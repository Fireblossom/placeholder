{
  "results": [
    {
      "context_text": "Generalized Planning Generalized planning extracts common patterns from a limited set of training solutions (i.e., plans) to solve unseen tasks within the same domain, which may be larger and more complex than the training tasks (Srivastava et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general concept of training solutions (plans). No clear, verifiable resource names are present.",
      "processing_time": 45.975112438201904,
      "citing_paper_id": "276409203",
      "cited_paper_id": 2320080
    },
    {
      "context_text": "Valmeekam et al. (2023b) employs LLMs to generate an initial plan, guiding the LPG planner (Gerevini et al., 2002), which iteratively refines it until a correct solution is found.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs and the LPG planner. No verifiable resources are identified.",
      "processing_time": 18.72508478164673,
      "citing_paper_id": "276409203",
      "cited_paper_id": 18976919
    },
    {
      "context_text": "For example, TTG (Ju et al., 2024) uses the LLM to convert travel planning requests of minimum total costs into Mixed Integer Linear Programming problems, then runs an optimizer such as SCIP (Bestuzheva et al., 2021) to provide the optimal plan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TTG' which is a method or system, not a dataset. No specific datasets are mentioned or used in the described research context.",
      "processing_time": 19.110106229782104,
      "citing_paper_id": "276409203",
      "cited_paper_id": 24331484
    },
    {
      "context_text": "For example, TTG (Ju et al., 2024) uses the LLM to convert travel planning requests of minimum total costs into Mixed Integer Linear Programming problems, then runs an optimizer such as SCIP (Bestuzheva et al., 2021) to provide the optimal plan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TTG' which is a method or system, not a dataset. No specific datasets are mentioned or used in the described research context.",
      "processing_time": 19.110106229782104,
      "citing_paper_id": "276409203",
      "cited_paper_id": 273507560
    },
    {
      "context_text": "…classical planners, which typically rely on search algorithms, heuristics and logical deductions and may not guarantee optimality (Russell and Norvig, 2016), optimizers, often using gradient-based meth-ods (e.g., Newton’s methods), can guarantee optimal solutions (Boyd and Vandenberghe, 2004).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses classical planners and optimizers. No verifiable resources are identified.",
      "processing_time": 28.829440355300903,
      "citing_paper_id": "276409203",
      "cited_paper_id": 37925315
    },
    {
      "context_text": "…to classical planners, which typically rely on search algorithms, heuristics, and logical deduction and may not guarantee optimality (Russell and Norvig, 2016), optimizers, often based on gradient-based methods (e.g., Newton’s methods), can guarantee optimal solutions (Boyd and Vandenberghe, 2004).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and approaches in optimization and planning.",
      "processing_time": 15.463813543319702,
      "citing_paper_id": "276409203",
      "cited_paper_id": 37925315
    },
    {
      "context_text": "For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALF-World"
      ],
      "dataset_descriptions": {
        "ALF-World": "Used to fine-tune GPT-2 for robotics planning, demonstrating its effectiveness in inferring detailed plans from high-level instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ALF-World' as a dataset used for fine-tuning GPT-2 to demonstrate its effectiveness in robotics planning. The dataset is specific and relevant to the research topic.",
      "processing_time": 26.85478401184082,
      "citing_paper_id": "276409203",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "For example, Jansen (2020) and Chalvatzaki et al. (2023) fine-tuned GPT-2 (Radford et al., 2019) on ALF-World, demonstrating its effectiveness in robotics planning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALF-World"
      ],
      "dataset_descriptions": {
        "ALF-World": "Used to fine-tune GPT-2 for robotics planning, demonstrating its effectiveness in inferring detailed plans from high-level instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ALF-World' as a dataset used for fine-tuning GPT-2 to demonstrate its effectiveness in robotics planning. The dataset is specific and relevant to the research topic.",
      "processing_time": 26.85478401184082,
      "citing_paper_id": "276409203",
      "cited_paper_id": 222066988
    },
    {
      "context_text": "To train a smaller planning model, Brahman et al. (2024) uses GPT-3 (Brown et al., 2020) as the teacher and T5 (Raffel et al., 2020) as the student, distilling the teacher’s planning capabilities into the more compact student model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GPT-3 and T5 for distilling planning capabilities, which are not datasets.",
      "processing_time": 35.96157908439636,
      "citing_paper_id": "276409203",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "This is crucial especially developing planners based on commercial LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the importance of developing planners based on commercial LLMs.",
      "processing_time": 22.02261185646057,
      "citing_paper_id": "276409203",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "This is crucial especially developing planners based on commercial LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the importance of developing planners based on commercial LLMs.",
      "processing_time": 22.02261185646057,
      "citing_paper_id": "276409203",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), and Guan et al. (2023) use LLMs to convert planning problems into PDDL (McDer-mott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers do not introduce any datasets either.",
      "processing_time": 29.568490982055664,
      "citing_paper_id": "276409203",
      "cited_paper_id": 220618602
    },
    {
      "context_text": "For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), and Guan et al. (2023) use LLMs to convert planning problems into PDDL (McDer-mott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers do not introduce any datasets either.",
      "processing_time": 29.568490982055664,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), and Guan et al. (2023) use LLMs to convert planning problems into PDDL (McDer-mott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers do not introduce any datasets either.",
      "processing_time": 29.568490982055664,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "For example, LLM+P (Liu et al., 2023a), LLM-DP (Dagan et al., 2023), and Guan et al. (2023) use LLMs to convert planning problems into PDDL (McDer-mott et al., 1998), solved by Fast Downward or BFS(f) (Lipovetzky et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers do not introduce any datasets either.",
      "processing_time": 29.568490982055664,
      "citing_paper_id": "276409203",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "…as follows: (1) Agentic tasks , including reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only task categories. No verifiable resources are identified.",
      "processing_time": 28.30222749710083,
      "citing_paper_id": "276409203",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "If an action isn’t exactly admissible, the closest valid action is retrieved instead (Huang et al., 2022a; Raman et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only actions and methods. No verifiable resources are identified.",
      "processing_time": 15.705335855484009,
      "citing_paper_id": "276409203",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Second, they can generate inadmissible actions and non-existent objects, requiring translation or expert intervention to correct them (Huang et al., 2022a; Raman et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with language models generating inadmissible actions and non-existent objects.",
      "processing_time": 30.40020203590393,
      "citing_paper_id": "276409203",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Second, they can generate inadmissible actions and non-existent objects, requiring translation or expert intervention to correct them (Huang et al., 2022a; Raman et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only issues with language models generating inadmissible actions and non-existent objects.",
      "processing_time": 30.40020203590393,
      "citing_paper_id": "276409203",
      "cited_paper_id": 273502635
    },
    {
      "context_text": "Unlike open-loop systems (Huang et al., 2022a), which lack feedback, closed-loop planning helps reduce hallucinations and enables LLMs to handle complex, long-horizon, and dynamic environments (Wang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the difference between open-loop and closed-loop planning systems.",
      "processing_time": 16.580662965774536,
      "citing_paper_id": "276409203",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Unlike open-loop systems (Huang et al., 2022a), which lack feedback, closed-loop planning helps reduce hallucinations and enables LLMs to handle complex, long-horizon, and dynamic environments (Wang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the difference between open-loop and closed-loop planning systems.",
      "processing_time": 16.580662965774536,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Hierarchical Planning is another common method for grounding actions in LLM planning (Huang et al., 2022a; Raman et al., 2022; Song et al., 2023; Hazra et al., 2024; Bhat et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works. No verifiable resources are identified.",
      "processing_time": 18.65826725959778,
      "citing_paper_id": "276409203",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Hierarchical Planning is another common method for grounding actions in LLM planning (Huang et al., 2022a; Raman et al., 2022; Song et al., 2023; Hazra et al., 2024; Bhat et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works. No verifiable resources are identified.",
      "processing_time": 18.65826725959778,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261100610
    },
    {
      "context_text": "…and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for representing domain details. No dataset names are present.",
      "processing_time": 31.16766095161438,
      "citing_paper_id": "276409203",
      "cited_paper_id": 246485514
    },
    {
      "context_text": "…and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for representing domain details. No dataset names are present.",
      "processing_time": 31.16766095161438,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for representing domain details. No dataset names are present.",
      "processing_time": 31.16766095161438,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for representing domain details. No dataset names are present.",
      "processing_time": 31.16766095161438,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods for representing domain details. No dataset names are present.",
      "processing_time": 31.16766095161438,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, in the Delivery dataset (Yang et al., 2022), models trained on small-scale deliveries (9–17 locations) can generalize to larger ones (70–100 locations) using the same core strategy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Delivery dataset"
      ],
      "dataset_descriptions": {
        "Delivery dataset": "Used to train and evaluate models on delivery route planning, specifically assessing generalization from small-scale (9–17 locations) to larger-scale (70–100 locations) deliveries."
      },
      "confidence_score": 1.0,
      "reasoning": "The 'Delivery dataset' is mentioned as a specific dataset used to train and evaluate models on delivery route planning tasks.",
      "processing_time": 40.29852294921875,
      "citing_paper_id": "276409203",
      "cited_paper_id": 248366629
    },
    {
      "context_text": "…its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al., 2023a), or even humans (Huang et al., 2022b), when the initial plan are inexecutable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methods and approaches for improving planning capabilities in LLMs.",
      "processing_time": 18.308411836624146,
      "citing_paper_id": "276409203",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "…its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al., 2023a), or even humans (Huang et al., 2022b), when the initial plan are inexecutable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methods and approaches for improving planning capabilities in LLMs.",
      "processing_time": 18.308411836624146,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b; Song et al., 2023; Lin et al., 2023a; Singh et al., 2023) or neural embeddings (Sharma et al., 2021; Ahn et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities of language models.",
      "processing_time": 17.551992416381836,
      "citing_paper_id": "276409203",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b; Song et al., 2023; Lin et al., 2023a; Singh et al., 2023) or neural embeddings (Sharma et al., 2021; Ahn et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities of language models.",
      "processing_time": 17.551992416381836,
      "citing_paper_id": "276409203",
      "cited_paper_id": 251979509
    },
    {
      "context_text": "The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b; Song et al., 2023; Lin et al., 2023a; Singh et al., 2023) or neural embeddings (Sharma et al., 2021; Ahn et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities of language models.",
      "processing_time": 17.551992416381836,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "The simplest way to do this is by feeding observed or available objects into the planner via prompts (Huang et al., 2022b; Song et al., 2023; Lin et al., 2023a; Singh et al., 2023) or neural embeddings (Sharma et al., 2021; Ahn et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities of language models.",
      "processing_time": 17.551992416381836,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Environment and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and representations. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 21.024113655090332,
      "citing_paper_id": "276409203",
      "cited_paper_id": 251979509
    },
    {
      "context_text": "Environment and domain details have been represented using tables (Lin et al., 2023a), condensed symbols (Hu et al., 2024), Pythonic code (Aghzal et al., 2023; Singh et al., 2023; Sun et al., 2024), neural embeddings (Li et al., 2022; Ahn et al., 2022), and graphs (Lin et al., 2024; Wu et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and representations. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 21.024113655090332,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "In terms of identifying unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024; Katz et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance issues. No verifiable resources are identified.",
      "processing_time": 29.94591236114502,
      "citing_paper_id": "276409203",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "In terms of identifying unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024; Katz…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their limitations in solving planning problems.",
      "processing_time": 17.871565580368042,
      "citing_paper_id": "276409203",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Also, the LLM has to accurately translate the domain and problem into the specific format (e.g., PDDL), required by these solvers (Guan et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a requirement for LLMs to translate domain and problems into specific formats like PDDL.",
      "processing_time": 30.593587160110474,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "LLM-as-a-Translator LLM-as-a-Translator converts between natural language (NL) and formal planning languages (e.g. PDDL), making classic planners more accessible to non-experts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or tool (LLM-as-a-Translator) and a general description of its function.",
      "processing_time": 32.88377499580383,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 259138811
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261245497
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "Recent work has used LLMs to translate natu-ral language descriptions into PDDL (Liu et al., 2023a; Guan et al., 2023; Xie et al., 2023; Dagan et al., 2023; Zhou et al., 2024; Tantakoun et al., 2025), LTL (Pan et al., 2023), and STL (Chen et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works using LLMs for translating natural language into formal languages like PDDL, LTL, and STL.",
      "processing_time": 19.994802474975586,
      "citing_paper_id": "276409203",
      "cited_paper_id": 277313841
    },
    {
      "context_text": "LLMs are highly sensitive to prompts (Sclar et al., 2024; Razavi et al., 2025), but most research relies on natural language without comparing them to alternative formats, such as PDDL or Python, for describing domains and problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the sensitivity of LLMs to different prompt formats.",
      "processing_time": 17.24255084991455,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "For representation , Planetarium (Zuo et al., 2024) assesses LLMs’ ability to translate natural language into PDDL.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Planetarium' as a method for assessing LLMs' ability to translate natural language into PDDL, which is not a dataset but a method or tool.",
      "processing_time": 21.734994649887085,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Beyond basic precondition and postcondition rules, planners must consider extra constraints, such as avoiding sugar when baking a cake for diabetics (Yuan et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general example of a constraint in planning. No verifiable resources are identified.",
      "processing_time": 30.82356548309326,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258564677
    },
    {
      "context_text": "For grounding , Open Grounded Planning (Guo et al., 2024) and Embodied Agent Inter-face (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several systems (CoScript, TravelPlanner, PPNL) but does not specify any datasets. The focus is on evaluating performance and planning problems, which suggests these are methods or tools rather than datasets.",
      "processing_time": 35.87276029586792,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258564677
    },
    {
      "context_text": "For grounding , Open Grounded Planning (Guo et al., 2024) and Embodied Agent Inter-face (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several systems (CoScript, TravelPlanner, PPNL) but does not specify any datasets. The focus is on evaluating performance and planning problems, which suggests these are methods or tools rather than datasets.",
      "processing_time": 35.87276029586792,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "For grounding , Open Grounded Planning (Guo et al., 2024) and Embodied Agent Inter-face (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several systems (CoScript, TravelPlanner, PPNL) but does not specify any datasets. The focus is on evaluating performance and planning problems, which suggests these are methods or tools rather than datasets.",
      "processing_time": 35.87276029586792,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "For grounding , Open Grounded Planning (Guo et al., 2024) and Embodied Agent Inter-face (Li et al., 2024f) evaluate performance in embodied environments, while CoScript (Yuan et al., 2023), TravelPlanner (Xie et al., 2023), and PPNL (Aghzal et al., 2023) focus on planning problems with constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several systems (CoScript, TravelPlanner, PPNL) but does not specify any datasets. The focus is on evaluating performance and planning problems, which suggests these are methods or tools rather than datasets.",
      "processing_time": 35.87276029586792,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Yuan et al. (2023) ranks InstructGPT-generated plans using co-sine similarity with task embeddings and selects the most similar one.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for ranking plans generated by InstructGPT.",
      "processing_time": 18.261309146881104,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258564677
    },
    {
      "context_text": "…into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023; Zhou et al., 2024; Sun et al., 2024; Silver et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the capabilities of LLMs in planning systems. No verifiable resources are identified.",
      "processing_time": 19.970137119293213,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "…into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023; Zhou et al., 2024; Sun et al., 2024; Silver et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the capabilities of LLMs in planning systems. No verifiable resources are identified.",
      "processing_time": 19.970137119293213,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023; Zhou et al., 2024; Sun et al., 2024; Silver et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the capabilities of LLMs in planning systems. No verifiable resources are identified.",
      "processing_time": 19.970137119293213,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…into formal representations and translating the resulting plans back into NL, LLMs reduce ambiguity, minimize hallucinations, and enable external validation, improving both usability and reliability in planning systems (Xie et al., 2023; Zhou et al., 2024; Sun et al., 2024; Silver et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the capabilities of LLMs in planning systems. No verifiable resources are identified.",
      "processing_time": 19.970137119293213,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261245497
    },
    {
      "context_text": "Silver et al. (2024) approached this by prompting LLMs to summarize the domain and generate a minimal, generalizable Python-based plan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving LLMs and Python-based planning.",
      "processing_time": 15.635464191436768,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "For generated plans, Pythonic code is a common alternative to natural language (Singh et al., 2023; Silver et al., 2024).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to Pythonic code in generated plans. No verifiable resources are identified.",
      "processing_time": 19.517284870147705,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "For generated plans, Pythonic code is a common alternative to natural language (Singh et al., 2023; Silver et al., 2024).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to Pythonic code in generated plans. No verifiable resources are identified.",
      "processing_time": 19.517284870147705,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al., 2023a), or even humans (Huang et al., 2022b), when the initial plan are…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methods and approaches. The context is about using feedback from different sources to improve planning capabilities of LLMs.",
      "processing_time": 20.590425491333008,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "…assesses state progress toward the goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al.,…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and metrics. The cited papers are referenced for their methodologies and findings, not for providing datasets.",
      "processing_time": 21.921481370925903,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "…Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 21.90471386909485,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "(3) World Models define state transitions based on action preconditions and effects, using LLMs (Hao et al., 2023), classical planners (Hirsch et al., 2024), or external environment simulators (Zhou et al., 2023a; Zhao et al., 2024; Koh et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on defining state transitions in World Models using various approaches.",
      "processing_time": 21.357524156570435,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "(3) World Models define state transitions based on action preconditions and effects, using LLMs (Hao et al., 2023), classical planners (Hirsch et al., 2024), or external environment simulators (Zhou et al., 2023a; Zhao et al., 2024; Koh et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on defining state transitions in World Models using various approaches.",
      "processing_time": 21.357524156570435,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269982497
    },
    {
      "context_text": "(3) World Models define state transitions based on action preconditions and effects, using LLMs (Hao et al., 2023), classical planners (Hirsch et al., 2024), or external environment simulators (Zhou et al., 2023a; Zhao et al., 2024; Koh et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on defining state transitions in World Models using various approaches.",
      "processing_time": 21.357524156570435,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "To ensure correctness, the LLM must work with classical sound and complete solvers (Guan et al., 2023; Hao et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to solvers and planning capabilities of LLMs.",
      "processing_time": 30.301677465438843,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "To ensure correctness, the LLM must work with classical sound and complete solvers (Guan et al., 2023; Hao et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to solvers and planning capabilities of LLMs.",
      "processing_time": 30.301677465438843,
      "citing_paper_id": "276409203",
      "cited_paper_id": 279621061
    },
    {
      "context_text": "…et al., 2024; Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action (Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (3) merging plans with the same prefix actions or subgoals to avoid duplicate world model checks…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.80453062057495,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…et al., 2024; Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action (Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (3) merging plans with the same prefix actions or subgoals to avoid duplicate world model checks…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 17.80453062057495,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Implicit systems only fix the failed action (Raman et al., 2022; Singh et al., 2023; Zhou et al., 2024; Prasad et al., 2023; Yang et al., 2024), while explicit systems regenerate the entire plan (Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches in planning capabilities of LLMs.",
      "processing_time": 17.769184112548828,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Implicit systems only fix the failed action (Raman et al., 2022; Singh et al., 2023; Zhou et al., 2024; Prasad et al., 2023; Yang et al., 2024), while explicit systems regenerate the entire plan (Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches in planning capabilities of LLMs.",
      "processing_time": 17.769184112548828,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261245497
    },
    {
      "context_text": "Implicit systems only fix the failed action (Raman et al., 2022; Singh et al., 2023; Zhou et al., 2024; Prasad et al., 2023; Yang et al., 2024), while explicit systems regenerate the entire plan (Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches in planning capabilities of LLMs.",
      "processing_time": 17.769184112548828,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "Implicit systems only fix the failed action (Raman et al., 2022; Singh et al., 2023; Zhou et al., 2024; Prasad et al., 2023; Yang et al., 2024), while explicit systems regenerate the entire plan (Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches in planning capabilities of LLMs.",
      "processing_time": 17.769184112548828,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270357425
    },
    {
      "context_text": "Closed-loop systems fall into two types: implicit and explicit (Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of closed-loop systems. No verifiable resources are identified.",
      "processing_time": 16.489901542663574,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Specifically, sequential decomposition (Wang et al., 2023b; Singh et al., 2023; Sun et al., 2024; Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or concepts related to sequential decomposition in planning.",
      "processing_time": 16.80573582649231,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Specifically, sequential decomposition (Wang et al., 2023b; Singh et al., 2023; Sun et al., 2024; Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or concepts related to sequential decomposition in planning.",
      "processing_time": 16.80573582649231,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Specifically, sequential decomposition (Wang et al., 2023b; Singh et al., 2023; Sun et al., 2024; Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or concepts related to sequential decomposition in planning.",
      "processing_time": 16.80573582649231,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, sequential decomposition (Wang et al., 2023b; Singh et al., 2023; Sun et al., 2024; Wu et al., 2024) requires that the precondition of the subsequent subgoal is the effect of the preceding subgoal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or concepts related to sequential decomposition in planning.",
      "processing_time": 16.80573582649231,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Sun et al. (2024) takes a different approach, first generating a plan with placeholders for objects, then filling in the blanks with observed objects during execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context describes a planning process using placeholders and observed objects, which does not indicate the use of a specific dataset.",
      "processing_time": 33.33775544166565,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Sun et al. (2024) takes a different approach, first generating a plan with placeholders for objects, then filling in the blanks with observed objects during execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context describes a planning process using placeholders and observed objects, which does not indicate the use of a specific dataset.",
      "processing_time": 33.33775544166565,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…calls, several tricks are used: (1) generating the entire plan in one shot instead of step-by-step to reduce redundant prompts (Hu et al., 2023b; Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities in language models.",
      "processing_time": 17.454509258270264,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…calls, several tricks are used: (1) generating the entire plan in one shot instead of step-by-step to reduce redundant prompts (Hu et al., 2023b; Sun et al., 2024; Gonzalez-Pumariega et al., 2024); (2) checking feasibility by world models only at the end of each subgoal, not after every action…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for planning capabilities in language models.",
      "processing_time": 17.454509258270264,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Alignment techniques such as RLHF (Ouyang et al., 2022) and DPO (Rafailov et al., 2024) may help alleviate this issue, as humans generally prefer shorter plans for their efficiency, simplicity, and cognitive ease.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only alignment techniques. The context focuses on methods and preferences, not on the use of datasets.",
      "processing_time": 30.756500720977783,
      "citing_paper_id": "276409203",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Fine-tuning Current LLMs are not specifically trained for agentic tasks like planning, and prompt-based methods, which do not update model parameters, cannot fundamentally improve performance in these areas (Chen et al., 2023a; Wang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general statements about LLMs and their capabilities. No verifiable resources are identified.",
      "processing_time": 19.485609769821167,
      "citing_paper_id": "276409203",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "Fine-tuning Current LLMs are not specifically trained for agentic tasks like planning, and prompt-based methods, which do not update model parameters, cannot fundamentally improve performance in these areas (Chen et al., 2023a; Wang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general statements about LLMs and their capabilities. No verifiable resources are identified.",
      "processing_time": 19.485609769821167,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263829338
    },
    {
      "context_text": "SayCanPay (Hazra et al., 2024) uses A* search with LLMs to generate the shortest possible plans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (A* search with LLMs) and a tool (SayCanPay).",
      "processing_time": 31.934789419174194,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261100610
    },
    {
      "context_text": "…the model adapts its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al., 2023a), or even humans (Huang et al., 2022b), when the initial…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models, methods, and papers. There are no clear identifiers for datasets.",
      "processing_time": 20.897934198379517,
      "citing_paper_id": "276409203",
      "cited_paper_id": 261245497
    },
    {
      "context_text": "…et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation tasks , including video (Lin et al., 2023b), image (Zala et al., 2023) and text generation (Moryossef et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of generation tasks. No clear, verifiable datasets are identified.",
      "processing_time": 31.037948846817017,
      "citing_paper_id": "276409203",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "Downstream tasks can be categorized as follows: (1) Agentic tasks , including reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation tasks , including video (Lin et al., 2023b), image (Zala et al., 2023) and text generation (Moryossef et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of downstream tasks. No verifiable resources are identified.",
      "processing_time": 18.929564237594604,
      "citing_paper_id": "276409203",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "…tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation tasks ,…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general task categories. No verifiable resources are identified.",
      "processing_time": 30.48445987701416,
      "citing_paper_id": "276409203",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "First, they might struggle to assess if a plan is achievable given a specific problem description (Aghzal et al., 2023; Kambhampati, 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of assessing plan achievability in LLMs.",
      "processing_time": 19.889153242111206,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "…unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024; Katz et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their performance on planning problems. No verifiable resources are identified.",
      "processing_time": 31.91129159927368,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "…unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024; Katz et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their performance on planning problems. No verifiable resources are identified.",
      "processing_time": 31.91129159927368,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269214439
    },
    {
      "context_text": "…unsolvable planning problems, those with inherently unachievable goals, even top LLMs (e.g., GPT-4 (Achiam et al., 2023)) and Large Reasoning Models (e.g., OpenAI O1 (Jaech et al., 2024)) struggle due to hallucination issues (Aghzal et al., 2023; Valmeekam et al., 2024; Katz et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their performance on planning problems. No verifiable resources are identified.",
      "processing_time": 31.91129159927368,
      "citing_paper_id": "276409203",
      "cited_paper_id": 272770270
    },
    {
      "context_text": "PPNL (Aghzal et al., 2023) can also evaluate a planner’s ability to identify unachievable goals (i.e., completeness) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or capability of evaluating planners. The context is about assessing the completeness of planners, which is a methodological concern rather than a dataset.",
      "processing_time": 23.540844678878784,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "…et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation tasks , including video (Lin et al., 2023b), image…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and references to other works. No verifiable resources are identified.",
      "processing_time": 16.75452423095703,
      "citing_paper_id": "276409203",
      "cited_paper_id": 263829697
    },
    {
      "context_text": "This approach has been increasingly adopted in recent LLM planning research (Guo et al., 2024; O’Donoghue et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 22.089861392974854,
      "citing_paper_id": "276409203",
      "cited_paper_id": 264172681
    },
    {
      "context_text": "ToolChain* (Zhuang et al., 2023) 6 Criterion IV: Representation (Tab.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'ToolChain*'. No verifiable datasets are referenced.",
      "processing_time": 16.42708396911621,
      "citing_paper_id": "276409203",
      "cited_paper_id": 264405734
    },
    {
      "context_text": "ToolChain* (Zhuang et al., 2023) combines A* tree search with an LLM, which suggests next steps and estimates heuristic scores, to create plans with the fewest tool API calls.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining A* tree search with an LLM. No verifiable resources are identified.",
      "processing_time": 21.831728219985962,
      "citing_paper_id": "276409203",
      "cited_paper_id": 264405734
    },
    {
      "context_text": "If a step goes beyond the executor’s capabilities (e.g., combining multiple allowed actions into one), the LLM planner should be reprompted to break it down until every step is executable (Prasad et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for planning with language models.",
      "processing_time": 15.246229887008667,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "If a step goes beyond the executor’s capabilities (e.g., combining multiple allowed actions into one), the LLM planner should be reprompted to break it down until every step is executable (Prasad et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for planning with language models.",
      "processing_time": 15.246229887008667,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "…where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one (Prasad et al., 2023; Dagan et al., 2023; Zhao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses planning capabilities in scenarios with uncertain object information.",
      "processing_time": 28.111308097839355,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "…where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one (Prasad et al., 2023; Dagan et al., 2023; Zhao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses planning capabilities in scenarios with uncertain object information.",
      "processing_time": 28.111308097839355,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "…where some object information are uncertain (e.g., needing to clean a cup that could be in a cabinet, drawer, or fridge), the planner can generate multiple possible plans, one for each scenario, and select the first feasible one (Prasad et al., 2023; Dagan et al., 2023; Zhao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses planning capabilities in scenarios with uncertain object information.",
      "processing_time": 28.111308097839355,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, Prasad et al. (2023) recursively break down the goal until each subgoal can be easily executed in the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for breaking down goals using language models.",
      "processing_time": 15.080301284790039,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "For example, Prasad et al. (2023) recursively break down the goal until each subgoal can be easily executed in the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for breaking down goals using language models.",
      "processing_time": 15.080301284790039,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "Closed-Loop Systems A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various studies and systems. The context focuses on describing the concept of closed-loop systems in LLM planning.",
      "processing_time": 19.425093173980713,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "Closed-Loop Systems A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various studies and systems. The context focuses on describing the concept of closed-loop systems in LLM planning.",
      "processing_time": 19.425093173980713,
      "citing_paper_id": "276409203",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "Closed-Loop Systems A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various studies and systems. The context focuses on describing the concept of closed-loop systems in LLM planning.",
      "processing_time": 19.425093173980713,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270357425
    },
    {
      "context_text": "Closed-Loop Systems A closed-loop system in LLM planning means the model adapts its plan based on feedback from executors (Prasad et al., 2023; Yang et al., 2024), simulators (Bhat et al., 2024), validators (Zhou et al., 2024; Silver et al., 2024), other LLMs (Wang et al., 2023b; Zhou et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various studies and systems. The context focuses on describing the concept of closed-loop systems in LLM planning.",
      "processing_time": 19.425093173980713,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270357425
    },
    {
      "context_text": "Inputs include domains (predicates and actions), problems (initial and goal states), and environmental observations, while outputs are the generated plans.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general inputs and outputs for planning systems. No verifiable resources are identified.",
      "processing_time": 16.704993963241577,
      "citing_paper_id": "276409203",
      "cited_paper_id": 267782588
    },
    {
      "context_text": "Beyond A* (Lehnert et al., 2024) trains a Transformer model, Searchformer, to mimic A* search paths for complex tasks like Maze navigation and Sokoban puzzles, optimizing for the fewest steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Searchformer) and tasks (Maze navigation, Sokoban puzzles). No verifiable datasets are referenced.",
      "processing_time": 20.473231077194214,
      "citing_paper_id": "276409203",
      "cited_paper_id": 267782588
    },
    {
      "context_text": "Moreover, standardizing trajectory formats from different tasks (Zhang et al., 2024; Chen et al., 2024b), as well as incorporating unsuccessful reasoning or planning trajectories (Wang et al., 2024; Chen et al., 2024b; Song et al., 2024) can further enhance learning and performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear identifiers for datasets are present.",
      "processing_time": 16.71390700340271,
      "citing_paper_id": "276409203",
      "cited_paper_id": 267897975
    },
    {
      "context_text": "Moreover, standardizing trajectory formats from different tasks (Zhang et al., 2024; Chen et al., 2024b), as well as incorporating unsuccessful reasoning or planning trajectories (Wang et al., 2024; Chen et al., 2024b; Song et al., 2024) can further enhance learning and performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear identifiers for datasets are present.",
      "processing_time": 16.71390700340271,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, standardizing trajectory formats from different tasks (Zhang et al., 2024; Chen et al., 2024b), as well as incorporating unsuccessful reasoning or planning trajectories (Wang et al., 2024; Chen et al., 2024b; Song et al., 2024) can further enhance learning and performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear identifiers for datasets are present.",
      "processing_time": 16.71390700340271,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "This approach avoids repeated LLM and world model calls at each node (Katz et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no clear identifiers for datasets in the citation context.",
      "processing_time": 18.121243000030518,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269214439
    },
    {
      "context_text": "(4) Evaluation assesses state progress toward the goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and evaluation metrics. The context focuses on evaluation methods such as scores, ratings, and log-likelihood scores.",
      "processing_time": 20.460227489471436,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269214439
    },
    {
      "context_text": "(4) Evaluation assesses state progress toward the goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and evaluation metrics. The context focuses on evaluation methods such as scores, ratings, and log-likelihood scores.",
      "processing_time": 20.460227489471436,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Search Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 18.86073660850525,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269214439
    },
    {
      "context_text": "Search Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 18.86073660850525,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al., 2023a) or reward models (Chen et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for evaluating LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 21.23209524154663,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269982497
    },
    {
      "context_text": "…goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al., 2023a) or reward models (Chen et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for evaluating LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 21.23209524154663,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…goal via scores computed by predefined functions (Katz et al., 2024), LLM/LVM ratings (Yao et al., 2024; Hao et al., 2023; Zhou et al., 2023a), log-likelihood scores (Hirsch et al., 2024), voting (Yao et al., 2024), self-consistency scores (Zhou et al., 2023a) or reward models (Chen et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for evaluating LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 21.23209524154663,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024), have been applied to improve LLM-based planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various search algorithms and their application to LLM-based planning.",
      "processing_time": 18.10552167892456,
      "citing_paper_id": "276409203",
      "cited_paper_id": 269982497
    },
    {
      "context_text": "In contrast, parallel decomposition (Yang et al., 2024) involves subgoals that share the same pre-condition and effect, where achieving the final goal requires completing only one of these subgoals.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept called 'parallel decomposition'. No verifiable resources are identified.",
      "processing_time": 19.38905096054077,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270357425
    },
    {
      "context_text": "In contrast, parallel decomposition (Yang et al., 2024) involves subgoals that share the same pre-condition and effect, where achieving the final goal requires completing only one of these subgoals.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept called 'parallel decomposition'. No verifiable resources are identified.",
      "processing_time": 19.38905096054077,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270357425
    },
    {
      "context_text": "Curtis et al. (2024) generates Pythonic plans with parameter ranges, tests them with a simulator or classifier, and prompts the LLM to revise if constraints are still violated.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating and testing plans using LLMs and simulators.",
      "processing_time": 19.38406252861023,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270371268
    },
    {
      "context_text": "Some studies (Singh et al., 2023; Aghzal et al., 2024) suggest that using Python to represent planning problems can improve performance, but automatically translating natural language problem descriptions into Python remains challenging, particularly for non-experts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of translating natural language problem descriptions into Python.",
      "processing_time": 18.431053638458252,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270562133
    },
    {
      "context_text": "Some studies (Singh et al., 2023; Aghzal et al., 2024) suggest that using Python to represent planning problems can improve performance, but automatically translating natural language problem descriptions into Python remains challenging, particularly for non-experts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of translating natural language problem descriptions into Python.",
      "processing_time": 18.431053638458252,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, Aghzal et al. (2024) found that LLM planners often fail to achieve optimality in path planning, frequently producing un-necessarily long plans.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only findings about LLM planners in path planning.",
      "processing_time": 17.667692184448242,
      "citing_paper_id": "276409203",
      "cited_paper_id": 270562133
    },
    {
      "context_text": "Brahman et al. (2024) applies a verifier-guided beam search, keeping the top-K plans based on correctness and constraint adherence at each step.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (verifier-guided beam search).",
      "processing_time": 18.443849802017212,
      "citing_paper_id": "276409203",
      "cited_paper_id": 271745636
    },
    {
      "context_text": "The most common scenarios include (1) Embodied environments , (2) Task scheduling , (3) Games , and (4) Task decomposition (Li et al., 2024d).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists general categories of scenarios for LLM-assisted AI planning.",
      "processing_time": 20.803468942642212,
      "citing_paper_id": "276409203",
      "cited_paper_id": 272367167
    },
    {
      "context_text": "Figure 1 presents commonly used planning datasets; readers can refer to Li et al. (2024d, 2025) for further details.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'planning datasets' but does not specify any particular dataset names. The reference to Li et al. (2024d, 2025) suggests that more details are available elsewhere, but no specific datasets are named in the given context.",
      "processing_time": 25.392174243927002,
      "citing_paper_id": "276409203",
      "cited_paper_id": 272367167
    },
    {
      "context_text": "LLM + Optimizer It combines the LLM, which turns user requests into symbolic optimization problems, with an optimizer that solves them and finds the best solution (Ju et al., 2024; Hao et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the combination of LLM and an optimizer for solving symbolic optimization problems.",
      "processing_time": 22.598827123641968,
      "citing_paper_id": "276409203",
      "cited_paper_id": 273374872
    },
    {
      "context_text": "LLM + Optimizer It combines the LLM, which turns user requests into symbolic optimization problems, with an optimizer that solves them and finds the best solution (Ju et al., 2024; Hao et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the combination of LLM and an optimizer for solving symbolic optimization problems.",
      "processing_time": 22.598827123641968,
      "citing_paper_id": "276409203",
      "cited_paper_id": 273507560
    },
    {
      "context_text": "…and enables LLMs to handle complex, long-horizon, and dynamic environments (Wang et al., 2023b LLM + Optimizer It combines the LLM, which turns user requests into symbolic optimization problems, with an optimizer that solves them and finds the best solution (Ju et al., 2024; Hao et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the combination of LLM and an optimizer for solving symbolic optimization problems.",
      "processing_time": 22.762498140335083,
      "citing_paper_id": "276409203",
      "cited_paper_id": 273374872
    },
    {
      "context_text": "For more details on LLM-as-a-Judge, please see Li et al. (2024a,b); Gu et al. (2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of dataset usage or specific data sources.",
      "processing_time": 29.090588569641113,
      "citing_paper_id": "276409203",
      "cited_paper_id": 274234014
    },
    {
      "context_text": "To improve cost-effectiveness, we may summarize problem descriptions and enhance heuristic evaluations, e.g., by improving LLM uncertainty estimation (Huang et al., 2024a) and verification (Li et al., 2024c).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches to improve LLM performance in planning tasks.",
      "processing_time": 28.900567770004272,
      "citing_paper_id": "276409203",
      "cited_paper_id": 274763098
    },
    {
      "context_text": "Multi-agent planning (Kono-lige and Nilsson, 1980; Torreno et al., 2017) is more challenging, as it involves multiple agents (e.g., robots) working together or competing in parallel.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts related to multi-agent planning. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 30.889700412750244,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024), have been applied to improve LLM-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 31.21476697921753,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Search Algorithm Search algorithms, including Breadth-First Search, Depth-First Search (Yao et al., 2024; Katz et al., 2024), Monte Carlo Tree Search (Hao et al., 2023; Zhou et al., 2023a; Zhao et al., 2024; Shi et al., 2025), and Greedy Best-First Search (Koh et al., 2024; Hirsch et al., 2024), have been applied to improve LLM-based planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms and their application to LLM-based planning.",
      "processing_time": 29.504488945007324,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general task categories. No clear, verifiable resources are identified.",
      "processing_time": 30.11588740348816,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general task categories. No clear, verifiable resources are identified.",
      "processing_time": 30.11588740348816,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of tasks. There are no clear identifiers for datasets.",
      "processing_time": 30.11278510093689,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…reasoning-oriented tasks, tool-use-oriented tasks, programming tasks, and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of tasks. There are no clear identifiers for datasets.",
      "processing_time": 30.11278510093689,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "This approach is motivated by two key insights: (1) focusing too narrowly on planning may degrade general capabilities (Chen et al., 2024b), and (2) agentic tasks share overlapping capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on insights and capabilities related to planning and agentic tasks.",
      "processing_time": 31.776515007019043,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "This approach is motivated by two key insights: (1) focusing too narrowly on planning may degrade general capabilities (Chen et al., 2024b), and (2) agentic tasks share overlapping capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on insights and capabilities related to planning and agentic tasks.",
      "processing_time": 31.776515007019043,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Like object grounding, the simplest way is to explicitly list all admissible actions in LLMs’ inputs (Singh et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for handling admissible actions in LLMs.",
      "processing_time": 30.885295867919922,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…and web tasks (Yu et al., 2018; Cobbe et al., 2021; Saparov and He, 2022; Zhou et al., 2023b; Deng et al., 2023; Liu et al., 2023b; Li et al., 2023; Xu et al., 2023; Jimenez et al., 2023; Ruan et al., 2023; Bairi et al., 2023; Li et al., 2024e), (2) Generation tasks , including video (Lin et al.,…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general task categories. There are no clear identifiers for datasets, and the context is too generic.",
      "processing_time": 32.306583642959595,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "For spatial reasoning and planning, (Hu et al., 2024) introduces Chain-of-Symbols (CoS), a compact symbolic representation that replaces natural language descriptions in CoT (Wei et al., 2022)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Symbols) and a technique (Chain-of-Thought).",
      "processing_time": 32.28112506866455,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "Hallucination LLMs often experience hallucinations (Huang et al., 2023), which present two major challenges in planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with LLMs. No verifiable resources are identified.",
      "processing_time": 31.476895809173584,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast to classical planners, which typically rely on search algorithms, heuristics, and logical deduction and may not guarantee optimality (Russell and Norvig, 2016), optimizers, often based on gradient-based methods (e.g., Newton’s methods), can guarantee optimal solutions (Boyd and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the differences between classical planners and optimizers.",
      "processing_time": 30.585559368133545,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "However, this method has limitations, such as position bias, length bias, self-inconsistency, and sensitivity to prompts (Zheng et al., 2023; Ye et al., 2024; Wei et al., 2024).",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodological limitations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 32.29019570350647,
      "citing_paper_id": "276409203",
      "cited_paper_id": null
    },
    {
      "context_text": "…has been explored in natural language processing (NLP) (Bauer and Bansal, 2018; Ning et al., 2020; Yu et al., 2020), whereas planning has not yet been very much represented within the NLP research community except in the context of text planning (Reiter and Dale, 2000; Gatt and Krahmer, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to NLP research and text planning. No verifiable resources are identified.",
      "processing_time": 32.8705050945282,
      "citing_paper_id": "270620065",
      "cited_paper_id": 16946362
    },
    {
      "context_text": "PDDL (Planning Domain Definition Language): PDDL (McDermott et al., 1998) is a standardized language used in artificial intelligence for representing planning problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "PDDL is a method or tool, not a dataset. It is used for representing planning problems in AI, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 35.41296029090881,
      "citing_paper_id": "270620065",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Specifically, we explore two distinct planning representations: the formal Planning Domain Definition Language (PDDL) (McDermott et al., 1998), which provides a standardized representation for classical planning problems and allows for rigorous plan validation; and natural language, which offers a…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only planning representations and languages. No verifiable resources are identified.",
      "processing_time": 31.44093132019043,
      "citing_paper_id": "270620065",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "…articulated through natural language; reasoning has been explored in natural language processing (NLP) (Bauer and Bansal, 2018; Ning et al., 2020; Yu et al., 2020), whereas planning has not yet been very much represented within the NLP research community except in the context of text planning…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and areas of research.",
      "processing_time": 30.06961727142334,
      "citing_paper_id": "270620065",
      "cited_paper_id": 209485573
    },
    {
      "context_text": "…abilities can be articulated through natural language; reasoning has been explored in natural language processing (NLP) (Bauer and Bansal, 2018; Ning et al., 2020; Yu et al., 2020), whereas planning has not yet been very much represented within the NLP research community except in the context…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to NLP research and reasoning. No verifiable resources are identified.",
      "processing_time": 32.85481333732605,
      "citing_paper_id": "270620065",
      "cited_paper_id": 218470560
    },
    {
      "context_text": "Research is already underway to leverage the commonsense knowledge of LLMs in real-world tasks (Huang et al., 2022; Singh et al., 2023; Ding et al., 2023) and to generate plans (Valmeekam et al., 2023; Hao et al., 2023; Guan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works leveraging LLMs for various tasks including planning. No verifiable resources are named.",
      "processing_time": 34.63632297515869,
      "citing_paper_id": "270620065",
      "cited_paper_id": 257496672
    },
    {
      "context_text": "Research is already underway to leverage the commonsense knowledge of LLMs in real-world tasks (Huang et al., 2022; Singh et al., 2023; Ding et al., 2023) and to generate plans (Valmeekam et al., 2023; Hao et al., 2023; Guan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works leveraging LLMs for various tasks including planning. No verifiable resources are named.",
      "processing_time": 34.63632297515869,
      "citing_paper_id": "270620065",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "Therefore, we chose the latter for our experiments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only indicates a choice made for experiments without specifying what was chosen.",
      "processing_time": 33.34467077255249,
      "citing_paper_id": "270620065",
      "cited_paper_id": 269626390
    },
    {
      "context_text": "There is another line of work that uses a hybrid approach, meaning that they either use an exter-nal tool to solve the planning tasks (Kambhampati et al., 2024; Hirsch et al., 2024), or reformulate the problem as another task such as SMT (Hao et al., 2024) and use an external tool to solve it.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools used for solving planning tasks. There are no clear identifiers for datasets in the text.",
      "processing_time": 34.187504053115845,
      "citing_paper_id": "270620065",
      "cited_paper_id": 279621061
    },
    {
      "context_text": "This capacity for strategic foresight is fundamental to intelligent behavior (Russell and Norvig, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is a reference to a concept in AI literature.",
      "processing_time": 32.837048053741455,
      "citing_paper_id": "270620065",
      "cited_paper_id": null
    },
    {
      "context_text": "BLEU [21] and ROUGE [22] is used as the evaluation metric.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BLEU and ROUGE as evaluation metrics, which are excluded as per instructions. No datasets are mentioned.",
      "processing_time": 32.250661849975586,
      "citing_paper_id": "268581113",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Pretrained Sentence Transformer [20] (all-mpnet-base-v2) is used as the retrieval model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (Sentence Transformer) but does not refer to any specific dataset. The citation is used to describe the model, not a dataset.",
      "processing_time": 34.615811586380005,
      "citing_paper_id": "268581113",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "As shown in Figure 1(a), we generate a large set of diverse instructions leveraging GPT-3 [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generating instructions using GPT-3, which is a model, not a dataset.",
      "processing_time": 35.363250732421875,
      "citing_paper_id": "268581113",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "They carefully design prompts [4, 14] so that the model can successfully generate device control plans for ambiguous/abstract instructions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of designing prompts for generating device control plans. No verifiable resources are identified.",
      "processing_time": 34.151220083236694,
      "citing_paper_id": "268581113",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "They carefully design prompts [4, 14] so that the model can successfully generate device control plans for ambiguous/abstract instructions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of designing prompts for generating device control plans. No verifiable resources are identified.",
      "processing_time": 34.151220083236694,
      "citing_paper_id": "268581113",
      "cited_paper_id": 258535542
    },
    {
      "context_text": "With the aid of proper in-context examples [3], models like GPT-3 [4] and GPT-4 [5] can perform aforementioned planning tasks for different home configurations seamlessly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their capabilities. The context is about the performance of models like GPT-3 and GPT-4 in planning tasks.",
      "processing_time": 36.52835464477539,
      "citing_paper_id": "268581113",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "With the aid of proper in-context examples [3], models like GPT-3 [4] and GPT-4 [5] can perform aforementioned planning tasks for different home configurations seamlessly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their capabilities. The context is about the performance of models like GPT-3 and GPT-4 in planning tasks.",
      "processing_time": 36.52835464477539,
      "citing_paper_id": "268581113",
      "cited_paper_id": 247155069
    },
    {
      "context_text": "With the aid of proper in-context examples [3], models like GPT-3 [4] and GPT-4 [5] can perform aforementioned planning tasks for different home configurations seamlessly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their capabilities. The context is about the performance of models like GPT-3 and GPT-4 in planning tasks.",
      "processing_time": 36.52835464477539,
      "citing_paper_id": "268581113",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Early works related to ‘Smart Home AI As-sistant’ or ‘Smart Home Voice Assistant’ that tries to address device control planning task, suffer from the requirement of predefined templates [6, 7] or predictable structures [8, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to earlier works and their limitations. No verifiable resources are identified.",
      "processing_time": 33.3123893737793,
      "citing_paper_id": "268581113",
      "cited_paper_id": 236258746
    },
    {
      "context_text": "Early works related to ‘Smart Home AI As-sistant’ or ‘Smart Home Voice Assistant’ that tries to address device control planning task, suffer from the requirement of predefined templates [6, 7] or predictable structures [8, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to earlier works and their limitations. No verifiable resources are identified.",
      "processing_time": 33.3123893737793,
      "citing_paper_id": "268581113",
      "cited_paper_id": 258218323
    },
    {
      "context_text": "They use in-context examples [3] to guide LLM for the desired task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using in-context examples.",
      "processing_time": 31.68866539001465,
      "citing_paper_id": "268581113",
      "cited_paper_id": 247155069
    },
    {
      "context_text": "For example, (Zhuo et al. 2020) use word vector embeddings and RNNs to support plan recognition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of word vector embeddings and RNNs for plan recognition.",
      "processing_time": 35.552727460861206,
      "citing_paper_id": "258947755",
      "cited_paper_id": 3752019
    },
    {
      "context_text": "The PDDL framework (Ghallab et al. 1998) defines a class of planning problems where each problem can be defined using a domain D , a start state I and a goal specification G .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the PDDL framework, which is a method for defining planning problems.",
      "processing_time": 35.0173077583313,
      "citing_paper_id": "258947755",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Recent works such as Decision Transformer(Chen et al. 2021) also employ large Transformer models to learn policies purely from trajectories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Decision Transformer) and its application. No verifiable resources are identified.",
      "processing_time": 33.90572690963745,
      "citing_paper_id": "258947755",
      "cited_paper_id": 235294299
    },
    {
      "context_text": "The work by (Pallagani et al. 2022) attempts to learn plans directly from action sequences generated by a planner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating plans using transformers.",
      "processing_time": 31.388729572296143,
      "citing_paper_id": "258947755",
      "cited_paper_id": 254854675
    },
    {
      "context_text": "A recent work (Valmeekam et al. 2023) investigates the planning abilities of various LLMs like GPT-3, InstructGPT and BLOOM.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their planning abilities.",
      "processing_time": 31.120733976364136,
      "citing_paper_id": "258947755",
      "cited_paper_id": null
    },
    {
      "context_text": "This leads to a natural question: can these models plan? A recent work (Valmeekam et al. 2023) investigates the planning abilities of various LLMs like GPT-3, InstructGPT and BLOOM.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and a general research question about their planning abilities.",
      "processing_time": 32.78584551811218,
      "citing_paper_id": "258947755",
      "cited_paper_id": null
    },
    {
      "context_text": "…multi-modal domains have given rise to Vision-Language Models (VLMs)(Yin et al., 2023) such as LLaVA(Liu et al., 2023) and CogVLM(Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA’s training on image-caption pairs from the CC3M datasets (Sharma et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CC3M"
      ],
      "dataset_descriptions": {
        "CC3M": "Used to train LLaVA, a vision-language model, on image-caption pairs, integrating textual and visual inputs to enhance multi-modal capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CC3M dataset, which is a specific, verifiable dataset used for training vision-language models. The dataset is clearly identified and its usage is described.",
      "processing_time": 42.02388858795166,
      "citing_paper_id": "267500377",
      "cited_paper_id": 51876975
    },
    {
      "context_text": "Meta-World provides a suite of 50 distinct robotic skills focused on fine manipulation.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Meta-World as a suite of robotic skills, which is not a dataset but a benchmark or evaluation framework. It does not meet the criteria for inclusion as a dataset.",
      "processing_time": 37.17538380622864,
      "citing_paper_id": "267500377",
      "cited_paper_id": 204852201
    },
    {
      "context_text": "These include the text-based multi-step tasks in ALFWorld (Shridhar et al., 2021) and Webshop (Yao et al., 2022), and robotics tasks, FrankaKitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2021), which are multimodal environments with texts and images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFWorld and Meta-World, which are environments used for training and evaluating models. However, they are not datasets but rather benchmarks or environments.",
      "processing_time": 36.48355197906494,
      "citing_paper_id": "267500377",
      "cited_paper_id": 204852201
    },
    {
      "context_text": "These include the text-based multi-step tasks in ALFWorld (Shridhar et al., 2021) and Webshop (Yao et al., 2022), and robotics tasks, FrankaKitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2021), which are multimodal environments with texts and images.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFWorld and Meta-World, which are environments used for training and evaluating models. However, they are not datasets but rather benchmarks or environments.",
      "processing_time": 36.48355197906494,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We evaluated our proposed technique in embodied multimodal agents on two benchmark environments: Franka Kitchen and Meta-World.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Franka Kitchen' and 'Meta-World' as benchmark environments, but 'Meta-World' is a benchmark suite, not a dataset. 'Franka Kitchen' is a specific environment but lacks clear dataset characteristics.",
      "processing_time": 38.557026624679565,
      "citing_paper_id": "267500377",
      "cited_paper_id": 204852201
    },
    {
      "context_text": "Among works (Madaan et al., 2022; Liu et al., 2021; Su et al., 2022) that seek to derive better answers from LLMs via memory, RAG (Lewis et al., 2020) is a notable method that combines retrieval-based mechanisms with generative models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RAG as a method combining retrieval-based mechanisms with generative models, but does not refer to any specific dataset.",
      "processing_time": 34.97364521026611,
      "citing_paper_id": "267500377",
      "cited_paper_id": 218869575
    },
    {
      "context_text": "Building on RAG, Reflexion (Shinn et al., 2023) requires LLMs to self-reflect on unsuccessful tasks for self-improvement in solving tasks over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Reflexion) that builds on another method (RAG). No verifiable resources are identified.",
      "processing_time": 36.72356367111206,
      "citing_paper_id": "267500377",
      "cited_paper_id": 218869575
    },
    {
      "context_text": "Building on RAG, Reflexion (Shinn et al., 2023) requires LLMs to self-reflect on unsuccessful tasks for self-improvement in solving tasks over time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Reflexion) that builds on another method (RAG). No verifiable resources are identified.",
      "processing_time": 36.72356367111206,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "ALFWorld success rate(",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a success rate in a context that suggests evaluation but lacks details.",
      "processing_time": 34.54427719116211,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Specifically, RAP achieves 33.6%, 13.0%, 18.2%, and 12.7% gain over ReAct on the ALFWorld, Webshop, Franka Kitchen, and Meta World benchmarks respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as specific, downloadable datasets. The names are used primarily for score comparison rather than as reusable data sources.",
      "processing_time": 36.46382522583008,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For instance, in ALFWorld (Shridhar et al., 2021), if an action plan like ”I need to find the watch” is generated, the retrieval key would be ”search watch”.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or environment (ALFWorld) for interactive learning.",
      "processing_time": 35.296968936920166,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "ALFWorld (Shridhar et al., 2021) is a synthetic text-based game that challenges an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld (Cˆot´e et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFWorld and TextWorld, but neither is a dataset. They are environments or platforms used for training agents. No specific, verifiable datasets are mentioned.",
      "processing_time": 37.59073328971863,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We perform evaluation on ALFWorld with GPT-3.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld' as a platform for evaluation, but it does not specify if ALFWorld is a dataset, a method, or a tool. Given the title, ALFWorld appears to be an environment rather than a dataset.",
      "processing_time": 39.86364150047302,
      "citing_paper_id": "267500377",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Furthermore, we utilize visual observation provided by ALF-World instead of textual observation, and perform an evaluation using similarity between textual retrieval key and image observation with CLIP (Radford et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of ALF-World for visual observation and CLIP for evaluating similarity, but neither are datasets. ALF-World is a simulation environment, and CLIP is a model. No specific datasets are mentioned.",
      "processing_time": 39.496774673461914,
      "citing_paper_id": "267500377",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "For images, the representations are generated with a CLIP-based Vision Transformer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CLIP-based Vision Transformer) for generating representations.",
      "processing_time": 36.443923473358154,
      "citing_paper_id": "267500377",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "A notable example is Chain-of-Thought (CoT) (Wei et al., 2022), where agents are encouraged to mirror human cognitive mechanisms by incorporating reasoning into intermediate steps for complex problem-solving tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought) for enhancing reasoning in large language models.",
      "processing_time": 38.822470903396606,
      "citing_paper_id": "267500377",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Previous works such as ReAct (Yao et al., 2023) have shown that LLMs can generate accurate actions as language agents by iteratively performing actions and reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ReAct) and its capabilities. The context focuses on the functionality of LLMs and the ReAct method.",
      "processing_time": 41.54920959472656,
      "citing_paper_id": "267500377",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Building on Reflexion, ExpeL (Zhao et al., 2023) passes all generated experiences into the LLM to reflect in a text-based manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Reflexion) and a new approach (ExpeL).",
      "processing_time": 41.22487449645996,
      "citing_paper_id": "267500377",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Building on Reflexion, ExpeL (Zhao et al., 2023) passes all generated experiences into the LLM to reflect in a text-based manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Reflexion) and a new approach (ExpeL).",
      "processing_time": 41.22487449645996,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Following previous works (Yao et al., 2023; Shinn et al., 2023; Prasad et al., 2023), RAP runs recursively until it reaches a depth (trial) of 3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only recursive processes in RAP. No verifiable resources are identified.",
      "processing_time": 40.94315767288208,
      "citing_paper_id": "267500377",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Following previous works (Yao et al., 2023; Shinn et al., 2023; Prasad et al., 2023), RAP runs recursively until it reaches a depth (trial) of 3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only recursive processes in RAP. No verifiable resources are identified.",
      "processing_time": 40.94315767288208,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "With a dynamic reasoning process, ReAct (Yao et al., 2023) interleaves generated actions and environmental states, improving the reasoning ability through action-state synergy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called ReAct which is not a dataset.",
      "processing_time": 41.216469526290894,
      "citing_paper_id": "267500377",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Extending beyond LLMs, multi-modal domains have given rise to Vision-Language Models (VLMs)(Yin et al., 2023) such as LLaVA(Liu et al., 2023) and CogVLM(Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA’s training on image-caption pairs from the CC3M datasets (Sharma et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CC3M"
      ],
      "dataset_descriptions": {
        "CC3M": "Used to train LLaVA, a Vision-Language Model, integrating textual and visual inputs through image-caption pairs, focusing on multi-modal capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CC3M dataset, which is a specific, verifiable dataset used for training VLMs. No other datasets are mentioned.",
      "processing_time": 51.420313119888306,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258179774
    },
    {
      "context_text": "Extending beyond LLMs, multi-modal domains have given rise to Vision-Language Models (VLMs)(Yin et al., 2023) such as LLaVA(Liu et al., 2023) and CogVLM(Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA’s training on image-caption pairs from the CC3M datasets…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CC3M"
      ],
      "dataset_descriptions": {
        "CC3M": "Used to train LLaVA, a Vision-Language Model, integrating textual and visual inputs through image-caption pairs, focusing on multi-modal capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CC3M dataset, which is a specific, verifiable dataset used for training Vision-Language Models. The dataset is clearly identified and used in the context of training LLaVA.",
      "processing_time": 52.97818326950073,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258179774
    },
    {
      "context_text": "We constructed embodied agents using two VLM foundations - LLaVA and CogView.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LLaVA and CogView as visual language model foundations but does not refer to them as datasets. They are excluded as they are models, not datasets.",
      "processing_time": 44.153178691864014,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258179774
    },
    {
      "context_text": "%) as compared to ReAct (61.8%), Reflexion (61.8",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance metrics. The cited paper title does not provide additional context to identify a dataset.",
      "processing_time": 43.10962104797363,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "%) achieve a significantly higher success rate compared to previous works such as ReAct(52.2%), Reflexion(74.6%), and ADaPT(71.6%).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons with other methods.",
      "processing_time": 41.51038980484009,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "%) achieves a higher success rate compared to previous studies such as ReAct (35.0%), Reflexion (35.0%), and ADaPT (43.0%).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance comparisons between different methods.",
      "processing_time": 41.71826434135437,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Following Reflexion (Shinn et al., 2023) and ADaPT (Prasad et al., 2023), we evaluated an agent across 100 instructions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of an agent across 100 instructions. No clear, verifiable dataset names are provided.",
      "processing_time": 44.32137370109558,
      "citing_paper_id": "267500377",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "These agents can be depicted into having 4 key aspects: Profile (agent characteristics), Memory (past information), Planning (future strategies) and Action (execution policies) (Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only aspects of autonomous agents. No verifiable resources are identified.",
      "processing_time": 42.41399836540222,
      "citing_paper_id": "267500377",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Knowledge-Augmented LLMs. Previous works (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2023) concentrate on knowledge augmentation in LLMs through retrieval.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works on knowledge augmentation in LLMs through retrieval.",
      "processing_time": 43.0896258354187,
      "citing_paper_id": "268248897",
      "cited_paper_id": 211204736
    },
    {
      "context_text": "Our fine-tuning framework leverages FastChat (Zheng et al., 2023) using DeepSpeed (Rasley et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions tools and methods (FastChat, DeepSpeed) but does not reference any specific datasets. The context is about leveraging these tools for fine-tuning, not using datasets.",
      "processing_time": 46.32076811790466,
      "citing_paper_id": "268248897",
      "cited_paper_id": 221191193
    },
    {
      "context_text": "Our fine-tuning framework leverages FastChat (Zheng et al., 2023) using DeepSpeed (Rasley et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions tools and methods (FastChat, DeepSpeed) but does not reference any specific datasets. The context is about leveraging these tools for fine-tuning, not using datasets.",
      "processing_time": 46.32076811790466,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "More details of HotpotQA are listed in Appendix B. ALFWorld (Shridhar et al., 2021) is an interactive, text-based household environment where agent challenge to complete six different types of multi-step tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFWorld, which is described as an interactive, text-based household environment. However, it is not a dataset but an environment for interactive learning.",
      "processing_time": 45.433276414871216,
      "citing_paper_id": "268248897",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Furthermore, following previous research (Shridhar et al., 2021; Yao et al., 2023b), we use a test set with 140 seen tasks and 134 un-seen tasks to evaluate our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a test set with seen and unseen tasks but does not provide a specific name for the dataset. The context is too generic to identify a specific, verifiable dataset.",
      "processing_time": 46.653162479400635,
      "citing_paper_id": "268248897",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We evaluate K NOW A GENT on HotpotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate KNOW A GENT on complex question-answering tasks, focusing on multi-hop reasoning and evidence retrieval.",
        "ALFWorld": "Used to assess KNOW A GENT's capabilities in interactive learning within embodied environments, emphasizing alignment between text and physical actions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, HotpotQA and ALFWorld, which are used for evaluating the system KNOW A GENT. HotpotQA is a question-answering dataset, and ALFWorld is a dataset for interactive learning in embodied environments.",
      "processing_time": 62.75641894340515,
      "citing_paper_id": "268248897",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We evaluate K NOW A GENT on HotpotQA (Yang et al., 2018) and ALF-World (Shridhar et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA",
        "ALF-World"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate KNOW A GENT's performance on complex question answering, focusing on multi-hop reasoning and evidence retrieval.",
        "ALF-World": "Used to assess KNOW A GENT's planning capabilities in an interactive, embodied environment, emphasizing alignment between text and actions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, HotpotQA and ALF-World, which are used to evaluate the performance of KNOW A GENT. HotpotQA is a question-answering dataset, and ALF-World is an environment for interactive learning.",
      "processing_time": 62.75303530693054,
      "citing_paper_id": "268248897",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Experimental results on HotpotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021) based on various backbone models demonstrate that K NOW A GENT can achieve comparable or superior performance to existing baselines.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate KNOWAGENT's performance on complex question answering, demonstrating comparable or superior results to existing baselines.",
        "ALFWorld": "Used to assess KNOWAGENT's capabilities in aligning text and embodied environments for interactive learning, showing strong performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, HotpotQA and ALFWorld, which are used to evaluate the performance of KNOWAGENT against existing baselines.",
      "processing_time": 57.07009029388428,
      "citing_paper_id": "268248897",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "LLM-based agents (Wang et al., 2023b; Xi et al., 2023; Durante et al., 2024) have emerged as one of the most prevalent AI systems after the rise of LLMs (Zhao et al., 2023; Qiao et al., 2023b; Zhu et al., 2023; Li et al., 2024a; Jiang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to LLM-based agents and their prevalence. No verifiable resources are identified.",
      "processing_time": 44.519906520843506,
      "citing_paper_id": "268248897",
      "cited_paper_id": 254854219
    },
    {
      "context_text": "As artificial intelligence (AI) advances, language agents are becoming increasingly vital for solving complex problems (Zhang et al., 2023; Sumers et al., 2024; Yang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to advancements in AI and language agents. No verifiable resources are identified.",
      "processing_time": 44.29256510734558,
      "citing_paper_id": "268248897",
      "cited_paper_id": 254854219
    },
    {
      "context_text": "With the emergence of Augmented Language Models (ALMs), many studies (Trivedi et al., 2023; Li et al., 2023c; Vu et al., 2023; Qiao et al., 2023a) have enhanced the reasoning capabilities of LLMs by incorporating knowledge from external tools such as search engines, knowledge bases, and Wikipedia…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only external tools and knowledge sources. The context focuses on enhancing LLMs' reasoning capabilities through these tools.",
      "processing_time": 46.0106999874115,
      "citing_paper_id": "268248897",
      "cited_paper_id": 254877499
    },
    {
      "context_text": "With the emergence of Augmented Language Models (ALMs), many studies (Trivedi et al., 2023; Li et al., 2023c; Vu et al., 2023; Qiao et al., 2023a) have enhanced the reasoning capabilities of LLMs by incorporating knowledge from external tools such as search engines, knowledge bases, and Wikipedia…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only external tools and knowledge sources. The context focuses on enhancing LLMs' reasoning capabilities through these tools.",
      "processing_time": 46.0106999874115,
      "citing_paper_id": "268248897",
      "cited_paper_id": 258832372
    },
    {
      "context_text": "…Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al., 2023; Chen et al., 2023c; Yin et al., 2023; Qiao et al., 2024), and the utilization of external tools (Schick et al., 2023; Qiao et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various research works and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.40129113197327,
      "citing_paper_id": "268248897",
      "cited_paper_id": 258832372
    },
    {
      "context_text": "…Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al., 2023; Chen et al., 2023c; Yin et al., 2023; Qiao et al., 2024), and the utilization of external tools (Schick et al., 2023; Qiao et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various research works and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.40129113197327,
      "citing_paper_id": "268248897",
      "cited_paper_id": 261048935
    },
    {
      "context_text": "Due to the rich parameterized knowledge within LLMs (Chen, 2023; Feng et al., 2023), some other works (Liu et al., 2022; Yu et al., 2023; Sun et al., 2023b) advocate for knowledge generation rather than retrieval.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works discussing knowledge generation in LLMs.",
      "processing_time": 43.641688108444214,
      "citing_paper_id": "268248897",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "…application and customization in different areas such as question-answering (Yao et al., 2023c; Yin et al., 2023), web browsing (Yao et al., 2022; Deng et al., 2023; Zhou et al., 2023a), robotics (Ichter et al., 2022; Ding et al., 2023) and so on, researchers are exploring Agent Tuning as a…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of LLMs. No verifiable resources are identified.",
      "processing_time": 44.276676177978516,
      "citing_paper_id": "268248897",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "…in different areas such as question-answering (Yao et al., 2023c; Yin et al., 2023), web browsing (Yao et al., 2022; Deng et al., 2023; Zhou et al., 2023a), robotics (Ichter et al., 2022; Ding et al., 2023) and so on, researchers are exploring Agent Tuning as a means to augment model…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and methods. The context is about the application of Agent Tuning in different domains, but no datasets are explicitly named or described.",
      "processing_time": 48.39813041687012,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "We employ Llama-2-{7,13,70}b-chat (Touvron et al., 2023) as the backbone models, and also apply K NOW A GENT to Vicuna (Zheng et al., 2023), Mis-tral (Jiang et al., 2023a), GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 45.138216495513916,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "We also explore other backbone models except for Llama with a 7b parameter scale, such as Vicuna-7b and Mistral-7b.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 44.2646803855896,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "…al., 2022; Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al., 2023; Chen et al., 2023c; Yin et al., 2023; Qiao et al., 2024), and the utilization of external tools (Schick et al., 2023; Qiao et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and concepts. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 47.22134327888489,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "5-16k (Zheng et al., 2023), Mistral-7B-Instruct-v0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model names and sizes. There are no clear identifiers for datasets.",
      "processing_time": 44.06302523612976,
      "citing_paper_id": "268248897",
      "cited_paper_id": 260351380
    },
    {
      "context_text": "Previous works primarily focus on unlocking the potential of LLMs as the core of language agents by leveraging human-crafted (Yao et al., 2023b; Li et al., 2023a; Talebi-rad and Nadiri, 2023; Qian et al., 2023) or machine-generated (Zhou et al., 2023b; Chen et al., 2023c,b) prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing human-crafted or machine-generated prompts for LLMs.",
      "processing_time": 45.351123571395874,
      "citing_paper_id": "268248897",
      "cited_paper_id": 261048935
    },
    {
      "context_text": "Recent research has explored various approaches to improve LLMs’ performance in complex environments: some works (Zhou et al., 2023b; Ye et al., 2023) introduced structured knowledge to regulate multi-agent work-flows, while others developed state-aware guidelines for environment-specific reasoning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No clear identifiers for datasets are present.",
      "processing_time": 44.47225499153137,
      "citing_paper_id": "268248897",
      "cited_paper_id": 265295561
    },
    {
      "context_text": "Guan et al. (2024) introduces AMOR, an agent framework that constructs its reasoning capabilities on a Finite State Machine (FSM).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an agent framework called AMOR. No verifiable resources are identified.",
      "processing_time": 44.760005235672,
      "citing_paper_id": "268248897",
      "cited_paper_id": 267406772
    },
    {
      "context_text": "5-turbo-0125 (OpenAI, 2022) and gpt-4-32K-0613.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no verifiable resources or datasets mentioned.",
      "processing_time": 44.463982820510864,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "5-Turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions models (GPT-3.5-Turbo and GPT-4) but does not reference any specific datasets. The context is about models, not datasets.",
      "processing_time": 48.57604932785034,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "…(LLMs), enhance their task planning capabilities through a variety of strategies including task decomposition (Wei et al., 2022; Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al., 2023; Chen et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various strategies for enhancing task planning capabilities in LLMs.",
      "processing_time": 45.3318989276886,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "…Language Models (LLMs), enhance their task planning capabilities through a variety of strategies including task decomposition (Wei et al., 2022; Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only strategies and methods for enhancing task planning capabilities in LLMs.",
      "processing_time": 45.086758613586426,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "…task planning capabilities through a variety of strategies including task decomposition (Wei et al., 2022; Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of labor (Hong et al., 2023; Chen et al., 2023c; Yin et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only strategies and methods for task planning capabilities in LLMs.",
      "processing_time": 44.7406280040741,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experiments across various models demonstrate that K NOW A GENT effectively competes with or surpasses other baselines, showcasing the benefits of integrating external action knowledge to stream-line planning processes and improve performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of a method called KNOW AGENT. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 47.82425045967102,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experiments across various models demonstrate that K NOW A GENT effectively competes with or surpasses other baselines, showcasing the benefits of integrating external action knowledge to stream-line planning processes and improve performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of a method called KNOW AGENT. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 47.82425045967102,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "Seen",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide any specific dataset names or verifiable resources. The term 'Seen' is too generic and does not refer to a specific dataset.",
      "processing_time": 47.16365027427673,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "However, when it comes to executing planning tasks, especially in open-source models, there remain issues (Liu et al., 2023a; Valmeekam et al., 2023; Guan et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to issues in planning tasks with open-source models.",
      "processing_time": 44.72761511802673,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "To overcome this, we leverage GPT-4 (OpenAI, 2023), known for its strong performance on such tasks (Liu et al., 2023a; Ouyang and Li, 2023), for initial construction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on leveraging GPT-4 for initial construction, which is a model, not a dataset.",
      "processing_time": 48.547999143600464,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "We choose the following baselines here: (1) Chain-of-Thought (CoT) (Wei et al., 2022) catalyzes in-depth reasoning in large language models (LLMs) by incorporating intermediate reasoning steps within examples.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought) used to enhance reasoning in LLMs.",
      "processing_time": 45.92760634422302,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "…built around Large Language Models (LLMs), enhance their task planning capabilities through a variety of strategies including task decomposition (Wei et al., 2022; Yao et al., 2023a; Wang et al., 2023a; Team, 2023), reflection (Shinn et al., 2023; Sun et al., 2023a), collaborative division of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only strategies for enhancing task planning capabilities in LLMs.",
      "processing_time": 44.71619200706482,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "We compare K NOW A GENT with various baselines including CoT (Wei et al., 2022), ReAct (Yao et al., 2023b), Reflexion (Shinn et al., 2023), FireAct (Chen et al., 2023a) and NAT (Wang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model or method names. There are no clear identifiers for datasets.",
      "processing_time": 45.31507349014282,
      "citing_paper_id": "268248897",
      "cited_paper_id": null
    },
    {
      "context_text": "For caption-related tasks, such as scene description and the selection of attention objects, we utilize the commonly employed language-based metrics to evaluate the sentence similarity at the word level, including METEOR [3], ROUGE [27], and CIDEr [49].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions language-based metrics but does not refer to any specific datasets. The metrics are used for evaluating sentence similarity in caption-related tasks.",
      "processing_time": 46.9467089176178,
      "citing_paper_id": "277824043",
      "cited_paper_id": 7164502
    },
    {
      "context_text": "In the finetuning stage, the model is trained by AdamW [34] optimizer with a batch size of 16.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the optimizer used during training.",
      "processing_time": 43.339869022369385,
      "citing_paper_id": "277824043",
      "cited_paper_id": 14337532
    },
    {
      "context_text": "HAD and Talk2Car both contain human like advice to best navigate the car [9,22], while LaMPilot contains labels meant to evaluate transition from human commands to drive action [35].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HAD",
        "Talk2Car",
        "LaMPilot"
      ],
      "dataset_descriptions": {
        "HAD": "Used to provide human-like advice for navigating self-driving vehicles, focusing on natural language interactions to guide driving actions.",
        "Talk2Car": "Used to provide human-like advice for navigating self-driving vehicles, focusing on natural language interactions to guide driving actions.",
        "LaMPilot": "Used to evaluate the transition from human commands to driving actions, focusing on the accuracy and effectiveness of command execution in self-driving vehicles."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions HAD, Talk2Car, and LaMPilot as datasets containing human-like advice or labels for evaluating transitions from human commands to driving actions.",
      "processing_time": 65.69415163993835,
      "citing_paper_id": "277824043",
      "cited_paper_id": 204241631
    },
    {
      "context_text": "Some MLLMs such as CLIP [44] and ALIGN [18] utilize contrastive learning to create a similar embedding space for both language and vision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (CLIP and ALIGN) and their use of contrastive learning. There are no verifiable resources that meet the criteria.",
      "processing_time": 48.78752589225769,
      "citing_paper_id": "277824043",
      "cited_paper_id": 231879586
    },
    {
      "context_text": "The objective of end-to-end autonomous driving is to create a fully differentiable system that spans from sensor input to control signals [42,57,61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the objective of end-to-end autonomous driving. No verifiable resources are identified.",
      "processing_time": 46.51954531669617,
      "citing_paper_id": "277824043",
      "cited_paper_id": 237194835
    },
    {
      "context_text": "The recent rapid development of multimodal LLMs (MLLMs) [1,24,31] and their excellent reasoning capabilities have led to a stream of applications in end-to-end autonomous driving [6,39,46,52,58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to multimodal LLMs and their applications in autonomous driving.",
      "processing_time": 45.89509558677673,
      "citing_paper_id": "277824043",
      "cited_paper_id": 248476411
    },
    {
      "context_text": "Conversely, Flamingo [1], Qwen-VL [2], BLIP-2 [24], utilize cross-attention and are able to extract a fixed number of visual tokens regardless of image resolution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. The context focuses on the use of cross-attention mechanisms in visual language models.",
      "processing_time": 47.97871279716492,
      "citing_paper_id": "277824043",
      "cited_paper_id": 248476411
    },
    {
      "context_text": "Beyond scene descriptions, DRAMA [36] and Rank2Tell [45] focus on risk object localization.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rank2Tell"
      ],
      "dataset_descriptions": {
        "Rank2Tell": "Used to evaluate multimodal driving scenarios, focusing on joint importance ranking and reasoning for risk object localization."
      },
      "confidence_score": 0.9,
      "reasoning": "DRAMA and Rank2Tell are mentioned in the context of risk object localization in driving scenarios. DRAMA is a method, and Rank2Tell is a dataset.",
      "processing_time": 53.14168047904968,
      "citing_paper_id": "277824043",
      "cited_paper_id": 252439240
    },
    {
      "context_text": "Beyond scene descriptions, DRAMA [36] and Rank2Tell [45] focus on risk object localization.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Rank2Tell"
      ],
      "dataset_descriptions": {
        "Rank2Tell": "Used to evaluate multimodal driving scenarios, focusing on joint importance ranking and reasoning for risk object localization."
      },
      "confidence_score": 0.9,
      "reasoning": "DRAMA and Rank2Tell are mentioned in the context of risk object localization in driving scenarios. DRAMA is a method, and Rank2Tell is a dataset.",
      "processing_time": 53.14168047904968,
      "citing_paper_id": "277824043",
      "cited_paper_id": 261706176
    },
    {
      "context_text": "The query-based 3D perception models [21, 28, 29, 55] consist of a shared visual encoder to extract multi-view image features, and a detection head f d .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the context.",
      "processing_time": 46.163650035858154,
      "citing_paper_id": "277824043",
      "cited_paper_id": 253735327
    },
    {
      "context_text": "Regarding temporal modeling, we store the perception queries with top-k classification scores into a memory bank and propagate them frame by frame [28, 59].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving temporal modeling and memory banks.",
      "processing_time": 44.36519408226013,
      "citing_paper_id": "277824043",
      "cited_paper_id": 253735327
    },
    {
      "context_text": "LLaVa [31], PaLM-E [12], PaLI [8], and RT2 [62] utilize self-attention for alignment by interleaving or concatenating image and text tokens in fixed sequence lengths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their methodologies. No verifiable datasets are referenced.",
      "processing_time": 45.58417892456055,
      "citing_paper_id": "277824043",
      "cited_paper_id": 257364842
    },
    {
      "context_text": "Our model uses EVA-02-L [14] as the vision encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "EVA-02-L is identified as a vision encoder, which is a model or method, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 48.27515625953674,
      "citing_paper_id": "277824043",
      "cited_paper_id": 257631498
    },
    {
      "context_text": "Pioneering work UniAD [17] and VAD [20] integrate modularized design of perception tasks such as object detection, tracking, and semantic segmentation into a unified planning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on integrating modularized design of perception tasks into a planning framework.",
      "processing_time": 47.95446991920471,
      "citing_paper_id": "277824043",
      "cited_paper_id": 257687420
    },
    {
      "context_text": "Recent works, e.g., MILE [15], ThinkTwice [19], VADv2 [7] leverage CARLA [11] as the simulator, which enables the creation of virtual environments with feedback from other agents.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CARLA as a simulator used in recent works for creating virtual environments with feedback from other agents. However, CARLA is a simulation platform, not a dataset.",
      "processing_time": 49.24417591094971,
      "citing_paper_id": "277824043",
      "cited_paper_id": 258587978
    },
    {
      "context_text": "Similarly, Drive Anywhere proposes a patch-aligned feature extraction for MLLMs that allow it to provide text query-able driving decisions [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for feature extraction in autonomous driving.",
      "processing_time": 45.568333864212036,
      "citing_paper_id": "277824043",
      "cited_paper_id": 264490392
    },
    {
      "context_text": "Besides model architectures, recent drive LLM-agent works also feature the importance of benchmarks [10,38,39,43,46,47].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of benchmarks. No specific dataset names are provided.",
      "processing_time": 45.856223344802856,
      "citing_paper_id": "277824043",
      "cited_paper_id": 266725320
    },
    {
      "context_text": "Besides model architectures, recent drive LLM-agent works also feature the importance of benchmarks [10,38,39,43,46,47].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of benchmarks. No specific dataset names are provided.",
      "processing_time": 45.856223344802856,
      "citing_paper_id": "277824043",
      "cited_paper_id": 267750682
    },
    {
      "context_text": "Despite the various QA setups, benchmarks that involve planning [10,46,47] still resort to an open-loop setting on real-world sessions (e.g., nuScenes) where expert trajectories are used.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "nuScenes"
      ],
      "dataset_descriptions": {
        "nuScenes": "Used to evaluate planning capabilities in autonomous driving, focusing on expert trajectories in real-world sessions to assess model performance."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'nuScenes' as a real-world session involving planning capabilities, which is relevant to the research topic. No other specific datasets are mentioned.",
      "processing_time": 53.51226592063904,
      "citing_paper_id": "277824043",
      "cited_paper_id": 266725320
    },
    {
      "context_text": "Despite the various QA setups, benchmarks that involve planning [10,46,47] still resort to an open-loop setting on real-world sessions (e.g., nuScenes) where expert trajectories are used.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "nuScenes"
      ],
      "dataset_descriptions": {
        "nuScenes": "Used to evaluate planning capabilities in autonomous driving, focusing on expert trajectories in real-world sessions to assess model performance."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'nuScenes' as a real-world session involving planning capabilities, which is relevant to the research topic. No other specific datasets are mentioned.",
      "processing_time": 53.51226592063904,
      "citing_paper_id": "277824043",
      "cited_paper_id": 267750682
    },
    {
      "context_text": "Although previous works [38,47] have demonstrated successful applications of LLM-agents in autonomous driving, a holistic and principled approach is still needed to fully extend the 2D understanding and reasoning capabilities of MLLMs into complex 3D scenes for understanding the 3D geometry and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the application of LLM-agents in autonomous driving. No clear identifiers for datasets are present.",
      "processing_time": 47.93570017814636,
      "citing_paper_id": "277824043",
      "cited_paper_id": 267750682
    },
    {
      "context_text": "Other works leverage MLLMs through graph-based VQA (DriveLM) [46] or chain-of-thought (CoT) design [47, 54].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on leveraging MLLMs through graph-based VQA and chain-of-thought design.",
      "processing_time": 49.03917694091797,
      "citing_paper_id": "277824043",
      "cited_paper_id": 267750682
    },
    {
      "context_text": "Further enhancements include Stentz’s Dynamic A* (D*) from 1994, which recalculates paths as the environment changes, proving effective for navigation in unknown or evolving terrains (Stentz, 1994).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Dynamic A*).",
      "processing_time": 44.98910975456238,
      "citing_paper_id": "270924281",
      "cited_paper_id": 229853
    },
    {
      "context_text": "Nash et al.’s Theta*, from 2007, allows line-of-sight checks between nodes, resulting in more direct paths (Nash et al., 2007).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Theta*) for path planning. The context is about the algorithm's capability to perform line-of-sight checks, which is not related to a dataset.",
      "processing_time": 50.29657006263733,
      "citing_paper_id": "270924281",
      "cited_paper_id": 5340880
    },
    {
      "context_text": "Koenig et al.’s Lifelong Planning A* (LPA*), introduced in 2004, incrementally updates paths in dynamic and large-scale environments (Koenig et al., 2004).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Lifelong Planning A*) for pathfinding in dynamic environments.",
      "processing_time": 47.3989794254303,
      "citing_paper_id": "270924281",
      "cited_paper_id": 7828197
    },
    {
      "context_text": "Korf’s Iterative Deepening A* (IDA*), from 1985, combines depth-first search with A*’s heuristic to create a memory-efficient approach (Korf, 1985).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IDA*).",
      "processing_time": 44.62141394615173,
      "citing_paper_id": "270924281",
      "cited_paper_id": 10956233
    },
    {
      "context_text": "However, as the environment and map scale up, algorithms like A* and its variants (Hart et al., 1968b; Korf et al., 2001; Harabor and Grastien, 2011; Jansen and Buro, 2007) encounter an exponential increase in computational and memory demands.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and their performance issues. No verifiable resources are identified.",
      "processing_time": 47.049583435058594,
      "citing_paper_id": "270924281",
      "cited_paper_id": 13195796
    },
    {
      "context_text": "However, as the environment and map scale up, algorithms like A* and its variants (Hart et al., 1968b; Korf et al., 2001; Harabor and Grastien, 2011; Jansen and Buro, 2007) encounter an exponential increase in computational and memory demands.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and their performance issues. No verifiable resources are identified.",
      "processing_time": 47.049583435058594,
      "citing_paper_id": "270924281",
      "cited_paper_id": 20817484
    },
    {
      "context_text": "Similarly, Pearl’s Best First Search (BFS), proposed in 1984, prioritizes nodes based on heuristic values but can lead to longer paths due to its focus on the most promising nodes (Pearl, 1984).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Best First Search).",
      "processing_time": 45.217241048812866,
      "citing_paper_id": "270924281",
      "cited_paper_id": 38193578
    },
    {
      "context_text": "Prior benchmarks (Cˆot´e et al., 2019; Shridhar et al., 2020a; Ruis et al., 2020; Wu et al., 2021) often neglect temporal planning aspects.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prior benchmarks which are excluded as they are primarily used for score comparison.",
      "processing_time": 47.38412117958069,
      "citing_paper_id": "270924281",
      "cited_paper_id": 212658007
    },
    {
      "context_text": "Prior benchmarks (Cˆot´e et al., 2019; Shridhar et al., 2020a; Ruis et al., 2020; Wu et al., 2021) often neglect temporal planning aspects.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prior benchmarks which are excluded as they are primarily used for score comparison.",
      "processing_time": 47.38412117958069,
      "citing_paper_id": "270924281",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Studies such as (Shridhar et al., 2020b; Song et al., 2023; Shah et al., 2023) explore LLMs in high-level planning, highlighting challenges in long-term planning and spatial reasoning (Aghzal et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only studies exploring LLMs in high-level planning. No clear identifiers for datasets are present.",
      "processing_time": 48.44293808937073,
      "citing_paper_id": "270924281",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "LLMs show varying proficiency in spatial reasoning (Ilharco et al., 2020; Patel and Pavlick, 2021; Bubeck et al., 2023; Abdou et al., 2021; Yang et al., 2023b), yet face limitations in spatial reasoning and planning (Agrawal, 2023; Xie et al., 2023; Wu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No clear identifiers for datasets are present.",
      "processing_time": 47.375823736190796,
      "citing_paper_id": "270924281",
      "cited_paper_id": 251647156
    },
    {
      "context_text": "LLMs show varying proficiency in spatial reasoning (Ilharco et al., 2020; Patel and Pavlick, 2021; Bubeck et al., 2023; Abdou et al., 2021; Yang et al., 2023b), yet face limitations in spatial reasoning and planning (Agrawal, 2023; Xie et al., 2023; Wu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No clear identifiers for datasets are present.",
      "processing_time": 47.375823736190796,
      "citing_paper_id": "270924281",
      "cited_paper_id": 257687798
    },
    {
      "context_text": "LLMs show varying proficiency in spatial reasoning (Ilharco et al., 2020; Patel and Pavlick, 2021; Bubeck et al., 2023; Abdou et al., 2021; Yang et al., 2023b), yet face limitations in spatial reasoning and planning (Agrawal, 2023; Xie et al., 2023; Wu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No clear identifiers for datasets are present.",
      "processing_time": 47.375823736190796,
      "citing_paper_id": "270924281",
      "cited_paper_id": 259341893
    },
    {
      "context_text": "This concept bears a re-semblance to the ReAct approach, Step Back QA, and Self Reflection (Yao et al., 2022; Zheng et al., 2023; Renze and Guven, 2024) in its processing step by step foundational principles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on comparing different methodologies rather than using a particular dataset.",
      "processing_time": 48.434606075286865,
      "citing_paper_id": "270924281",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "This concept bears a re-semblance to the ReAct approach, Step Back QA, and Self Reflection (Yao et al., 2022; Zheng et al., 2023; Renze and Guven, 2024) in its processing step by step foundational principles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on comparing different methodologies rather than using a particular dataset.",
      "processing_time": 48.434606075286865,
      "citing_paper_id": "270924281",
      "cited_paper_id": 263830368
    },
    {
      "context_text": "Our study further evaluates LLMs in robot motion and path planning contexts, addressing limitations in end-to-end planning (Liu et al., 2023; Chen et al., 2023b; Xie et al., 2023; Silver et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets, and the context focuses on evaluating LLMs in planning contexts.",
      "processing_time": 50.091060400009155,
      "citing_paper_id": "270924281",
      "cited_paper_id": 259138811
    },
    {
      "context_text": "Understanding the interplay between high-level and low-level planning is crucial (Latif, 2024; Yang et al., 2023a; Ding et al., 2024; Zhou et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 48.808356523513794,
      "citing_paper_id": "270924281",
      "cited_paper_id": 264305599
    },
    {
      "context_text": "Understanding the interplay between high-level and low-level planning is crucial (Latif, 2024; Yang et al., 2023a; Ding et al., 2024; Zhou et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 48.808356523513794,
      "citing_paper_id": "270924281",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent work, such as the LLM3 framework (Wang et al., 2024) leverages pre-trained LLMs to integrate symbolic task planning with continuous motion generation through motion failure reasoning, where LLM3 iteratively refines both symbolic actions and continuous parameters, significantly improving…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called LLM3. The context focuses on the integration of symbolic task planning and continuous motion generation.",
      "processing_time": 49.50450825691223,
      "citing_paper_id": "270924281",
      "cited_paper_id": 268531200
    },
    {
      "context_text": "Meanwhile, Large Language Models (LLMs) have achieved notable milestones in various planning tasks (Naveed et al., 2023; Yin et al., 2023; Chen et al., 2023a; Shinn et al., 2024; Dou et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to achievements in planning tasks by LLMs.",
      "processing_time": 47.02865529060364,
      "citing_paper_id": "270924281",
      "cited_paper_id": null
    },
    {
      "context_text": "Although some works have focused on the safety of embodied agent planning [36, 37], they merely use AI-generated images as substitutes for the embodied simulation environment which the agent can not truly interact with.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of AI-generated images, but these are not identified as a specific dataset.",
      "processing_time": 49.16610074043274,
      "citing_paper_id": "274789097",
      "cited_paper_id": 58377815
    },
    {
      "context_text": "Although some works have focused on the safety of embodied agent planning [36, 37], they merely use AI-generated images as substitutes for the embodied simulation environment which the agent can not truly interact with.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of AI-generated images, but these are not identified as a specific dataset.",
      "processing_time": 49.16610074043274,
      "citing_paper_id": "274789097",
      "cited_paper_id": 271769045
    },
    {
      "context_text": "Most benchmarks about embodied LLM agents primarily focus on their planning capabilities in structured, fixed-format tasks, while overlooking the risks of hazardous tasks [8, 21, 22, 26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general benchmarks which are excluded according to the instructions.",
      "processing_time": 46.40275478363037,
      "citing_paper_id": "274789097",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "It covers 10 common risks to humans and property, and also includes three distinct categories of tasks: detailed tasks, abstract tasks, and long-horizon tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only task categories. The context is too generic and lacks specific identifiers.",
      "processing_time": 47.34382128715515,
      "citing_paper_id": "274789097",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "To ensure 100% executability and evaluability of all tasks, the generated data undergoes a two-step filtering process followed by human annotation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a general data processing method.",
      "processing_time": 47.01214933395386,
      "citing_paper_id": "274789097",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "By combining these two metrics, we aim to achieve a more comprehensive evaluation of agent performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about evaluating agent performance. The cited paper title suggests a benchmark, but no specific dataset is named in the context.",
      "processing_time": 50.052655935287476,
      "citing_paper_id": "274789097",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "ALFRED[22], while widely used by LLM-based embodied agents, has a limited range of task types and supported actions, and its outdated version makes it difficult to expand into safety issues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ALFRED but does not refer to it as a dataset. It is described as a benchmark with limitations, which suggests it is a method or benchmark rather than a dataset.",
      "processing_time": 49.880459785461426,
      "citing_paper_id": "274789097",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Recently, embodied AI has attracted substantial attention for its capacity to dynamically perceive, understand, and interact with the physical world [4, 10, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to embodied AI. No dataset names are present in the text.",
      "processing_time": 48.78152894973755,
      "citing_paper_id": "274789097",
      "cited_paper_id": 231839855
    },
    {
      "context_text": "Next, all feasible instructions are converted into embeddings using the OpenAI embedding model[18].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of an embedding model. No verifiable resources are identified.",
      "processing_time": 48.161916971206665,
      "citing_paper_id": "274789097",
      "cited_paper_id": 246275593
    },
    {
      "context_text": "Safety risks of LLM agents have become a key research focus [2, 19, 20, 31], with various benchmarks evaluating safety at the text level [13, 27, 32].",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'benchmarks evaluating safety at the text level' but does not specify any particular dataset names. The cited paper title 'SafeText: A Benchmark for Exploring Physical Safety in Language Models' suggests a benchmark, but it is not a specific, downloadable dataset.",
      "processing_time": 53.00779128074646,
      "citing_paper_id": "274789097",
      "cited_paper_id": 252968000
    },
    {
      "context_text": "We select eight task-planning-related works as baselines[3, 5, 6, 24, 25, 28, 30, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only task-planning-related works as baselines. No verifiable resources are identified.",
      "processing_time": 49.12337899208069,
      "citing_paper_id": "274789097",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "We select eight task-planning-related works as baselines[3, 5, 6, 24, 25, 28, 30, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only task-planning-related works as baselines. No verifiable resources are identified.",
      "processing_time": 49.12337899208069,
      "citing_paper_id": "274789097",
      "cited_paper_id": 259342833
    },
    {
      "context_text": "We select eight task-planning-related works as baselines[3, 5, 6, 24, 25, 28, 30, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only task-planning-related works as baselines. No verifiable resources are identified.",
      "processing_time": 49.12337899208069,
      "citing_paper_id": "274789097",
      "cited_paper_id": 271854813
    },
    {
      "context_text": "Numerous recent studies have shown that embodied LLM agents can achieve decent success rates and have a promising future in task planning [25, 28, 33].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general findings about embodied LLM agents. No dataset names are present in the text.",
      "processing_time": 49.451555252075195,
      "citing_paper_id": "274789097",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Numerous recent studies have shown that embodied LLM agents can achieve decent success rates and have a promising future in task planning [25, 28, 33].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general findings about embodied LLM agents. No dataset names are present in the text.",
      "processing_time": 49.451555252075195,
      "citing_paper_id": "274789097",
      "cited_paper_id": 259342833
    },
    {
      "context_text": "To improve task success rates, ReAct [30] and LLM-Planner [25] leverage observations of objects in the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models (ReAct and LLM-Planner).",
      "processing_time": 48.93743181228638,
      "citing_paper_id": "274789097",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "• LLM-Planner [25] leverages LLMs for few-shot planning to generate task plans for embodied agents based on nat-ural language commands, updating plans with physical grounding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called LLM-Planner. The context focuses on the capabilities and usage of LLM-Planner rather than any dataset.",
      "processing_time": 51.4180326461792,
      "citing_paper_id": "274789097",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Behavior1K [14] created 1,000 tasks tailored to human preferences through surveys, achieving a balance between task diversity and physical realism based on the OMNIGIBSON platform.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Behavior1K"
      ],
      "dataset_descriptions": {
        "Behavior1K": "Used to create 1,000 tasks tailored to human preferences, balancing task diversity and physical realism in embodied AI research using the OMNIGIBSON platform."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'Behavior1K' as a benchmark with 1,000 tasks, which is a specific dataset used for embodied AI research. It is clearly identified and used for creating tasks tailored to human preferences.",
      "processing_time": 60.31880283355713,
      "citing_paper_id": "274789097",
      "cited_paper_id": 255198985
    },
    {
      "context_text": "It determines task success by verifying whether the goal conditions have been met after executing the generated plan, as widely used in existing benchmarks [6, 14, 22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to benchmarks which are excluded according to the instructions.",
      "processing_time": 48.359748125076294,
      "citing_paper_id": "274789097",
      "cited_paper_id": 255198985
    },
    {
      "context_text": "Unlike previous benchmarks[6, 14] that evaluate the agent’s performance by checking the environment state from the execution perspective, we further propose a LLM-based evaluation method from the semantic perspective.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to previous benchmarks. The cited paper title suggests a benchmark, but it is not used as a dataset in the given context.",
      "processing_time": 51.07060241699219,
      "citing_paper_id": "274789097",
      "cited_paper_id": 255198985
    },
    {
      "context_text": "Multiple embodied LLM agents can execute more complicated tasks together by communication[9, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of multiple embodied LLM agents executing tasks through communication.",
      "processing_time": 49.103551149368286,
      "citing_paper_id": "274789097",
      "cited_paper_id": 259342833
    },
    {
      "context_text": "• CoELA [33] integrates reasoning, language comprehen-Table 2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called CoELA. The context is too limited to infer any dataset usage.",
      "processing_time": 50.18756723403931,
      "citing_paper_id": "274789097",
      "cited_paper_id": 259342833
    },
    {
      "context_text": "Some studies have explored LLM safety in embodied environments: [16, 34] demonstrated that JailBreak attacks can trigger dangerous actions in both simulations and real-world settings, while [17] examined anomaly detection using scene graphs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only studies and methods. The focus is on the findings and methodologies of the cited works.",
      "processing_time": 49.822256088256836,
      "citing_paper_id": "274789097",
      "cited_paper_id": 270123427
    },
    {
      "context_text": "Specifically, We conduct experiments with three open-source LLMs—Llama3-8B [7], Qwen2-7B [29] and DeepSeek-V2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (Llama3-8B, Qwen2-7B, DeepSeek-V2) but does not refer to any specific datasets. The context is about using these models for experiments, not datasets.",
      "processing_time": 52.46439528465271,
      "citing_paper_id": "274789097",
      "cited_paper_id": 271212307
    },
    {
      "context_text": "Specifically, We conduct experiments with three open-source LLMs—Llama3-8B [7], Qwen2-7B [29] and DeepSeek-V2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (Llama3-8B, Qwen2-7B, DeepSeek-V2) but does not refer to any specific datasets. The context is about using these models for experiments, not datasets.",
      "processing_time": 52.46439528465271,
      "citing_paper_id": "274789097",
      "cited_paper_id": 271571434
    },
    {
      "context_text": "By generating non-interactive scenarios via LLMs and diffusion models, RiskAwareBench and EAR-Bench [36, 37] assessed physical risks in high-level planning.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RiskAwareBench and EAR-Bench, but these are benchmark suites, not datasets. No specific datasets are mentioned.",
      "processing_time": 50.351983070373535,
      "citing_paper_id": "274789097",
      "cited_paper_id": 271769045
    },
    {
      "context_text": "In the C ODE G ENERATOR ’s prompt, we explain the meanings of different stages and fields in the JSON representation and ask LLMs to follow Python and Z3 SMT syntax (De Moura & Bjørner, 2008).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Z3 SMT solver) and a programming language (Python).",
      "processing_time": 50.16352081298828,
      "citing_paper_id": "273374872",
      "cited_paper_id": 15912959
    },
    {
      "context_text": "…CoT depends on task-specific examples to achieve strong performance, domain files written in the planning domain definition language (PDDL) (Aeronautiques et al., 1998; Haslum et al., 2019) are required for (Liu et al., 2023), codes that encode domains into a mixed-integer linear program…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to PDDL, which is a language for defining planning problems, not a dataset.",
      "processing_time": 50.883039236068726,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "In this work, we compare the SMT solver with popular PDDL and MILP solvers and choose SMT solver with following reasons: SMT allows explicit goal and constraint assertion from scratch, which could be used to solve both single-step multi-constraint problems and multi-step problems, while PDDL solvers require a PDDL domain file and a PDDL problem file with strictly fixed formats, which limits its capability to solve non-PDDL problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only solvers and their capabilities. The citation is used to explain the choice of SMT solver over PDDL and MILP solvers.",
      "processing_time": 52.81597280502319,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "For multi-step problems, since Code tries to use a PDDL planner, which requires LLM to generate fixed-format PDDL domain and problem files, it almost always fails to generate and call them correctly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of PDDL for planning, which is a method or language, not a dataset.",
      "processing_time": 52.937949895858765,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "(Liu et al., 2023; Guan et al., 2023; Zhou et al., 2024; Xie et al., 2023) leverages PDDL planner to aid the planning processes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PDDL planner' but does not refer to a specific dataset. PDDL is a planning domain definition language, not a dataset.",
      "processing_time": 52.941248178482056,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "(Liu et al., 2023; Guan et al., 2023; Zhou et al., 2024; Xie et al., 2023) leverages PDDL planner to aid the planning processes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PDDL planner' but does not refer to a specific dataset. PDDL is a planning domain definition language, not a dataset.",
      "processing_time": 52.941248178482056,
      "citing_paper_id": "273374872",
      "cited_paper_id": 261245497
    },
    {
      "context_text": "SMT is complete and sound, that is, it guarantees to find the optimal plan, while PDDL planners are not guaranteed to be complete.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the properties of SMT and PDDL planners.",
      "processing_time": 52.3123197555542,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "For example, CoT depends on task-specific examples to achieve strong performance, domain files written in the planning domain definition language (PDDL) (Aeronautiques et al., 1998; Haslum et al., 2019) are required for (Liu et al., 2023), codes that encode domains into a mixed-integer linear program (MILP) are necessary for (Li et al., 2023), and external constraint critics are needed for (Gundawar et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on the requirements for different approaches in planning capabilities, such as PDDL, MILP, and constraint critics.",
      "processing_time": 55.10206651687622,
      "citing_paper_id": "273374872",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "…examples to achieve strong performance, domain files written in the planning domain definition language (PDDL) (Aeronautiques et al., 1998; Haslum et al., 2019) are required for (Liu et al., 2023), codes that encode domains into a mixed-integer linear program (MILP) are necessary for (Li…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to PDDL and MILP, which are methods or tools, not datasets.",
      "processing_time": 53.32851815223694,
      "citing_paper_id": "273374872",
      "cited_paper_id": 132910614
    },
    {
      "context_text": "Baselines We compare LLMFP against 1) Direct: LLM direct plan generation, 2) CoT: chain-of-thought prompting (Wei et al., 2022) by asking LLMs to reason before generating the final answer, and 3) Code: prompts LLM to generate Python codes to solve the problem, allowing the use of any package or…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for comparing LLMFP against other techniques.",
      "processing_time": 53.50029444694519,
      "citing_paper_id": "273374872",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "The remarkable capabilities of LLMs in reasoning (Wei et al., 2022; Kojima et al., 2022; Yao et al., 2022; 2024) and tool-use (Qin et al., 2023; Schick et al., 2024) brings up interests of utilizing LLMs to solve planning problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs in reasoning and tool-use. No verifiable resources are identified.",
      "processing_time": 55.39862823486328,
      "citing_paper_id": "273374872",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "The remarkable capabilities of LLMs in reasoning (Wei et al., 2022; Kojima et al., 2022; Yao et al., 2022; 2024) and tool-use (Qin et al., 2023; Schick et al., 2024) brings up interests of utilizing LLMs to solve planning problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs in reasoning and tool-use. No verifiable resources are identified.",
      "processing_time": 55.39862823486328,
      "citing_paper_id": "273374872",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "To improve LLMs reasoning and planning capabilities for more complex problems, chain-of-thought (CoT) prompting appends reasoning steps before the answer to encourage LLMs to perform step-by-step reasoning (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (chain-of-thought prompting) used to improve LLMs' reasoning and planning capabilities.",
      "processing_time": 55.394431591033936,
      "citing_paper_id": "273374872",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "As existing LLMs still do not have the capability to perform long-horizon reasoning for complex tasks (Achiam et al., 2023; Valmeekam et al., 2022; 2023; Kambhampati et al.)",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to limitations of LLMs in performing long-horizon reasoning for complex tasks.",
      "processing_time": 55.39026141166687,
      "citing_paper_id": "273374872",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Traditionally, symbolic approaches have been used for robotic planning tasks such as STRIPS or SHOP [12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only symbolic approaches and planners. No verifiable resources are identified.",
      "processing_time": 54.49010610580444,
      "citing_paper_id": "271965695",
      "cited_paper_id": 2329216
    },
    {
      "context_text": "Since its introduction in 1998 the Plan Domain Definition Language (PDDL) and extensions of it have been predominantly been used for planning [15, 16, 17, 18, 19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PDDL but does not refer to it as a dataset. It is described as a language for planning, which is more methodological than a dataset.",
      "processing_time": 56.0353946685791,
      "citing_paper_id": "271965695",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Since its introduction in 1998 the Plan Domain Definition Language (PDDL) and extensions of it have been predominantly been used for planning [15, 16, 17, 18, 19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PDDL but does not refer to it as a dataset. It is described as a language for planning, which is more methodological than a dataset.",
      "processing_time": 56.0353946685791,
      "citing_paper_id": "271965695",
      "cited_paper_id": 123106881
    },
    {
      "context_text": "The number of elemental steps to fulfill a task ranges between 1 and 173 steps, which is considerably longer than in existing robotic planning benchmarks [35, 36].",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'existing robotic planning benchmarks' but does not specify any particular dataset names. The cited papers are benchmarks but do not contain specific dataset names.",
      "processing_time": 56.03172254562378,
      "citing_paper_id": "271965695",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "The number of elemental steps to fulfill a task ranges between 1 and 173 steps, which is considerably longer than in existing robotic planning benchmarks [35, 36].",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'existing robotic planning benchmarks' but does not specify any particular dataset names. The cited papers are benchmarks but do not contain specific dataset names.",
      "processing_time": 56.03172254562378,
      "citing_paper_id": "271965695",
      "cited_paper_id": 255198985
    },
    {
      "context_text": "This is useful because LLMs are notoriously bad in counting tasks [34].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of LLMs. The cited paper title suggests a focus on counting tasks but does not name a dataset.",
      "processing_time": 56.71021389961243,
      "citing_paper_id": "271965695",
      "cited_paper_id": 229363334
    },
    {
      "context_text": "Providing a LLM with domain-specific knowledge is usually done by a human expert through detailed instruction or specialized training data [5, 1, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to 'specialized training data' in a generic sense without naming any particular dataset.",
      "processing_time": 56.02362418174744,
      "citing_paper_id": "271965695",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "For comparison we used the models SayCan [5] and Progprompt [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context is about comparing models, not using datasets.",
      "processing_time": 54.90329599380493,
      "citing_paper_id": "271965695",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "In the model SayCan, grounding is achieved the LLM scores the likelihood for action being the next step in a plan [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (SayCan) and a general concept (grounding).",
      "processing_time": 55.70848345756531,
      "citing_paper_id": "271965695",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "• We evaluate our approach against the state of the art models Progprompt[1] and SayCan[5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context focuses on evaluating the approach against other models, not using datasets.",
      "processing_time": 56.00446844100952,
      "citing_paper_id": "271965695",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "For this reason, recent studies utilised Large Language Models (LLMs) that are able to perform planning by generating sequences of elemental actions that satisfy some given goal [1, 2, 3, 4, 5, 6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of Large Language Models (LLMs) for planning. No verifiable resources are identified.",
      "processing_time": 56.672958850860596,
      "citing_paper_id": "271965695",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "For example by letting a model use template-functions that map to elemental actions [1, 4, 29, 30] or by letting it use predefined class structures that translate well into real-world concepts of interest [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. There are no clear identifiers for datasets in the context.",
      "processing_time": 55.87024164199829,
      "citing_paper_id": "271965695",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "This method extends the traditional Chain of Thought approach [31], because the code can be compiled and executed to calculate an answer [10, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.682281494140625,
      "citing_paper_id": "271965695",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "This method extends the traditional Chain of Thought approach [31], because the code can be compiled and executed to calculate an answer [10, 32, 33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.682281494140625,
      "citing_paper_id": "271965695",
      "cited_paper_id": 267406155
    },
    {
      "context_text": "With their training data usually containing code data, LLMs can be reasonable code generators [25, 26, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'training data' generically, which is not a specific dataset.",
      "processing_time": 56.99410343170166,
      "citing_paper_id": "271965695",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "This strategy can be applied in the planning domain, by letting the LLM translate a problem into PDDL and let a symbolic solver determine the correct plan [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving LLMs and PDDL for planning.",
      "processing_time": 55.68443560600281,
      "citing_paper_id": "271965695",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "One method to mitigate the limitations of LLMs is to refrain from allowing the LLM to generate the plan entirely, instead letting it make calls to external solvers [22, 23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches involving external solvers.",
      "processing_time": 55.02673697471619,
      "citing_paper_id": "271965695",
      "cited_paper_id": 258833332
    },
    {
      "context_text": "A challenge with plan generation by the means of a LLM is that even though they are usually able to produce good sounding high-level plans, they struggle to translate the plan into a sequence of executable elemental actions [20, 3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge with LLMs in plan generation. No verifiable resources are identified.",
      "processing_time": 56.66681170463562,
      "citing_paper_id": "271965695",
      "cited_paper_id": 259342833
    },
    {
      "context_text": "VizAssist[43]enables itsuserstospecifytheirdataobjectivesintermsofanalytic tasks(e.g.,correlate,compare)andconsidersthesetasksin combinationwithexistingperceptualguidelinesasinputtoage-neticalgorithmforrecommendingvisualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a tool called VizAssist which is used for recommending visualizations based on user objectives and perceptual guidelines.",
      "processing_time": 57.73815846443176,
      "citing_paper_id": "273950419",
      "cited_paper_id": 536614
    },
    {
      "context_text": "Through a review of visual analytics literature, we identify : C1 Accommodating diverse analysis requirements: Data An-alysts often face the immense challenge of navigating a vast exploration space, where the analytical process is dynamic and iterative [4].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges faced by data analysts in visual analytics.",
      "processing_time": 55.31558609008789,
      "citing_paper_id": "273950419",
      "cited_paper_id": 2307186
    },
    {
      "context_text": "While using the system, tasks may evolve based on the insights gained, necessitating ongoing iterations until the analytical goals are achieved [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the iterative nature of tasks in visual analytics.",
      "processing_time": 56.43123483657837,
      "citing_paper_id": "273950419",
      "cited_paper_id": 2307186
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 2366653
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 52012313
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 221308729
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 236090307
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 245986665
    },
    {
      "context_text": "Our research is related to prior studies on visualization recommendations, task-driven data exploration, and LLM application in data exploration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 56.136961221694946,
      "citing_paper_id": "273950419",
      "cited_paper_id": 252089119
    },
    {
      "context_text": "Additionally, when tasks involve multiple visualizations, these must be integrated into a linked view to enable more effective interactive exploration [57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using multiple views in information visualization.",
      "processing_time": 55.49144101142883,
      "citing_paper_id": "273950419",
      "cited_paper_id": 2738870
    },
    {
      "context_text": "Formachinelearn-ingmethods,Data2Vis[23]introducesanend-to-endtrainable neuraltranslationmodelforautomaticallygeneratingvisual-izationsfromgivendatasets.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Data2Vis' but does not refer to it as a dataset. It is described as a method or model for generating visualizations.",
      "processing_time": 57.57618069648743,
      "citing_paper_id": "273950419",
      "cited_paper_id": 4706694
    },
    {
      "context_text": "Gotz and Wen [42] present a prototype system that observes interaction patterns (e.g., repeatedly changing ﬁlters or swappingattributes)toinferanalytictaskssuchascomparisonor trendanalysisandcorrespondinglyrecommendsvisualizations suchassmallmultiplesorlinecharts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a prototype system and interaction patterns. No clear, verifiable dataset names are provided.",
      "processing_time": 56.6401252746582,
      "citing_paper_id": "273950419",
      "cited_paper_id": 9225837
    },
    {
      "context_text": "Qu and Hullman [30] proposes coordination principles to keep consistency.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only coordination principles for visualization authoring.",
      "processing_time": 54.99873614311218,
      "citing_paper_id": "273950419",
      "cited_paper_id": 12316599
    },
    {
      "context_text": "(cid:2) Color mapping: We built a data-to-color mapping rule withinthetaskspacebasedonQuandHullman’sguidelines [30],whereonecolorcorrespondstoonedatadimension without reuse.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method for data-to-color mapping based on guidelines from a cited paper.",
      "processing_time": 57.12596821784973,
      "citing_paper_id": "273950419",
      "cited_paper_id": 12316599
    },
    {
      "context_text": "Saket et al. [40] conducted a study to assess the effectiveness of ﬁve canonical visualizations on ten low-level analytic tasks [41] and developed a recommendation engine based on their study’s ﬁndings.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a study and its findings. The context focuses on the effectiveness of visualizations and the development of a recommendation engine.",
      "processing_time": 58.15083432197571,
      "citing_paper_id": "273950419",
      "cited_paper_id": 13771609
    },
    {
      "context_text": "Saket et al. [40] conducted a study to assess the effectiveness of ﬁve canonical visualizations on ten low-level analytic tasks [41] and developed a recommendation engine based on their study’s ﬁndings.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a study and its findings. The context focuses on the effectiveness of visualizations and the development of a recommendation engine.",
      "processing_time": 58.15083432197571,
      "citing_paper_id": "273950419",
      "cited_paper_id": 14217178
    },
    {
      "context_text": "A key challenge is that this process is iterative, requiring continual reﬁnement based on evolving needs [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general process in visual analytics.",
      "processing_time": 56.62415552139282,
      "citing_paper_id": "273950419",
      "cited_paper_id": 16238232
    },
    {
      "context_text": "A key challenge is that this process is iterative, requiring continual reﬁnement based on evolving needs [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general process in visual analytics.",
      "processing_time": 56.62415552139282,
      "citing_paper_id": "273950419",
      "cited_paper_id": 246240065
    },
    {
      "context_text": "A key challenge is that this process is iterative, requiring continual reﬁnement based on evolving needs [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general process in visual analytics.",
      "processing_time": 56.62415552139282,
      "citing_paper_id": "273950419",
      "cited_paper_id": null
    },
    {
      "context_text": "Casner [39] presents one of the earliest examples of visualization systems that suggest charts based on a user’s task (e.g., ﬁnding direct ﬂight routes or a table to see ﬂight information).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a visualization system. The context is about the system's functionality, not a dataset.",
      "processing_time": 57.113943576812744,
      "citing_paper_id": "273950419",
      "cited_paper_id": 16261041
    },
    {
      "context_text": "Rule-basedmethods,suchasVoyager[21]andCompassQL[22], utilizethevisualizationprinciplestoconstructvisualmapping andallowuserstochoosetheirinteresteddatapropertiesand visualencodingtocreatevisualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only rule-based methods and visualization principles. No verifiable resources are identified.",
      "processing_time": 56.39126133918762,
      "citing_paper_id": "273950419",
      "cited_paper_id": 16716972
    },
    {
      "context_text": "Recentresearch focuses on data-driven or natural language-based visual data exploration, with an emphasis on automatic visualization generation [5], [6] or insight mining [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and approaches. No verifiable resources are identified.",
      "processing_time": 56.38757371902466,
      "citing_paper_id": "273950419",
      "cited_paper_id": 33551774
    },
    {
      "context_text": "Recentresearch focuses on data-driven or natural language-based visual data exploration, with an emphasis on automatic visualization generation [5], [6] or insight mining [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and approaches. No verifiable resources are identified.",
      "processing_time": 56.38757371902466,
      "citing_paper_id": "273950419",
      "cited_paper_id": 251253219
    },
    {
      "context_text": "Foresight[8] usestaskslikedistributions,outliers,andcorrelationstoguide insightdiscoveryandgroupingrecommendations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods used for recommending visual insights.",
      "processing_time": 55.6084771156311,
      "citing_paper_id": "273950419",
      "cited_paper_id": 33551774
    },
    {
      "context_text": "Dziban [32] is a visualization API using anchored recommendation and extending Draco [33] to reason about multiple views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Dziban and Draco, which are tools or methods, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 57.91250777244568,
      "citing_paper_id": "273950419",
      "cited_paper_id": 49575415
    },
    {
      "context_text": "Dziban [32] is a visualization API using anchored recommendation and extending Draco [33] to reason about multiple views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Dziban and Draco, which are tools or methods, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 57.91250777244568,
      "citing_paper_id": "273950419",
      "cited_paper_id": 210926937
    },
    {
      "context_text": "Prompting techniques: 1) Few-shot prompting [61]: We provide examples to assist the model in better grasping the requirements of the outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to few-shot prompting as a technique, which is not a dataset.",
      "processing_time": 57.664270639419556,
      "citing_paper_id": "273950419",
      "cited_paper_id": 61817892
    },
    {
      "context_text": "Prompting techniques: 1) Few-shot prompting [61]: We provide examples to assist the model in better grasping the requirements of the outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to few-shot prompting as a technique, which is not a dataset.",
      "processing_time": 57.664270639419556,
      "citing_paper_id": "273950419",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We could conduct further qualitative experiments to test the differences in decision-making paths when experts in visualization are assisted by agents [73], [74], comparing these paths visually using graph algorithms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a suggestion for future qualitative experiments. No verifiable resources are identified.",
      "processing_time": 56.90461444854736,
      "citing_paper_id": "273950419",
      "cited_paper_id": 160018920
    },
    {
      "context_text": "We could conduct further qualitative experiments to test the differences in decision-making paths when experts in visualization are assisted by agents [73], [74], comparing these paths visually using graph algorithms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a suggestion for future qualitative experiments. No verifiable resources are identified.",
      "processing_time": 56.90461444854736,
      "citing_paper_id": "273950419",
      "cited_paper_id": 204960852
    },
    {
      "context_text": "Guided by the design requirements, we propose a pipeline of LLM agent-based task planning and an interface to support interactive visual data exploration with assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to task planning and data exploration. No verifiable resources are identified.",
      "processing_time": 57.66686511039734,
      "citing_paper_id": "273950419",
      "cited_paper_id": 189048448
    },
    {
      "context_text": "Guided by the design requirements, we propose a pipeline of LLM agent-based task planning and an interface to support interactive visual data exploration with assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to task planning and data exploration. No verifiable resources are identified.",
      "processing_time": 57.66686511039734,
      "citing_paper_id": "273950419",
      "cited_paper_id": 201093978
    },
    {
      "context_text": "Guided by the design requirements, we propose a pipeline of LLM agent-based task planning and an interface to support interactive visual data exploration with assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to task planning and data exploration. No verifiable resources are identified.",
      "processing_time": 57.66686511039734,
      "citing_paper_id": "273950419",
      "cited_paper_id": 206805969
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 199405608
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 201093978
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 257365015
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 263834585
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 265659006
    },
    {
      "context_text": "The reasoning abilities that enable autonomous planning and execution of analytical tasks [9], [10], [11], while code generation capability support creating insightful visualizations efﬁciently [12], [13], [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models and tools. No verifiable resources are identified.",
      "processing_time": 57.294485092163086,
      "citing_paper_id": "273950419",
      "cited_paper_id": 266693892
    },
    {
      "context_text": "AsAltaironlysupportsbasicdatatransformation,weleverage Python’sﬂexiblelibraries,allowingformorecomplexanalysis, suchasregressionandclustering.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods for data transformation and analysis.",
      "processing_time": 55.76856827735901,
      "citing_paper_id": "273950419",
      "cited_paper_id": 206805969
    },
    {
      "context_text": "In future studies, we could explore how different levels of agency and automation affect task performance and user satisfaction [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential area for future research.",
      "processing_time": 55.59002685546875,
      "citing_paper_id": "273950419",
      "cited_paper_id": 210926937
    },
    {
      "context_text": "ATENA [38] shapes EDA into a Markov Decision Process (MDP) model using a deep reinforcement learning architecture to effectively optimize notebook generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ATENA) and a model (MDP).",
      "processing_time": 57.078511476516724,
      "citing_paper_id": "273950419",
      "cited_paper_id": 218981884
    },
    {
      "context_text": "Table2Charts [25] recommends visualizations by learning patterns between tables and visualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Table2Charts but does not indicate it is a dataset. It appears to be a method or tool for recommending visualizations.",
      "processing_time": 57.881858587265015,
      "citing_paper_id": "273950419",
      "cited_paper_id": 235359276
    },
    {
      "context_text": "Acorpusofresearchhasbeenproposedon automaticvisualizationrecommendations[18],[19],[20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide specific, verifiable dataset names. It mentions a 'corpus of research' which is too generic and lacks a specific identifier.",
      "processing_time": 58.52853870391846,
      "citing_paper_id": "273950419",
      "cited_paper_id": 235651885
    },
    {
      "context_text": "Acorpusofresearchhasbeenproposedon automaticvisualizationrecommendations[18],[19],[20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide specific, verifiable dataset names. It mentions a 'corpus of research' which is too generic and lacks a specific identifier.",
      "processing_time": 58.52853870391846,
      "citing_paper_id": "273950419",
      "cited_paper_id": 259241297
    },
    {
      "context_text": "MultiVision [35] and DashBot [5] recommend dashboards given an input dataset in an end-to-end manner using deep learning models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MultiVision and DashBot, which are tools or methods for dashboard recommendation and generation, not datasets. No specific datasets are mentioned.",
      "processing_time": 58.07694697380066,
      "citing_paper_id": "273950419",
      "cited_paper_id": 236034184
    },
    {
      "context_text": "MultiVision [35] and DashBot [5] recommend dashboards given an input dataset in an end-to-end manner using deep learning models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MultiVision and DashBot, which are tools or methods for dashboard recommendation and generation, not datasets. No specific datasets are mentioned.",
      "processing_time": 58.07694697380066,
      "citing_paper_id": "273950419",
      "cited_paper_id": 251253219
    },
    {
      "context_text": "Unlike end-to-end deep learning methods that directly learn from datasets to generate visualizations, knowledge graph-based approaches, such as AdaVis [27], KG4VIS [28], Lodestar [29], leverage structured information about data and relationships to recommend visualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.65151381492615,
      "citing_paper_id": "273950419",
      "cited_paper_id": 236447420
    },
    {
      "context_text": "Unlike end-to-end deep learning methods that directly learn from datasets to generate visualizations, knowledge graph-based approaches, such as AdaVis [27], KG4VIS [28], Lodestar [29], leverage structured information about data and relationships to recommend visualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.65151381492615,
      "citing_paper_id": "273950419",
      "cited_paper_id": 236985291
    },
    {
      "context_text": "Unlike end-to-end deep learning methods that directly learn from datasets to generate visualizations, knowledge graph-based approaches, such as AdaVis [27], KG4VIS [28], Lodestar [29], leverage structured information about data and relationships to recommend visualizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.65151381492615,
      "citing_paper_id": "273950419",
      "cited_paper_id": 262055731
    },
    {
      "context_text": "Sun et al. [31] investigate different linking techniques based on data relationships.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only 'data relationships' which is too generic.",
      "processing_time": 56.16951656341553,
      "citing_paper_id": "273950419",
      "cited_paper_id": 236945351
    },
    {
      "context_text": "TaskVis[44] recommends visualizations under speciﬁc tasks through answer set programming.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a system called TaskVis, which is a method or tool, not a dataset.",
      "processing_time": 58.892804861068726,
      "citing_paper_id": "273950419",
      "cited_paper_id": 238353829
    },
    {
      "context_text": "TaskVis[44] recommends visualizations under speciﬁc tasks through answer set programming.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a system called TaskVis, which is a method or tool, not a dataset.",
      "processing_time": 58.892804861068726,
      "citing_paper_id": "273950419",
      "cited_paper_id": 240302757
    },
    {
      "context_text": "EDAssistant [37] recommends code by analyzing associations between APIs in a large notebook collection.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'large notebook collection' but does not provide a specific name or identifier for this collection. It is a generic reference and thus does not meet the criteria for inclusion.",
      "processing_time": 59.427969455718994,
      "citing_paper_id": "273950419",
      "cited_paper_id": 245144651
    },
    {
      "context_text": "2) Chain-of-thoughts [52]: We guide the model through a thought plan with step-by-step instructions, which is helpful in reasoning processes, such as task execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for guiding models through reasoning processes.",
      "processing_time": 56.157930850982666,
      "citing_paper_id": "273950419",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Recently, prior methods have utilized LLMs with input-output prompting, CoT [52], ToT [53] or GoT [54] to perform complex task planning and execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (CoT, ToT, GoT) used with LLMs for task planning and execution.",
      "processing_time": 59.406920194625854,
      "citing_paper_id": "273950419",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Recently, prior methods have utilized LLMs with input-output prompting, CoT [52], ToT [53] or GoT [54] to perform complex task planning and execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (CoT, ToT, GoT) used with LLMs for task planning and execution.",
      "processing_time": 59.406920194625854,
      "citing_paper_id": "273950419",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Recently, prior methods have utilized LLMs with input-output prompting, CoT [52], ToT [53] or GoT [54] to perform complex task planning and execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods (CoT, ToT, GoT) used with LLMs for task planning and execution.",
      "processing_time": 59.406920194625854,
      "citing_paper_id": "273950419",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "Compared to our sub-mission to the VAST Challenge [63], where we spent 2 people, 2 days, and 6 hours on data preprocessing (including tagging, categorizing, and removing spam messages) plus 2 people, 7 days, and 6 hours on interface construction and analysis, totaling around 108 hours.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only describes the time spent on data preprocessing and interface construction for a challenge.",
      "processing_time": 58.26195216178894,
      "citing_paper_id": "273950419",
      "cited_paper_id": 247022700
    },
    {
      "context_text": "However, this process requires substantial knowledge of both visualization principles and coding skills, resulting in inefﬁciencies and delays [58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the challenges of developing data visualization services.",
      "processing_time": 57.60747027397156,
      "citing_paper_id": "273950419",
      "cited_paper_id": 247897432
    },
    {
      "context_text": "C2 Developing visualization and data mining tools is time-consuming: Analysts often lack proﬁciency in developing and managing VA systems, which makes it challenging for them to select and apply appropriate data modeling and visualization techniques [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in developing visualization and data mining tools.",
      "processing_time": 56.528785705566406,
      "citing_paper_id": "273950419",
      "cited_paper_id": 251371611
    },
    {
      "context_text": "Inadditiontovisual generation based on a single task, Med-ley [45] recommends multi-view collections based on several analytic intents, and views and widgets can be selected to compose a variety of dashboards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called Medley. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.48533630371094,
      "citing_paper_id": "273950419",
      "cited_paper_id": 251371611
    },
    {
      "context_text": "Ma et al. [50] and JarviX[12] introduce systems that automate the data exploration process by identifying suitable analysis intents and generating insights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only systems that automate data exploration and generate insights.",
      "processing_time": 56.297285079956055,
      "citing_paper_id": "273950419",
      "cited_paper_id": 257912912
    },
    {
      "context_text": "Ma et al. [50] and JarviX[12] introduce systems that automate the data exploration process by identifying suitable analysis intents and generating insights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only systems that automate data exploration and generate insights.",
      "processing_time": 56.297285079956055,
      "citing_paper_id": "273950419",
      "cited_paper_id": 265659006
    },
    {
      "context_text": "Talk2Data [56] presents a natural language interface that enables users to explore visual data through question decomposition.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called Talk2Data. The context focuses on the functionality of Talk2Data rather than the use of a specific dataset.",
      "processing_time": 60.775249004364014,
      "citing_paper_id": "273950419",
      "cited_paper_id": 258685266
    },
    {
      "context_text": "During the exploration process, users can ask various questions and discuss them with LLM in the dialog box, such as the problem analysis method, understanding of the answer, and explanation for the recommendation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general interaction process with LLMs. No verifiable resources are identified.",
      "processing_time": 57.82209062576294,
      "citing_paper_id": "273950419",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Furthermore, visual analytics incorporates visualization techniques into the EDA process, expanding the task space.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of incorporating visualization techniques into EDA.",
      "processing_time": 58.24669933319092,
      "citing_paper_id": "273950419",
      "cited_paper_id": 258841776
    },
    {
      "context_text": "Furthermore, visual analytics incorporates visualization techniques into the EDA process, expanding the task space.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of incorporating visualization techniques into EDA.",
      "processing_time": 58.24669933319092,
      "citing_paper_id": "273950419",
      "cited_paper_id": 259137864
    },
    {
      "context_text": "Thereisalackofapproaches supportingtaskplanning,VAmethodsimplementation,and interactiveanalysiswithhuman-agentcollaboration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a lack of approaches for task planning, VA methods implementation, and interactive analysis with human-agent collaboration.",
      "processing_time": 60.498908281326294,
      "citing_paper_id": "273950419",
      "cited_paper_id": 259137864
    },
    {
      "context_text": "Thereisalackofapproaches supportingtaskplanning,VAmethodsimplementation,and interactiveanalysiswithhuman-agentcollaboration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a lack of approaches for task planning, VA methods implementation, and interactive analysis with human-agent collaboration.",
      "processing_time": 60.498908281326294,
      "citing_paper_id": "273950419",
      "cited_paper_id": 261100760
    },
    {
      "context_text": "7(b3), the differences between product categories are minor, with all reaching maximum proﬁt at a 10% discount.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It discusses profit differences in product categories, which is unrelated to the topic of planning capabilities of LLMs.",
      "processing_time": 60.28046917915344,
      "citing_paper_id": "273950419",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "7(b3), the differences between product categories are minor, with all reaching maximum proﬁt at a 10% discount.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It discusses profit differences in product categories, which is unrelated to the topic of planning capabilities of LLMs.",
      "processing_time": 60.28046917915344,
      "citing_paper_id": "273950419",
      "cited_paper_id": 266176951
    },
    {
      "context_text": "7(b3), the differences between product categories are minor, with all reaching maximum proﬁt at a 10% discount.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It discusses profit differences in product categories, which is unrelated to the topic of planning capabilities of LLMs.",
      "processing_time": 60.28046917915344,
      "citing_paper_id": "273950419",
      "cited_paper_id": 271533450
    },
    {
      "context_text": "7(b3), the differences between product categories are minor, with all reaching maximum proﬁt at a 10% discount.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It discusses profit differences in product categories, which is unrelated to the topic of planning capabilities of LLMs.",
      "processing_time": 60.28046917915344,
      "citing_paper_id": "273950419",
      "cited_paper_id": 271544382
    },
    {
      "context_text": "In addition, interactions other than natural language can be designed to facilitate user input of intentions, such as sketching [71] or generating widgets [72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to interactions and methods for user input, which are not datasets.",
      "processing_time": 58.66554570198059,
      "citing_paper_id": "273950419",
      "cited_paper_id": 263671690
    },
    {
      "context_text": "In addition, interactions other than natural language can be designed to facilitate user input of intentions, such as sketching [71] or generating widgets [72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to interactions and methods for user input, which are not datasets.",
      "processing_time": 58.66554570198059,
      "citing_paper_id": "273950419",
      "cited_paper_id": 267060885
    },
    {
      "context_text": "For visualization generation and recommendations, LLM4Vis [14] and ChartGPT [46] utilize LLMs to choose appropriate visualizations from natural language instructions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods (LLMs, LLM4Vis, ChartGPT).",
      "processing_time": 58.23134779930115,
      "citing_paper_id": "273950419",
      "cited_paper_id": 263834585
    },
    {
      "context_text": "Besides, a memory module [70] can be integrated to make the agent evolve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a memory module which is likely a method or component of a system.",
      "processing_time": 57.564733266830444,
      "citing_paper_id": "273950419",
      "cited_paper_id": 263909014
    },
    {
      "context_text": "NL2Rigel [48] showcases the LLM’s ability to convert instructions into comprehensive data visualizations and tables.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of LLMs to convert instructions into data visualizations and tables.",
      "processing_time": 58.00977444648743,
      "citing_paper_id": "273950419",
      "cited_paper_id": 264930510
    },
    {
      "context_text": "In addition to data attributes when recommending visualizations, some visualization recommendation systems are task-driven recommendation systems that consider one or more analytic tasks (e.g., correlate, analyze trend).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about visualization recommendation systems and analytic tasks.",
      "processing_time": 56.789947509765625,
      "citing_paper_id": "273950419",
      "cited_paper_id": 265019126
    },
    {
      "context_text": "A future direction could be to utilize caching mechanisms such as GPTCache [66] to explore acceleration strategies in data analysis scenarios.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a caching mechanism. The context is about future directions and does not describe actual usage of a dataset.",
      "processing_time": 58.64561653137207,
      "citing_paper_id": "273950419",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "Text2Analysis [51] offers a framework for categorizing data analysis tasks, establishing a structured approach to tackling common analytical challenges ranging from basic operations to forecasting and chart generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for categorizing data analysis tasks.",
      "processing_time": 56.47269082069397,
      "citing_paper_id": "273950419",
      "cited_paper_id": 266435934
    },
    {
      "context_text": "Li et al. [47] evaluate the capability of GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets used in the evaluation of GPT-3. It only refers to the evaluation of the model's capabilities.",
      "processing_time": 58.80983519554138,
      "citing_paper_id": "273950419",
      "cited_paper_id": 267069400
    },
    {
      "context_text": "LEVA [17] integrates LLMs in VA systems to recommend insights for a given task but still cannot generate visualization and data modelingmethodsadaptedtotasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (LEVA) that integrates LLMs in VA systems. There are no clear identifiers for datasets.",
      "processing_time": 59.98318958282471,
      "citing_paper_id": "273950419",
      "cited_paper_id": 268239825
    },
    {
      "context_text": "Ye et al. (2023) designed an Elo-based Self-Judgment Mechanism (Elo and Sloan, 1978) that use ChatGPT as a decision maker.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Elo-based Self-Judgment Mechanism) and a tool (ChatGPT).",
      "processing_time": 59.12378764152527,
      "citing_paper_id": "272827086",
      "cited_paper_id": 142610973
    },
    {
      "context_text": "Comparison between Tag Extraction and Re-triever: Previous works use a dense retriever, following Sentence BERT (Reimers and Gurevych, 2019), to select the top-K related APIs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Sentence BERT) used for selecting related APIs. No dataset names are present.",
      "processing_time": 58.42984199523926,
      "citing_paper_id": "272827086",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "React (Yao et al., 2022) interacts with external tools multiple rounds that follow the \"Thought-Action-Action Input-Observation\" format.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called ReAct. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.11569571495056,
      "citing_paper_id": "272827086",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…large language models (LLMs) have shown their remarkable ability in utilizing various external tools, such as Hugging-Face models (Shen et al., 2023; Wu et al., 2023), real-world applications (Liu et al., 2023b; Wang et al., 2023b), and massive APIs (Tang et al., 2023; Liang et al., 2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models, tools, and APIs. No verifiable datasets are identified.",
      "processing_time": 58.187663555145264,
      "citing_paper_id": "272827086",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Qiao et al. (2023) enhances the model through feedback derived from tool execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing models through feedback from tool execution.",
      "processing_time": 57.16101622581482,
      "citing_paper_id": "272827086",
      "cited_paper_id": 258832372
    },
    {
      "context_text": "However, our multi-granularity instruction mechanism can also be applied to other public datasets, like APIBench(Patil et al., 2023) and ToolAlpaca(Tang et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "APIBench",
        "ToolAlpaca"
      ],
      "dataset_descriptions": {
        "APIBench": "Mentioned as a public dataset, but specific usage in the research context is not detailed.",
        "ToolAlpaca": "Mentioned as a public dataset, but specific usage in the research context is not detailed."
      },
      "confidence_score": 0.5,
      "reasoning": "The context mentions 'APIBench' and 'ToolAlpaca' as public datasets, but does not specify their usage in the research. The cited paper title does not provide additional clarity on the nature of these resources.",
      "processing_time": 73.29999160766602,
      "citing_paper_id": "272827086",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "APIBench (Patil et al., 2023) collects 1,716 machine learning APIs from 3 public model hubs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "APIBench"
      ],
      "dataset_descriptions": {
        "APIBench": "Used to connect large language models with a diverse set of machine learning APIs, enhancing the model's capabilities by integrating external services and functionalities."
      },
      "confidence_score": 0.8,
      "reasoning": "APIBench is a collection of machine learning APIs, which is not a traditional dataset but a resource. However, it is specific and verifiable, and it is used in the context of connecting large language models with APIs.",
      "processing_time": 68.2526261806488,
      "citing_paper_id": "272827086",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "To simulate complex real-world tasks, previous studies have continuously increased the size of the external tool pool and the complexity of task instructions (Yang et al., 2023; Real User 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general approach of increasing the size of the external tool pool and task complexity.",
      "processing_time": 58.158912897109985,
      "citing_paper_id": "272827086",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "Following (Chen et al., 2023), we set the maximum sequence length to 8192.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological detail about sequence length.",
      "processing_time": 56.42974543571472,
      "citing_paper_id": "272827086",
      "cited_paper_id": 259262376
    },
    {
      "context_text": "Tool-augmented LLMs Datasets: The research community collects diverse datasets to facilitate research on tool-enhanced LLMs. API-Bank (Li et al., 2023) provides a benchmark that includes 264 annotated dialogues and 568 APIs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "API-Bank"
      ],
      "dataset_descriptions": {
        "API-Bank": "Used to evaluate tool-augmented LLMs, specifically assessing performance on 264 annotated dialogues and 568 APIs, focusing on the integration and effectiveness of external tools."
      },
      "confidence_score": 1.0,
      "reasoning": "API-Bank is mentioned as a benchmark that includes annotated dialogues and APIs, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 67.91848492622375,
      "citing_paper_id": "272827086",
      "cited_paper_id": 276344352
    },
    {
      "context_text": "The ability of high-level planning languages like PDDL [28] to root linguistic directives with symbolic representations has been extensively studied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a planning language (PDDL). The context focuses on the capabilities of PDDL for symbolic representation and temporal planning, which is not a dataset.",
      "processing_time": 61.25942420959473,
      "citing_paper_id": "268723699",
      "cited_paper_id": 1397894
    },
    {
      "context_text": "We use GMapping [35] to create a map using scan data and provide it to the language model as environment information.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "GMapping is mentioned as a tool used to create a map using scan data, which is then provided to the language model as environment information. However, GMapping is a method, not a dataset.",
      "processing_time": 61.54542589187622,
      "citing_paper_id": "268723699",
      "cited_paper_id": 40099475
    },
    {
      "context_text": "• We evaluated the performance of proposed system with SOTA path planners (A*[2] and RRT [24]) and observed least distance travelled and shortest planning time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (A* and RRT) used for path planning evaluation.",
      "processing_time": 57.9381046295166,
      "citing_paper_id": "268723699",
      "cited_paper_id": 59524001
    },
    {
      "context_text": "• We evaluated the performance of proposed system with SOTA path planners (A*[2] and RRT [24]) and observed least distance travelled and shortest planning time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (A* and RRT) used for path planning evaluation.",
      "processing_time": 57.9381046295166,
      "citing_paper_id": "268723699",
      "cited_paper_id": 210971709
    },
    {
      "context_text": "A recent technique [32] illustrates how multi-task reinforcement learning in a discrete state and action space may map phrases to robotic actions (navigation, pick, and place).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique. The context focuses on the application of multi-task reinforcement learning for mapping language to robotic actions.",
      "processing_time": 59.92824625968933,
      "citing_paper_id": "268723699",
      "cited_paper_id": 67788344
    },
    {
      "context_text": "According to a study [31], a model that learns to map phrases to a list of subgoals can automatically break down tasks for these planners.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model capability. No dataset names are present in the citation context.",
      "processing_time": 57.91856598854065,
      "citing_paper_id": "268723699",
      "cited_paper_id": 84186410
    },
    {
      "context_text": "5, created by OpenAI, has demonstrated outstanding performance in various natural language processing tasks, including language production, question answering, and machine translation [12], [13], [14].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of a language model in various tasks.",
      "processing_time": 57.10861921310425,
      "citing_paper_id": "268723699",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Despite these advances, conventional path-planning frameworks often fal-ter in navigating complex environments and in formulating dependable strategies amidst fluctuating environmental conditions [9], [10], [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general challenges in path planning. No verifiable resources are identified.",
      "processing_time": 57.92266249656677,
      "citing_paper_id": "268723699",
      "cited_paper_id": 184469323
    },
    {
      "context_text": "5-turbo uses A* [2] algorithm for path finding from a grid, and upon prompt tuning, it changed the navigation algorithm to Hierarchical Annotated A* [34] for efficiency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions algorithms (A* and Hierarchical Annotated A*) but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.91376543045044,
      "citing_paper_id": "268723699",
      "cited_paper_id": 210971709
    },
    {
      "context_text": "5-turbo uses A* [2] algorithm for path finding from a grid, and upon prompt tuning, it changed the navigation algorithm to Hierarchical Annotated A* [34] for efficiency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions algorithms (A* and Hierarchical Annotated A*) but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.91376543045044,
      "citing_paper_id": "268723699",
      "cited_paper_id": 220828823
    },
    {
      "context_text": "The literature has seen the emergence of potent methodologies in the realm of path planning, including graph optimization path planning [2], [3], [4], heuristic-based strategies [5], and the exploration of rapidly exploring random trees (RRTs) [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and algorithms. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.73136925697327,
      "citing_paper_id": "268723699",
      "cited_paper_id": 210971709
    },
    {
      "context_text": "The literature has seen the emergence of potent methodologies in the realm of path planning, including graph optimization path planning [2], [3], [4], heuristic-based strategies [5], and the exploration of rapidly exploring random trees (RRTs) [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and algorithms. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.73136925697327,
      "citing_paper_id": "268723699",
      "cited_paper_id": 249461514
    },
    {
      "context_text": "The literature has seen the emergence of potent methodologies in the realm of path planning, including graph optimization path planning [2], [3], [4], heuristic-based strategies [5], and the exploration of rapidly exploring random trees (RRTs) [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and algorithms. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.73136925697327,
      "citing_paper_id": "268723699",
      "cited_paper_id": 252693035
    },
    {
      "context_text": "The literature has seen the emergence of potent methodologies in the realm of path planning, including graph optimization path planning [2], [3], [4], heuristic-based strategies [5], and the exploration of rapidly exploring random trees (RRTs) [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and algorithms. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.73136925697327,
      "citing_paper_id": "268723699",
      "cited_paper_id": 259991167
    },
    {
      "context_text": "5-turbo excels in these tasks, which is attributed to its capacity to identify patterns in vast amounts of data [15], [16], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capacity of models to identify patterns in data. No verifiable resources are named.",
      "processing_time": 58.72665214538574,
      "citing_paper_id": "268723699",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "5-turbo excels in these tasks, which is attributed to its capacity to identify patterns in vast amounts of data [15], [16], [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capacity of models to identify patterns in data. No verifiable resources are named.",
      "processing_time": 58.72665214538574,
      "citing_paper_id": "268723699",
      "cited_paper_id": 267616761
    },
    {
      "context_text": "For instance, A study [18], [19], [20] demonstrated that GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No verifiable resources are identified.",
      "processing_time": 57.69206786155701,
      "citing_paper_id": "268723699",
      "cited_paper_id": 246681316
    },
    {
      "context_text": "For instance, A study [18], [19], [20] demonstrated that GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No verifiable resources are identified.",
      "processing_time": 57.69206786155701,
      "citing_paper_id": "268723699",
      "cited_paper_id": 266051233
    },
    {
      "context_text": "For instance, A study [18], [19], [20] demonstrated that GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No verifiable resources are identified.",
      "processing_time": 57.69206786155701,
      "citing_paper_id": "268723699",
      "cited_paper_id": 277626231
    },
    {
      "context_text": "R O ] 27 M a r 2024 [21], [22], [23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets or resources. It only lists citation numbers without providing context or details about the resources used.",
      "processing_time": 59.257575273513794,
      "citing_paper_id": "268723699",
      "cited_paper_id": 266162896
    },
    {
      "context_text": "One well-known example of rule-based planning is the Intelligent Driver Model (IDM) [29], which is designed to help vehicles follow a leading vehicle while maintaining a safe following distance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Intelligent Driver Model).",
      "processing_time": 57.089720010757446,
      "citing_paper_id": "266693892",
      "cited_paper_id": 1100293
    },
    {
      "context_text": "The strong performances of IDM [29] and PDM-Closed [10] on the closed-loop nuPlan evaluations demonstrate that rule-based planners are capable of successfully maneuvering the vast majority of driving scenarios.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of rule-based planners on the nuPlan evaluations. The nuPlan dataset is implied but not explicitly named.",
      "processing_time": 60.14353561401367,
      "citing_paper_id": "266693892",
      "cited_paper_id": 1100293
    },
    {
      "context_text": "Rule-based planning involves the use of explicit rules to guide the decision-making process in autonomous vehicles [2, 7, 11, 16, 26, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only rule-based planning in autonomous vehicles. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.50643348693848,
      "citing_paper_id": "266693892",
      "cited_paper_id": 1906145
    },
    {
      "context_text": "Rule-based planning involves the use of explicit rules to guide the decision-making process in autonomous vehicles [2, 7, 11, 16, 26, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only rule-based planning in autonomous vehicles. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.50643348693848,
      "citing_paper_id": "266693892",
      "cited_paper_id": 12198630
    },
    {
      "context_text": "Rule-based planning involves the use of explicit rules to guide the decision-making process in autonomous vehicles [2, 7, 11, 16, 26, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only rule-based planning in autonomous vehicles. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.50643348693848,
      "citing_paper_id": "266693892",
      "cited_paper_id": 15693605
    },
    {
      "context_text": "In addition to pure rule-based planning, previous research has explored hybrid approaches that combine rule-based decision-making with machine learning components [6–10, 12, 13, 15, 22, 23, 25, 32, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous research on hybrid approaches combining rule-based decision-making with machine learning.",
      "processing_time": 58.69069170951843,
      "citing_paper_id": "266693892",
      "cited_paper_id": 233324485
    },
    {
      "context_text": "In addition to pure rule-based planning, previous research has explored hybrid approaches that combine rule-based decision-making with machine learning components [6–10, 12, 13, 15, 22, 23, 25, 32, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous research on hybrid approaches combining rule-based decision-making with machine learning.",
      "processing_time": 58.69069170951843,
      "citing_paper_id": "266693892",
      "cited_paper_id": 257687420
    },
    {
      "context_text": "Instruct-GPT [20], a specialized version of OpenAI’s GPT models, is fine-tuned for superior adherence to user instructions, offering more precise and context-relevant responses across various applications like content creation and information retrieval.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Instruct-GPT) and its capabilities. No verifiable resources are identified.",
      "processing_time": 59.23759150505066,
      "citing_paper_id": "266693892",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "A recent approach [1], allows robots to understand # Task Overview You are the decision-making agent for a self-driving car in a simulated 2D top-down environment.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task overview for a robotic system. No verifiable resources are identified.",
      "processing_time": 58.68254518508911,
      "citing_paper_id": "266693892",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "The introduction of Large Language Models (LLMs) for complex reasoning tasks, as highlighted in works like those by Brohan et al. [3] and Ahn et al. [1], and their application in agentic control as demonstrated by Wang et al. [31], marks a significant advancement in this field.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss the capabilities and applications of LLMs in complex reasoning and robotic control.",
      "processing_time": 59.85394906997681,
      "citing_paper_id": "266693892",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "The introduction of Large Language Models (LLMs) for complex reasoning tasks, as highlighted in works like those by Brohan et al. [3] and Ahn et al. [1], and their application in agentic control as demonstrated by Wang et al. [31], marks a significant advancement in this field.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss the capabilities and applications of LLMs in complex reasoning and robotic control.",
      "processing_time": 59.85394906997681,
      "citing_paper_id": "266693892",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "The introduction of Large Language Models (LLMs) for complex reasoning tasks, as highlighted in works like those by Brohan et al. [3] and Ahn et al. [1], and their application in agentic control as demonstrated by Wang et al. [31], marks a significant advancement in this field.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that discuss the capabilities and applications of LLMs in complex reasoning and robotic control.",
      "processing_time": 59.85394906997681,
      "citing_paper_id": "266693892",
      "cited_paper_id": 260293142
    },
    {
      "context_text": "Another work [14], proposed the use of environment feedback to form an inner monologue, enhancing planning and interaction in robotics by integrating perception models and pre-trained skills for improved completion of complex, long-horizon tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving environment feedback and integration of perception models and pre-trained skills.",
      "processing_time": 58.08564591407776,
      "citing_paper_id": "266693892",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "“ReAct” [34] introduces a new paradigm that enhances the capabilities of large language models in complex tasks requiring both reasoning and decision-making, by prompting these models to generate verbal reasoning traces and actions in an interleaved manner, enabling dynamic reasoning and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or approach called 'ReAct'. There are no clear identifiers for datasets.",
      "processing_time": 59.00504422187805,
      "citing_paper_id": "266693892",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Similarly, it was shown in [24] that enhancing LLMs with physical grounding, allowing them to generate and update plans that are contextually relevant to the current environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for enhancing LLMs with physical grounding.",
      "processing_time": 58.07679104804993,
      "citing_paper_id": "266693892",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Large Language Models (LLMs) like GPT [4], its succes-sors GPT-3 and GPT4 [19], and its open-source counterparts Llama [27] and Llama2 [28] are a type of artificial intelligence that are designed to understand, generate, and manipulate human language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.213677167892456,
      "citing_paper_id": "266693892",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "Large Language Models (LLMs) like GPT [4], its succes-sors GPT-3 and GPT4 [19], and its open-source counterparts Llama [27] and Llama2 [28] are a type of artificial intelligence that are designed to understand, generate, and manipulate human language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.213677167892456,
      "citing_paper_id": "266693892",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Large Language Models (LLMs) like GPT [4], its succes-sors GPT-3 and GPT4 [19], and its open-source counterparts Llama [27] and Llama2 [28] are a type of artificial intelligence that are designed to understand, generate, and manipulate human language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.213677167892456,
      "citing_paper_id": "266693892",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "Current directions of research such as certified reasoning [21] and retrieval feedback [35] seek to mitigate this, but significant work remains, especially in high-risk domains like autonomous driving where accuracy is paramount and human lives are at stake.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only research directions and their applications. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 59.47123956680298,
      "citing_paper_id": "266693892",
      "cited_paper_id": 258841029
    },
    {
      "context_text": "Current directions of research such as certified reasoning [21] and retrieval feedback [35] seek to mitigate this, but significant work remains, especially in high-risk domains like autonomous driving where accuracy is paramount and human lives are at stake.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only research directions and their applications. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 59.47123956680298,
      "citing_paper_id": "266693892",
      "cited_paper_id": 259095869
    },
    {
      "context_text": "Additionally, we investigate the possibility of using an open-source LLM, Llama2-7B [28], as the LLM for LLM-A SSIST .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Llama2-7B). No verifiable datasets are referenced.",
      "processing_time": 58.99006795883179,
      "citing_paper_id": "266693892",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "Specifically, can gains in solving math problems transfer to other reasoning domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019),…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various reasoning domains and tasks. The cited paper title does not help in identifying a dataset.",
      "processing_time": 59.650853395462036,
      "citing_paper_id": "280146966",
      "cited_paper_id": 1553193
    },
    {
      "context_text": "…domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019), instruction following (Zhou et al., 2023)) that do not require extensive reasoning?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains and tasks. No verifiable resources are identified.",
      "processing_time": 58.29156517982483,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "…domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019), instruction following (Zhou et al., 2023)) that do not require extensive reasoning?",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains and tasks. No verifiable resources are identified.",
      "processing_time": 58.29156517982483,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "We collected the following datasets that are mainly composed of problems with factual answers, which means that they do not need a reasoning process to give the answer: • CoQA (Reddy et al., 2019): It has 127K questions in dialogues over passages, focusing on maintaining context and coreference across turns.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoQA"
      ],
      "dataset_descriptions": {
        "CoQA": "Used to evaluate conversational question answering, focusing on maintaining context and coreference across dialogue turns."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CoQA dataset, which is used for conversational question answering with a focus on context and coreference.",
      "processing_time": 64.41072726249695,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202541184
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "We used LLM-Harness (Gao et al., 2024) to evaluate the models’ performance on Olympiad-Bench, ACPBench, HeadQA, CoQA, HaluEval, MC-TACO and used Eval-Chemy (Raoof et al., 2025) MATH500, AIME24, AIME25, GPQA-Diamond, LiveCodeBench, IFEval.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Olympiad-Bench",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "HaluEval",
        "MC-TACO",
        "Eval-Chemy",
        "MATH500",
        "AIME24",
        "AIME25",
        "GPQA-Diamond",
        "LiveCodeBench",
        "IFEval"
      ],
      "dataset_descriptions": {
        "Olympiad-Bench": "Used to evaluate the problem-solving capabilities of LLMs on olympiad-level questions, focusing on complex reasoning and domain-specific knowledge.",
        "ACPBench": "Used to assess the ability of LLMs to generate and evaluate arguments, emphasizing logical consistency and persuasiveness.",
        "HeadQA": "Used to test the medical knowledge and reasoning abilities of LLMs through a series of clinical questions.",
        "CoQA": "Used to evaluate conversational question answering, focusing on the ability of LLMs to understand and respond to follow-up questions in a dialogue.",
        "HaluEval": "Used to measure the hallucination tendencies of LLMs, assessing their tendency to generate false information.",
        "MC-TACO": "Used to evaluate the ability of LLMs to generate coherent and contextually appropriate text in multi-document summarization tasks.",
        "Eval-Chemy": "Used to assess the mathematical reasoning and problem-solving skills of LLMs through a variety of math problems.",
        "MATH500": "Used to evaluate the mathematical reasoning and problem-solving skills of LLMs through a set of 500 math problems.",
        "AIME24": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 24 AIME-level problems.",
        "AIME25": "Used to assess the advanced mathematical reasoning and problem-solving skills of LLMs through a set of 25 AIME-level problems.",
        "GPQA-Diamond": "Used to evaluate the graduate-level reasoning and problem-solving skills of LLMs through a set of Google-proof questions.",
        "LiveCodeBench": "Used to assess the code generation and execution capabilities of LLMs, focusing on live coding tasks.",
        "IFEval": "Used to evaluate the instruction-following capabilities of LLMs, focusing on the ability to execute instructions accurately."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and evaluation frameworks, which are used to assess the performance of large language models on various tasks.",
      "processing_time": 145.63391304016113,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279154475
    },
    {
      "context_text": "For OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, and IFEval, we used greedy sampling and sampled only once.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks/challenges but does not specify them as datasets. However, ACPBench is mentioned in a paper that focuses on reasoning about action, change, and planning, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 70.67832922935486,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "For OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, and IFEval, we used greedy sampling and sampled only once.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks/challenges but does not specify them as datasets. However, ACPBench is mentioned in a paper that focuses on reasoning about action, change, and planning, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 70.67832922935486,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "For OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, and IFEval, we used greedy sampling and sampled only once.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks/challenges but does not specify them as datasets. However, ACPBench is mentioned in a paper that focuses on reasoning about action, change, and planning, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 70.67832922935486,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "For OlympiadBench, ACPBench, HeadQA, CoQA, HaluEval, and IFEval, we used greedy sampling and sampled only once.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning in large language models, focusing on the ability to understand and predict outcomes of actions and changes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks/challenges but does not specify them as datasets. However, ACPBench is mentioned in a paper that focuses on reasoning about action, change, and planning, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 70.67832922935486,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "…the following datasets that are mainly composed of problems with factual answers, which means that they do not need a reasoning process to give the answer: • CoQA (Reddy et al., 2019): It has 127K questions in dialogues over passages, focusing on maintaining context and coreference across turns.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoQA"
      ],
      "dataset_descriptions": {
        "CoQA": "Used to evaluate conversational question answering, focusing on maintaining context and coreference across dialogue turns."
      },
      "confidence_score": 1.0,
      "reasoning": "CoQA is identified as a dataset used for evaluating conversational question answering, specifically focusing on context and coreference resolution.",
      "processing_time": 64.21476221084595,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202541184
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267770504
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "AIME24",
        "AIME25",
        "OlympiadBench",
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA",
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical reasoning capabilities, focusing on solving complex mathematical problems.",
        "AIME24": "Used to assess advanced mathematical problem-solving skills, particularly in algebra and geometry.",
        "AIME25": "Used to assess advanced mathematical problem-solving skills, particularly in combinatorics and number theory.",
        "OlympiadBench": "Used to evaluate performance on international mathematics olympiad problems, testing deep mathematical reasoning.",
        "LiveCodeBench": "Used to evaluate code generation and execution capabilities, focusing on real-world programming tasks.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving in various domains, including medical and scientific reasoning.",
        "ACPBench": "Used to evaluate agent planning and decision-making in complex environments, focusing on language-based tasks.",
        "HeadQA": "Used to evaluate medical reasoning and question-answering, focusing on clinical scenarios and patient care.",
        "CoQA": "Used to evaluate conversational question-answering, focusing on multi-turn dialogues and contextual understanding.",
        "IFEval": "Used to evaluate instruction-following capabilities, focusing on the ability to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate the tendency of large language models to generate hallucinations, focusing on factual accuracy.",
        "MC-TACO": "Used to evaluate commonsense reasoning and story comprehension, focusing on narrative understanding and logical inference."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and datasets, but only those that are specific and verifiable are included. The others are excluded as they are either benchmarks or not clearly identified.",
      "processing_time": 128.28990983963013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "…general reasoning questions, such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "CoQA": "Used to evaluate conversational question answering, focusing on multi-turn dialogue and context understanding in language models.",
        "IFEval": "Used to assess factual and alignment tasks, specifically evaluating the accuracy and consistency of language model responses.",
        "HaluEval": "Used to evaluate factual and alignment tasks, focusing on the reliability and coherence of language model outputs.",
        "MC-TACO": "Used to evaluate multi-choice question answering, particularly in the context of complex reasoning and comprehension tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several datasets used for evaluating language models on various tasks, including language-based agent planning tasks, which aligns with the research topic on planning capabilities of LLMs.",
      "processing_time": 81.97643232345581,
      "citing_paper_id": "280146966",
      "cited_paper_id": 52055325
    },
    {
      "context_text": "…agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering and instruction-following.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks/challenges but does not refer to them as specific, downloadable datasets. These are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 60.28607130050659,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202541184
    },
    {
      "context_text": "• MC-TACO (Zhou et al., 2019): It is a multiple-choice benchmark designed to evaluate models’ temporal commonsense, covering duration, ordering, typical time, frequency, and stationarity.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "MC-TACO": "Used to evaluate models' temporal commonsense, focusing on aspects such as duration, ordering, typical time, frequency, and stationarity in planning scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "MC-TACO is a specific benchmark dataset designed to evaluate temporal commonsense in models, which aligns with the research topic of planning capabilities in LLMs.",
      "processing_time": 67.78623700141907,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202541184
    },
    {
      "context_text": "…however, guides models by rewarding accurate and logically coherent reasoning steps without explicit step-by-step supervision, allowing exploration and optimization of reasoning pathways through feedback loops (Chu et al., 2025; Liu et al., 2025; Wang et al., 2024c; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methods and approaches for guiding and optimizing reasoning in language models.",
      "processing_time": 58.6251859664917,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "…however, guides models by rewarding accurate and logically coherent reasoning steps without explicit step-by-step supervision, allowing exploration and optimization of reasoning pathways through feedback loops (Chu et al., 2025; Liu et al., 2025; Wang et al., 2024c; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methods and approaches for guiding and optimizing reasoning in language models.",
      "processing_time": 58.6251859664917,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267412590
    },
    {
      "context_text": "…however, guides models by rewarding accurate and logically coherent reasoning steps without explicit step-by-step supervision, allowing exploration and optimization of reasoning pathways through feedback loops (Chu et al., 2025; Liu et al., 2025; Wang et al., 2024c; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methods and approaches for guiding and optimizing reasoning in language models.",
      "processing_time": 58.6251859664917,
      "citing_paper_id": "280146966",
      "cited_paper_id": 275932560
    },
    {
      "context_text": "…however, guides models by rewarding accurate and logically coherent reasoning steps without explicit step-by-step supervision, allowing exploration and optimization of reasoning pathways through feedback loops (Chu et al., 2025; Liu et al., 2025; Wang et al., 2024c; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses methods and approaches for guiding and optimizing reasoning in language models.",
      "processing_time": 58.6251859664917,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279447674
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267412590
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 275932560
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 276116814
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278911988
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279447674
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context focuses on methods and findings rather than reusable datasets.",
      "processing_time": 59.782777309417725,
      "citing_paper_id": "280146966",
      "cited_paper_id": null
    },
    {
      "context_text": "Such RL-enhanced fine-tuning has achieved state-of-the-art results on benchmarks and competitive programming challenges (Hendrycks et al., 2021b; Lambert et al., 2025; Team, 2025a; Team et al., 2025).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'benchmarks and competitive programming challenges' but does not specify any particular datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.5342583656311,
      "citing_paper_id": "280146966",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "Such RL-enhanced fine-tuning has achieved state-of-the-art results on benchmarks and competitive programming challenges (Hendrycks et al., 2021b; Lambert et al., 2025; Team, 2025a; Team et al., 2025).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'benchmarks and competitive programming challenges' but does not specify any particular datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.5342583656311,
      "citing_paper_id": "280146966",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Such RL-enhanced fine-tuning has achieved state-of-the-art results on benchmarks and competitive programming challenges (Hendrycks et al., 2021b; Lambert et al., 2025; Team, 2025a; Team et al., 2025).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'benchmarks and competitive programming challenges' but does not specify any particular datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.5342583656311,
      "citing_paper_id": "280146966",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "LiveCodeBench"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical problem-solving capabilities, focusing on complex algebraic and geometric problems in large language models.",
        "LiveCodeBench": "Applied to assess code generation and execution skills, specifically evaluating the ability to solve programming challenges in a live coding environment."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks, but only 'MATH500' and 'LiveCodeBench' are specific, verifiable datasets. 'AIME24', 'AIME25', and 'OlympiadBench' are not clearly identified as datasets.",
      "processing_time": 77.41200399398804,
      "citing_paper_id": "280146966",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "Namely, we select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "LiveCodeBench"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical problem-solving capabilities, focusing on complex algebraic and geometric problems in large language models.",
        "LiveCodeBench": "Applied to assess code generation and execution skills, specifically evaluating the ability to solve programming challenges in a live coding environment."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks, but only 'MATH500' and 'LiveCodeBench' are specific, verifiable datasets. 'AIME24', 'AIME25', and 'OlympiadBench' are not clearly identified as datasets.",
      "processing_time": 77.41200399398804,
      "citing_paper_id": "280146966",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "…that are composed of mathematical problems, which means that they typically need a mathematical reasoning process to get the answer: • MATH500 (Hendrycks et al., 2021b): A curated subset of 500 problems sampled from the broader MATH dataset, covering topics like algebra, combinatorics,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical problem-solving capabilities, focusing on algebra and combinatorics problems. The dataset is a curated subset of 500 problems from the broader MATH dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MATH500', a specific subset of the MATH dataset, which is used for evaluating mathematical problem-solving capabilities.",
      "processing_time": 68.6464011669159,
      "citing_paper_id": "280146966",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "…that are composed of mathematical problems, which means that they typically need a mathematical reasoning process to get the answer: • MATH500 (Hendrycks et al., 2021b): A curated subset of 500 problems sampled from the broader MATH dataset, covering topics like algebra, combinatorics,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate mathematical problem-solving capabilities, focusing on algebra and combinatorics problems. The dataset is a curated subset of 500 problems from the broader MATH dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MATH500', a specific subset of the MATH dataset, which is used for evaluating mathematical problem-solving capabilities.",
      "processing_time": 68.6464011669159,
      "citing_paper_id": "280146966",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Indeed, o1 and similar reasoning models are built on strong general-purpose bases to retain broad knowledge (Hendrycks et al., 2021a; OpenAI, 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. The context is about the general knowledge and reasoning abilities of models.",
      "processing_time": 59.78323841094971,
      "citing_paper_id": "280146966",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
      "processing_time": 67.914790391922,
      "citing_paper_id": "280146966",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
      "processing_time": 67.914790391922,
      "citing_paper_id": "280146966",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Over the past years, the community has raced to push large language models (LLMs) to new heights on math-centric reasoning benchmarks such as MATH (Hendrycks et al., 2021b) and",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate large language models on mathematical problem-solving, focusing on the ability to reason through complex equations and problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MATH' as a benchmark for evaluating LLMs on mathematical problem-solving. The cited paper 'Measuring Mathematical Problem Solving With the MATH Dataset' confirms that MATH is a dataset.",
      "processing_time": 67.914790391922,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270380229
    },
    {
      "context_text": "The chain-of-thought prompting strategy introduced by Wei et al. (2022) encourages models to produce step-by-step explanations, significantly boosting performance in symbolic reasoning tasks (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2022; Yu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting) and its impact on model performance.",
      "processing_time": 59.58306574821472,
      "citing_paper_id": "280146966",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "The chain-of-thought prompting strategy introduced by Wei et al. (2022) encourages models to produce step-by-step explanations, significantly boosting performance in symbolic reasoning tasks (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2022; Yu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting) and its impact on model performance.",
      "processing_time": 59.58306574821472,
      "citing_paper_id": "280146966",
      "cited_paper_id": 256415991
    },
    {
      "context_text": "The chain-of-thought prompting strategy introduced by Wei et al. (2022) encourages models to produce step-by-step explanations, significantly boosting performance in symbolic reasoning tasks (Lambert et al., 2025; Longpre et al., 2023; Wei et al., 2022; Yu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting) and its impact on model performance.",
      "processing_time": 59.58306574821472,
      "citing_paper_id": "280146966",
      "cited_paper_id": 262084051
    },
    {
      "context_text": "SFT methods predominantly utilize annotated reasoning trajectories or solution traces, directly training models to replicate explicit reasoning sequences from datasets (Wang et al., 2023; Wei et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. It focuses on the use of annotated reasoning trajectories or solution traces, which are not specific datasets.",
      "processing_time": 61.38041639328003,
      "citing_paper_id": "280146966",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "SFT methods predominantly utilize annotated reasoning trajectories or solution traces, directly training models to replicate explicit reasoning sequences from datasets (Wang et al., 2023; Wei et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. It focuses on the use of annotated reasoning trajectories or solution traces, which are not specific datasets.",
      "processing_time": 61.38041639328003,
      "citing_paper_id": "280146966",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Interestingly, models heavily fine-tuned for formal reasoning sometimes falter on more general language tasks (Kumar et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about model performance.",
      "processing_time": 57.54924178123474,
      "citing_paper_id": "280146966",
      "cited_paper_id": 247011290
    },
    {
      "context_text": "This clarity has made math a popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that focus on improving mathematical reasoning in LLMs.",
      "processing_time": 59.57180738449097,
      "citing_paper_id": "280146966",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "This clarity has made math a popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that focus on improving mathematical reasoning in LLMs.",
      "processing_time": 59.57180738449097,
      "citing_paper_id": "280146966",
      "cited_paper_id": 261030818
    },
    {
      "context_text": "This clarity has made math a popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that focus on improving mathematical reasoning in LLMs.",
      "processing_time": 59.57180738449097,
      "citing_paper_id": "280146966",
      "cited_paper_id": 261696697
    },
    {
      "context_text": "This clarity has made math a popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that focus on improving mathematical reasoning in LLMs.",
      "processing_time": 59.57180738449097,
      "citing_paper_id": "280146966",
      "cited_paper_id": 266209760
    },
    {
      "context_text": "This clarity has made math a popular proxy task of LLM reasoning, and researchers have developed increasingly sophisticated training recipes to maximize model performance on mathematical reasoning (Luo et al., 2023; Shao et al., 2024; Wang et al., 2024a; Wei et al., 2023; Yue et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that focus on improving mathematical reasoning in LLMs.",
      "processing_time": 59.57180738449097,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "Mathematics is often considered the foundational language of science, and enabling machines to reason precisely over math is central to the long-term vision of automated scientific discovery (Mishra et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the importance of mathematical reasoning in automated scientific discovery.",
      "processing_time": 59.12047624588013,
      "citing_paper_id": "280146966",
      "cited_paper_id": 253237047
    },
    {
      "context_text": "• HaluEval (Li et al., 2023): It contains human-annotated samples where models must distinguish factual content from hallucinations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HaluEval"
      ],
      "dataset_descriptions": {
        "HaluEval": "Used to evaluate the ability of large language models to distinguish factual content from hallucinations, providing human-annotated samples for benchmarking."
      },
      "confidence_score": 1.0,
      "reasoning": "HaluEval is a specific, verifiable dataset used for evaluating hallucinations in large language models. The context clearly indicates its use as a benchmark for distinguishing factual content from hallucinations.",
      "processing_time": 67.88638162612915,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "…code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as commonsense question answering…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "CoQA": "Used for evaluating conversational question answering, focusing on multi-turn dialogue and contextual understanding.",
        "IFEval": "Used for assessing factual consistency and alignment in language models, particularly in information extraction tasks.",
        "HaluEval": "Used to evaluate hallucination in large language models, focusing on the accuracy and reliability of generated content.",
        "MC-TACO": "Used for evaluating commonsense reasoning, focusing on multi-choice questions that require understanding of everyday scenarios."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several datasets but does not specify their usage in the current research. However, they are listed as part of non-reasoning tasks, suggesting they are used for evaluation.",
      "processing_time": 81.89943480491638,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "For HaluEval, the performance is the accuracy averaged on 3 tasks with zero-shot.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'HaluEval' but does not refer to it as a dataset. It is described as a benchmark, which is excluded unless it refers to a specific, downloadable dataset.",
      "processing_time": 62.14215302467346,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258832847
    },
    {
      "context_text": "…of LLMs. Recent advancements in large language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing advancements in LLMs and specialized fine-tuning methods.",
      "processing_time": 60.003801584243774,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258833200
    },
    {
      "context_text": "…other reasoning domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019), instruction following (Zhou et al., 2023)) that do not require…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists various domains and tasks where LLMs have been applied.",
      "processing_time": 60.20466470718384,
      "citing_paper_id": "280146966",
      "cited_paper_id": 258967391
    },
    {
      "context_text": "…fine-tuned using reinforcement learning (RL) (Su et al., 2025; Yeo et al., 2025) exhibit much stronger generalization to non-math tasks than those trained with supervised fine-tuning (SFT) (Yue et al., 2024a,b), which often show signs of catastrophic forgetting over a wide range of non-math tasks.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings. The context focuses on comparing reinforcement learning and supervised fine-tuning for training models.",
      "processing_time": 60.73120045661926,
      "citing_paper_id": "280146966",
      "cited_paper_id": 261696697
    },
    {
      "context_text": "…in solving math problems transfer to other reasoning domains (e.g., scientific QA (Welbl et al., 2017), coding (Jain et al., 2025), agent planning (Xie et al., 2024), logical deduction (Dziri et al., 2024)) and to tasks (e.g., conversational QA (Reddy et al., 2019), instruction following (Zhou et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references various reasoning domains and tasks where the capabilities of language models are applied.",
      "processing_time": 60.727495431900024,
      "citing_paper_id": "280146966",
      "cited_paper_id": 264172893
    },
    {
      "context_text": "And for IFEval, we used strict instruction accuracy as the score.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a scoring method for IFEval. No verifiable resources are identified.",
      "processing_time": 59.34810447692871,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "…such as medical reasoning, code generation, and language-based agent planning tasks; (3) non-reasoning tasks : CoQA (Reddy et al., 2019), IFEval (Zhou et al., 2023), HaluEval (Li et al., 2023), MC-TACO (Zhou et al., 2019), which contain factual, alignment, or conversational problems such as…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoQA",
        "IFEval",
        "HaluEval",
        "MC-TACO"
      ],
      "dataset_descriptions": {
        "CoQA": "Used to evaluate conversational question answering, focusing on multi-turn dialogue and contextual understanding in large language models.",
        "IFEval": "Used to assess instruction-following capabilities, specifically evaluating the ability of models to execute complex instructions accurately.",
        "HaluEval": "Used to evaluate hallucination in language models, focusing on the generation of factually incorrect statements during task execution.",
        "MC-TACO": "Used to evaluate multi-choice question answering, focusing on the ability of models to select correct answers from multiple options in various contexts."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used for evaluating large language models, particularly in non-reasoning tasks. These datasets are explicitly named and are relevant to the topic of planning capabilities.",
      "processing_time": 84.5480043888092,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "• IFEval (Zhou et al., 2023): It contains over 500 prompts, each embedding verifiable instructions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IFEval"
      ],
      "dataset_descriptions": {
        "IFEval": "Used to evaluate the instruction-following capabilities of large language models, focusing on the accuracy and reliability of responses to verifiable instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "IFEval is a specific dataset containing prompts with verifiable instructions, which is directly relevant to evaluating the planning capabilities of LLMs.",
      "processing_time": 66.12765622138977,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "For instance, UniReason- Qwen3-14B-SFT-no-think demonstrates KL divergences of 0.372 and 0.283 on MATH-500 and IFEval respectively compared to the backbone model, whereas UniReason-Qwen3-14B(RL) achieves considerably lower KL divergences of only 0.084 and 0.019 on the corresponding tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH-500",
        "IFEval"
      ],
      "dataset_descriptions": {
        "MATH-500": "Used to evaluate the performance of UniReason-Qwen3-14B-SFT-no-think and UniReason-Qwen3-14B(RL) models, focusing on KL divergence in mathematical reasoning tasks.",
        "IFEval": "Used to assess the instruction-following capabilities of UniReason-Qwen3-14B-SFT-no-think and UniReason-Qwen3-14B(RL) models, measuring KL divergence in instruction-following tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, MATH-500 and IFEval, which are used to evaluate the performance of different models in terms of KL divergence.",
      "processing_time": 81.2589476108551,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265157752
    },
    {
      "context_text": "• GPQA-Diamond (Rein et al., 2024): It is a graduate-level question-answering dataset that contains multiple-choice questions in biology, physics, and chemistry.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GPQA-Diamond"
      ],
      "dataset_descriptions": {
        "GPQA-Diamond": "Used to evaluate graduate-level question-answering systems, focusing on multiple-choice questions in biology, physics, and chemistry."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions a specific dataset, GPQA-Diamond, which is used for evaluating question-answering systems at a graduate level.",
      "processing_time": 66.79650330543518,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "On MATH500, AIME24, AIME25, GPQA-Diamond, and LiveCodeBench, we used 0.6 as temperature, and 0.95 as top-p value.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. They are likely part of a suite used for evaluation, which is excluded according to the instructions.",
      "processing_time": 61.315866231918335,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "…(He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LiveCodeBench",
        "GPQA-Diamond",
        "ACPBench",
        "HeadQA"
      ],
      "dataset_descriptions": {
        "LiveCodeBench": "Used to evaluate code generation and reasoning capabilities, focusing on live coding tasks and problem-solving skills.",
        "GPQA-Diamond": "Used to assess graduate-level reasoning and problem-solving skills, containing complex questions that require deep understanding and critical thinking.",
        "ACPBench": "Used to evaluate advanced computational problem-solving abilities, focusing on algorithmic and logical reasoning tasks.",
        "HeadQA": "Used to assess medical reasoning and knowledge, containing questions that require understanding of medical concepts and clinical scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several datasets that contain reasoning tasks, including mathematical problems and general reasoning questions. These datasets are likely used to evaluate the performance of models on various reasoning tasks.",
      "processing_time": 83.66230058670044,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "For GPQA-Diamond, LiveCodeBench and MATH 500, our score is the average accuracy over 3 samples.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions GPQA-Diamond, LiveCodeBench, and MATH 500, but these are likely benchmarks or challenges rather than specific, reusable datasets. No clear dataset names are provided.",
      "processing_time": 62.78145432472229,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "The rank shift is calculated as the difference in token rankings between the fine-tuned model and the backbone model for each token (Li et al., 2025c; Lin et al., 2023).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating rank shift between models.",
      "processing_time": 58.149051666259766,
      "citing_paper_id": "280146966",
      "cited_paper_id": 265608902
    },
    {
      "context_text": "The rank shift is calculated as the difference in token rankings between the fine-tuned model and the backbone model for each token (Li et al., 2025c; Lin et al., 2023).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating rank shift between models.",
      "processing_time": 58.149051666259766,
      "citing_paper_id": "280146966",
      "cited_paper_id": 276422318
    },
    {
      "context_text": "Wang et al. (2024b) found that fine-tuning on a narrow set of instruction types can degrade a model’s performance on other skills.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about fine-tuning methods.",
      "processing_time": 58.144853353500366,
      "citing_paper_id": "280146966",
      "cited_paper_id": 266359670
    },
    {
      "context_text": "For RL, we apply a standard GRPO (Shao et al., 2024) setup using answer correctness as the reward.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GRPO) and a reward mechanism (answer correctness).",
      "processing_time": 59.68772578239441,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "Xu et al. (2024) proposes a quantitative framework for assessing ideas that leverages hidden representations from LLMs to predict the merit of scientific ideas.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for assessing ideas using LLMs. No verifiable resources are identified.",
      "processing_time": 60.4159893989563,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "Xu et al. (2024) proposes a quantitative framework for assessing ideas that leverages hidden representations from LLMs to predict the merit of scientific ideas.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for assessing ideas using LLMs. No verifiable resources are identified.",
      "processing_time": 60.4159893989563,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270380229
    },
    {
      "context_text": "• OlympiadBench (He et al., 2024): Problems sourced from international olympiads (e.g., IMO and regional contests).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OlympiadBench"
      ],
      "dataset_descriptions": {
        "OlympiadBench": "Used to evaluate models on challenging scientific problems from international olympiads, focusing on bilingual multimodal capabilities and promoting AGI development."
      },
      "confidence_score": 1.0,
      "reasoning": "OlympiadBench is a specific, verifiable dataset used for evaluating models on challenging scientific problems from international olympiads.",
      "processing_time": 67.35361933708191,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267770504
    },
    {
      "context_text": "…select benchmarks from the following three categories: (1) math reasoning tasks : MATH500 (Hendrycks et al., 2021b), AIME24, AIME25, OlympiadBench (He et al., 2024), which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al.,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH500",
        "OlympiadBench"
      ],
      "dataset_descriptions": {
        "MATH500": "Used to evaluate math reasoning capabilities, focusing on solving complex mathematical problems in the context of large language models.",
        "OlympiadBench": "Used to assess advanced reasoning skills, particularly in solving olympiad-level bilingual multimodal scientific problems, promoting AGI development."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks, but only 'MATH500' and 'OlympiadBench' are specific, verifiable datasets. 'AIME24', 'AIME25', 'LiveCodeBench', and 'GPQA-Diamond' are excluded as they are either task names or benchmarks without clear dataset identifiers.",
      "processing_time": 80.6513204574585,
      "citing_paper_id": "280146966",
      "cited_paper_id": 267770504
    },
    {
      "context_text": "Fine-tuning for reasoning models not only boosts task performance but also alters the model’s internal representations (Sheng et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about fine-tuning effects on model representations.",
      "processing_time": 59.285714864730835,
      "citing_paper_id": "280146966",
      "cited_paper_id": 269457450
    },
    {
      "context_text": "Follow-up research introduced reinforcement fine-tuning precisely to address this gap, aiming to adapt a generalist model’s reasoning to new domains with limited data (Zhang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting models. No verifiable resources are identified.",
      "processing_time": 59.476561069488525,
      "citing_paper_id": "280146966",
      "cited_paper_id": 269457450
    },
    {
      "context_text": "Reasoning Fine-Tuning of LLMs. Recent advancements in large language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing advancements in fine-tuning methods for enhancing reasoning capabilities in LLMs.",
      "processing_time": 61.27391052246094,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270380229
    },
    {
      "context_text": "Recent studies have begun to probe how CoT-based fine-tuning changes the latent space of LLMs (Wang et al., 2025; Xu et al., 2025a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No verifiable resources are identified.",
      "processing_time": 60.135464906692505,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270380229
    },
    {
      "context_text": "Recent studies have begun to probe how CoT-based fine-tuning changes the latent space of LLMs (Wang et al., 2025; Xu et al., 2025a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No verifiable resources are identified.",
      "processing_time": 60.135464906692505,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278788524
    },
    {
      "context_text": "Supervised Fine-Tuning vs. Reinforcement Learning for LLMs. Fine-tuning methods for reasoning typically fall into two major categories: supervised fine-tuning and reinforcement learning (Chen et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for training LLMs. No verifiable resources are identified.",
      "processing_time": 60.64354729652405,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270380229
    },
    {
      "context_text": "Supervised Fine-Tuning vs. Reinforcement Learning for LLMs. Fine-tuning methods for reasoning typically fall into two major categories: supervised fine-tuning and reinforcement learning (Chen et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for training LLMs. No verifiable resources are identified.",
      "processing_time": 60.64354729652405,
      "citing_paper_id": "280146966",
      "cited_paper_id": 270560678
    },
    {
      "context_text": "…which contain mathematical problems only; (2) other reasoning tasks : LiveCodeBench (Jain et al., 2025), GPQA-Diamond (Rein et al., 2024), ACPBench (Kokel et al., 2025), HeadQA (Vilares and Gómez-Rodríguez, 2019), which contain more general reasoning questions, such as medical reasoning, code…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning, focusing on the ability of models to handle complex reasoning tasks involving sequences of actions and their effects."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions several benchmarks, but they are primarily used for score comparison rather than as specific, downloadable datasets. However, 'ACPBench' is highlighted as containing reasoning tasks related to action, change, and planning, which aligns with the research topic.",
      "processing_time": 72.7703025341034,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "For ACPBench, we only used multiple choices, and averaged the score for all 10 tasks as the final score.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ACPBench but does not refer to it as a dataset. It is described as a benchmark or challenge, which is excluded unless it refers to a specific, downloadable dataset.",
      "processing_time": 63.22210073471069,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "• ACPBench (Kokel et al., 2025): It has 7 atomic reasoning tasks around 13 classical planning domains.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning tasks in classical planning domains, focusing on action, change, and planning with 7 atomic tasks across 13 domains."
      },
      "confidence_score": 1.0,
      "reasoning": "ACPBench is mentioned as a benchmark with specific tasks and domains, which fits the criteria for a dataset.",
      "processing_time": 66.73661398887634,
      "citing_paper_id": "280146966",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "Lobo et al. (2024) find that task-specific fine-tuning can reduce the faith-fulness of a model’s generated reasoning chains, indicating shifts in its underlying inference mechanisms.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the impact of fine-tuning on model reasoning but does not mention any specific datasets.",
      "processing_time": 59.44017267227173,
      "citing_paper_id": "280146966",
      "cited_paper_id": 274234789
    },
    {
      "context_text": "Subsequent extensions, such as DeepSeek-R1 (Team, 2025a), have integrated reinforcement learning approaches alongside CoT, optimizing models through reward-driven policy improvements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the integration of reinforcement learning and CoT in LLMs.",
      "processing_time": 61.564032793045044,
      "citing_paper_id": "280146966",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "A steady stream of reasoning-tuned models (Guha et al., 2025; Muennighoff et al., 2025) now advances the state of the art on math benchmarks nearly every week, with some even surpassing the average performance of human experts (OpenAI, 2024; Team, 2025a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in reasoning-tuned models and their performance on math benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 62.02190685272217,
      "citing_paper_id": "280146966",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "A steady stream of reasoning-tuned models (Guha et al., 2025; Muennighoff et al., 2025) now advances the state of the art on math benchmarks nearly every week, with some even surpassing the average performance of human experts (OpenAI, 2024; Team, 2025a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in reasoning-tuned models and their performance on math benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 62.02190685272217,
      "citing_paper_id": "280146966",
      "cited_paper_id": 276079693
    },
    {
      "context_text": "A steady stream of reasoning-tuned models (Guha et al., 2025; Muennighoff et al., 2025) now advances the state of the art on math benchmarks nearly every week, with some even surpassing the average performance of human experts (OpenAI, 2024; Team, 2025a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in reasoning-tuned models and their performance on math benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 62.02190685272217,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279154475
    },
    {
      "context_text": "Across families and sizes, models fine-tuned using reinforcement learning (RL) (Su et al., 2025; Yeo et al., 2025) exhibit much stronger generalization to non-math tasks than those trained with supervised fine-tuning (SFT) (Yue et al., 2024a,b), which often show signs of catastrophic forgetting…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to reinforcement learning and supervised fine-tuning.",
      "processing_time": 60.09386420249939,
      "citing_paper_id": "280146966",
      "cited_paper_id": 276116814
    },
    {
      "context_text": "Across families and sizes, models fine-tuned using reinforcement learning (RL) (Su et al., 2025; Yeo et al., 2025) exhibit much stronger generalization to non-math tasks than those trained with supervised fine-tuning (SFT) (Yue et al., 2024a,b), which often show signs of catastrophic forgetting…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to reinforcement learning and supervised fine-tuning.",
      "processing_time": 60.09386420249939,
      "citing_paper_id": "280146966",
      "cited_paper_id": 277451641
    },
    {
      "context_text": "…language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al., 2025; Ziegler et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing specialized fine-tuning methods for enhancing reasoning capabilities in language models.",
      "processing_time": 61.23383617401123,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278715317
    },
    {
      "context_text": "To further validate our observation, we apply our controlled study pipeline also for General-Reasoner (Ma et al., 2025), an RL-tuned reasoning model that are also math experts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model called 'General-Reasoner'. No verifiable datasets are referenced.",
      "processing_time": 60.93938851356506,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "…explore the effect of training data distribution for SFT-based reasoning models, we also distill a larger and more comprehensive dataset collected from General-Reasoner (Ma et al., 2025), which contains 232K examples across reasoning and non-reasoning tasks (e.g., Math, Chemistry, Business).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "General-Reasoner"
      ],
      "dataset_descriptions": {
        "General-Reasoner": "Used to explore the effect of training data distribution on SFT-based reasoning models, containing 232K examples across various reasoning and non-reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset 'General-Reasoner' with a clear size and content description, which is relevant to the research topic of planning capabilities of LLMs.",
      "processing_time": 70.24383997917175,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "To further explore the effect of training data distribution for SFT-based reasoning models, we also distill a larger and more comprehensive dataset collected from General-Reasoner (Ma et al., 2025), which contains 232K examples across reasoning and non-reasoning tasks (e.g., Math, Chemistry, Business).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "General-Reasoner"
      ],
      "dataset_descriptions": {
        "General-Reasoner": "Used to explore the effect of training data distribution for SFT-based reasoning models, containing 232K examples across various reasoning and non-reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset 'General-Reasoner' with a clear size and scope, which is used to explore the effect of training data distribution for SFT-based reasoning models.",
      "processing_time": 70.65627026557922,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "This additional distilled set is used to train the General-Reasoner model using supervised fine-tuning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a 'distilled set' which is too generic and lacks a clear identifier.",
      "processing_time": 62.36316275596619,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "The majority of user-facing applications, question answering, dialogue, instruction following, require broader linguistic and commonsense competence that math alone does not test (Ma et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general requirements for linguistic and commonsense competence in LLMs.",
      "processing_time": 60.59272742271423,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "Then we finetune the Qwen3-14B base model using the distilled dataset and name the model as General-Reasoner-Qwen3-14B(SFT) to directly compare with the RL-based General-Reasoner for a fairer and more comprehensive controlled study towards SFT and RL.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'distilled dataset' but does not provide a specific name or identifier. The term 'distilled dataset' is too generic and lacks the necessary specificity to be considered a verifiable resource.",
      "processing_time": 63.87067699432373,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278768680
    },
    {
      "context_text": "Recent studies (Xu et al., 2025a; Zheng et al., 2025) demonstrate that PCA shift analysis provides a sensitive and interpretable measure of representational changes relevant to task performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PCA shift analysis). The context focuses on the use of PCA shift analysis as a measure of representational changes in task performance.",
      "processing_time": 63.34750533103943,
      "citing_paper_id": "280146966",
      "cited_paper_id": 278788524
    },
    {
      "context_text": "Recent studies (Xu et al., 2025a; Zheng et al., 2025) demonstrate that PCA shift analysis provides a sensitive and interpretable measure of representational changes relevant to task performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PCA shift analysis). The context focuses on the use of PCA shift analysis as a measure of representational changes in task performance.",
      "processing_time": 63.34750533103943,
      "citing_paper_id": "280146966",
      "cited_paper_id": 280000246
    },
    {
      "context_text": "…advancements in large language models have notably emphasized specialized fine-tuning methods to enhance reasoning capabilities (Chen et al., 2023; Feng et al., 2025; Li et al., 2025a,b; Liu et al., 2025; Wang et al., 2024c; Wong et al., 2025; Xu et al., 2025b; Yang et al., 2025; Yeo et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing advancements in large language models and specialized fine-tuning methods.",
      "processing_time": 61.52107906341553,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279070806
    },
    {
      "context_text": "Recent works have also stepped into analyzing the cross-domain performance of reasoning models (Sun et al., 2025), especially for RL-based approaches (Cheng et al., 2025; Hu et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works and their focus on cross-domain performance of reasoning models, particularly RL-based approaches.",
      "processing_time": 62.347960233688354,
      "citing_paper_id": "280146966",
      "cited_paper_id": 279447674
    },
    {
      "context_text": "Recent works have also stepped into analyzing the cross-domain performance of reasoning models (Sun et al., 2025), especially for RL-based approaches (Cheng et al., 2025; Hu et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works and their focus on cross-domain performance of reasoning models, particularly RL-based approaches.",
      "processing_time": 62.347960233688354,
      "citing_paper_id": "280146966",
      "cited_paper_id": 280000246
    },
    {
      "context_text": "Recent works have also stepped into analyzing the cross-domain performance of reasoning models (Sun et al., 2025), especially for RL-based approaches (Cheng et al., 2025; Hu et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works and their focus on cross-domain performance of reasoning models, particularly RL-based approaches.",
      "processing_time": 62.347960233688354,
      "citing_paper_id": "280146966",
      "cited_paper_id": null
    },
    {
      "context_text": "The appealing performance on math reasoning is understandable: problems are well-posed, solutions are unambiguous, and evaluation is easily verifiable, often just a single number or expression (Luo et al., 2025).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general characteristics of math reasoning problems.",
      "processing_time": 59.58597755432129,
      "citing_paper_id": "280146966",
      "cited_paper_id": null
    },
    {
      "context_text": "We fine-tune Qwen3-14B (Team, 2025b) on the high-quality math dataset derived from MATH and DeepScaler (Luo et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH",
        "DeepScaler"
      ],
      "dataset_descriptions": {
        "MATH": "Used to fine-tune Qwen3-14B, focusing on improving mathematical reasoning capabilities through high-quality math problems.",
        "DeepScaler": "Used to fine-tune Qwen3-14B, enhancing the model's ability to handle complex mathematical scaling tasks."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'high-quality math dataset derived from MATH and DeepScaler'. MATH is a known dataset, and DeepScaler appears to be a specific resource. However, 'high-quality math dataset' is too generic.",
      "processing_time": 77.08148455619812,
      "citing_paper_id": "280146966",
      "cited_paper_id": null
    },
    {
      "context_text": "To automate the generation of BT-based task plans in the context of sequential robot manipulation, progress has been made by using (i) symbolic planning [4], (ii) learning from demonstration [5], and (iii) reinforcement learning [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper title 'Learning Behavior Trees From Demonstration' does not introduce a dataset.",
      "processing_time": 62.65977430343628,
      "citing_paper_id": "272690069",
      "cited_paper_id": 199541993
    },
    {
      "context_text": "However, as shown in [16], large-scale applications of classical planning are still limited due to the exponential increase of complexity in the state space.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of classical planning methods.",
      "processing_time": 60.31288385391235,
      "citing_paper_id": "272690069",
      "cited_paper_id": 210836616
    },
    {
      "context_text": "It generates action sequences for small-scale, well-defined problems, leveraging search algorithms to reach the goal state from the initial state under the closed-world assumption [14]–[16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing production planning and search algorithms.",
      "processing_time": 60.76034903526306,
      "citing_paper_id": "272690069",
      "cited_paper_id": 210836616
    },
    {
      "context_text": "Benefiting from its features like the state-free tree structure [20], flexible modification [2], and good interpretability [21], it has become a potential task plan representation in the robotics field.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to features and benefits of behavior trees in robotics and AI.",
      "processing_time": 61.17431855201721,
      "citing_paper_id": "272690069",
      "cited_paper_id": 218595872
    },
    {
      "context_text": "Due to its limitation in scalability [2], behavior trees (BTs), which represent policies in a state-implicit, hierarchical tree structure, have gained increasing popularity for complex task planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (behavior trees) and its application in complex task planning.",
      "processing_time": 61.49676561355591,
      "citing_paper_id": "272690069",
      "cited_paper_id": 218595872
    },
    {
      "context_text": "Other research focuses on generating valid BTs by applying genetic programming [26], [27], grammatical programming [28], [29], and value function-based policies [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 61.6920211315155,
      "citing_paper_id": "272690069",
      "cited_paper_id": 233322292
    },
    {
      "context_text": "Since the work of [4], BTs have been increasingly applied in the robot planning field with respect to condition dependency [22], success belief [23], reactiveness [24], and logical formal check [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to behavior trees (BTs) and their applications in robot planning. No verifiable resources are identified.",
      "processing_time": 63.45636820793152,
      "citing_paper_id": "272690069",
      "cited_paper_id": 245023520
    },
    {
      "context_text": "Since the work of [4], BTs have been increasingly applied in the robot planning field with respect to condition dependency [22], success belief [23], reactiveness [24], and logical formal check [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to behavior trees (BTs) and their applications in robot planning. No verifiable resources are identified.",
      "processing_time": 63.45636820793152,
      "citing_paper_id": "272690069",
      "cited_paper_id": 257512189
    },
    {
      "context_text": "Despite being more efficient to program, maintain, and modify than Finite State Machines [3], manually programming BTs still requires significant effort.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between Behavior Trees and Finite State Machines. No verifiable resources are identified.",
      "processing_time": 62.47942090034485,
      "citing_paper_id": "272690069",
      "cited_paper_id": 252284068
    },
    {
      "context_text": "In this step, the necessary task knowledge is provided in natural language similar to the way in [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for providing task knowledge.",
      "processing_time": 60.992382526397705,
      "citing_paper_id": "272690069",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "Some research tries to utilize LLMs in generating complete problem descriptions [13], [31]–[34] and tracking world states [7], [35], [36] to support the planning process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning and problem-solving contexts.",
      "processing_time": 62.13752269744873,
      "citing_paper_id": "272690069",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "Some research tries to utilize LLMs in generating complete problem descriptions [13], [31]–[34] and tracking world states [7], [35], [36] to support the planning process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning and problem-solving contexts.",
      "processing_time": 62.13752269744873,
      "citing_paper_id": "272690069",
      "cited_paper_id": 269005338
    },
    {
      "context_text": "As mentioned in [13] and [4], conflicts between nodes can happen when BTs have a series of deeply nested steps.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about conflicts in Behavior Trees.",
      "processing_time": 60.98016953468323,
      "citing_paper_id": "272690069",
      "cited_paper_id": 269005338
    },
    {
      "context_text": "R O ] 18 J un 2025 specifications and initialize a BT expansion algorithm [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or algorithm. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 62.45795154571533,
      "citing_paper_id": "272690069",
      "cited_paper_id": 269005338
    },
    {
      "context_text": "Some models use open-ended paths to states (Rafferty et al., 2016), while others use deep learning-based, long-term memory capabilities essential for learning (Piech et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 62.45993399620056,
      "citing_paper_id": "270559967",
      "cited_paper_id": 9944290
    },
    {
      "context_text": "This approach, from which we draw inspiration, updates the likelihood of these states based on performance (Corbett and Anderson, 1994; Yudelson et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and methods. The context is about drawing inspiration from an approach that updates state likelihood based on performance.",
      "processing_time": 63.672016859054565,
      "citing_paper_id": "270559967",
      "cited_paper_id": 15120295
    },
    {
      "context_text": "There have been several works exploring Socratic reasoning in education (Herbel-Eisenmann and Breyfogle, 2005; Wang and Demszky, 2024; Alic et al., 2022; Demszky and Hill, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works exploring Socratic reasoning in education.",
      "processing_time": 61.665008306503296,
      "citing_paper_id": "270559967",
      "cited_paper_id": 250390581
    },
    {
      "context_text": "…rapidly expanding conversational and reasoning abilities of large language models (LLMs), there has been a substantial rise in demand for exploiting their capabilities within a multitude of educational applications (Kasneci et al., 2023) in order to widen accessibility via personalized feed-back.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general capabilities of LLMs in educational applications.",
      "processing_time": 61.661195516586304,
      "citing_paper_id": "270559967",
      "cited_paper_id": 257445349
    },
    {
      "context_text": "This is a more effective learning strategy because the weight of learning falls on the Student as they must put in effort to answer a question as opposed to solely relying on the model (Cotton, 1988; Kasneci et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a learning strategy involving a 'Student' and a 'model'.",
      "processing_time": 62.5955970287323,
      "citing_paper_id": "270559967",
      "cited_paper_id": 257445349
    },
    {
      "context_text": "…et al., 2023b,a) has highlighted the poor performance of prompting-based methods in performing Socratic Reasoning for the education domain (Achiam et al., 2023), even with Chain-of-Thought (CoT) (Wei et al., 2022), as they often give away answers without asking clarifying questions, or…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to the performance of prompting-based methods in Socratic Reasoning.",
      "processing_time": 62.92984914779663,
      "citing_paper_id": "270559967",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "…in performing Socratic Reasoning for the education domain (Achiam et al., 2023), even with Chain-of-Thought (CoT) (Wei et al., 2022), as they often give away answers without asking clarifying questions, or the questions are unrelated to the student’s response or original bug (Achiam et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to Socratic Reasoning and Chain-of-Thought in the context of educational applications of LLMs.",
      "processing_time": 64.0771095752716,
      "citing_paper_id": "270559967",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Student: The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of LLMs for educational purposes. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 64.98342084884644,
      "citing_paper_id": "270559967",
      "cited_paper_id": 264491049
    },
    {
      "context_text": "Student: The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of LLMs for educational purposes. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 64.98342084884644,
      "citing_paper_id": "270559967",
      "cited_paper_id": 267069469
    },
    {
      "context_text": "Student: The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. the use of LLMs for providing feedback and guidance to students (Wang et al., 2023; Kazemitabaar et al., 2024; Sheese et al., 2024; Lyu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of LLMs for educational purposes. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 64.98342084884644,
      "citing_paper_id": "270559967",
      "cited_paper_id": 269292966
    },
    {
      "context_text": "However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024).",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses limitations in existing work regarding single-turn feedback.",
      "processing_time": 32.395268201828,
      "citing_paper_id": "270559967",
      "cited_paper_id": 267069469
    },
    {
      "context_text": "However, existing work fails to account for such nuances and assumes single-turn feedback (Kazemitabaar et al., 2024; Wang et al., 2023; Lyu et al., 2024).",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses limitations in existing work regarding single-turn feedback.",
      "processing_time": 32.395268201828,
      "citing_paper_id": "270559967",
      "cited_paper_id": 269292966
    },
    {
      "context_text": "CodeAid (Kazemitabaar et al., 2024) is an assistive tool that helps students debug their code.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an assistive tool called CodeAid. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 64.49160552024841,
      "citing_paper_id": "270559967",
      "cited_paper_id": 267069469
    },
    {
      "context_text": "In order to evaluate TreeInstruct, we utilize a proxy Student agent based on the Mistral-7B-Instruct model (Jiang et al., 2023) to mimic the abilities of a student while responding to the Instructor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (Mistral-7B-Instruct) and a proxy Student agent, which are not datasets.",
      "processing_time": 65.49266266822815,
      "citing_paper_id": "270559967",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional methods make use of domain-specific languages like PDDL [23], [24] and finite state machines [6], along with grammars and semantic parsing [25], search techniques [26], and heuristics [27] to find solutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers are about PDDL, which is a language for defining planning problems, not a dataset.",
      "processing_time": 65.24074053764343,
      "citing_paper_id": "273228575",
      "cited_paper_id": 1397894
    },
    {
      "context_text": "Traditional methods make use of domain-specific languages like PDDL [23], [24] and finite state machines [6], along with grammars and semantic parsing [25], search techniques [26], and heuristics [27] to find solutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers are about PDDL, which is a language for defining planning problems, not a dataset.",
      "processing_time": 65.24074053764343,
      "citing_paper_id": "273228575",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Traditional methods make use of domain-specific languages like PDDL [23], [24] and finite state machines [6], along with grammars and semantic parsing [25], search techniques [26], and heuristics [27] to find solutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers are about PDDL, which is a language for defining planning problems, not a dataset.",
      "processing_time": 65.24074053764343,
      "citing_paper_id": "273228575",
      "cited_paper_id": 267068760
    },
    {
      "context_text": "The creation of such preconditions generally lacks scalability [23], [24], but recent work has explored the ability of LLMs for precondition generation [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to papers discussing planning domain definition languages. No verifiable resources are identified.",
      "processing_time": 63.876636028289795,
      "citing_paper_id": "273228575",
      "cited_paper_id": 1397894
    },
    {
      "context_text": "The creation of such preconditions generally lacks scalability [23], [24], but recent work has explored the ability of LLMs for precondition generation [44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to papers discussing planning domain definition languages. No verifiable resources are identified.",
      "processing_time": 63.876636028289795,
      "citing_paper_id": "273228575",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Despite the maturity of fundamental robotic skills like navigation, grasping, and object manipulation [16]–[19], the integration of modern vision models with these foundational capabilities often yields underwhelming results.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and integration challenges. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 64.4761962890625,
      "citing_paper_id": "273228575",
      "cited_paper_id": 6138957
    },
    {
      "context_text": "Despite the maturity of fundamental robotic skills like navigation, grasping, and object manipulation [16]–[19], the integration of modern vision models with these foundational capabilities often yields underwhelming results.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and integration challenges. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 64.4761962890625,
      "citing_paper_id": "273228575",
      "cited_paper_id": 219964473
    },
    {
      "context_text": "In simulation, we used the AI2Thor embodied agent simulator [22] to benchmark performance across a diverse set of tasks and environments, allowing for the controlled assessment of our approach.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "AI2Thor is a simulator, not a dataset, and is used to benchmark performance in a controlled environment.",
      "processing_time": 63.49622082710266,
      "citing_paper_id": "273228575",
      "cited_paper_id": 28328610
    },
    {
      "context_text": "Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to external tools and textual feedback. No clear, verifiable datasets are identified.",
      "processing_time": 64.31056046485901,
      "citing_paper_id": "273228575",
      "cited_paper_id": 201782024
    },
    {
      "context_text": "Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to external tools and textual feedback. No clear, verifiable datasets are identified.",
      "processing_time": 64.31056046485901,
      "citing_paper_id": "273228575",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to external tools and textual feedback. No clear, verifiable datasets are identified.",
      "processing_time": 64.31056046485901,
      "citing_paper_id": "273228575",
      "cited_paper_id": 257205781
    },
    {
      "context_text": "Leveraging external knowledge in tool-based agents requires API calls to external tools [38], [39] or textual feedback from the operating environment [33], [40], [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to external tools and textual feedback. No clear, verifiable datasets are identified.",
      "processing_time": 64.31056046485901,
      "citing_paper_id": "273228575",
      "cited_paper_id": 257757298
    },
    {
      "context_text": "Even the most † Project Lead *Equal Contribution expansive robotic models typically function optimally only within familiar settings [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general statements about robotic models. No verifiable resources are identified.",
      "processing_time": 64.04065418243408,
      "citing_paper_id": "273228575",
      "cited_paper_id": 254591260
    },
    {
      "context_text": "Our results, summarized in Table II, show that the baseline methods, including ReAct [34] and Tree of Thoughts (ToT) [45], both using an 8-billion-parameter LLM, achieved task completion rates of 10% and 8%, respectively.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing task completion rates of different methods using LLMs.",
      "processing_time": 65.46549987792969,
      "citing_paper_id": "273228575",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Inspired by [42], ConceptAgent agent extends prior work in LLM-guided tree search to the real world domain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to extending prior work in LLM-guided tree search. No verifiable resources are identified.",
      "processing_time": 65.61588191986084,
      "citing_paper_id": "273228575",
      "cited_paper_id": 258841057
    },
    {
      "context_text": "For embodied tasks, the state-action space explodes at even small depths in the tree search, requiring a powerful heuristic to filter out irrelevant states [42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in embodied tasks. No verifiable resources are identified.",
      "processing_time": 64.7809510231018,
      "citing_paper_id": "273228575",
      "cited_paper_id": 258841057
    },
    {
      "context_text": "Inspired by [5], our perception pipeline leverages several off-the-shelf computer vision models to accurately segment, embed, and project objects into a 3 dimensional pointcloud where open vocabulary object retrieval can be performed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only off-the-shelf computer vision models. The context focuses on the methodology and tools used in the perception pipeline.",
      "processing_time": 66.01302409172058,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "Leveraging realtime incremental 3D scene graphs for grounding [5] and large pretrained multi-modal models [30], our approach can formulate execution paths that were not preconceived, resulting in a capability to adapt and use affordances in ways that were not explicitly encoded by humans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of 3D scene graphs and multi-modal models, which are not datasets.",
      "processing_time": 66.52947425842285,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "This lack of adapt-ability stands in stark contrast to the remarkable capabilities demonstrated by large-scale vision models, which excel in tasks such as semantic comprehension, object detection, and bridging visual and linguistic representations [5], [9]–[15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of large-scale vision models. No dataset names are present in the citation span.",
      "processing_time": 65.60007429122925,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "In our work, we improve scalability by re-imagining ConceptGraphs [5] as a service with limited scope.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called ConceptGraphs. The context focuses on improving scalability by re-imagining ConceptGraphs as a service.",
      "processing_time": 67.10968017578125,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "Full scene understanding is achieved through a combination of an incrementally-updating language-aligned 3d scene graph [5] and large pretrained multi-modal models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (language-aligned 3D scene graph) and a type of model (large pretrained multi-modal models).",
      "processing_time": 67.11449670791626,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "Many of these systems rely on being given a prescanned environment which limits their use to well known environments [5], [6].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of systems relying on prescanned environments. No verifiable resources are identified.",
      "processing_time": 65.99331068992615,
      "citing_paper_id": "273228575",
      "cited_paper_id": 263134620
    },
    {
      "context_text": "Many of these systems rely on being given a prescanned environment which limits their use to well known environments [5], [6].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of systems relying on prescanned environments. No verifiable resources are identified.",
      "processing_time": 65.99331068992615,
      "citing_paper_id": "273228575",
      "cited_paper_id": 267068760
    },
    {
      "context_text": "Every 5 iterations, the scene layout would be randomized, and every 10 iterations, the scene would be decluttered like prior work [6], where for each declutter procedure, we select the select 1/3 of the objects that showed difficulty for the system in terms of semantic ambiguity or grasping…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or procedure for decluttering scenes, which is not a dataset.",
      "processing_time": 66.35598826408386,
      "citing_paper_id": "273228575",
      "cited_paper_id": 267068760
    },
    {
      "context_text": "To benchmark the system’s performance, we began with object rearrangement tasks, modeled after of experiments of prior work [6], Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to prior work for object rearrangement tasks.",
      "processing_time": 65.05575013160706,
      "citing_paper_id": "273228575",
      "cited_paper_id": 267068760
    },
    {
      "context_text": "We leverage the pre-trained bounding-box Faster R-CNN model to map images into text descriptions, and confirm a drop in accuracy for all models: Llama 8B ( − 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a pre-trained model and a drop in accuracy for various models.",
      "processing_time": 66.2240822315216,
      "citing_paper_id": "275133533",
      "cited_paper_id": 9953039
    },
    {
      "context_text": "Our environment has a one-to-one pixel mapping with the real Minecraft interface and supports multiple resolutions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a description of an environment setup. No verifiable resources are identified.",
      "processing_time": 65.82099604606628,
      "citing_paper_id": "275133533",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Our environment has a one-to-one pixel mapping with the real Minecraft interface and supports multiple resolutions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a description of an environment setup. No verifiable resources are identified.",
      "processing_time": 65.82099604606628,
      "citing_paper_id": "275133533",
      "cited_paper_id": 249017698
    },
    {
      "context_text": "Our environment has a one-to-one pixel mapping with the real Minecraft interface and supports multiple resolutions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a description of an environment setup. No verifiable resources are identified.",
      "processing_time": 65.82099604606628,
      "citing_paper_id": "275133533",
      "cited_paper_id": 277467626
    },
    {
      "context_text": "Using image manipulation to overlay items on top of the inventory background significantly improves performance, as the observations do not require the Java game engine.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method for improving performance in a game environment using image manipulation.",
      "processing_time": 66.21656584739685,
      "citing_paper_id": "275133533",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "01 ) and uses 100 times as many tokens, since every image is converted into a long sequence of tokens.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes a characteristic of the data used, which is not sufficient to identify a specific dataset.",
      "processing_time": 66.92072367668152,
      "citing_paper_id": "275133533",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "The simplest strategy, ReAct [Yao et al. , 2023b], consists of interleaving actions with ‘thinking’ steps where the LLM is allowed un-constrained generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ReAct) and a benchmark (ALFRED). The context focuses on the methodology and does not provide information about datasets used.",
      "processing_time": 68.25344443321228,
      "citing_paper_id": "275133533",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "We fine-tune a Llama 3.1 8B model on oracle planning trajectories from the training set using LoRA [Hu et al. , 2021] ( r = 64 , α = 32 ).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'oracle planning trajectories from the training set' but does not specify a named dataset. The citation is about a method (LoRA) rather than a dataset.",
      "processing_time": 67.87276434898376,
      "citing_paper_id": "275133533",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "In Plancraft, we collect the pages of recipes available on the Minecraft Wiki and use them to build a knowledge base to evaluate RAG capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Minecraft Wiki recipe pages"
      ],
      "dataset_descriptions": {
        "Minecraft Wiki recipe pages": "Used to build a knowledge base for evaluating RAG capabilities, focusing on the integration of web-based information into language model evaluations."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions collecting recipe pages from the Minecraft Wiki to build a knowledge base for evaluating RAG capabilities. This is a specific, verifiable resource used in the research.",
      "processing_time": 76.18425369262695,
      "citing_paper_id": "275133533",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "In Plancraft, we collect the pages of recipes available on the Minecraft Wiki and use them to build a knowledge base to evaluate RAG capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Minecraft Wiki recipe pages"
      ],
      "dataset_descriptions": {
        "Minecraft Wiki recipe pages": "Used to build a knowledge base for evaluating RAG capabilities, focusing on the integration of web-based information into language model evaluations."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions collecting recipe pages from the Minecraft Wiki to build a knowledge base for evaluating RAG capabilities. This is a specific, verifiable resource used in the research.",
      "processing_time": 76.18425369262695,
      "citing_paper_id": "275133533",
      "cited_paper_id": 249017698
    },
    {
      "context_text": "These datasets vary significantly in the types of environments in which actions are executed, ranging from simplified text-only worlds to multi-modal environments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to datasets in general terms without naming any.",
      "processing_time": 39.664326190948486,
      "citing_paper_id": "275133533",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "BabyAI [Chevalier-Boisvert et al. , 2019] is a 2D maze-solving game with natural language instructions.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BabyAI as a 2D maze-solving game with natural language instructions, which is relevant to the planning capabilities of LLMs. However, it does not specify that BabyAI is a dataset.",
      "processing_time": 69.75760269165039,
      "citing_paper_id": "275133533",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Various strategies have been proposed to leverage LLMs as agents with interactive feedback [Yao et al. , 2023b; Huang et al. , 2022; Shinn et al. , 2023; Yao et al. , 2023a; Wang et al. , 2022b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various strategies and papers. No verifiable resources are identified.",
      "processing_time": 66.76828193664551,
      "citing_paper_id": "275133533",
      "cited_paper_id": 249017698
    },
    {
      "context_text": "Various strategies have been proposed to leverage LLMs as agents with interactive feedback [Yao et al. , 2023b; Huang et al. , 2022; Shinn et al. , 2023; Yao et al. , 2023a; Wang et al. , 2022b].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various strategies and papers. No verifiable resources are identified.",
      "processing_time": 66.76828193664551,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "But LLMs still exhibit problematic issues that limit their reliability and usefulness for general-purpose agents – hallucinations and brittleness to inputs, limited context windows, and lack of grounding when placed in new environments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general limitations of LLMs. No verifiable resources are identified.",
      "processing_time": 66.61340379714966,
      "citing_paper_id": "275133533",
      "cited_paper_id": 249017698
    },
    {
      "context_text": "Because gpt-4o-mini supports image inputs, we also evaluate passing images directly to the model instead of the text observations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model capability. No verifiable resources are identified.",
      "processing_time": 66.61010241508484,
      "citing_paper_id": "275133533",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Because gpt-4o-mini supports image inputs, we also evaluate passing images directly to the model instead of the text observations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model capability. No verifiable resources are identified.",
      "processing_time": 66.61010241508484,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "This has inspired numerous research efforts [Nakano et al. , 2022; Schick et al. , 2023; Parisi et al. , 2022; Yang et al. , 2023; Patil et al. , 2023] to integrate and evaluate tool use (or external actions) into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research efforts and tool integration into LLMs.",
      "processing_time": 67.21688151359558,
      "citing_paper_id": "275133533",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "This has inspired numerous research efforts [Nakano et al. , 2022; Schick et al. , 2023; Parisi et al. , 2022; Yang et al. , 2023; Patil et al. , 2023] to integrate and evaluate tool use (or external actions) into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research efforts and tool integration into LLMs.",
      "processing_time": 67.21688151359558,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "This has inspired numerous research efforts [Nakano et al. , 2022; Schick et al. , 2023; Parisi et al. , 2022; Yang et al. , 2023; Patil et al. , 2023] to integrate and evaluate tool use (or external actions) into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research efforts and tool integration into LLMs.",
      "processing_time": 67.21688151359558,
      "citing_paper_id": "275133533",
      "cited_paper_id": null
    },
    {
      "context_text": "With the increased performance and affordability of Large Language Models (LLMs) [OpenAI, 2024; Dubey et al. , 2024], LLM-based agents have exploded in popularity [Xi et al. , 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the performance and popularity of LLMs. No verifiable resources are identified.",
      "processing_time": 68.22445821762085,
      "citing_paper_id": "275133533",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "With the increased performance and affordability of Large Language Models (LLMs) [OpenAI, 2024; Dubey et al. , 2024], LLM-based agents have exploded in popularity [Xi et al. , 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the performance and popularity of LLMs. No verifiable resources are identified.",
      "processing_time": 68.22445821762085,
      "citing_paper_id": "275133533",
      "cited_paper_id": null
    },
    {
      "context_text": "For all methods, we report the task success rate – whether or not the goal item has been crafted, the plan length – the number of environment actions the agent has taken (i.e. smelt and move ), and the total number of tokens used (both input and output combined).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and evaluation criteria. No verifiable resources are identified.",
      "processing_time": 66.87236046791077,
      "citing_paper_id": "275133533",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Even though they’re not designed as planners, they are increasingly applied to embodied environments where various methods and strategies [Yao et al. , 2023b; Shinn et al. , 2023; Yao et al. , 2023a] aim to maximise the performance of autonomous agents [Xi et al. , 2024].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and strategies in the context of improving autonomous agent performance.",
      "processing_time": 67.83293533325195,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Other broader strategies, such as Re-flexion [Shinn et al. , 2023] or Inner Monologue [Huang et al. , 2022] try to promote self-corrective behaviour through exter-nal modules or steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on describing strategies and their purposes.",
      "processing_time": 67.64328145980835,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Knowledge Base Minecraft has a unique advantage in that it is a real game, rather than a construction designed for agent evaluation, and therefore there exists a variety of online content to assist human players.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to online content for human players, which is too generic.",
      "processing_time": 68.03388833999634,
      "citing_paper_id": "275133533",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Of the datasets that we compare to, only BabyAI [Chevalier-Boisvert et al. , 2019] and ALFRED [Shridhar et al. , 2020] release handcrafted policies or solvers.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BabyAI",
        "ALFRED"
      ],
      "dataset_descriptions": {
        "BabyAI": "Used to compare planning capabilities, specifically evaluating handcrafted policies or solvers in a grid-world environment.",
        "ALFRED": "Used to compare planning capabilities, specifically evaluating handcrafted policies or solvers in a simulated home environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'BabyAI' and 'ALFRED' as datasets that release handcrafted policies or solvers. These are specific datasets used for comparing planning capabilities in the research.",
      "processing_time": 80.89023351669312,
      "citing_paper_id": "275133533",
      "cited_paper_id": 259370794
    },
    {
      "context_text": "Most existing benchmarks for evaluating LLM agents focus almost exclusively on success rates [Shridhar et al. , 2021; Yao et al. , 2022; Liu et al. , 2023; Xi et al. , 2024]: that is, the proportion of trials in which the agent achieved a goal state.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only benchmarks which are excluded according to the instructions.",
      "processing_time": 67.02519869804382,
      "citing_paper_id": "275133533",
      "cited_paper_id": 260682249
    },
    {
      "context_text": "Several works have taken advantage of knowledge bases in Minecraft in conjunction with the game environment.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to knowledge bases in Minecraft, which are not clearly identified as reusable datasets.",
      "processing_time": 68.75027275085449,
      "citing_paper_id": "275133533",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "…2023] Web ✗ ✗ ✗ ✗ WebShop [Yao et al. , 2022] Web ✓ ✗ ✗ ✗ MiniWoB++ [Liu et al. , 2018] Web ✓ ✗ ✗ ✗ WebArena [Zhou et al. , 2023] Web ✗ ✗ ✗ ✗ GAIA [Mialon et al. , 2023] Web ✓ ✓ † ✗ ✗ VirtualHome [Puig et al. , 2018] Home ✓ ✗ ✗ ✗ ALFWorld [Shridhar et al. , 2021] Home ✗ ✗ ✗ ✗ ALFRED [Shridhar et…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several web and home environments, but only GAIA is described as a benchmark, which could imply a dataset. However, the title confirms GAIA is a benchmark for General AI Assistants, not a dataset.",
      "processing_time": 32.7150981426239,
      "citing_paper_id": "275133533",
      "cited_paper_id": 265351664
    },
    {
      "context_text": "Some datasets, such as GAIA [Mialon et al. , 2023], even evaluate agents without any restriction on the tools allowed.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAIA"
      ],
      "dataset_descriptions": {
        "GAIA": "Used to evaluate general AI assistants without restrictions on tools, focusing on the capabilities and performance of AI agents in various tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "GAIA is mentioned as a benchmark, but since it is used to evaluate agents and involves specific datasets, it is included as a dataset.",
      "processing_time": 75.15212798118591,
      "citing_paper_id": "275133533",
      "cited_paper_id": 265351664
    },
    {
      "context_text": "The basic idea is to harvest LLMs for information, which, if novel to the agent, may result in better behaviours than they would perform otherwise.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a general idea about using LLMs for information harvesting. The cited paper title suggests a benchmark, but it is not clear if it is a dataset or a challenge.",
      "processing_time": 71.7569169998169,
      "citing_paper_id": "275133533",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "BabyAI implements a Bot Agent with handcrafted policies that can act as a teacher agent for learners in the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Bot Agent) within the BabyAI environment. The cited paper title suggests a benchmark, but it is not a dataset.",
      "processing_time": 69.46257090568542,
      "citing_paper_id": "275133533",
      "cited_paper_id": 277467626
    },
    {
      "context_text": "We evaluate both the open-source Llama 3.1 8B and Llama 3.3 70B models [Dubey et al. , 2024] as well as gpt-4o-mini [OpenAI, 2024] in our Plancraft environment.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and an environment but does not specify any datasets. The context is focused on evaluating models in a specific environment.",
      "processing_time": 68.34794187545776,
      "citing_paper_id": "275133533",
      "cited_paper_id": null
    },
    {
      "context_text": "The ability to use tools is invaluable as it extends an organism’s reach and enhances its capacity to interact with objects and the environment [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about tool use. No verifiable resources are identified.",
      "processing_time": 67.992520570755,
      "citing_paper_id": "274610213",
      "cited_paper_id": 10097708
    },
    {
      "context_text": "Being able to use tools is a widely recognised indicator of intelligence across species [1], [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to tool use as an indicator of intelligence.",
      "processing_time": 67.60164284706116,
      "citing_paper_id": "274610213",
      "cited_paper_id": 10097708
    },
    {
      "context_text": "Affordance models are a common technique used for tool feature selection and tool classification [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique. The cited paper title confirms the focus on computational models rather than datasets.",
      "processing_time": 68.85370373725891,
      "citing_paper_id": "274610213",
      "cited_paper_id": 26553672
    },
    {
      "context_text": "These experiments include validating the task decomposition performance in a single and dual-arm robot setup, the robustness of the affordance and manoeuvrability model in various shapes of tools, and evaluating the overall performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general experimental setups and evaluations. No clear identifiers for datasets are present.",
      "processing_time": 68.46161270141602,
      "citing_paper_id": "274610213",
      "cited_paper_id": 60447873
    },
    {
      "context_text": "The keypoint-inspired learning approach (similar to [20]) yields comparable results to our method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method. The context is about comparing results of methods, not using datasets.",
      "processing_time": 68.59085607528687,
      "citing_paper_id": "274610213",
      "cited_paper_id": 80628296
    },
    {
      "context_text": "Humans, for instance, have demonstrated mastery of tool use for over two million years.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It is a general statement about human tool use.",
      "processing_time": 67.76653814315796,
      "citing_paper_id": "274610213",
      "cited_paper_id": 140952910
    },
    {
      "context_text": "To address this research gap, in this work, we propose a novel LLM-based manoeuvrability-driven method with the following original contributions: (1) We develop an effective model to represent the geometric-mechanical relations and manoeuvrability of tools and objects; (2) We propose a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only contributions to a method. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 69.0703055858612,
      "citing_paper_id": "274610213",
      "cited_paper_id": 189656355
    },
    {
      "context_text": "Inspired by the animal manipulation study in [3] (where a crow uses a tool to get the food from the box slot by rotating and dragging the tool outwards), we adopt a similar approach to retrieve the object from confined spaces.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a study on animal behavior, which is not a dataset.",
      "processing_time": 68.83762502670288,
      "citing_paper_id": "274610213",
      "cited_paper_id": 199437458
    },
    {
      "context_text": "Being able to understand the geometric-mechanical relations between the tools-objects-environments allows certain species (e.g., apes and crows [3]) to reach food in narrow constrained spaces.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about tool use in certain species. No verifiable resources are identified.",
      "processing_time": 69.06397080421448,
      "citing_paper_id": "274610213",
      "cited_paper_id": 199437458
    },
    {
      "context_text": "In terms of LLM-based task decomposition, we assess the success rates of our approach with zero-shot and few-shot learning methods [18], DELTA [11], SayPlan [12], and fine-tuning on a smaller dataset, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'zero-shot and few-shot learning methods' and 'fine-tuning on a smaller dataset', but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 21.507381916046143,
      "citing_paper_id": "274610213",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "In terms of LLM-based task decomposition, we assess the success rates of our approach with zero-shot and few-shot learning methods [18], DELTA [11], SayPlan [12], and fine-tuning on a smaller dataset, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'zero-shot and few-shot learning methods' and 'fine-tuning on a smaller dataset', but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 21.507381916046143,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "In this study, we focus on using the side part of a tool to interact with the manipulandum .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method or experimental setup but does not reference any named dataset.",
      "processing_time": 69.04548287391663,
      "citing_paper_id": "274610213",
      "cited_paper_id": 221668003
    },
    {
      "context_text": "The same principles of physical augmentation and its associated non-prehensile manipulation capabilities also apply to robotic systems [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to principles and capabilities, which are not verifiable resources.",
      "processing_time": 69.406414270401,
      "citing_paper_id": "274610213",
      "cited_paper_id": 253410677
    },
    {
      "context_text": "For example, robots can identify the tool type, potential uses, and contact approaches based on the tool’s geometry, see e.g., [2], [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of robots in identifying tools. No verifiable resources are referenced.",
      "processing_time": 68.81813168525696,
      "citing_paper_id": "274610213",
      "cited_paper_id": 255826858
    },
    {
      "context_text": "Effective tool utilisation by a robot involves primarily two aspects: (1) task planning and (2) tool movement [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only aspects of robot tool use. There are no verifiable resources or datasets mentioned.",
      "processing_time": 16.038355588912964,
      "citing_paper_id": "274610213",
      "cited_paper_id": 255826858
    },
    {
      "context_text": "Notably, the challenge of applying incremental control on the stepping motion of the tool within a confined area has not been well-addressed by previous studies [2], [6], [13]–[16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous studies. There are no clear identifiers for datasets, models, or other resources.",
      "processing_time": 69.58309674263,
      "citing_paper_id": "274610213",
      "cited_paper_id": 255826858
    },
    {
      "context_text": "We define a tool as a manipulable object that is graspable by a robot, a manipulandum [6] as an object (e.g. a block) that is manipulated via a tool, and a wall as a static non-manipulable object.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only definitions of terms used in robotics research.",
      "processing_time": 67.68137240409851,
      "citing_paper_id": "274610213",
      "cited_paper_id": 255826858
    },
    {
      "context_text": "Passive affordance is given from a static non-manipulable object.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a concept related to passive affordance.",
      "processing_time": 68.75538849830627,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259737661
    },
    {
      "context_text": "The methodologies evaluated include Zero-shot learning, Few-shot learning, DELTA [11], SayPlan [12], planning domain definition language (PDDL) [21], behaviour tree [22], and Fine-tuning with 200 data, and our proposed approach.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Fine-tuning with 200 data' but does not specify a named dataset. No other datasets are mentioned.",
      "processing_time": 69.5824863910675,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Similarly, even when more information is given through domain knowledge and graphs [11], [12], the LLM still struggles to generate a reasonable list for tasks involving both arms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'domain knowledge and graphs'. No clear, verifiable resource names are provided.",
      "processing_time": 70.05772829055786,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "However, recent trends have been pushing towards the use of LLMs to leverage the domain knowledge for semantically decomposing and planning the execution of manipulation tasks [10]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general trends in using LLMs for task planning. No verifiable resources are named.",
      "processing_time": 69.82196593284607,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "However, recent trends have been pushing towards the use of LLMs to leverage the domain knowledge for semantically decomposing and planning the execution of manipulation tasks [10]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general trends in using LLMs for task planning. No verifiable resources are named.",
      "processing_time": 69.82196593284607,
      "citing_paper_id": "274610213",
      "cited_paper_id": 265295011
    },
    {
      "context_text": "11, 12, 13, where x and y refer to the coordinates of the items, such as the position of the block.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes a method or approach involving coordinates and positions.",
      "processing_time": 68.96445536613464,
      "citing_paper_id": "274610213",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "The combination of traditional motion planners with LLMs has been explored in [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of methods. No verifiable resources are identified.",
      "processing_time": 68.4785885810852,
      "citing_paper_id": "274610213",
      "cited_paper_id": 265295011
    },
    {
      "context_text": "…the geometric-mechanical relations and manoeuvrability of tools and objects; (2) We propose a non-prehensile strategy to manoeuvre objects under different constraints with tools; (3) We evaluate the performance of the proposed methodology with real-world experiments on a dual-arm robotic system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodology and experimental setup. No verifiable resources are identified.",
      "processing_time": 68.59863638877869,
      "citing_paper_id": "274610213",
      "cited_paper_id": 268777626
    },
    {
      "context_text": "IV gives final conclusions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only states 'IV gives final conclusions.' which does not provide any information about the use of datasets.",
      "processing_time": 71.07998991012573,
      "citing_paper_id": "274610213",
      "cited_paper_id": null
    },
    {
      "context_text": "LLM-based Agents Reinforcement learning has been extensively employed in the training of autonomous agents (Ribeiro, 2002; Isbell et al., 2001; Mnih et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to reinforcement learning in training autonomous agents.",
      "processing_time": 68.04533123970032,
      "citing_paper_id": "268032489",
      "cited_paper_id": 15238391
    },
    {
      "context_text": "Instead, the use of human-written exam questions as an assessment paradigm (Hu et al., 2024; Hendrycks et al., 2021; Liang et al., 2022; Srivastava et al., 2023) has become increasingly mainstream, focusing primarily on testing the world knowledge and reasoning capabilities in a multi-task…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human-written exam questions' as an assessment paradigm but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 70.83585286140442,
      "citing_paper_id": "268032489",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "Instead, the use of human-written exam questions as an assessment paradigm (Hu et al., 2024; Hendrycks et al., 2021; Liang et al., 2022; Srivastava et al., 2023) has become increasingly mainstream, focusing primarily on testing the world knowledge and reasoning capabilities in a multi-task…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human-written exam questions' as an assessment paradigm but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 70.83585286140442,
      "citing_paper_id": "268032489",
      "cited_paper_id": 263625818
    },
    {
      "context_text": "Instead, the use of human-written exam questions as an assessment paradigm (Hu et al., 2024; Hendrycks et al., 2021; Liang et al., 2022; Srivastava et al., 2023) has become increasingly mainstream, focusing primarily on testing the world knowledge and reasoning capabilities in a multi-task…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human-written exam questions' as an assessment paradigm but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 70.83585286140442,
      "citing_paper_id": "268032489",
      "cited_paper_id": null
    },
    {
      "context_text": "To ensure user-friendliness and robust scalability in LLMA RENA , we have developed the environment using PettingZoo (Terry et al., 2021) as a foundation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PettingZoo, which is a toolkit for multi-agent reinforcement learning, not a dataset. No datasets are mentioned in the citation.",
      "processing_time": 70.0394697189331,
      "citing_paper_id": "268032489",
      "cited_paper_id": 222066674
    },
    {
      "context_text": "By simply creating environments that adhere to the PettingZoo interface specifications 1 , they can seamlessly add these to LLMA RENA for evaluating the diverse capabilities of LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a framework (PettingZoo) for multi-agent reinforcement learning environments. No verifiable datasets are referenced.",
      "processing_time": 20.67790937423706,
      "citing_paper_id": "268032489",
      "cited_paper_id": 222066674
    },
    {
      "context_text": "With the superior capabilities of LLMs, the utilization of LLMs as cognitive entities and controllers for agents has garnered widespread acclaim (Huang et al., 2022; Park et al., 2023; Sumers et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the capabilities of LLMs and their use as cognitive entities and controllers for agents.",
      "processing_time": 44.573209285736084,
      "citing_paper_id": "268032489",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "With the superior capabilities of LLMs, the utilization of LLMs as cognitive entities and controllers for agents has garnered widespread acclaim (Huang et al., 2022; Park et al., 2023; Sumers et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the capabilities of LLMs and their use as cognitive entities and controllers for agents.",
      "processing_time": 44.573209285736084,
      "citing_paper_id": "268032489",
      "cited_paper_id": null
    },
    {
      "context_text": "LLM Evaluation With the rapid development of LLMs, the evaluation practices of traditional NLP tasks (Hu et al., 2023; Bang et al., 2023; Zhong et al., 2023; Qin et al., 2023; Hu et al., 2022) limit their performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general evaluation practices of traditional NLP tasks. No verifiable resources are identified.",
      "processing_time": 18.427682161331177,
      "citing_paper_id": "268032489",
      "cited_paper_id": 256827430
    },
    {
      "context_text": "LLM Evaluation With the rapid development of LLMs, the evaluation practices of traditional NLP tasks (Hu et al., 2023; Bang et al., 2023; Zhong et al., 2023; Qin et al., 2023; Hu et al., 2022) limit their performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general evaluation practices of traditional NLP tasks. No verifiable resources are identified.",
      "processing_time": 18.427682161331177,
      "citing_paper_id": "268032489",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent large language models (LLMs) have greatly promoted the progress of the field of natural language processing (NLP) due to their excellent zero-shot capabilities in various downstream tasks (Ko-jima et al., 2022; Yang et al., 2023; Touvron et al., 2023a,b; OpenAI, 2022, 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the capabilities of large language models. No verifiable resources are identified.",
      "processing_time": 19.07923722267151,
      "citing_paper_id": "268032489",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "…examine large language models from multiple dimensions, such as adaptability to professional fields (Xiang et al., 2023), real-world application (Li et al., 2023b; Liu et al., 2024c,b,a), robustness (Zhu et al., 2023), multi-modal capabilities (Fu et al., 2023), etc. Recently, the trend of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general aspects of research on large language models. No verifiable resources are identified.",
      "processing_time": 15.613115549087524,
      "citing_paper_id": "268032489",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "Firstly, static datasets used in benchmarks like AgentBench might lead to issues such as data leakage and over-fitting, as LLMs could have already encountered this data during their pre-training phase (Zhou et al., 2023b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AgentBench' but does not indicate it is a dataset. It is likely a benchmark or leaderboard, which is excluded according to the instructions.",
      "processing_time": 31.48966932296753,
      "citing_paper_id": "268032489",
      "cited_paper_id": 265019021
    },
    {
      "context_text": "…empirical studies indicate that collaborative and adversarial frameworks (Li et al., 2023a; Chen et al., 2023; Chan et al., 2023) involving multiple LLMs surpass the capabilities of singular agents across various tasks and may lead to the emergence of social phenomena (Park et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to empirical studies and frameworks. No verifiable resources are identified.",
      "processing_time": 28.244041681289673,
      "citing_paper_id": "268032489",
      "cited_paper_id": null
    },
    {
      "context_text": "The collected data is used to train models using Imitation Learning [35], taking RGB-D images and the robot arm’s joint state as input and predicting joint angle movement sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'collected data' but does not specify a named dataset. The focus is on the methodology (Imitation Learning) rather than a specific dataset.",
      "processing_time": 18.412068843841553,
      "citing_paper_id": "265498448",
      "cited_paper_id": 103456
    },
    {
      "context_text": "In classical planning [27]–[29], these representations are predefined, often using predicates to track state changes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts in classical planning. No verifiable resources are identified.",
      "processing_time": 17.420745372772217,
      "citing_paper_id": "265498448",
      "cited_paper_id": 17746168
    },
    {
      "context_text": "We design our experiments with the Fetch mobile manipulator robot [37] in a typical real home environment consisting of four rooms: kitchen, meeting room, living room and office room.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes an experimental setup using a robot in a simulated home environment.",
      "processing_time": 26.967609167099,
      "citing_paper_id": "265498448",
      "cited_paper_id": 42886148
    },
    {
      "context_text": "A. Simulation Experiments 1) Experimental settings in Simulation: We evaluate our method using VirtualHome [36], a large-scale household simulation with a variety of interactive objects, receptacles, and rooms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VirtualHome"
      ],
      "dataset_descriptions": {
        "VirtualHome": "Used to evaluate the method in a large-scale household simulation, focusing on interactions with various objects, receptacles, and rooms."
      },
      "confidence_score": 0.8,
      "reasoning": "VirtualHome is a simulation environment, not a traditional dataset, but it is used as a reusable resource for evaluating methods in household simulations.",
      "processing_time": 23.528393983840942,
      "citing_paper_id": "265498448",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "Most works assume the availability of all objects and their accurate attributes in the environment [5], [7], [8], [15], [22] or directly detect them from the perception system [4], [6], [12]–[14], [21], using them as state representations for LLMs to reason and plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and assumptions about the availability of objects and attributes in the environment for LLM planning.",
      "processing_time": 16.6906635761261,
      "citing_paper_id": "265498448",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Most works assume the availability of all objects and their accurate attributes in the environment [5], [7], [8], [15], [22] or directly detect them from the perception system [4], [6], [12]–[14], [21], using them as state representations for LLMs to reason and plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and assumptions about the availability of objects and attributes in the environment for LLM planning.",
      "processing_time": 16.6906635761261,
      "citing_paper_id": "265498448",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Most works assume the availability of all objects and their accurate attributes in the environment [5], [7], [8], [15], [22] or directly detect them from the perception system [4], [6], [12]–[14], [21], using them as state representations for LLMs to reason and plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and assumptions about the availability of objects and attributes in the environment for LLM planning.",
      "processing_time": 16.6906635761261,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "Most works assume the availability of all objects and their accurate attributes in the environment [5], [7], [8], [15], [22] or directly detect them from the perception system [4], [6], [12]–[14], [21], using them as state representations for LLMs to reason and plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and assumptions about the availability of objects and attributes in the environment for LLM planning.",
      "processing_time": 16.6906635761261,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "We compare our method with three baselines: Inner-Monologue [4], ProgPrompt [5], and a naive version of LLM without our LLM-State representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models. No verifiable resources are identified.",
      "processing_time": 14.140483617782593,
      "citing_paper_id": "265498448",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "…from LLMs to solve complex planning tasks, by decomposing instructions into action sequences [3], [4], [6]–[13], generating executable code [1], [5], [14], [15], translating natural language into formal specifications [16]–[18], and providing auxiliary information for classical planners…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various capabilities of LLMs in planning tasks. No verifiable resources are identified.",
      "processing_time": 15.276841163635254,
      "citing_paper_id": "265498448",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "…from LLMs to solve complex planning tasks, by decomposing instructions into action sequences [3], [4], [6]–[13], generating executable code [1], [5], [14], [15], translating natural language into formal specifications [16]–[18], and providing auxiliary information for classical planners…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various capabilities of LLMs in planning tasks. No verifiable resources are identified.",
      "processing_time": 15.276841163635254,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "Some approaches use exhaustive object and attribute descriptions in prompts [5], [9], [10], while others rely on hand-engineered and task-specific state representations [3], [6], [7], [14], [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 27.55057191848755,
      "citing_paper_id": "265498448",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Some approaches use exhaustive object and attribute descriptions in prompts [5], [9], [10], while others rely on hand-engineered and task-specific state representations [3], [6], [7], [14], [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 27.55057191848755,
      "citing_paper_id": "265498448",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Some approaches use exhaustive object and attribute descriptions in prompts [5], [9], [10], while others rely on hand-engineered and task-specific state representations [3], [6], [7], [14], [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 27.55057191848755,
      "citing_paper_id": "265498448",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Some approaches use exhaustive object and attribute descriptions in prompts [5], [9], [10], while others rely on hand-engineered and task-specific state representations [3], [6], [7], [14], [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 27.55057191848755,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Some approaches use exhaustive object and attribute descriptions in prompts [5], [9], [10], while others rely on hand-engineered and task-specific state representations [3], [6], [7], [14], [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 27.55057191848755,
      "citing_paper_id": "265498448",
      "cited_paper_id": null
    },
    {
      "context_text": "For the naive LLM, we directly prompt the observations and action history to the LLM, similar to previous work [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about prompting an LLM with observations and action history, which is a methodological detail.",
      "processing_time": 32.95155334472656,
      "citing_paper_id": "265498448",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "…how to leverage the common sense knowledge from LLMs to solve complex planning tasks, by decomposing instructions into action sequences [3], [4], [6]–[13], generating executable code [1], [5], [14], [15], translating natural language into formal specifications [16]–[18], and providing auxiliary…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 15.576431512832642,
      "citing_paper_id": "265498448",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Some works employ self-reflection techniques [23], [24] or external models for failure explanation [12], but these free-form summaries may be inefficient for complex, long-horizon tasks requiring explicit state tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 19.7650363445282,
      "citing_paper_id": "265498448",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Some works employ self-reflection techniques [23], [24] or external models for failure explanation [12], but these free-form summaries may be inefficient for complex, long-horizon tasks requiring explicit state tracking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 19.7650363445282,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "…from LLMs to solve complex planning tasks, by decomposing instructions into action sequences [3], [4], [6]–[13], generating executable code [1], [5], [14], [15], translating natural language into formal specifications [16]–[18], and providing auxiliary information for classical planners [19]–[22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities of LLMs in planning tasks. No verifiable resources are identified.",
      "processing_time": 31.44883155822754,
      "citing_paper_id": "265498448",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "…to leverage the common sense knowledge from LLMs to solve complex planning tasks, by decomposing instructions into action sequences [3], [4], [6]–[13], generating executable code [1], [5], [14], [15], translating natural language into formal specifications [16]–[18], and providing auxiliary…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and applications of LLMs. No verifiable resources are identified.",
      "processing_time": 16.66242790222168,
      "citing_paper_id": "265498448",
      "cited_paper_id": 258564887
    },
    {
      "context_text": "Image tagging is obtained using the model in [31], and Grounding DINO [32] provides bounding boxes.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'image tagging' and 'bounding boxes' but does not specify any datasets. The cited papers are models, not datasets.",
      "processing_time": 15.562756299972534,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259089333
    },
    {
      "context_text": "Current Large Language Models’ (LLMs) advanced abilities, like common-sense reasoning, enable them to tackle these challenges [1], [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 17.046862840652466,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "Recent methods explore dynamic memory with reflection [24], semantic search in scene graph [7], multi-sensory summary [12], and training task-conditioned state descriptions [30] to achieve more flexible state representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 16.654670476913452,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "Recent methods explore dynamic memory with reflection [24], semantic search in scene graph [7], multi-sensory summary [12], and training task-conditioned state descriptions [30] to achieve more flexible state representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 16.654670476913452,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Recent methods explore dynamic memory with reflection [24], semantic search in scene graph [7], multi-sensory summary [12], and training task-conditioned state descriptions [30] to achieve more flexible state representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 16.654670476913452,
      "citing_paper_id": "265498448",
      "cited_paper_id": 260125969
    },
    {
      "context_text": "Despite not requiring the formal definition of state space and transition models, these LLM-based methods either utilize a massive array of ground truth objects or manually engineered state representation, proving insufficient for long-horizon tasks in open-world settings [3]– [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches. No clear, verifiable resource names are provided.",
      "processing_time": 27.740682125091553,
      "citing_paper_id": "265498448",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Related research also investigates accepting encoded images as state representations and outputting subsequent actions [11], [25], [26]. where the perception and reasoning modules are coupled.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about perception, reasoning, and action in robotic control. No verifiable resources are identified.",
      "processing_time": 20.440667629241943,
      "citing_paper_id": "265498448",
      "cited_paper_id": 260293142
    },
    {
      "context_text": "Related research also investigates accepting encoded images as state representations and outputting subsequent actions [11], [25], [26]. where the perception and reasoning modules are coupled.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about perception, reasoning, and action in robotic control. No verifiable resources are identified.",
      "processing_time": 20.440667629241943,
      "citing_paper_id": "265498448",
      "cited_paper_id": 260438420
    },
    {
      "context_text": "Humans manage these uncertainties with situational awareness, whose lack is a major cause of accidents from human errors [2, 3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to human errors and situational awareness. No verifiable resources are identified.",
      "processing_time": 15.252827167510986,
      "citing_paper_id": "266551253",
      "cited_paper_id": 8347993
    },
    {
      "context_text": "When prompted for plan-+ ning that involves perception, comprehension, and projection [2], or when provided with reasoning feedback, the LLM shows enhanced deductive reasoning skills that require perspective-taking and consideration of potential outcomes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 17.031490802764893,
      "citing_paper_id": "266551253",
      "cited_paper_id": 8347993
    },
    {
      "context_text": "Motivated by discussions of inconsistent human evaluation in Iskender et al. [40] and the inadequate quality of automatic metrics highlighted in Sottano et al. [41], we introduce a rank-based scoring (RBS) method to help mitigate potential reliability issues when evaluating FSM plans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and challenges in human evaluation.",
      "processing_time": 27.728739023208618,
      "citing_paper_id": "266551253",
      "cited_paper_id": 233305616
    },
    {
      "context_text": "Most existing studies focus on task-specific planning or rely on assumptions about predefined steps under controlled conditions [19, 12, 13, 20, 18, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to existing studies and their focus on task-specific planning or predefined steps.",
      "processing_time": 20.196052074432373,
      "citing_paper_id": "266551253",
      "cited_paper_id": 248986485
    },
    {
      "context_text": "Most existing studies focus on task-specific planning or rely on assumptions about predefined steps under controlled conditions [19, 12, 13, 20, 18, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to existing studies and their focus on task-specific planning or predefined steps.",
      "processing_time": 20.196052074432373,
      "citing_paper_id": "266551253",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 248986485
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 250311260
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 258960174
    },
    {
      "context_text": "Current AI systems that operate on rigid, context-insensitive rules are at risk of producing unintended outcomes when deployed in complex, real-world environments [19, 12, 13, 20, 18, 21, 22, 15, 23, 24, 16, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references the risk of unintended outcomes in AI systems deployed in complex environments.",
      "processing_time": 19.399877786636353,
      "citing_paper_id": "266551253",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "This indicates that the prompt is particularly valuable in complex, nuanced planning situations that require perspective-taking and consideration of potential outcomes (for ablation studies, see Appendix B) [48, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to ablation studies and the value of prompts in complex planning situations.",
      "processing_time": 17.56012797355652,
      "citing_paper_id": "266551253",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Additionally, conventional agents typically generate plans reactively, only in response to explicit instructions or demands [22, 15, 23, 24, 16, 25, 26, 27], rather than proactively.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the reactive nature of conventional agents in generating plans.",
      "processing_time": 17.556431770324707,
      "citing_paper_id": "266551253",
      "cited_paper_id": 250311260
    },
    {
      "context_text": "Additionally, conventional agents typically generate plans reactively, only in response to explicit instructions or demands [22, 15, 23, 24, 16, 25, 26, 27], rather than proactively.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the reactive nature of conventional agents in generating plans.",
      "processing_time": 17.556431770324707,
      "citing_paper_id": "266551253",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Additionally, conventional agents typically generate plans reactively, only in response to explicit instructions or demands [22, 15, 23, 24, 16, 25, 26, 27], rather than proactively.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the reactive nature of conventional agents in generating plans.",
      "processing_time": 17.556431770324707,
      "citing_paper_id": "266551253",
      "cited_paper_id": 258960174
    },
    {
      "context_text": "Additionally, conventional agents typically generate plans reactively, only in response to explicit instructions or demands [22, 15, 23, 24, 16, 25, 26, 27], rather than proactively.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the reactive nature of conventional agents in generating plans.",
      "processing_time": 17.556431770324707,
      "citing_paper_id": "266551253",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "The dataset comprises over 500 hazardous home scenarios, specifically curated to fill gaps often ignored in academic research, such as scenarios typically avoided by embodied agents and agent simulations [14, 32, 12, 13, 15, 16, 17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hazardous home scenarios"
      ],
      "dataset_descriptions": {
        "hazardous home scenarios": "Used to address gaps in academic research by providing over 500 scenarios typically avoided by embodied agents and agent simulations, focusing on planning capabilities of language models."
      },
      "confidence_score": 0.85,
      "reasoning": "The context describes a specific dataset with a clear identifier and purpose, which is used to address gaps in academic research related to hazardous home scenarios.",
      "processing_time": 25.602171421051025,
      "citing_paper_id": "266551253",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "The dataset comprises over 500 hazardous home scenarios, specifically curated to fill gaps often ignored in academic research, such as scenarios typically avoided by embodied agents and agent simulations [14, 32, 12, 13, 15, 16, 17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hazardous home scenarios"
      ],
      "dataset_descriptions": {
        "hazardous home scenarios": "Used to address gaps in academic research by providing over 500 scenarios typically avoided by embodied agents and agent simulations, focusing on planning capabilities of language models."
      },
      "confidence_score": 0.85,
      "reasoning": "The context describes a specific dataset with a clear identifier and purpose, which is used to address gaps in academic research related to hazardous home scenarios.",
      "processing_time": 25.602171421051025,
      "citing_paper_id": "266551253",
      "cited_paper_id": 266906759
    },
    {
      "context_text": "Nonetheless, the simulations for robots and agents [15, 16, 17, 18] ignore real-world hazardous interactions that are prevalent and essential to consider in daily scenarios.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general simulations for robots and agents. No verifiable resources are identified.",
      "processing_time": 16.62321376800537,
      "citing_paper_id": "266551253",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Nonetheless, the simulations for robots and agents [15, 16, 17, 18] ignore real-world hazardous interactions that are prevalent and essential to consider in daily scenarios.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general simulations for robots and agents. No verifiable resources are identified.",
      "processing_time": 16.62321376800537,
      "citing_paper_id": "266551253",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Textual descriptions generated by GPT-4V [34] and expert-validated solutions provide a robust framework for evaluating the planning capabilities of LLMs.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only textual descriptions generated by GPT-4V and expert-validated solutions. These are not considered datasets according to the extraction rules.",
      "processing_time": 32.21878361701965,
      "citing_paper_id": "266551253",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "5 [34], Claude-2 [42], alongside open-source alternatives such as LLama-2 [43], LLava [44], Vicuna [45], MiniGPT-4 [46], and CodeLLama [47], on their ability to perform hazard planning using scene-informed one-shot prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 15.517471313476562,
      "citing_paper_id": "266551253",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "5 [34], Claude-2 [42], alongside open-source alternatives such as LLama-2 [43], LLava [44], Vicuna [45], MiniGPT-4 [46], and CodeLLama [47], on their ability to perform hazard planning using scene-informed one-shot prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 15.517471313476562,
      "citing_paper_id": "266551253",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "5 [34], Claude-2 [42], alongside open-source alternatives such as LLama-2 [43], LLava [44], Vicuna [45], MiniGPT-4 [46], and CodeLLama [47], on their ability to perform hazard planning using scene-informed one-shot prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 15.517471313476562,
      "citing_paper_id": "266551253",
      "cited_paper_id": null
    },
    {
      "context_text": "This selection provides a thoughtful baseline for functionality, drawing on insights from some of the leading projects in intelligent robotics [19, 12, 13, 35, 20, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to projects in intelligent robotics. No clear, verifiable datasets are identified.",
      "processing_time": 18.978323459625244,
      "citing_paper_id": "266551253",
      "cited_paper_id": 264935717
    },
    {
      "context_text": "In light of the constraints imposed by image generation models such as DALL·E [33], the production of images depicting hazardous scenarios is limited.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (DALL·E) which is excluded according to the rules.",
      "processing_time": 19.346988439559937,
      "citing_paper_id": "266551253",
      "cited_paper_id": 265185500
    },
    {
      "context_text": "…that prohibit interactions with humans or animals, the handling of sharp objects, and involvement in dangerous settings such as those with water and electricity, aiming to define research boundaries and simplify complex issues, akin to the approach seen in the Google RT-X Series [12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a series of papers related to robotic agents. No verifiable resources are identified.",
      "processing_time": 18.617566347122192,
      "citing_paper_id": "266551253",
      "cited_paper_id": 266906759
    },
    {
      "context_text": "Such collaboration leverages the individual strengths of each agent [28, 29, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of a reusable resource being used.",
      "processing_time": 16.99310827255249,
      "citing_paper_id": "266551253",
      "cited_paper_id": 268042527
    },
    {
      "context_text": "Prior investigations on the applications of LLMs within the scope of computational models like finite state machines and behaviour trees, specifically targeting certain tasks or sub-tasks completion [9, 10, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general computational models and tasks. No clear, verifiable datasets are identified.",
      "processing_time": 15.208061218261719,
      "citing_paper_id": "266551253",
      "cited_paper_id": 268531996
    },
    {
      "context_text": "Prior investigations on the applications of LLMs within the scope of computational models like finite state machines and behaviour trees, specifically targeting certain tasks or sub-tasks completion [9, 10, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general computational models and tasks. No clear, verifiable datasets are identified.",
      "processing_time": 15.208061218261719,
      "citing_paper_id": "266551253",
      "cited_paper_id": null
    },
    {
      "context_text": "Without this understanding, seemingly beneficial actions can have unintended consequences [5, 6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, methods, or other resources. It only refers to general findings or concepts.",
      "processing_time": 16.98844575881958,
      "citing_paper_id": "266551253",
      "cited_paper_id": null
    },
    {
      "context_text": "The application of LLMs for SAP introduces a significant paradigm shift due to the inherently infinite state space of the open world, which is in stark contrast to the relatively confined state spaces observed in traditional game strategies [7, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to traditional game strategies. There are no verifiable resources or datasets mentioned.",
      "processing_time": 27.45319366455078,
      "citing_paper_id": "266551253",
      "cited_paper_id": null
    },
    {
      "context_text": "Methods such as multimodal deep learning and Transformer models have contributed to enhancing multimodal integration [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 28.53959894180298,
      "citing_paper_id": "272579800",
      "cited_paper_id": 246634264
    },
    {
      "context_text": "This feedback mechanism helps optimize the overall efficiency of the multi-agent systems [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a feedback mechanism in multi-agent systems.",
      "processing_time": 28.5245304107666,
      "citing_paper_id": "272579800",
      "cited_paper_id": 259360395
    },
    {
      "context_text": "For instance, the ChatDev project utilizes multiple agents within a multi-agent system to perform different roles, including CEO, Chief Product Officer, and Chief Technology Officer, thereby advancing various stages of software development [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a project that uses multiple agents in a multi-agent system. There are no clear identifiers for datasets.",
      "processing_time": 19.32464623451233,
      "citing_paper_id": "272579800",
      "cited_paper_id": 259936967
    },
    {
      "context_text": "For example, ToolLLM provides a comprehensive data-building, model training, and evaluation template, facilitating the development of more functional agents [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool or method called ToolLLM. The context focuses on the capabilities of ToolLLM rather than a specific dataset.",
      "processing_time": 19.906773805618286,
      "citing_paper_id": "272579800",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "These limitations severely hindered the development of agents [2-3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to limitations in the development of agents.",
      "processing_time": 17.30479645729065,
      "citing_paper_id": "272579800",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "To mitigate or eliminate the impact of hallucinations, several methods have been proposed, such as data optimization, adversarial training, and model improvements, aiming to reduce data-related hallucinations [13-14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods to address hallucinations in LLMs.",
      "processing_time": 13.329432964324951,
      "citing_paper_id": "272579800",
      "cited_paper_id": 265067168
    },
    {
      "context_text": "Currently, LLM can construct environments categorized into virtual environments, real physical environments, and non-specific environments [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of environments that LLMs can construct. There are no verifiable resources or datasets mentioned.",
      "processing_time": 19.884843826293945,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "With continuous technological advancements, LLM-based multi-agent systems are expected to demonstrate their unique potential and value in the near future [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about LLM-based multi-agent systems.",
      "processing_time": 13.323755264282227,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "The natural language understanding capabilities of LLM enable efficient and smooth communication among agents [5],[10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs in multi-agent communication.",
      "processing_time": 14.02588415145874,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "The current role assignment methods mainly include predefined, model-generated, and data-derived roles [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for role assignment. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 19.33118486404419,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "Additionally, communication structures can take various forms, such as hierarchical communication, decentralized communication, and centralized communication [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of communication structures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 14.796806573867798,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "These advantages demonstrate the unique capabilities of LLM-based multi-agent systems in handling complex, dynamic, multi-domain tasks and highlight their promising development prospects [1], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the capabilities and prospects of LLM-based multi-agent systems.",
      "processing_time": 20.353482246398926,
      "citing_paper_id": "272579800",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "These advantages demonstrate the unique capabilities of LLM-based multi-agent systems in handling complex, dynamic, multi-domain tasks and highlight their promising development prospects [1], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the capabilities and prospects of LLM-based multi-agent systems.",
      "processing_time": 20.353482246398926,
      "citing_paper_id": "272579800",
      "cited_paper_id": 269921354
    },
    {
      "context_text": "MARL integrates reinforcement learning with game theory and other techniques, enabling multiple agents to interact and make decisions in high-dimensional and dynamic real-world scenarios to accomplish more complex tasks [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and approaches. There are no clear identifiers for datasets in the given context.",
      "processing_time": 15.166210412979126,
      "citing_paper_id": "272579800",
      "cited_paper_id": 269921354
    },
    {
      "context_text": "Researchers began combining deep learning methods with traditional reinforcement learning algorithms, leading to the formation of numerous deep reinforcement learning algorithms, significantly advancing the research and application of single-agent reinforcement learning [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the combination of deep learning and reinforcement learning methods.",
      "processing_time": 25.939188957214355,
      "citing_paper_id": "272579800",
      "cited_paper_id": 276489554
    },
    {
      "context_text": "For example, Littman proposed a multi-agent reinforcement learning framework based on Markov decision processes, laying the mathematical foundation for solving multi-agent reinforcement learning problems [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological framework. The context focuses on the theoretical contribution rather than empirical data.",
      "processing_time": 16.19452977180481,
      "citing_paper_id": "272579800",
      "cited_paper_id": 276489554
    },
    {
      "context_text": "Existing research efforts aim to promote system scalability through methods such as modular design and distributed architectures [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for promoting system scalability.",
      "processing_time": 26.159780740737915,
      "citing_paper_id": "272579800",
      "cited_paper_id": 276489554
    },
    {
      "context_text": "Current research, such as establishing knowledge-sharing mechanisms and consensus algorithms, aims to promote information exchange among multiple agents to acquire collective intelligence [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts related to multi-agent systems and collective intelligence.",
      "processing_time": 14.206669807434082,
      "citing_paper_id": "272579800",
      "cited_paper_id": null
    },
    {
      "context_text": "The success of LLMs in Natural Language Generation [Radford et al., 2018] and Natural Language Understanding [Vaswani et al., 2017, Devlin et al., 2019] has sparked interest in exploring reasoning capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and their capabilities in NLP tasks.",
      "processing_time": 26.657366275787354,
      "citing_paper_id": "276421789",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Large language models (LLMs) [Brown et al., 2020] have demonstrated exceptional performance across a range of natural language processing (NLP) tasks, including question answering, machine translation, sentiment analysis, and text summarization [Devlin et al., 2019, Vaswani et al., 2017].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 14.77451467514038,
      "citing_paper_id": "276421789",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Large language models (LLMs) [Brown et al., 2020] have demonstrated exceptional performance across a range of natural language processing (NLP) tasks, including question answering, machine translation, sentiment analysis, and text summarization [Devlin et al., 2019, Vaswani et al., 2017].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 14.77451467514038,
      "citing_paper_id": "276421789",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "To be specific, in StrategyQA, the binary output (yes or no) enables LLaMA models to effectively utilize the generated facts, leading to improved performance.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'StrategyQA' but does not provide enough information to determine if it is a dataset or a benchmark. The title suggests it is a benchmark, so it is excluded.",
      "processing_time": 31.043839931488037,
      "citing_paper_id": "276421789",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "Since RAP requires extensive prompt engineering to frame all tasks as planning problems, we evaluate it on a subset of tasks, including GSM8K, AQuA, ProntoQA, StrategyQA, Game of 24, Binpacking, Blocksworld, and Rubik’s Cube.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AQuA",
        "ProntoQA",
        "StrategyQA",
        "Game of 24",
        "Binpacking",
        "Blocksworld",
        "Rubik’s Cube"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate RAP on grade school math word problems, focusing on multi-step reasoning and problem-solving strategies.",
        "AQuA": "Used to assess RAP on multiple-choice questions requiring implicit reasoning, emphasizing logical and commonsense inference.",
        "ProntoQA": "Used to test RAP on complex question answering tasks, evaluating its ability to handle implicit reasoning and contextual understanding.",
        "StrategyQA": "Used to evaluate RAP on strategic reasoning tasks, assessing its capability to plan and execute multi-step solutions.",
        "Game of 24": "Used to assess RAP on arithmetic puzzle solving, focusing on numerical reasoning and strategic planning.",
        "Binpacking": "Used to evaluate RAP on optimization problems, testing its ability to efficiently pack items into bins.",
        "Blocksworld": "Used to assess RAP on planning tasks in a simulated environment, evaluating its ability to solve block manipulation problems.",
        "Rubik’s Cube": "Used to test RAP on algorithmic problem solving, focusing on step-by-step planning to solve the cube."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several task datasets but does not specify their use in the current research. The cited paper title suggests a QA benchmark, which aligns with some of the mentioned datasets.",
      "processing_time": 64.32373809814453,
      "citing_paper_id": "276421789",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "StrategyQA is a benchmark designed to assess a model’s ability to perform implicit multi-step reasoning using general knowledge or common sense facts.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions StrategyQA as a benchmark but does not refer to it as a dataset. It is used to assess models' reasoning capabilities.",
      "processing_time": 15.723986625671387,
      "citing_paper_id": "276421789",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "We evaluate this type of reasoning using the StrategyQA Geva et al. [2021] and HotPotQA Yang et al. [2018] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "StrategyQA",
        "HotPotQA"
      ],
      "dataset_descriptions": {
        "StrategyQA": "Used to evaluate implicit reasoning strategies in question answering, focusing on multi-step reasoning and strategy formulation.",
        "HotPotQA": "Applied to assess complex question answering, emphasizing multi-hop reasoning and evidence aggregation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets used for evaluating reasoning capabilities, which are directly relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 25.35502552986145,
      "citing_paper_id": "276421789",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "A range of datasets have been introduced to evaluate reasoning, covering tasks in arithmetic [Ling et al., 2017, Cobbe et al., 2021], logic [Chollet, 2019, Wang et al., 2022], common sense [Yang et al., 2018, Geva et al., 2021], and algorithmic reasoning [Yao et al., 2024].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a range of datasets but does not specify their names. It only refers to types of tasks covered by these datasets, which are not specific enough to identify as datasets.",
      "processing_time": 19.29596447944641,
      "citing_paper_id": "276421789",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "A range of datasets have been introduced to evaluate reasoning, covering tasks in arithmetic [Ling et al., 2017, Cobbe et al., 2021], logic [Chollet, 2019, Wang et al., 2022], common sense [Yang et al., 2018, Geva et al., 2021], and algorithmic reasoning [Yao et al., 2024].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a range of datasets but does not specify their names. It only refers to types of tasks covered by these datasets, which are not specific enough to identify as datasets.",
      "processing_time": 19.29596447944641,
      "citing_paper_id": "276421789",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "We evaluate the arithmetic reasoning of LLMs, on GSM8K [Cobbe et al., 2021] and AQuA [Ling et al., 2017] benchmark.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AQuA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate the arithmetic reasoning capabilities of LLMs, focusing on solving math word problems with step-by-step solutions.",
        "AQuA": "Used to assess the arithmetic reasoning of LLMs, specifically through multiple-choice questions that require understanding and solving mathematical problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, GSM8K and AQuA, which are used to evaluate the arithmetic reasoning capabilities of LLMs.",
      "processing_time": 29.01939845085144,
      "citing_paper_id": "276421789",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "LLMs have been employed as planners or high-level controllers for robotic tasks Liu et al. [2023], Huang et al. [2022] and as agents for web navigation [Deng et al., 2024], scientific discovery [Wang et al., 2024], and autonomous vehicles [Yang et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of LLMs in various domains. No verifiable resources are named.",
      "processing_time": 16.151591062545776,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "LLMs have been employed as planners or high-level controllers for robotic tasks Liu et al. [2023], Huang et al. [2022] and as agents for web navigation [Deng et al., 2024], scientific discovery [Wang et al., 2024], and autonomous vehicles [Yang et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of LLMs in various domains. No verifiable resources are named.",
      "processing_time": 16.151591062545776,
      "citing_paper_id": "276421789",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "LLMs have been employed as planners or high-level controllers for robotic tasks Liu et al. [2023], Huang et al. [2022] and as agents for web navigation [Deng et al., 2024], scientific discovery [Wang et al., 2024], and autonomous vehicles [Yang et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of LLMs in various domains. No verifiable resources are named.",
      "processing_time": 16.151591062545776,
      "citing_paper_id": "276421789",
      "cited_paper_id": 264935408
    },
    {
      "context_text": ") [Wei et al., 2022] and Self Consistency (SC) [Wang et al., 2023] improves.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 17.876744270324707,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246411621
    },
    {
      "context_text": ") [Wei et al., 2022] and Self Consistency (SC) [Wang et al., 2023] improves.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 17.876744270324707,
      "citing_paper_id": "276421789",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For instance, Chain-of-Thought prompting (CoT) [Wei et al., 2022] and its variants [Zhou et al., 2023, Kojima et al., 2022] decompose problems into sequential steps, while self-consistency [Wang et al., 2023] refines CoT by aggregating multiple responses through voting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses methods and approaches in large language models, specifically Chain-of-Thought prompting and self-consistency, but does not mention any specific datasets.",
      "processing_time": 17.875100135803223,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "For instance, Chain-of-Thought prompting (CoT) [Wei et al., 2022] and its variants [Zhou et al., 2023, Kojima et al., 2022] decompose problems into sequential steps, while self-consistency [Wang et al., 2023] refines CoT by aggregating multiple responses through voting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses methods and approaches in large language models, specifically Chain-of-Thought prompting and self-consistency, but does not mention any specific datasets.",
      "processing_time": 17.875100135803223,
      "citing_paper_id": "276421789",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For instance, Chain-of-Thought prompting (CoT) [Wei et al., 2022] and its variants [Zhou et al., 2023, Kojima et al., 2022] decompose problems into sequential steps, while self-consistency [Wang et al., 2023] refines CoT by aggregating multiple responses through voting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses methods and approaches in large language models, specifically Chain-of-Thought prompting and self-consistency, but does not mention any specific datasets.",
      "processing_time": 17.875100135803223,
      "citing_paper_id": "276421789",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Chain of Thought (CoT) enables LLMs to solve complex problems by breaking them into intermediate reasoning steps, improving their logical coherence and accuracy Wei et al. [2022].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain of Thought) for improving LLM performance.",
      "processing_time": 14.748721361160278,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "For instance, Chain-of-Thought [Wei et al., 2022] encourages step-by-step reasoning, while Tree-of-Thought [Yao et al., 2024] chooses optimal reasoning paths using tree search.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (Chain-of-Thought and Tree-of-Thought) but does not reference any specific datasets. The context focuses on reasoning capabilities and methodologies rather than data sources.",
      "processing_time": 20.084203243255615,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246411621
    },
    {
      "context_text": ") [Wei et al., 2022] solves problems through a linear sequence of reasoning steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for solving problems using large language models.",
      "processing_time": 15.696598052978516,
      "citing_paper_id": "276421789",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Self Consistency (SC) extends CoT by generating multiple reasoning paths for a problem and selecting the most consistent answer through majority voting [Wang et al., 2023].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Self Consistency (SC).",
      "processing_time": 15.13829517364502,
      "citing_paper_id": "276421789",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Self-Consistency (SC) [Wang et al., 2023] extends CoT by selecting answers through majority voting over multiple reasoning chains.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Self-Consistency (SC).",
      "processing_time": 15.692805528640747,
      "citing_paper_id": "276421789",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Reasoning as Planning (RAP) Hao et al. [2023] combines Monte Carlo Tree Search (MCTS) with the LLM as a world model to reward reasoning steps and guide tree growth toward the answer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method combining MCTS with LLMs.",
      "processing_time": 15.689282417297363,
      "citing_paper_id": "276421789",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Reasoning as Planning with World Models (RAP) reformulates reasoning as a planning problem, where the LLM acts as both the reasoning agent and the world model [Hao et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach.",
      "processing_time": 14.426133632659912,
      "citing_paper_id": "276421789",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Tree of Thought [Yao et al., 2024], Graph of Thought [Besta et al., 2024], and Monte Carlo Tree Search [Hao et al., 2023, Zhou et al., 2024] enhance problem-solving by systematically exploring reasoning paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets within the text.",
      "processing_time": 18.50854182243347,
      "citing_paper_id": "276421789",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Furthermore, a comparison between ToT and Reasoning as Planning with World Models (RAP) [Hao et al., 2023] shows that RAP outperforms ToT in planning and arithmetic reasoning tasks but lags in commonsense reasoning while performing equally in algorithmic reasoning tasks.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (ToT and RAP).",
      "processing_time": 17.424131393432617,
      "citing_paper_id": "276421789",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Building on their success in these diverse domains, researchers are increasingly using LLMs as AI agents [Deng et al., 2024, Wang et al., 2024] for complex tasks, such as robotics [Liu et al., 2023] and scientific discovery [Wang et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and applications of LLMs. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 21.007482528686523,
      "citing_paper_id": "276421789",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "Our evaluation focuses on four planning problems: BlocksWorld [Valmeekam et al., 2023], Rubik’s Cube [Ding et al., 2024], TripPlan, and CalendarPlan [Zheng et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific planning problems but does not refer to any datasets, models, or methods. The names provided are problem domains rather than verifiable resources.",
      "processing_time": 20.453869819641113,
      "citing_paper_id": "276421789",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "LLM Planning involves constructing a sequence of actions to achieve defined goals [Valmeekam et al., 2023, Zheng et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general concept of LLM Planning. No verifiable resources are identified.",
      "processing_time": 19.590460777282715,
      "citing_paper_id": "276421789",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "Beyond NLP, LLMs have also been adapted for multimodal tasks involving vision [Parashar et al., 2024, Lin et al., 2025] and audio [Wu et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multimodal tasks involving vision and audio. No verifiable resources are named.",
      "processing_time": 27.581164836883545,
      "citing_paper_id": "276421789",
      "cited_paper_id": 267094920
    },
    {
      "context_text": "Beyond NLP, LLMs have also been adapted for multimodal tasks involving vision [Parashar et al., 2024, Lin et al., 2025] and audio [Wu et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multimodal tasks involving vision and audio. No verifiable resources are named.",
      "processing_time": 27.581164836883545,
      "citing_paper_id": "276421789",
      "cited_paper_id": 267759583
    },
    {
      "context_text": "Despite their broad adoption, studies reveal that LLMs often struggle to generate valid plans for complex tasks [Kambhampati et al., 2024, Xie et al., 2024].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs struggle with generating valid plans for complex tasks. No verifiable resources are identified.",
      "processing_time": 28.974542140960693,
      "citing_paper_id": "276421789",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "Inference Time Techniques for LLMs are methods applied during output generation to improve performance, and alignment with downstream tasks [Welleck et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques for improving LLM performance during inference time.",
      "processing_time": 27.349321365356445,
      "citing_paper_id": "276421789",
      "cited_paper_id": 270703266
    },
    {
      "context_text": "These methods have demonstrated strong results in arithmetic [Kumar et al., 2024a] and gameplay tasks [Schultz et al., 2024].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance in arithmetic and gameplay tasks.",
      "processing_time": 27.144306898117065,
      "citing_paper_id": "276421789",
      "cited_paper_id": 274788614
    },
    {
      "context_text": "…advancements in embodied artificial intelligence (AI) have attracted significant attention due to their ability to process natural language instructions, perceive environments, engage in reasoning, and interact with the physical world (Anderson et al., 2018; Hua et al., 2021; Károly et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in embodied AI and references to papers that discuss vision-and-language navigation.",
      "processing_time": 28.682910442352295,
      "citing_paper_id": "277955666",
      "cited_paper_id": 4673790
    },
    {
      "context_text": "To optimize Safe-Align, we reconstruct the existing embodied dataset, ActivityPrograms (Puig et al., 2018), to create a corresponding pairwise safety preference dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ActivityPrograms"
      ],
      "dataset_descriptions": {
        "ActivityPrograms": "Reconstructed to create a pairwise safety preference dataset, focusing on optimizing Safe-Align by simulating household activities via programs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the reconstruction of an existing embodied dataset, ActivityPrograms, to create a new safety preference dataset. ActivityPrograms is a specific, verifiable dataset.",
      "processing_time": 37.88244295120239,
      "citing_paper_id": "277955666",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "In the SFT phase, we use two parts of the training data: we split the ActivityPro-grams (Puig et al., 2018)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ActivityPrograms"
      ],
      "dataset_descriptions": {
        "ActivityPrograms": "Used to train models in the Supervised Fine-Tuning (SFT) phase, focusing on simulating household activities via programs. The dataset provides structured activity sequences for training."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'ActivityPrograms' which appears to be a specific dataset used for training. The cited paper title 'VirtualHome: Simulating Household Activities Via Programs' supports this interpretation.",
      "processing_time": 39.79328417778015,
      "citing_paper_id": "277955666",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "Unlike existing benchmarks (Puig et al., 2018; Savva et al., 2019; Shridhar et al., 2020; Li et al., 2024), which are primarily used to assess the task execution capabilities of embodied agents, SafePlan-Bench is designed to evaluate the safety of embodied task planning.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SafePlan-Bench' as a new benchmark designed to evaluate the safety of embodied task planning, distinguishing it from existing benchmarks. However, 'SafePlan-Bench' is not a traditional dataset but a benchmark. No other specific datasets are mentioned.",
      "processing_time": 22.86303734779358,
      "citing_paper_id": "277955666",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "Unlike existing benchmarks (Puig et al., 2018; Savva et al., 2019; Shridhar et al., 2020; Li et al., 2024), which are primarily used to assess the task execution capabilities of embodied agents, SafePlan-Bench is designed to evaluate the safety of embodied task planning.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SafePlan-Bench' as a new benchmark designed to evaluate the safety of embodied task planning, distinguishing it from existing benchmarks. However, 'SafePlan-Bench' is not a traditional dataset but a benchmark. No other specific datasets are mentioned.",
      "processing_time": 22.86303734779358,
      "citing_paper_id": "277955666",
      "cited_paper_id": 91184540
    },
    {
      "context_text": "Finally, we design a modular and extensible safety detector, further integrating it into the VirtualHome (Puig et al., 2018) simulator to jointly evaluate task success and safety.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the VirtualHome simulator but does not refer to it as a dataset. It is used as a simulation environment for evaluating task success and safety.",
      "processing_time": 44.950642824172974,
      "citing_paper_id": "277955666",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "Specifically, we evaluate the task-planning safety of various embodied agents on the VirtualHome (Puig et al., 2018) and Behavior-1k (Li et al., 2024) benchmarks, which encompass a wide range of tasks, and record their performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VirtualHome",
        "Behavior-1k"
      ],
      "dataset_descriptions": {
        "VirtualHome": "Used to evaluate task-planning safety of embodied agents, focusing on simulating household activities via programs.",
        "Behavior-1k": "Used to evaluate task-planning safety of embodied agents, encompassing a wide range of tasks and recording performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two benchmarks, VirtualHome and Behavior-1k, which are used to evaluate task-planning safety of embodied agents. These are specific, verifiable resources.",
      "processing_time": 44.036322832107544,
      "citing_paper_id": "277955666",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "For embodied agents, a key objective is to assist humans in completing tasks (Savva et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general objective for embodied agents. No verifiable resources are identified.",
      "processing_time": 15.677558660507202,
      "citing_paper_id": "277955666",
      "cited_paper_id": 91184540
    },
    {
      "context_text": "Then, based on the Bradley-Terry model (Bradley and Terry, 1952), we derive our objective: Details of the statement and proof can be found in Appendix C.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a statistical model. No dataset names are present in the citation span.",
      "processing_time": 28.948245525360107,
      "citing_paper_id": "277955666",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "• Lora (Hu et al., 2021) adapts the pre-trained LLMs to the embodied domain.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting pre-trained LLMs to the embodied domain.",
      "processing_time": 13.467833042144775,
      "citing_paper_id": "277955666",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "To generate diverse tasks automatically and efficiently, we use LLMs, such as GPT-4, as the core tool (Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of LLMs like GPT-4. No verifiable resources are identified.",
      "processing_time": 18.43605875968933,
      "citing_paper_id": "277955666",
      "cited_paper_id": 254877310
    },
    {
      "context_text": "Inspired by DPO (Rafailov et al., 2023), we propose Safe-Align which treats physical-world safety knowledge as a form of human preference and aligns low-level action sequences step by step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DPO) and a proposed approach (Safe-Align).",
      "processing_time": 29.25084662437439,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Finally, we obtain our objective by substituting Equation 14 into DPO objective as Equation 5.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to mathematical equations.",
      "processing_time": 26.716835498809814,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "DPO is one of the most popular preference optimization methods currently available.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DPO. There are no verifiable resources or datasets mentioned.",
      "processing_time": 29.538706302642822,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "DPO by-passes the process of fitting a preference model by reparameterizing the reward function r using a closed-form expression with the optimal policy: where Z ( x ) = y π ref ( y | x ) exp 1 β r ( x, y ) is the partition function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for reparameterizing the reward function in reinforcement learning.",
      "processing_time": 13.904045820236206,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Direct Preference Optimization (DPO) (Rafailov et al., 2023) simplifies the RLHF by eliminating the need for reward models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DPO. The context is about simplifying RLHF, which is a process, not a dataset.",
      "processing_time": 20.605568647384644,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "First, compared to the world-model and Lora fine-tuning methods, DPO, SimPO, and Safe-Align all exhibit better safety performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their comparative performance.",
      "processing_time": 14.362365007400513,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "To address this issue, we treat atomic actions as fundamental optimization units, drawing inspiration from Step-DPO (Lai et al., 2024), and focus on learning the actions where errors first emerge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Step-DPO) which is not a dataset.",
      "processing_time": 18.14461398124695,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "By incorporating this reward formulation into the BradleyTerry (BT) ranking objective: DPO policy objective becomes: SimPO observe that: (1) DPO requires a reference model π ref during trainging, which incurs additional memory and computational costs; (2) there is a mismatch between the reward ranking and the log-liklihood ranking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects of DPO and SimPO.",
      "processing_time": 14.080450296401978,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Aligning LLMs with human preferences is a widely adopted and effective approach to address this challenge (Ouyang et al., 2022; Bai et al., 2022; Rafailov et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods for aligning LLMs with human preferences.",
      "processing_time": 18.139196634292603,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "We focus more on the location where error occur, and therefore assign a smaller weight to the corresponding parts in the reward function: By plugging Equation.12 into Equation.10, we obtain our objective as Equation 4 C.3 Derivation of the Adjusted DPO Approach For a given preference sample pair: y w = { a w 1 , a w 2 , . . . , a w | y w | } , y l = { a l 1 , a l 2 , . . . , a l | y l | } , where the first k actions are identical.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only equations and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 19.19055938720703,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Thus, we can obtain the modified likelihood ˆ π θ ( y | x ) : specifically, if µ = 1 the approach reduces to the standard DPO algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a modification to an algorithm. There are no verifiable resources or datasets mentioned.",
      "processing_time": 30.31971049308777,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Then the likelihood of response in DPO can be rewritten as: Our objective is to reduce the focus of DPO on the redundancy the identical action sequences while preserving safety preference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a method (DPO) and its objective. No verifiable resources are identified.",
      "processing_time": 16.42020273208618,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Additionally, although DPO shows an improvement in safety compared to the baselines, Rouge-L drops significantly, indicating that DPO sacrifices task-planning ability in favor of learning safety.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (Rouge-L) and a method (DPO). No verifiable resources are identified.",
      "processing_time": 19.522019863128662,
      "citing_paper_id": "277955666",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Previous studies (Huang et al., 2023; Ren et al., 2023; Liu et al., 2024a) have highlighted internal issues within LLMs, particularly regarding hallucinations and knowledge misalignment.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only internal issues within LLMs such as hallucinations and knowledge misalignment.",
      "processing_time": 14.646538734436035,
      "citing_paper_id": "277955666",
      "cited_paper_id": 259342058
    },
    {
      "context_text": "Moreover, in embodied AI, LLM-Personalize (Han et al., 2024) aligns LLM planners with human preferences through Reinforced Self-Training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LLM-Personalize) and a technique (Reinforced Self-Training).",
      "processing_time": 20.37593150138855,
      "citing_paper_id": "277955666",
      "cited_paper_id": 269293529
    },
    {
      "context_text": "CLMASP (Lin et al., 2024) refers to the process of applying a logical post-processing step to the output.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or process called CLMASP. The title confirms that CLMASP is a method, not a dataset.",
      "processing_time": 20.373161792755127,
      "citing_paper_id": "277955666",
      "cited_paper_id": 270258182
    },
    {
      "context_text": "All baseline methods and our approach underwent a unified post-processing procedure (Lin et al., 2024). and the experimental results presented are based on single-run experiments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a post-processing procedure and experimental setup.",
      "processing_time": 26.01545286178589,
      "citing_paper_id": "277955666",
      "cited_paper_id": 270258182
    },
    {
      "context_text": "Additionally, we perform an executability check (Lin et al., 2024) on the generated unsafe action sequence y l to ensure that it is both executable and presents potential safety hazards.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for checking the executability of generated actions.",
      "processing_time": 14.062601089477539,
      "citing_paper_id": "277955666",
      "cited_paper_id": 270258182
    },
    {
      "context_text": "1-8B-Instruct, Llama-3-8B, and Mistral-7B-Instruct-v0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions model names, which are excluded according to the instructions. No datasets are mentioned.",
      "processing_time": 26.683571815490723,
      "citing_paper_id": "277955666",
      "cited_paper_id": null
    },
    {
      "context_text": "1-8B-Instruct, Llama-3-8B and Mistral-7B-Instruct-v0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no verifiable resources or datasets mentioned.",
      "processing_time": 28.346676111221313,
      "citing_paper_id": "277955666",
      "cited_paper_id": null
    },
    {
      "context_text": "We perform Safe-Align with two families of models, Llama-3-8B (Dubey et al., 2024) and Mistral-7B (Jiang et al., 2023), and apply it to Llama-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 26.872838258743286,
      "citing_paper_id": "277955666",
      "cited_paper_id": null
    },
    {
      "context_text": "However, our preliminary experiments reveal that, due to the potential biases present in LLMs, their detection results are unreliable (Chen et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to potential biases in LLMs.",
      "processing_time": 28.09646201133728,
      "citing_paper_id": "277955666",
      "cited_paper_id": null
    },
    {
      "context_text": "Similar to (Singh et al., 2023; Hao et al., 2023), we define success as follows: during the execution of a script, the state should reach the final state obtained by executing the ground truth script, which includes both the states of the objects in the environment and the relationships between the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or other resources. It focuses on defining success criteria for script execution in a planning context.",
      "processing_time": 18.364967107772827,
      "citing_paper_id": "277955666",
      "cited_paper_id": null
    },
    {
      "context_text": "We show that Pyperplan equipped with the heuristics from DeepSeek R1 ( h R1 ) surpasses commonly used heuristics implemented in Fast Downward [38], a state-of-the-art planner written in C++. More-over, h R1 is also competitive with h WLFGPR [11], the state-of-the-art in heuristic learning for…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods. The citation is focused on comparing heuristics and planners, not on using a particular dataset.",
      "processing_time": 20.354973077774048,
      "citing_paper_id": "277940718",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Despite Pyperplan being much slower compared to state-of-the-art planners [38, 47, 66] due to its Python implementation, our method outperforms h FF also in the standard C++ implementations available in Fast Downward [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to planning systems and methods. No verifiable resources are identified.",
      "processing_time": 16.777287244796753,
      "citing_paper_id": "277940718",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Despite Pyperplan being much slower compared to state-of-the-art planners [38, 47, 66] due to its Python implementation, our method outperforms h FF also in the standard C++ implementations available in Fast Downward [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to planning systems and methods. No verifiable resources are identified.",
      "processing_time": 16.777287244796753,
      "citing_paper_id": "277940718",
      "cited_paper_id": 19104534
    },
    {
      "context_text": "Despite Pyperplan being much slower compared to state-of-the-art planners [38, 47, 66] due to its Python implementation, our method outperforms h FF also in the standard C++ implementations available in Fast Downward [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to planning systems and methods. No verifiable resources are identified.",
      "processing_time": 16.777287244796753,
      "citing_paper_id": "277940718",
      "cited_paper_id": 273589540
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 5753706
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6552475
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 11337237
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 19104534
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 19182808
    },
    {
      "context_text": "Nowadays, most classical planners rely on heuristic search algorithms to ﬁnd plans [6, 41, 38, 56, 47, 78, 69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planners and heuristic search algorithms. No verifiable resources are identified.",
      "processing_time": 15.572787046432495,
      "citing_paper_id": "277940718",
      "cited_paper_id": 210928826
    },
    {
      "context_text": "Moreover, all of these planners are implemented on top of the Fast Downward planning system [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Fast Downward planning system, which is a method or tool, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 19.120188236236572,
      "citing_paper_id": "277940718",
      "cited_paper_id": 17305
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 1478442
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 2432261
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6552475
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 16652519
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 19104534
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 245827334
    },
    {
      "context_text": "While there are lots of search improvements that one could evaluate on top of GBFS [e.g., 41, 47, 55, 58], we limit ourselves to “pure” GBFS planners as this is the most commonly used version in the classical planning literature [e.g., 16, 23, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to various planning systems and methods. The focus is on the evaluation of 'pure' GBFS planners, which are methods, not datasets.",
      "processing_time": 30.882188081741333,
      "citing_paper_id": "277940718",
      "cited_paper_id": 265957829
    },
    {
      "context_text": "The combination of planning and learning to create heuristic functions has a long tradition [62, 12, 61, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies and theories. The cited papers also do not provide clear dataset names.",
      "processing_time": 19.115183115005493,
      "citing_paper_id": "277940718",
      "cited_paper_id": 2126705
    },
    {
      "context_text": "The combination of planning and learning to create heuristic functions has a long tradition [62, 12, 61, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies and theories. The cited papers also do not provide clear dataset names.",
      "processing_time": 19.115183115005493,
      "citing_paper_id": "277940718",
      "cited_paper_id": 7138783
    },
    {
      "context_text": "The combination of planning and learning to create heuristic functions has a long tradition [62, 12, 61, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies and theories. The cited papers also do not provide clear dataset names.",
      "processing_time": 19.115183115005493,
      "citing_paper_id": "277940718",
      "cited_paper_id": 10448069
    },
    {
      "context_text": "The combination of planning and learning to create heuristic functions has a long tradition [62, 12, 61, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies and theories. The cited papers also do not provide clear dataset names.",
      "processing_time": 19.115183115005493,
      "citing_paper_id": "277940718",
      "cited_paper_id": 17099383
    },
    {
      "context_text": "We then include the following ﬁle contents to the prompt: Items 4–9 illustrate what domain-dependent heuristics can look like for two example domains [49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only domain-dependent heuristics. No clear identifiers for datasets are present.",
      "processing_time": 16.017903804779053,
      "citing_paper_id": "277940718",
      "cited_paper_id": 2730999
    },
    {
      "context_text": "Classical planning tasks are usually described using the Planning Domain Deﬁnition Language (PDDL) [49, 36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PDDL, which is a language for describing planning tasks, not a dataset. No specific datasets are mentioned.",
      "processing_time": 29.755446672439575,
      "citing_paper_id": "277940718",
      "cited_paper_id": 2730999
    },
    {
      "context_text": "Classical planning tasks are usually described using the Planning Domain Deﬁnition Language (PDDL) [49, 36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PDDL, which is a language for describing planning tasks, not a dataset. No specific datasets are mentioned.",
      "processing_time": 29.755446672439575,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "…implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg [37]; and the additive heuristic, h add [6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various heuristics and methods but does not refer to any specific datasets. The cited papers are about planning heuristics and algorithms, not datasets.",
      "processing_time": 18.735431671142578,
      "citing_paper_id": "277940718",
      "cited_paper_id": 5642010
    },
    {
      "context_text": "…implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg [37]; and the additive heuristic, h add [6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various heuristics and methods but does not refer to any specific datasets. The cited papers are about planning heuristics and algorithms, not datasets.",
      "processing_time": 18.735431671142578,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6552475
    },
    {
      "context_text": "…implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg [37]; and the additive heuristic, h add [6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various heuristics and methods but does not refer to any specific datasets. The cited papers are about planning heuristics and algorithms, not datasets.",
      "processing_time": 18.735431671142578,
      "citing_paper_id": "277940718",
      "cited_paper_id": 8065594
    },
    {
      "context_text": "…implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg [37]; and the additive heuristic, h add [6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various heuristics and methods but does not refer to any specific datasets. The cited papers are about planning heuristics and algorithms, not datasets.",
      "processing_time": 18.735431671142578,
      "citing_paper_id": "277940718",
      "cited_paper_id": 11337237
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6384075
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 10684261
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 126643659
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 235349223
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 235586367
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 247778891
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 248721898
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 256274690
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 259854663
    },
    {
      "context_text": "Among other options, this knowledge can take the form of policies [e.g., 25, 75], heuristics [e.g., 73, 11], sketches [e.g., 18, 19], planning programs [e.g., 64, 65], or planner conﬁgurations [e.g., 20, 67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various forms of knowledge representation and planning methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 17.704450845718384,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268680720
    },
    {
      "context_text": "The LLM-generated heuristics outperform state-of-the-art heuristics, such as h FF [41], in terms of solved tasks and are competitive in the number of required state expansions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a planning system and heuristic methods. The context is about comparing performance of LLM-generated heuristics with existing heuristics.",
      "processing_time": 18.728938341140747,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6552475
    },
    {
      "context_text": "We now compare the LLM-generated heuristics to two baselines: breadth-ﬁrst search (BrFS), which uses no heuristic guidance, 2 and GBFS with the h FF heuristic [41], which is one of the most commonly used heuristics for satisﬁcing planning [e.g., 8, 13, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It discusses heuristics and planning methods but does not reference any named, verifiable datasets.",
      "processing_time": 28.837756633758545,
      "citing_paper_id": "277940718",
      "cited_paper_id": 6552475
    },
    {
      "context_text": "We now compare the LLM-generated heuristics to two baselines: breadth-ﬁrst search (BrFS), which uses no heuristic guidance, 2 and GBFS with the h FF heuristic [41], which is one of the most commonly used heuristics for satisﬁcing planning [e.g., 8, 13, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It discusses heuristics and planning methods but does not reference any named, verifiable datasets.",
      "processing_time": 28.837756633758545,
      "citing_paper_id": "277940718",
      "cited_paper_id": 265130666
    },
    {
      "context_text": "We now compare the LLM-generated heuristics to two baselines: breadth-ﬁrst search (BrFS), which uses no heuristic guidance, 2 and GBFS with the h FF heuristic [41], which is one of the most commonly used heuristics for satisﬁcing planning [e.g., 8, 13, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It discusses heuristics and planning methods but does not reference any named, verifiable datasets.",
      "processing_time": 28.837756633758545,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "We now compare the LLM-generated heuristics to two baselines: breadth-ﬁrst search (BrFS), which uses no heuristic guidance, 2 and GBFS with the h FF heuristic [41], which is one of the most commonly used heuristics for satisﬁcing planning [e.g., 8, 13, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It discusses heuristics and planning methods but does not reference any named, verifiable datasets.",
      "processing_time": 28.837756633758545,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "…R1 heuristics, to GBFS in Fast Downward using one of the many satisﬁcing heuristics implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only heuristics and methods used in planning algorithms. There are no clear identifiers for datasets.",
      "processing_time": 28.569645881652832,
      "citing_paper_id": "277940718",
      "cited_paper_id": 8623866
    },
    {
      "context_text": "…using one of the many satisﬁcing heuristics implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several heuristics but does not reference any specific datasets. The cited paper title 'Landmarks Revisited' does not indicate a dataset.",
      "processing_time": 19.929755210876465,
      "citing_paper_id": "277940718",
      "cited_paper_id": 10791590
    },
    {
      "context_text": "Both domains admit polynomial solving strategies: Blocksworld is 2-approximable [33] (by destroying all stacks and then building the goal stacks) and even optimal Spanner plans can be computed in polynomial time (by picking up exactly the ﬁrst n spanners).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses planning strategies in the Blocks-World domain.",
      "processing_time": 13.10633373260498,
      "citing_paper_id": "277940718",
      "cited_paper_id": 11481542
    },
    {
      "context_text": "In heuristic search algorithms—such as A [35], weighted-A [54], and greedy best-ﬁrst search [17]—this heuristic guides the search towards promising states and thereby reduces the search effort.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses heuristic search algorithms but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 15.541202783584595,
      "citing_paper_id": "277940718",
      "cited_paper_id": 21698093
    },
    {
      "context_text": "In heuristic search algorithms—such as A [35], weighted-A [54], and greedy best-ﬁrst search [17]—this heuristic guides the search towards promising states and thereby reduces the search effort.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses heuristic search algorithms but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 15.541202783584595,
      "citing_paper_id": "277940718",
      "cited_paper_id": 206799161
    },
    {
      "context_text": "In heuristic search algorithms—such as A [35], weighted-A [54], and greedy best-ﬁrst search [17]—this heuristic guides the search towards promising states and thereby reduces the search effort.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses heuristic search algorithms but does not mention any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 15.541202783584595,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "We include the distilled version to Qwen 14B [3] to evaluate the impact of smaller models in our pipeline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (Qwen 14B) and discusses evaluating smaller models in a pipeline.",
      "processing_time": 17.705299615859985,
      "citing_paper_id": "277940718",
      "cited_paper_id": 28467205
    },
    {
      "context_text": "For Gripper, we provide a Python function computing the perfect heuristic h ∗ as input, while for Logistics we encode the simple “single visit and load/unload counting heuristic” by Paul et al. [52].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or heuristics. The context is about encoding heuristics for logistics planning problems.",
      "processing_time": 29.986632108688354,
      "citing_paper_id": "277940718",
      "cited_paper_id": 32556638
    },
    {
      "context_text": "The search is usually guided by a heuristic function h which maps each state s to an estimate h ( s ) ∈ R +0 ∪ ∞ that estimates the cost of reaching the goal state from s [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a heuristic function used in search algorithms.",
      "processing_time": 13.370503425598145,
      "citing_paper_id": "277940718",
      "cited_paper_id": 38193578
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 208512881
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 219188633
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 237581157
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 250334236
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 250406114
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 254017827
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 266359688
    },
    {
      "context_text": "There are two main paradigms for learning heuristic functions in classical planning: task-dependent [21, 22, 51, 4] and domain-dependent [70, 74, 10, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only paradigms for learning heuristic functions in classical planning. No verifiable resources are identified.",
      "processing_time": 19.903151512145996,
      "citing_paper_id": "277940718",
      "cited_paper_id": 271504408
    },
    {
      "context_text": "We used a disjoint set of ten domains from the Autoscale benchmark set [79] for exploratory experiments while developing our pipeline.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Autoscale benchmark set"
      ],
      "dataset_descriptions": {
        "Autoscale benchmark set": "Used for exploratory experiments during pipeline development, focusing on classical planning capabilities across ten distinct domains."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Autoscale benchmark set' which is a specific benchmark set used for exploratory experiments in classical planning. It is a reusable resource and fits the criteria for a dataset.",
      "processing_time": 24.060574769973755,
      "citing_paper_id": "277940718",
      "cited_paper_id": 235428843
    },
    {
      "context_text": "We chose Python because LLMs generate correct code for Python more often than for other languages [45], and because the code injection is simpler in Python due to its ﬂexibility and due to it being an interpreted language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a preference for Python in the context of LLMs generating code.",
      "processing_time": 18.3511164188385,
      "citing_paper_id": "277940718",
      "cited_paper_id": 246527904
    },
    {
      "context_text": "This idea is similar to what has been done by AlphaCode [45] in the context of code generation, and to parallel sampling [7] for test-time compute [72, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not introduce new datasets either.",
      "processing_time": 15.505933046340942,
      "citing_paper_id": "277940718",
      "cited_paper_id": 246527904
    },
    {
      "context_text": "This idea is similar to what has been done by AlphaCode [45] in the context of code generation, and to parallel sampling [7] for test-time compute [72, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not introduce new datasets either.",
      "processing_time": 15.505933046340942,
      "citing_paper_id": "277940718",
      "cited_paper_id": 271571035
    },
    {
      "context_text": "This idea is similar to what has been done by AlphaCode [45] in the context of code generation, and to parallel sampling [7] for test-time compute [72, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not introduce new datasets either.",
      "processing_time": 15.505933046340942,
      "citing_paper_id": "277940718",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "This idea is similar to what has been done by AlphaCode [45] in the context of code generation, and to parallel sampling [7] for test-time compute [72, 44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not introduce new datasets either.",
      "processing_time": 15.505933046340942,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276482470
    },
    {
      "context_text": "A common issue when using LLMs for planning is that they produce invalid plans [82, 83].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general issue with LLMs producing invalid plans. No verifiable resources are identified.",
      "processing_time": 28.282763719558716,
      "citing_paper_id": "277940718",
      "cited_paper_id": 256846992
    },
    {
      "context_text": "A common issue when using LLMs for planning is that they produce invalid plans [82, 83].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general issue with LLMs producing invalid plans. No verifiable resources are identified.",
      "processing_time": 28.282763719558716,
      "citing_paper_id": "277940718",
      "cited_paper_id": 273098440
    },
    {
      "context_text": "For example, there is an extensive body of work exploring the potential of LLMs to convert problems described in natural language into PDDL tasks [e.g., 32, 26, 50, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers exploring LLMs' capabilities in converting natural language to PDDL tasks.",
      "processing_time": 17.04693031311035,
      "citing_paper_id": "277940718",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "For example, there is an extensive body of work exploring the potential of LLMs to convert problems described in natural language into PDDL tasks [e.g., 32, 26, 50, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers exploring LLMs' capabilities in converting natural language to PDDL tasks.",
      "processing_time": 17.04693031311035,
      "citing_paper_id": "277940718",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "For example, there is an extensive body of work exploring the potential of LLMs to convert problems described in natural language into PDDL tasks [e.g., 32, 26, 50, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers exploring LLMs' capabilities in converting natural language to PDDL tasks.",
      "processing_time": 17.04693031311035,
      "citing_paper_id": "277940718",
      "cited_paper_id": 269614003
    },
    {
      "context_text": "For example, there is an extensive body of work exploring the potential of LLMs to convert problems described in natural language into PDDL tasks [e.g., 32, 26, 50, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers exploring LLMs' capabilities in converting natural language to PDDL tasks.",
      "processing_time": 17.04693031311035,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": 258865907
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": 260440590
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": 269614003
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276079564
    },
    {
      "context_text": "In the context of classical planning, LLMs have been used, for example, to plan directly [e.g., 81], to create planning models from natural language [e.g., 32, 26, 50], to compute generalized policies [71], and to create heuristics for numeric planning [80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general uses of LLMs in planning. No clear, verifiable resource names are provided.",
      "processing_time": 29.148289918899536,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "Silver et al. [71] also use LLMs to generate Python code for solving classical planning tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for generating Python code to solve planning tasks.",
      "processing_time": 16.328053951263428,
      "citing_paper_id": "277940718",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "…one of the many satisﬁcing heuristics implemented in the planner: the goal-count heuristic, h GC [24]; the landmark count heuristic, h lmc [57, 9]; the C++ implementation of the FF heuristic, h FF [41]; the context-enhanced additive heuristic, h cea [39]; the causal graph heuristic, h cg…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several heuristics but does not refer to any specific datasets. The context is focused on describing various heuristics used in planning, which are methods rather than datasets.",
      "processing_time": 31.674747228622437,
      "citing_paper_id": "277940718",
      "cited_paper_id": 259320426
    },
    {
      "context_text": "Yet, Valmeekam et al. [81] show that LLMs cannot reliably solve even small classical planning tasks when used for end-to-end plan generation.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a critique of LLMs' planning abilities.",
      "processing_time": 14.538418531417847,
      "citing_paper_id": "277940718",
      "cited_paper_id": 260440590
    },
    {
      "context_text": "Classical planning is a fundamental problem in Artiﬁcial Intelligence (AI), with applications ranging from robotics to game playing [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to classical planning in AI.",
      "processing_time": 26.803868532180786,
      "citing_paper_id": "277940718",
      "cited_paper_id": 262269888
    },
    {
      "context_text": "Outside the area of PDDL planning, Romera-Paredes et al. [59] use a search in function space to help to solve combinatorial problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for solving combinatorial problems using function space search.",
      "processing_time": 16.690313816070557,
      "citing_paper_id": "277940718",
      "cited_paper_id": 266223700
    },
    {
      "context_text": "To generate the heuristics, we use the APIs from two different families of LLMs: Gemini [29, 30], with the models Gemini 2.0 Flash (stable release 001) and Gemini 2.0 Flash Thinking (version 01-21); and DeepSeek [14, 15], with the models DeepSeek V3, DeepSeek R1 Distill Qwen 14B, and DeepSeek R1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and APIs from LLMs but does not refer to any specific datasets. The focus is on the models themselves rather than datasets.",
      "processing_time": 29.956530809402466,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268297180
    },
    {
      "context_text": "To generate the heuristics, we use the APIs from two different families of LLMs: Gemini [29, 30], with the models Gemini 2.0 Flash (stable release 001) and Gemini 2.0 Flash Thinking (version 01-21); and DeepSeek [14, 15], with the models DeepSeek V3, DeepSeek R1 Distill Qwen 14B, and DeepSeek R1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and APIs from LLMs but does not refer to any specific datasets. The focus is on the models themselves rather than datasets.",
      "processing_time": 29.956530809402466,
      "citing_paper_id": "277940718",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "To generate the heuristics, we use the APIs from two different families of LLMs: Gemini [29, 30], with the models Gemini 2.0 Flash (stable release 001) and Gemini 2.0 Flash Thinking (version 01-21); and DeepSeek [14, 15], with the models DeepSeek V3, DeepSeek R1 Distill Qwen 14B, and DeepSeek R1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and APIs from LLMs but does not refer to any specific datasets. The focus is on the models themselves rather than datasets.",
      "processing_time": 29.956530809402466,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "…from DeepSeek R1 ( h R1 ) surpasses commonly used heuristics implemented in Fast Downward [38], a state-of-the-art planner written in C++. More-over, h R1 is also competitive with h WLFGPR [11], the state-of-the-art in heuristic learning for classical planning implemented on top of Fast Downward.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons between different heuristic methods and planners.",
      "processing_time": 12.674062013626099,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268680720
    },
    {
      "context_text": "We also compare it to h WLFGPR [11], which uses statistical learning methods together with the Weisfeiler-Leman algorithm to learn domain-dependent heuristics, and is considered the state-of-the-art in classical planning for heuristic learning. h WLFGPR is also implemented on top of Fast Downward.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (h WLFGPR) and a planner (Fast Downward).",
      "processing_time": 14.921265602111816,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268680720
    },
    {
      "context_text": "Currently, the strongest approach in domain-dependent heuristic learning is h WLFGPR [11], which we compare to above.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (h WLFGPR) for heuristic learning. The context is focused on comparing approaches rather than using a dataset.",
      "processing_time": 18.022600173950195,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268680720
    },
    {
      "context_text": "For example, the winners of all tracks of the last IPC, in 2023, are implemented in C++ [77].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the implementation language of the winners of the 2023 International Planning Competition.",
      "processing_time": 15.496972560882568,
      "citing_paper_id": "277940718",
      "cited_paper_id": 268961846
    },
    {
      "context_text": "Katz et al. [43] highlight the high computational cost of using LLMs for end-to-end plan generation, particularly when multiple inferences are required.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the computational cost of using LLMs for plan generation.",
      "processing_time": 14.219362020492554,
      "citing_paper_id": "277940718",
      "cited_paper_id": 269214439
    },
    {
      "context_text": "Moreover, techniques such as supervised ﬁne-tuning and chain-of-thought fail to generalize to out-of-distribution tasks [5, 76], and even LLMs explicitly designed for reasoning tasks cannot solve typical planning problems [83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and limitations of LLMs in planning tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 18.289817571640015,
      "citing_paper_id": "277940718",
      "cited_paper_id": 269626390
    },
    {
      "context_text": "Moreover, techniques such as supervised ﬁne-tuning and chain-of-thought fail to generalize to out-of-distribution tasks [5, 76], and even LLMs explicitly designed for reasoning tasks cannot solve typical planning problems [83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and limitations of LLMs in planning tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 18.289817571640015,
      "citing_paper_id": "277940718",
      "cited_paper_id": 270620065
    },
    {
      "context_text": "Moreover, techniques such as supervised ﬁne-tuning and chain-of-thought fail to generalize to out-of-distribution tasks [5, 76], and even LLMs explicitly designed for reasoning tasks cannot solve typical planning problems [83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and limitations of LLMs in planning tasks. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 18.289817571640015,
      "citing_paper_id": "277940718",
      "cited_paper_id": 273098440
    },
    {
      "context_text": "Nonetheless, Rossetti et al. [60] show that a GPT model trained from scratch on solved planning tasks from a ﬁxed domain can achieve competitive performance compared to other learning approaches when training and test sets share the same distribution.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'solved planning tasks from a fixed domain' but does not specify a named dataset. The reference is to a method or finding rather than a reusable dataset.",
      "processing_time": 19.03825044631958,
      "citing_paper_id": "277940718",
      "cited_paper_id": 270158113
    },
    {
      "context_text": "Arguably, the LLM could have created a simpler heuristic if the implicit assumptions of the domain were explicit [31]: the PDDL domain allows for arbitrary connections between locations, but all instances assume a one-way corridor.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about the representation of planning domains in PDDL.",
      "processing_time": 15.202505111694336,
      "citing_paper_id": "277940718",
      "cited_paper_id": 270170904
    },
    {
      "context_text": "This is similar to what Brown et al. [7] report, and in their case, some problems required even larger temperature.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a finding about temperature settings in sampling.",
      "processing_time": 14.92904782295227,
      "citing_paper_id": "277940718",
      "cited_paper_id": 271571035
    },
    {
      "context_text": "Additionally, Huang et al. [42] use reinforcement learning with partial rewards to increase the LLM performance in end-to-end plan generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (reinforcement learning with partial rewards).",
      "processing_time": 15.483020544052124,
      "citing_paper_id": "277940718",
      "cited_paper_id": 274776284
    },
    {
      "context_text": "In contrast to the work by Tuisov et al. [80], we do not need to generate heuristics for each task we want to solve.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological difference from another work.",
      "processing_time": 13.554321765899658,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276079564
    },
    {
      "context_text": "The work by Tuisov et al. [80] is the most similar to ours, and we draw inspiration from their work.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a similar work.",
      "processing_time": 14.54681944847107,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276079564
    },
    {
      "context_text": "Our approach is cheaper than the one by Tuisov et al. [80] because their method generates multiple heuristics per task, which increases the costs proportionally to the number of tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. The context is about the cost-effectiveness of the approaches, not the use of datasets.",
      "processing_time": 19.02603769302368,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276079564
    },
    {
      "context_text": "Another recent approach is the one by Ling et al. [46], where an LLM generates a set of candidate heuristics which are then evaluated on a training set and the best heuristics is returned to the LLM for reﬁnement.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a training set, which is too generic.",
      "processing_time": 17.622118949890137,
      "citing_paper_id": "277940718",
      "cited_paper_id": 276618008
    },
    {
      "context_text": "The full details about the task sets can be found online [63].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to task sets which are not clearly defined as a dataset.",
      "processing_time": 17.213323831558228,
      "citing_paper_id": "277940718",
      "cited_paper_id": null
    },
    {
      "context_text": "Interactive Recommender Systems (IRS) enhance user engagement and improve recommendation accuracy by actively involving users through interactions such as ratings, clicks, and natural language input [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of Interactive Recommender Systems. No verifiable resources are identified.",
      "processing_time": 18.646284580230713,
      "citing_paper_id": "280010952",
      "cited_paper_id": 6326401
    },
    {
      "context_text": "This interactive approach not only ensures personalized recommendations but also heightens user engagement and satisfaction [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a survey paper on interactive recommender systems, which is not a dataset.",
      "processing_time": 40.95856237411499,
      "citing_paper_id": "280010952",
      "cited_paper_id": 6326401
    },
    {
      "context_text": "Building on this, we proposed multiple recommendation scenarios with three difficulty levels and introduced different user questioning semantics, inspired by [29] and [47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works for inspiration. No clear, verifiable datasets are identified.",
      "processing_time": 18.640898942947388,
      "citing_paper_id": "280010952",
      "cited_paper_id": 19100351
    },
    {
      "context_text": "Building on this, we proposed multiple recommendation scenarios with three difficulty levels and introduced different user questioning semantics, inspired by [29] and [47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works for inspiration. No clear, verifiable datasets are identified.",
      "processing_time": 18.640898942947388,
      "citing_paper_id": "280010952",
      "cited_paper_id": 209515572
    },
    {
      "context_text": "For example, Sun et al. [35] used a belief tracker and a deep policy network to guide recommendation actions based on user intent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (belief tracker and deep policy network). The context is about using these methods to guide recommendation actions based on user intent.",
      "processing_time": 21.07008934020996,
      "citing_paper_id": "280010952",
      "cited_paper_id": 47012216
    },
    {
      "context_text": "They dynamically generate responses based on user input, adopting an end-to-end framework that integrates recommendation and dialogue generation [37, 55, 57], handling multi-turn interactions [24, 43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks for integrating recommendation and dialogue generation.",
      "processing_time": 17.65204429626465,
      "citing_paper_id": "280010952",
      "cited_paper_id": 53513571
    },
    {
      "context_text": "They dynamically generate responses based on user input, adopting an end-to-end framework that integrates recommendation and dialogue generation [37, 55, 57], handling multi-turn interactions [24, 43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks for integrating recommendation and dialogue generation.",
      "processing_time": 17.65204429626465,
      "citing_paper_id": "280010952",
      "cited_paper_id": 220404390
    },
    {
      "context_text": "They dynamically generate responses based on user input, adopting an end-to-end framework that integrates recommendation and dialogue generation [37, 55, 57], handling multi-turn interactions [24, 43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks for integrating recommendation and dialogue generation.",
      "processing_time": 17.65204429626465,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "While structured and controllable, early attribute-based approaches [20, 21, 56, 60] lack the flexibility to handle complex or ambiguous user needs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and their limitations. No verifiable resources are identified.",
      "processing_time": 26.54077672958374,
      "citing_paper_id": "280010952",
      "cited_paper_id": 209366827
    },
    {
      "context_text": "While structured and controllable, early attribute-based approaches [20, 21, 56, 60] lack the flexibility to handle complex or ambiguous user needs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and their limitations. No verifiable resources are identified.",
      "processing_time": 26.54077672958374,
      "citing_paper_id": "280010952",
      "cited_paper_id": 219124336
    },
    {
      "context_text": "While structured and controllable, early attribute-based approaches [20, 21, 56, 60] lack the flexibility to handle complex or ambiguous user needs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and their limitations. No verifiable resources are identified.",
      "processing_time": 26.54077672958374,
      "citing_paper_id": "280010952",
      "cited_paper_id": 220280457
    },
    {
      "context_text": "While structured and controllable, early attribute-based approaches [20, 21, 56, 60] lack the flexibility to handle complex or ambiguous user needs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and their limitations. No verifiable resources are identified.",
      "processing_time": 26.54077672958374,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional IRS relies on predefined rules and templates, guiding users through questions about item attributes to refine recommendations [20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to traditional IRS using predefined rules and templates.",
      "processing_time": 26.93399691581726,
      "citing_paper_id": "280010952",
      "cited_paper_id": 209366827
    },
    {
      "context_text": "• Success Rate (SR) [17]: We identify two primary failure cases in recommendation tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only failure cases in recommendation tasks.",
      "processing_time": 26.06063222885132,
      "citing_paper_id": "280010952",
      "cited_paper_id": 214774912
    },
    {
      "context_text": "We applied the designed user query modeling method in Section 4.1 to construct data for experiments on three well-known datasets: Amazon Clothing & Shoes (hereafter referred to as Amazon Clothing), Amazon Beauty , and Amazon Music [27].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Amazon Clothing & Shoes",
        "Amazon Beauty",
        "Amazon Music"
      ],
      "dataset_descriptions": {
        "Amazon Clothing & Shoes": "Used to construct data for experiments on user query modeling, focusing on the effectiveness of the method in a retail context.",
        "Amazon Beauty": "Used to construct data for experiments on user query modeling, focusing on the effectiveness of the method in a retail context.",
        "Amazon Music": "Used to construct data for experiments on user query modeling, focusing on the effectiveness of the method in a retail context."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for experiments, which are clearly named and relevant to the research topic.",
      "processing_time": 47.71117615699768,
      "citing_paper_id": "280010952",
      "cited_paper_id": 220551746
    },
    {
      "context_text": "CRS engages users through multi-round natural language interactions [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to multi-round natural language interactions.",
      "processing_time": 27.117645978927612,
      "citing_paper_id": "280010952",
      "cited_paper_id": 231698518
    },
    {
      "context_text": "Early methods relied on traditional machine learning techniques [1, 19], which later evolved into neural network approaches such as LSTM and RNN [4, 9, 18, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.339508533477783,
      "citing_paper_id": "280010952",
      "cited_paper_id": 231730714
    },
    {
      "context_text": "Early methods relied on traditional machine learning techniques [1, 19], which later evolved into neural network approaches such as LSTM and RNN [4, 9, 18, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.339508533477783,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Early methods relied on traditional machine learning techniques [1, 19], which later evolved into neural network approaches such as LSTM and RNN [4, 9, 18, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.339508533477783,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Early methods relied on traditional machine learning techniques [1, 19], which later evolved into neural network approaches such as LSTM and RNN [4, 9, 18, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.339508533477783,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Early methods relied on traditional machine learning techniques [1, 19], which later evolved into neural network approaches such as LSTM and RNN [4, 9, 18, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 29.339508533477783,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "The advent of LLMs like BERT and GPT shifted the focus to fine-tuning pre-trained models for intent recognition tasks [8, 14, 22, 38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models and the concept of fine-tuning. No verifiable resources are identified.",
      "processing_time": 29.89427423477173,
      "citing_paper_id": "280010952",
      "cited_paper_id": 237581476
    },
    {
      "context_text": "The advent of LLMs like BERT and GPT shifted the focus to fine-tuning pre-trained models for intent recognition tasks [8, 14, 22, 38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models and the concept of fine-tuning. No verifiable resources are identified.",
      "processing_time": 29.89427423477173,
      "citing_paper_id": "280010952",
      "cited_paper_id": 246575985
    },
    {
      "context_text": "The advent of LLMs like BERT and GPT shifted the focus to fine-tuning pre-trained models for intent recognition tasks [8, 14, 22, 38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models and the concept of fine-tuning. No verifiable resources are identified.",
      "processing_time": 29.89427423477173,
      "citing_paper_id": "280010952",
      "cited_paper_id": 269539563
    },
    {
      "context_text": "The advent of LLMs like BERT and GPT shifted the focus to fine-tuning pre-trained models for intent recognition tasks [8, 14, 22, 38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models and the concept of fine-tuning. No verifiable resources are identified.",
      "processing_time": 29.89427423477173,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "The advent of LLMs like BERT and GPT shifted the focus to fine-tuning pre-trained models for intent recognition tasks [8, 14, 22, 38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models and the concept of fine-tuning. No verifiable resources are identified.",
      "processing_time": 29.89427423477173,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• ReAct [50]: Reasoning and execution are performed alternately through a loop of Thought, Action, and Observation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called ReAct. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 30.171443700790405,
      "citing_paper_id": "280010952",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Reflection and refinement enhance adaptability by revising plans after failures, as in Reflexion [31], an improvement on ReAct [50], but are less effective in real-time situations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 29.633395671844482,
      "citing_paper_id": "280010952",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Reflection and refinement enhance adaptability by revising plans after failures, as in Reflexion [31], an improvement on ReAct [50], but are less effective in real-time situations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 29.633395671844482,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "In contrast, the Interleaved method, exemplified by ReAct [50], can dynamically decide the next plan based on the feedback from sub-task execution, allowing for the completion of more complex tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ReAct) and its capabilities. No verifiable resources are identified.",
      "processing_time": 29.887413501739502,
      "citing_paper_id": "280010952",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Task decomposition simplifies complex tasks into sub-tasks, as seen in methods like CoT [46] and Plan&Solve [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on task decomposition and the use of specific methods like CoT and Plan&Solve.",
      "processing_time": 31.846018075942993,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "• Plan&Solve [41]: A complete plan is generated before execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'Plan&Solve'.",
      "processing_time": 28.17934250831604,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "The Decomposition-First method, exemplified by Plan-and-Solve [41], generates a complete plan at once, creating strong associations between subtasks and the original task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Plan-and-Solve'. No datasets are referenced for use in the research context.",
      "processing_time": 31.243014574050903,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "While existing methods like CoT [46] or Reflexion [31] improve success rates in regular tasks, they still struggle to effectively handle such complex tasks, with average failure rates over 60%(see Table 3).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance metrics.",
      "processing_time": 27.519710779190063,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "• Reflexion [31]: An enhancement of ReAct, incorporating a self-reflection mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Reflexion which enhances ReAct with a self-reflection mechanism.",
      "processing_time": 30.155946493148804,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "The rise of LLMs has fueled significant advancements in recommender systems [12, 40, 48], including more flexible IRS capable of deeper understanding of such complex or ambiguous input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in recommender systems and IRS capabilities. No verifiable resources are named.",
      "processing_time": 30.73890256881714,
      "citing_paper_id": "280010952",
      "cited_paper_id": 258987581
    },
    {
      "context_text": "The rise of LLMs has fueled significant advancements in recommender systems [12, 40, 48], including more flexible IRS capable of deeper understanding of such complex or ambiguous input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in recommender systems and IRS capabilities. No verifiable resources are named.",
      "processing_time": 30.73890256881714,
      "citing_paper_id": "280010952",
      "cited_paper_id": 274437509
    },
    {
      "context_text": "The rise of LLMs has fueled significant advancements in recommender systems [12, 40, 48], including more flexible IRS capable of deeper understanding of such complex or ambiguous input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in recommender systems and IRS capabilities. No verifiable resources are named.",
      "processing_time": 30.73890256881714,
      "citing_paper_id": "280010952",
      "cited_paper_id": 276450249
    },
    {
      "context_text": "Moreover, prompt-driven approaches have emerged, enabling dynamic adaptation to conversational contexts without extensive re-training [7, 26, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 27.512821912765503,
      "citing_paper_id": "280010952",
      "cited_paper_id": 260865987
    },
    {
      "context_text": "Moreover, prompt-driven approaches have emerged, enabling dynamic adaptation to conversational contexts without extensive re-training [7, 26, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 27.512821912765503,
      "citing_paper_id": "280010952",
      "cited_paper_id": 263832826
    },
    {
      "context_text": "Moreover, prompt-driven approaches have emerged, enabling dynamic adaptation to conversational contexts without extensive re-training [7, 26, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 27.512821912765503,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "With advancements in natural language processing, especially the rise of LLM-powered agents, IRS has significantly improved in dialogue understanding and generation [15], enabling more natural user interactions and greater accuracy in completing recommendation tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only advancements in NLP and LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 30.73402976989746,
      "citing_paper_id": "280010952",
      "cited_paper_id": 261395685
    },
    {
      "context_text": "• InteRecAgent [15]: InteRecAgent introduces an interactive recommendation framework based on tool invocation, and incorpo-ratesmechanismsincludingthedynamic demonstration-augmented task planning to enhance the agent’s ability to complete recommendation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called InteRecAgent. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 32.077247619628906,
      "citing_paper_id": "280010952",
      "cited_paper_id": 261395685
    },
    {
      "context_text": "Despite these advancements, existing agents for interactive recommendation [15] still face challenges when handling complex and diverse user intents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in the field of interactive recommendation systems.",
      "processing_time": 29.044347286224365,
      "citing_paper_id": "280010952",
      "cited_paper_id": 261395685
    },
    {
      "context_text": "Leveraging models like GPT-4, these systems better understand user inputs and generate more personalized dialogues [15, 30, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 31.834808588027954,
      "citing_paper_id": "280010952",
      "cited_paper_id": 261395685
    },
    {
      "context_text": "Leveraging models like GPT-4, these systems better understand user inputs and generate more personalized dialogues [15, 30, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 31.834808588027954,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267897865
    },
    {
      "context_text": "Leveraging models like GPT-4, these systems better understand user inputs and generate more personalized dialogues [15, 30, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 31.834808588027954,
      "citing_paper_id": "280010952",
      "cited_paper_id": 278904757
    },
    {
      "context_text": "Unlike rule-based simulations, LLMs capture latent user preferences and behavioral patterns [25], enabling better simulations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of LLMs. No verifiable resources are identified.",
      "processing_time": 30.463886737823486,
      "citing_paper_id": "280010952",
      "cited_paper_id": 261823722
    },
    {
      "context_text": "There are some other recommender system frameworks based on LLM-Agents, such as [52] and [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other recommender system frameworks based on LLM-Agents. No verifiable resources are identified.",
      "processing_time": 33.270241260528564,
      "citing_paper_id": "280010952",
      "cited_paper_id": 264128019
    },
    {
      "context_text": "There are some other recommender system frameworks based on LLM-Agents, such as [52] and [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other recommender system frameworks based on LLM-Agents. No verifiable resources are identified.",
      "processing_time": 33.270241260528564,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, LLMs such as ChatGPT have demonstrated zero-shot capabilities in intent recognition [2, 33], further enhancing IRS by reducing reliance on large labeled datasets.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the zero-shot capabilities of LLMs like ChatGPT in intent recognition, which is a method or finding rather than a dataset.",
      "processing_time": 35.2901177406311,
      "citing_paper_id": "280010952",
      "cited_paper_id": 264146567
    },
    {
      "context_text": "Recently, LLMs such as ChatGPT have demonstrated zero-shot capabilities in intent recognition [2, 33], further enhancing IRS by reducing reliance on large labeled datasets.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the zero-shot capabilities of LLMs like ChatGPT in intent recognition, which is a method or finding rather than a dataset.",
      "processing_time": 35.2901177406311,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• MACRS [6]: MACRS uses a multi-agent system where agents execute tasks and propose responses, with the best response selected as the output.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or system called MACRS. The title confirms it is a system, not a dataset.",
      "processing_time": 32.06598782539368,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267406588
    },
    {
      "context_text": "Several methods can enhance planning in LLM-powered agents, with task decomposition and reflection and refinement being prominent approaches [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for enhancing planning in LLM-powered agents.",
      "processing_time": 29.310840129852295,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267411892
    },
    {
      "context_text": "There are two methods of agent planning: Decomposition-First and Interleaved [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods of agent planning. There are no verifiable resources or datasets mentioned.",
      "processing_time": 30.995254039764404,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267411892
    },
    {
      "context_text": "Equipped with a dense retriever, such as BGE-Reranker [5], it matches the relevant items from the candidate pool according to the similarity between the candidate description and the input attribute keywords.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BGE-Reranker) which is excluded. No verifiable resources are identified.",
      "processing_time": 32.058308839797974,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267413218
    },
    {
      "context_text": "• BGE-Reranker [5]: BGE-Reranker is a model that directly calculates the similarity between a query and document, producing a relevance score without embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model called BGE-Reranker. The context and title do not provide information about datasets used.",
      "processing_time": 32.926063776016235,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267413218
    },
    {
      "context_text": "The third category contains different state-of-the-art interactive recommender agent systems powered by LLMs, including: • MACRec [44]: The MACRec framework introduces multiple agents, where the Manager oversees task allocation and assigns agents to complete the recommendation task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or system (MACRec). There are no clear identifiers for datasets.",
      "processing_time": 31.537895917892456,
      "citing_paper_id": "280010952",
      "cited_paper_id": 267897865
    },
    {
      "context_text": "A more flexible approach is using task experience, like BoT [49], which leverages high-level insights from prior problem-solving to enhance real-time thought processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'Buffer of Thoughts' (BoT).",
      "processing_time": 31.204520225524902,
      "citing_paper_id": "280010952",
      "cited_paper_id": 270285926
    },
    {
      "context_text": "To enhance the agent’s reasoning and planning capabilities by leveraging prior experience, we apply a Thought Pattern Distillation (TPD) method to extract high-level thoughts from the agent’s and human experts’ experiences, inspired by [49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Thought Pattern Distillation) and a general reference to experiences. No clear, verifiable datasets are identified.",
      "processing_time": 33.81654596328735,
      "citing_paper_id": "280010952",
      "cited_paper_id": 270285926
    },
    {
      "context_text": "Different from [49], our proposed Thought Pattern Distillation (TPD) method considers not only the successful experiences of agents but also incorporates corrections from failed experiences and experts’ insights to form the thought patterns, for agents often struggle to complete more complex tasks…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Thought Pattern Distillation) and a general reference to 'experiences' and 'thought patterns'. No verifiable resources are identified.",
      "processing_time": 34.90449142456055,
      "citing_paper_id": "280010952",
      "cited_paper_id": 270285926
    },
    {
      "context_text": "Our study, in alignment with prior research[49], identifies that the root cause of this problem is that LLMs exhibit constrained generalization capabilities in complex tasks and lack advanced planning mechanisms necessary for tackling the multifaceted demands of complex recommendation problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with LLMs. No verifiable resources are identified.",
      "processing_time": 31.19754433631897,
      "citing_paper_id": "280010952",
      "cited_paper_id": 270285926
    },
    {
      "context_text": "To handle complex user intents, the recommender agent system needs to not only generate efficient task decomposition plans but also ensure that the execution of subtasks can dynamically adapt to uncertainties and changes during task progress [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for a system to handle complex user intents and adapt to uncertainties.",
      "processing_time": 31.52778387069702,
      "citing_paper_id": "280010952",
      "cited_paper_id": 276742020
    },
    {
      "context_text": "To achieve intelligent and efficient quantitative evaluation, we developed an LLM-driven user behavior simulation inspired by [42] and [58], to evaluate the effectiveness of recommendations for queries constructed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating recommendations using LLM-driven user behavior simulation.",
      "processing_time": 30.976792097091675,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "To achieve intelligent and efficient quantitative evaluation, we developed an LLM-driven user behavior simulation inspired by [42] and [58], to evaluate the effectiveness of recommendations for queries constructed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating recommendations using LLM-driven user behavior simulation.",
      "processing_time": 30.976792097091675,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• NDCG [32]: We adopt Normalized Discounted Cumulative Gain (NDCG) to evaluate the ranking quality of the recommendation results.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (NDCG) which is excluded according to the instructions.",
      "processing_time": 31.523807287216187,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "The first category includes different ranking models, including: • BM25 [36]: BM25 is a classic probabilistic ranking model that calculates document relevance for ranking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a ranking model (BM25).",
      "processing_time": 30.086799383163452,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "The second category includes various agent planning methods, including: • Zero-shot[45] : The large model autonomously decides on Execu-tor Agent invocation based solely on the user query and Executor Agent description, without additional prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only planning methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 31.798185348510742,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Equipped with tools like the Google Search API and an Attribute Mapper, it retrieves information from the internet and matches attributes from the domain knowledge base[59].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 32.47442436218262,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• One-shot[3] : Each execution agent is provided with an example of how to be invoked.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only describes a method or approach for providing an example to execution agents.",
      "processing_time": 32.4721200466156,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• Hit Ratio (HR) [54]: We consider the hit rate as the frequency of items in the recommendation list that meet the user’s needs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (Hit Ratio) which is excluded according to the instructions.",
      "processing_time": 32.03030872344971,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Following the common practice in sequential recommendation modeling [51], the last interaction in a user’s history is designated as the target item.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a common practice in sequential recommendation modeling.",
      "processing_time": 32.02854323387146,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Subsequently, an LLM generates an initial atomic query from the user’s perspective, leveraging information pertaining to this target item [51, 53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process involving LLMs generating queries. No clear, verifiable resource names are provided.",
      "processing_time": 33.995471715927124,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "Subsequently, an LLM generates an initial atomic query from the user’s perspective, leveraging information pertaining to this target item [51, 53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process involving LLMs generating queries. No clear, verifiable resource names are provided.",
      "processing_time": 33.995471715927124,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "• BGE-M3 [23]: BGE-M3 is a versatile embedding model that enables dense, and sparse retrieval across inputs of varying lengths.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BGE-M3 as an embedding model, which is not a dataset but a method or model. No datasets are mentioned.",
      "processing_time": 33.78994154930115,
      "citing_paper_id": "280010952",
      "cited_paper_id": null
    },
    {
      "context_text": "R O ] 27 J a n 2025 cesses [18–20], game-theoretic approaches [21,22], auction-and market-based strategies [23,24], and optimization-based techniques [25,26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 34.647801876068115,
      "citing_paper_id": "275931867",
      "cited_paper_id": 3628605
    },
    {
      "context_text": "R O ] 27 J a n 2025 cesses [18–20], game-theoretic approaches [21,22], auction-and market-based strategies [23,24], and optimization-based techniques [25,26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 34.647801876068115,
      "citing_paper_id": "275931867",
      "cited_paper_id": 13253834
    },
    {
      "context_text": "The mission in Multi-Robot Task Allocation (MRTA) problems can be categorized as elemental, simple, compound, or complex [16,17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of tasks in MRTA problems. No verifiable resources are identified.",
      "processing_time": 33.631896018981934,
      "citing_paper_id": "275931867",
      "cited_paper_id": 10878315
    },
    {
      "context_text": "The mission in Multi-Robot Task Allocation (MRTA) problems can be categorized as elemental, simple, compound, or complex [16,17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of tasks in MRTA problems. No verifiable resources are identified.",
      "processing_time": 33.631896018981934,
      "citing_paper_id": "275931867",
      "cited_paper_id": 12515065
    },
    {
      "context_text": "As we look toward the future, we anticipate a society where robots with diverse capabilities become integral to daily life, contributing to a pro-social and har-monious society [3,4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the societal impact and design considerations of social robots.",
      "processing_time": 34.86724281311035,
      "citing_paper_id": "275931867",
      "cited_paper_id": 17341026
    },
    {
      "context_text": "As we look toward the future, we anticipate a society where robots with diverse capabilities become integral to daily life, contributing to a pro-social and har-monious society [3,4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the societal impact and design considerations of social robots.",
      "processing_time": 34.86724281311035,
      "citing_paper_id": "275931867",
      "cited_paper_id": 19182772
    },
    {
      "context_text": "While there are many robot-and task-specific planning approaches, ranging from dexterous manipulation [7,8] to trajectory planning for autonomous vehicles [9–11], the literature lacks generalized mission planning approaches for heterogeneous multi-robot teams.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 34.379374504089355,
      "citing_paper_id": "275931867",
      "cited_paper_id": 38899272
    },
    {
      "context_text": "While there are many robot-and task-specific planning approaches, ranging from dexterous manipulation [7,8] to trajectory planning for autonomous vehicles [9–11], the literature lacks generalized mission planning approaches for heterogeneous multi-robot teams.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 34.379374504089355,
      "citing_paper_id": "275931867",
      "cited_paper_id": 207424229
    },
    {
      "context_text": "While there are many robot-and task-specific planning approaches, ranging from dexterous manipulation [7,8] to trajectory planning for autonomous vehicles [9–11], the literature lacks generalized mission planning approaches for heterogeneous multi-robot teams.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 34.379374504089355,
      "citing_paper_id": "275931867",
      "cited_paper_id": 255825556
    },
    {
      "context_text": "While there are many robot-and task-specific planning approaches, ranging from dexterous manipulation [7,8] to trajectory planning for autonomous vehicles [9–11], the literature lacks generalized mission planning approaches for heterogeneous multi-robot teams.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 34.379374504089355,
      "citing_paper_id": "275931867",
      "cited_paper_id": 268856519
    },
    {
      "context_text": "Additionally, we would like to fine-tune [38] the LLM for hierarchical tree construction task to obtain better quality trees.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (hierarchical tree construction). The cited paper title suggests methods for fine-tuning, not datasets.",
      "processing_time": 36.97654056549072,
      "citing_paper_id": "275931867",
      "cited_paper_id": 211132951
    },
    {
      "context_text": "Moreover, limited studies that are designed for class CD problems, such as those utilizing Generalized Partial Global Planning (GPGP) [27] and hierarchical trees [28], rely heavily on human expertise for task decomposition and treat task allocation and scheduling as separate processes from task…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of GPGP and hierarchical trees, which are methods, not datasets.",
      "processing_time": 37.94330096244812,
      "citing_paper_id": "275931867",
      "cited_paper_id": 212703531
    },
    {
      "context_text": "…studies that are designed for class CD problems, such as those utilizing Generalized Partial Global Planning (GPGP) [27] and hierarchical trees [28], rely heavily on human expertise for task decomposition and treat task allocation and scheduling as separate processes from task decomposition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of GPGP and hierarchical trees in mission planning, which are methods rather than datasets.",
      "processing_time": 40.05401396751404,
      "citing_paper_id": "275931867",
      "cited_paper_id": 237581307
    },
    {
      "context_text": "Recent advancements in Large Language Models (LLMs) such as GPT [13], Gemini [14], and LLaMA [15] have demonstrated emergent capabilities that suggest they might be useful in addressing the shortcomings of mission planning systems to reason over diverse and novel situations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 39.16342067718506,
      "citing_paper_id": "275931867",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "Recent advancements in Large Language Models (LLMs) such as GPT [13], Gemini [14], and LLaMA [15] have demonstrated emergent capabilities that suggest they might be useful in addressing the shortcomings of mission planning systems to reason over diverse and novel situations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 39.16342067718506,
      "citing_paper_id": "275931867",
      "cited_paper_id": null
    },
    {
      "context_text": "In the TAEMS framework [30], this utility is often defined as a function of a triple ( q a ( i ) , d a ( i ) , c a ( i )) , where q a ( i ) represents the quality of the action, d a ( i ) represents the time duration of the action, and c a ( i ) estimates the cost for the action (energy…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework and its components. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 39.69317388534546,
      "citing_paper_id": "275931867",
      "cited_paper_id": null
    },
    {
      "context_text": "Nevertheless, challenges remain, including limitations in multimodal image analysis, reliance on operator experience for dose determination, and the lack of individualized treatment planning Zhou and Yufeng [2017], which hinder broader clinical adoption and application.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges and limitations in the field.",
      "processing_time": 38.72717022895813,
      "citing_paper_id": "278912066",
      "cited_paper_id": 13405741
    },
    {
      "context_text": "In terms of treatment efficacy, machine learning models have also been applied to predict non-perfusion volume reduction and residual tissue regeneration following FUAS Zhang et al. [2022].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the application of machine learning models for predicting treatment outcomes. No clear, verifiable dataset names are provided.",
      "processing_time": 40.83137655258179,
      "citing_paper_id": "278912066",
      "cited_paper_id": 250114239
    },
    {
      "context_text": "More recently, domain-specific LLMs like GatorTron Peng et al. [2023], PubMedGPT, and Med-PaLM Singhal et al. [2023] have demonstrated superior capabilities in medical text understanding and generation, particularly in electronic health records (EHRs), clinical question answering, disease prediction—showing increasing potential for interactive reasoning with clinicians.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities in medical text understanding and generation.",
      "processing_time": 39.679646015167236,
      "citing_paper_id": "278912066",
      "cited_paper_id": 255124952
    },
    {
      "context_text": "More recently, domain-specific LLMs like GatorTron Peng et al. [2023], PubMedGPT, and Med-PaLM Singhal et al. [2023] have demonstrated superior capabilities in medical text understanding and generation, particularly in electronic health records (EHRs), clinical question answering, disease…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities in medical text understanding and generation.",
      "processing_time": 39.898035526275635,
      "citing_paper_id": "278912066",
      "cited_paper_id": 255124952
    },
    {
      "context_text": "…and XGBoost have been developed to estimate therapeutic parameters Hu et al. [2023], while others employed Deep Multimodal Teacher-Student (MMTS) frameworks to reconstruct temperature distributions from ultrasound echo signals, enabling thermal-feedback-driven dose prediction Luan et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the development and application of machine learning models for therapeutic parameter estimation and temperature distribution reconstruction.",
      "processing_time": 18.15820050239563,
      "citing_paper_id": "278912066",
      "cited_paper_id": 267642722
    },
    {
      "context_text": "…been introduced to support clinical diagnosis Li et al. [2024] Yan et al. [2024], decision-making Tang et al. [2023] Kim et al. [2024a], report generation Chen et al. Sharma [2024], education Yu et al. [2024] Huang et al. [2024], and healthcare management Lan et al. [2024] Mukherjee et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 41.65098857879639,
      "citing_paper_id": "278912066",
      "cited_paper_id": 271162268
    },
    {
      "context_text": "…been introduced to support clinical diagnosis Li et al. [2024] Yan et al. [2024], decision-making Tang et al. [2023] Kim et al. [2024a], report generation Chen et al. Sharma [2024], education Yu et al. [2024] Huang et al. [2024], and healthcare management Lan et al. [2024] Mukherjee et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 41.65098857879639,
      "citing_paper_id": "278912066",
      "cited_paper_id": 272827463
    },
    {
      "context_text": "Concurrently, agent-based technologies have been introduced to support clinical diagnosis Li et al. [2024] Yan et al. [2024], decision-making Tang et al. [2023] Kim et al. [2024a], report generation Chen et al. Sharma [2024], education Yu et al. [2024] Huang et al. [2024], and healthcare management Lan et al. [2024] Mukherjee et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of agent-based technologies. No dataset names are present in the text.",
      "processing_time": 41.04259729385376,
      "citing_paper_id": "278912066",
      "cited_paper_id": 271162268
    },
    {
      "context_text": "This under-scores the urgent need for a more powerful and unified framework capable of multimodal semantic understanding, autonomous reasoning, and cross-task generalization to advance automated and personalized treatment planning Zhang et al. [2025] AlSaad et al. [2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for a more powerful framework. No verifiable resources are identified.",
      "processing_time": 40.81863570213318,
      "citing_paper_id": "278912066",
      "cited_paper_id": 272559061
    },
    {
      "context_text": "…multimodal data integration, knowledge representation, autonomous reasoning, and tool utilization Raiaan et al. [2024] Han et al. [2024] Yao et al. [2025], LLM-based medical agents are poised to become a key enabler for addressing real-world challenges in medical and health domains…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and applications of LLMs in medical and health domains.",
      "processing_time": 40.80512976646423,
      "citing_paper_id": "278912066",
      "cited_paper_id": 273482255
    },
    {
      "context_text": "To ensure data quality and clinical relevance during fine-tuning, this study uses real-world patient data, which better reflect the complexities of clinical practice compared to publicly available or synthetic datasets Xie et al. [2024].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'real-world patient data' but does not provide a specific, identifiable dataset name. The reference is too generic and lacks a clear identifier.",
      "processing_time": 42.50454783439636,
      "citing_paper_id": "278912066",
      "cited_paper_id": 274823073
    },
    {
      "context_text": "…knowledge representation, autonomous reasoning, and tool utilization Raiaan et al. [2024] Han et al. [2024] Yao et al. [2025], LLM-based medical agents are poised to become a key enabler for addressing real-world challenges in medical and health domains Vrdoljak and Boban [2025] Yang et al. [2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing LLMs in medical contexts. No verifiable resources are identified.",
      "processing_time": 41.852179288864136,
      "citing_paper_id": "278912066",
      "cited_paper_id": 276913297
    },
    {
      "context_text": "For instance, speculative decoding (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2022) speeds up generation by having a small model draft token sequences that a large model verifies.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 41.26551961898804,
      "citing_paper_id": "279391526",
      "cited_paper_id": 53208380
    },
    {
      "context_text": "For instance, speculative decoding (Stern et al., 2018; Leviathan et al., 2023; Xia et al., 2022) speeds up generation by having a small model draft token sequences that a large model verifies.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 41.26551961898804,
      "citing_paper_id": "279391526",
      "cited_paper_id": 254096365
    },
    {
      "context_text": "Our approach shares several structural similarities with mixture-of-experts (MoE) models (Jordan and Jacobs, 1994; Cai et al., 2024): First, sampling a plan g ∼ P ( g | x ) corresponds to selecting an expert based on the input, akin to input-dependent gating.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach similar to mixture-of-experts models. No verifiable datasets are referenced.",
      "processing_time": 42.07438945770264,
      "citing_paper_id": "279391526",
      "cited_paper_id": 67000854
    },
    {
      "context_text": "Our approach shares several structural similarities with mixture-of-experts (MoE) models (Jordan and Jacobs, 1994; Cai et al., 2024): First, sampling a plan g ∼ P ( g | x ) corresponds to selecting an expert based on the input, akin to input-dependent gating.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach similar to mixture-of-experts models. No verifiable datasets are referenced.",
      "processing_time": 42.07438945770264,
      "citing_paper_id": "279391526",
      "cited_paper_id": 279586232
    },
    {
      "context_text": "We use the 500-problem test set of the MBPP dataset (Austin et al., 2021), which consists of crowd-sourced Python code generation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MBPP"
      ],
      "dataset_descriptions": {
        "MBPP": "Used to evaluate code generation capabilities of large language models, focusing on a 500-problem test set of crowd-sourced Python tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MBPP dataset, which is a specific, verifiable dataset used for evaluating code generation tasks.",
      "processing_time": 49.4964485168457,
      "citing_paper_id": "279391526",
      "cited_paper_id": 237142385
    },
    {
      "context_text": "…efforts typically fall into two directions: parallel sampling , which generates multiple outputs and selects the best among them (Wei et al., 2022; Wang et al., 2022), and sequential sampling (or iterative refinement), where a model revises its outputs over multiple passes (Madaan et al., 2023;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers are used to describe different sampling strategies in language models.",
      "processing_time": 42.479469299316406,
      "citing_paper_id": "279391526",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "These efforts typically fall into two directions: parallel sampling , which generates multiple outputs and selects the best among them (Wei et al., 2022; Wang et al., 2022), and sequential sampling (or iterative refinement), where a model revises its outputs over multiple passes (Madaan et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 41.41971302032471,
      "citing_paper_id": "279391526",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "So, x = 8 k + 5 for some integer k . agreement among solutions as an indicator of correctness (Wang et al., 2022; Du et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods or findings.",
      "processing_time": 40.505393743515015,
      "citing_paper_id": "279391526",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Smaller models, in * Equal contribution. contrast, are far more deployable, but their limited capacity hinders them on complex tasks (Kojima et al., 2022; Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the limitations of smaller models compared to larger ones.",
      "processing_time": 41.02146863937378,
      "citing_paper_id": "279391526",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For example, users typically interact with language models via mobile or edge devices, which cannot host large models (Leviathan et al., 2023; Chowdhery et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing the interaction with language models on mobile or edge devices.",
      "processing_time": 42.0700159072876,
      "citing_paper_id": "279391526",
      "cited_paper_id": 254096365
    },
    {
      "context_text": "We evaluate performance using the separate hidden test cases from the MBPP-ET dataset (Dong et al., 2025 Models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MBPP-ET"
      ],
      "dataset_descriptions": {
        "MBPP-ET": "Used to evaluate the performance of code generation models, specifically focusing on hidden test cases to assess model robustness and generalization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MBPP-ET dataset, which is a specific, verifiable dataset used for evaluating code generation models.",
      "processing_time": 50.15609836578369,
      "citing_paper_id": "279391526",
      "cited_paper_id": 256105296
    },
    {
      "context_text": "As larger models continue to improve in capability, they also grow increasingly costly to run (Achiam et al., 2023; Schick et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the cost of running larger models.",
      "processing_time": 40.794163942337036,
      "citing_paper_id": "279391526",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Planning has been primarily studied in robotics, agent systems, and, more recently, in vision-language-action models as a core mechanism for structured decision-making (Kim et al., 2024; Black et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of study. There are no clear identifiers for datasets in the given context.",
      "processing_time": 42.79966330528259,
      "citing_paper_id": "279391526",
      "cited_paper_id": 264172455
    },
    {
      "context_text": "…outputs and selects the best among them (Wei et al., 2022; Wang et al., 2022), and sequential sampling (or iterative refinement), where a model revises its outputs over multiple passes (Madaan et al., 2023; Zelikman et al., 2022; Lee et al., 2025; Kumar et al., 2024; Muen-nighoff et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 42.79698204994202,
      "citing_paper_id": "279391526",
      "cited_paper_id": 276482377
    },
    {
      "context_text": "…outputs and selects the best among them (Wei et al., 2022; Wang et al., 2022), and sequential sampling (or iterative refinement), where a model revises its outputs over multiple passes (Madaan et al., 2023; Zelikman et al., 2022; Lee et al., 2025; Kumar et al., 2024; Muen-nighoff et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 42.79698204994202,
      "citing_paper_id": "279391526",
      "cited_paper_id": null
    },
    {
      "context_text": "…outputs and selects the best among them (Wei et al., 2022; Wang et al., 2022), and sequential sampling (or iterative refinement), where a model revises its outputs over multiple passes (Madaan et al., 2023; Zelikman et al., 2022; Lee et al., 2025; Kumar et al., 2024; Muen-nighoff et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 42.79698204994202,
      "citing_paper_id": "279391526",
      "cited_paper_id": null
    },
    {
      "context_text": "For cost, we use the actual per-problem API price based on real-world model pricing (OpenAI, 2025; Google DeepMind, 2024).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to API pricing from OpenAI and Google DeepMind, which are not datasets.",
      "processing_time": 43.68387508392334,
      "citing_paper_id": "279391526",
      "cited_paper_id": null
    },
    {
      "context_text": "AutoMix (Aggarwal et al., 2024) similarly trains a routing policy, using confidence scores derived from verification prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AutoMix) and its training approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 44.26664614677429,
      "citing_paper_id": "279391526",
      "cited_paper_id": null
    },
    {
      "context_text": "Key milestones in their development include bidirectional transformers for contextual encoding [7], encoder-decoder architecture for text-to-text tasks [8], and GPT-3 autoregressive model [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.115599155426025,
      "citing_paper_id": "279447299",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Key milestones in their development include bidirectional transformers for contextual encoding [7], encoder-decoder architecture for text-to-text tasks [8], and GPT-3 autoregressive model [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.115599155426025,
      "citing_paper_id": "279447299",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "In the literature, LLM-based Planning Agents (LPA), still do not have a common architecture and might include different sets of components [20, 21, 22, 23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature about LLM-based Planning Agents. No verifiable resources are identified.",
      "processing_time": 43.364657163619995,
      "citing_paper_id": "279447299",
      "cited_paper_id": 256868474
    },
    {
      "context_text": "In the literature, LLM-based Planning Agents (LPA), still do not have a common architecture and might include different sets of components [20, 21, 22, 23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature about LLM-based Planning Agents. No verifiable resources are identified.",
      "processing_time": 43.364657163619995,
      "citing_paper_id": "279447299",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "In the literature, LLM-based Planning Agents (LPA), still do not have a common architecture and might include different sets of components [20, 21, 22, 23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature about LLM-based Planning Agents. No verifiable resources are identified.",
      "processing_time": 43.364657163619995,
      "citing_paper_id": "279447299",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "As a result, modern solutions utilize semantic search and Retrieval-Augmented Generation (RAG) [2], to enhance their capabilities by using various prompting techniques [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general techniques and approaches. No verifiable resources are identified.",
      "processing_time": 41.818520307540894,
      "citing_paper_id": "279447299",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "As a result, modern solutions utilize semantic search and Retrieval-Augmented Generation (RAG) [2], to enhance their capabilities by using various prompting techniques [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general techniques and approaches. No verifiable resources are identified.",
      "processing_time": 41.818520307540894,
      "citing_paper_id": "279447299",
      "cited_paper_id": null
    },
    {
      "context_text": "Models used in our implementation include Mixtral 8×7B (32k) [25], LLaMA3–70B (8.2k), and LLaMA3–8B (8k) [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is focused on describing the models used in the implementation.",
      "processing_time": 43.114131450653076,
      "citing_paper_id": "279447299",
      "cited_paper_id": 266844877
    },
    {
      "context_text": "Models used in our implementation include Mixtral 8×7B (32k) [25], LLaMA3–70B (8.2k), and LLaMA3–8B (8k) [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is focused on describing the models used in the implementation.",
      "processing_time": 43.114131450653076,
      "citing_paper_id": "279447299",
      "cited_paper_id": null
    },
    {
      "context_text": "Factors like the number of reasoning steps in CoT prompts further influence effectiveness, with more steps benefiting complex tasks [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a factor influencing the effectiveness of CoT prompts in LLMs.",
      "processing_time": 42.465075969696045,
      "citing_paper_id": "279447299",
      "cited_paper_id": 266902900
    },
    {
      "context_text": "Experiments in the FA domain already demonstrated the benefits of these methods in improving the efficiency of information retrieval and decision-making processes [4].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and their benefits in the FA domain.",
      "processing_time": 41.59538197517395,
      "citing_paper_id": "279447299",
      "cited_paper_id": 273042295
    },
    {
      "context_text": "More recent studies [6, 7] explore LLMs to generate long-horizon navigation plans from topological maps [8, 9] or graph-based scene representations (scene graph) [10, 11, 12, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.960022926330566,
      "citing_paper_id": "278310557",
      "cited_paper_id": 214754592
    },
    {
      "context_text": "More recent studies [6, 7] explore LLMs to generate long-horizon navigation plans from topological maps [8, 9] or graph-based scene representations (scene graph) [10, 11, 12, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.960022926330566,
      "citing_paper_id": "278310557",
      "cited_paper_id": 232404173
    },
    {
      "context_text": "More recent studies [6, 7] explore LLMs to generate long-horizon navigation plans from topological maps [8, 9] or graph-based scene representations (scene graph) [10, 11, 12, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.960022926330566,
      "citing_paper_id": "278310557",
      "cited_paper_id": 248721683
    },
    {
      "context_text": "More recent studies [6, 7] explore LLMs to generate long-horizon navigation plans from topological maps [8, 9] or graph-based scene representations (scene graph) [10, 11, 12, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.960022926330566,
      "citing_paper_id": "278310557",
      "cited_paper_id": 274131897
    },
    {
      "context_text": "More recent studies [6, 7] explore LLMs to generate long-horizon navigation plans from topological maps [8, 9] or graph-based scene representations (scene graph) [10, 11, 12, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 43.960022926330566,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "Therefore, recent efforts utilized topological maps [8, 9] and scene graphs [10, 11, 12, 13] to encode semantic and spatial relationships from raw sensor data (point clouds, videos, images).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'topological maps' and 'scene graphs' but does not refer to any specific, named datasets. These terms are used to describe methods or approaches rather than verifiable datasets.",
      "processing_time": 46.8419086933136,
      "citing_paper_id": "278310557",
      "cited_paper_id": 214754592
    },
    {
      "context_text": "Therefore, recent efforts utilized topological maps [8, 9] and scene graphs [10, 11, 12, 13] to encode semantic and spatial relationships from raw sensor data (point clouds, videos, images).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'topological maps' and 'scene graphs' but does not refer to any specific, named datasets. These terms are used to describe methods or approaches rather than verifiable datasets.",
      "processing_time": 46.8419086933136,
      "citing_paper_id": "278310557",
      "cited_paper_id": 232404173
    },
    {
      "context_text": "Therefore, recent efforts utilized topological maps [8, 9] and scene graphs [10, 11, 12, 13] to encode semantic and spatial relationships from raw sensor data (point clouds, videos, images).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'topological maps' and 'scene graphs' but does not refer to any specific, named datasets. These terms are used to describe methods or approaches rather than verifiable datasets.",
      "processing_time": 46.8419086933136,
      "citing_paper_id": "278310557",
      "cited_paper_id": 248721683
    },
    {
      "context_text": "Therefore, recent efforts utilized topological maps [8, 9] and scene graphs [10, 11, 12, 13] to encode semantic and spatial relationships from raw sensor data (point clouds, videos, images).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'topological maps' and 'scene graphs' but does not refer to any specific, named datasets. These terms are used to describe methods or approaches rather than verifiable datasets.",
      "processing_time": 46.8419086933136,
      "citing_paper_id": "278310557",
      "cited_paper_id": 274131897
    },
    {
      "context_text": "We conduct experiments using the iGibson simulation platform [51, 52] with the Fetch robot, adopting the setup from [17].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the iGibson simulation platform but does not refer to any specific dataset. The platform is a simulation environment, not a dataset.",
      "processing_time": 44.98734402656555,
      "citing_paper_id": "278310557",
      "cited_paper_id": 227347434
    },
    {
      "context_text": "We conduct experiments using the iGibson simulation platform [51, 52] with the Fetch robot, adopting the setup from [17].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the iGibson simulation platform but does not refer to any specific dataset. The platform is a simulation environment, not a dataset.",
      "processing_time": 44.98734402656555,
      "citing_paper_id": "278310557",
      "cited_paper_id": 236957210
    },
    {
      "context_text": "We conduct experiments using the iGibson simulation platform [51, 52] with the Fetch robot, adopting the setup from [17].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the iGibson simulation platform but does not refer to any specific dataset. The platform is a simulation environment, not a dataset.",
      "processing_time": 44.98734402656555,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "Such setups are unsuitable for deployment on edge devices and raise concerns about increased carbon emissions [14].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concerns about carbon emissions from training large neural networks.",
      "processing_time": 41.804781913757324,
      "citing_paper_id": "278310557",
      "cited_paper_id": 233324338
    },
    {
      "context_text": "To address this, [18, 19, 20, 21] experiment the effect of different prompt templates in LLMs’ reasoning, in search of optimal prompting strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the experimentation with different prompt templates in LLMs' reasoning.",
      "processing_time": 43.08703947067261,
      "citing_paper_id": "278310557",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "To address this, [18, 19, 20, 21] experiment the effect of different prompt templates in LLMs’ reasoning, in search of optimal prompting strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the experimentation with different prompt templates in LLMs' reasoning.",
      "processing_time": 43.08703947067261,
      "citing_paper_id": "278310557",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "We apply chain-of-thought prompting [18] similarly to [17] to generate responses for both the LLM and the SLM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting).",
      "processing_time": 41.79374051094055,
      "citing_paper_id": "278310557",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "We apply chain-of-thought prompting [18] similarly to [17] to generate responses for both the LLM and the SLM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting).",
      "processing_time": 41.79374051094055,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "This process begins with the construction of the Navigation Voronoi Graph G V = ( V , E ) , following the approach introduced in Hydra [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called Hydra. There are no clear identifiers for datasets in the context.",
      "processing_time": 44.56820321083069,
      "citing_paper_id": "278310557",
      "cited_paper_id": 248913107
    },
    {
      "context_text": "For mobile devices, typically possessing around 6GB of memory, [31] suggests that SLMs contain fewer than 10 billion params.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a suggestion about the parameter size of smaller language models for mobile devices.",
      "processing_time": 43.31264686584473,
      "citing_paper_id": "278310557",
      "cited_paper_id": 256390607
    },
    {
      "context_text": "Extending the setup in [17], our task builds on prior work [43, 44, 45] by requiring the robot M to interact with the environment Env —for example, opening doors or containers—to uncover occluded or hidden objects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general task setup involving a robot interacting with an environment. No verifiable resources are named.",
      "processing_time": 45.18351626396179,
      "citing_paper_id": "278310557",
      "cited_paper_id": 258947447
    },
    {
      "context_text": "Extending the setup in [17], our task builds on prior work [43, 44, 45] by requiring the robot M to interact with the environment Env —for example, opening doors or containers—to uncover occluded or hidden objects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general task setup involving a robot interacting with an environment. No verifiable resources are named.",
      "processing_time": 45.18351626396179,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "More recent research, on the other hand, highlights the adaptability of LLMs to human-defined rules [22], suggesting promising applications in long-horizon planning tasks in robotics [23, 17, 24, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research findings about LLMs and their adaptability to human-defined rules and potential applications in robotics.",
      "processing_time": 45.823243856430054,
      "citing_paper_id": "278310557",
      "cited_paper_id": 264935717
    },
    {
      "context_text": "More recent research, on the other hand, highlights the adaptability of LLMs to human-defined rules [22], suggesting promising applications in long-horizon planning tasks in robotics [23, 17, 24, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research findings about LLMs and their adaptability to human-defined rules and potential applications in robotics.",
      "processing_time": 45.823243856430054,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "More recent research, on the other hand, highlights the adaptability of LLMs to human-defined rules [22], suggesting promising applications in long-horizon planning tasks in robotics [23, 17, 24, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research findings about LLMs and their adaptability to human-defined rules and potential applications in robotics.",
      "processing_time": 45.823243856430054,
      "citing_paper_id": "278310557",
      "cited_paper_id": 272367253
    },
    {
      "context_text": "We also provide GPT-4o’s performance, which is used in article MoMaLLM [17], for comparison.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with GPT-4o's performance. No verifiable resources are identified.",
      "processing_time": 44.560839891433716,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "Inspired by [17], we then decompose G V into multiple room-based Navigation Voronoi Graphs G R V by removing edges and nodes near doorways from G V .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for decomposing graphs. The context is about graph manipulation, not data usage.",
      "processing_time": 44.23029017448425,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "SayPlan [6], MoMaLLM [17] employ scene graphs to generate long-horizon navigation plans with LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods (scene graphs) used for generating navigation plans with LLMs.",
      "processing_time": 44.540982723236084,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268379149
    },
    {
      "context_text": "SayPlan [6], MoMaLLM [17] employ scene graphs to generate long-horizon navigation plans with LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods (scene graphs) used for generating navigation plans with LLMs.",
      "processing_time": 44.540982723236084,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "Multiple studies [32, 33, 34, 35, 36] have explored techniques for transferring knowledge from large models (LLMs) to smaller ones (SLMs).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques for knowledge transfer between large and small models.",
      "processing_time": 43.08258509635925,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268417079
    },
    {
      "context_text": "Multiple studies [32, 33, 34, 35, 36] have explored techniques for transferring knowledge from large models (LLMs) to smaller ones (SLMs).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general techniques for knowledge transfer between large and small models.",
      "processing_time": 43.08258509635925,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "For examples, [27] defines SLMs as models with fewer than 1 billion params, while [28, 29, 30] consider the term SLMs relative to the larger counterparts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discussions about the definition of small language models (SLMs).",
      "processing_time": 43.64126658439636,
      "citing_paper_id": "278310557",
      "cited_paper_id": 268889483
    },
    {
      "context_text": "VLMs have become a prevalent solution by leveraging foundation model reasoning for long-horizon planning tasks [4, 5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that VLMs are used for long-horizon planning tasks. No verifiable resources are identified.",
      "processing_time": 45.803059339523315,
      "citing_paper_id": "278310557",
      "cited_paper_id": 273098318
    },
    {
      "context_text": "Leveraging these works, [4, 5] propose Vision-Language Models (VLMs) for long-horizon planning tasks, aiming to enable robots to perceive and traverse large-scale and complex environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the proposal of Vision-Language Models (VLMs) for long-horizon planning tasks.",
      "processing_time": 45.5149621963501,
      "citing_paper_id": "278310557",
      "cited_paper_id": 273098318
    },
    {
      "context_text": "Please refer to [53] for more information about the benchmark.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to a benchmark. The cited paper title suggests a game-related dataset, but no specific dataset is named in the citation context.",
      "processing_time": 46.937591791152954,
      "citing_paper_id": "278310557",
      "cited_paper_id": 276580691
    },
    {
      "context_text": "We use a text-based interactive game benchmark [53], designed to evaluate various logical reasoning skills of language models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a 'text-based interactive game benchmark' which is likely a specific dataset or collection of games used to evaluate language models. However, the name of the dataset is not explicitly provided.",
      "processing_time": 47.6352014541626,
      "citing_paper_id": "278310557",
      "cited_paper_id": 276580691
    },
    {
      "context_text": "[42] specifically surveys what works and what does not for RL for reasoning in SLMs. Inspired by these findings, our work proposes a fine-tuning strategy that utilize SFT interleaved with reward-customized RL to stabilize and enhance the outcomes of the SLMs for path planning.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses reinforcement learning (RL) for reasoning in small language models (LLMs) but does not mention any specific datasets. The focus is on methodologies and findings.",
      "processing_time": 46.552069902420044,
      "citing_paper_id": "278310557",
      "cited_paper_id": 277150647
    },
    {
      "context_text": "This observation matches the findings in [42].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to findings. There is no indication of a reusable resource or dataset being used.",
      "processing_time": 45.15986680984497,
      "citing_paper_id": "278310557",
      "cited_paper_id": 277150647
    },
    {
      "context_text": "An interesting study [41] suggests that SFT tends to lead models to memorize the training data, whereas RL, especially when guided by outcome-based rewards, enables models to generalize across both rule-based textual and visual variants, thus enhancing the language model’s outcomes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between SFT and RL methods. No verifiable resources are identified.",
      "processing_time": 44.52318811416626,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "Consequently, this leads to inefficient re-planning and frequently results in inexecutable action sequences [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes a problem related to re-planning and action sequences.",
      "processing_time": 45.503031969070435,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "However, existing methods rely on pre-trained LLMs for direct reasoning without fine-tuning, leading to inefficient re-planning and inexecutable action sequences [6].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological limitation of existing approaches.",
      "processing_time": 42.73338198661804,
      "citing_paper_id": "278310557",
      "cited_paper_id": null
    },
    {
      "context_text": "Aggregating multiple LLM responses can further improve performance (Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving LLM performance.",
      "processing_time": 42.41353368759155,
      "citing_paper_id": "273654271",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Approaches like Chain-of-Thought (CoT) (Wang et al., 2022) enhance reasoning by prompting LLMs to generate step-by-step solutions, while Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (Besta et al., 2024) provide more structured reasoning processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for enhancing reasoning in LLMs, such as Chain-of-Thought (CoT), Tree of Thought (ToT), and Graph of Thought (GoT). No specific datasets are mentioned.",
      "processing_time": 48.55514097213745,
      "citing_paper_id": "273654271",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Approaches like Chain-of-Thought (CoT) (Wang et al., 2022) enhance reasoning by prompting LLMs to generate step-by-step solutions, while Tree of Thought (ToT) (Yao et al., 2023) and Graph of Thought (Besta et al., 2024) provide more structured reasoning processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for enhancing reasoning in LLMs, such as Chain-of-Thought (CoT), Tree of Thought (ToT), and Graph of Thought (GoT). No specific datasets are mentioned.",
      "processing_time": 48.55514097213745,
      "citing_paper_id": "273654271",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "We add several baselines such as the Direct , Few-shot , chain-of-thought ( CoT ) prompting (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about adding baselines, which are methods, not datasets.",
      "processing_time": 45.99656128883362,
      "citing_paper_id": "273654271",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Other studies incorporate an auxiliary LLM to provide natural language feedback, helping the main LLM reflect on and correct mistakes (Wang and Li, 2023; Akyürek et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches involving LLMs. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 46.53708744049072,
      "citing_paper_id": "273654271",
      "cited_paper_id": 258685337
    },
    {
      "context_text": "Other studies incorporate an auxiliary LLM to provide natural language feedback, helping the main LLM reflect on and correct mistakes (Wang and Li, 2023; Akyürek et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches involving LLMs. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 46.53708744049072,
      "citing_paper_id": "273654271",
      "cited_paper_id": 267212063
    },
    {
      "context_text": "Other studies incorporate an auxiliary LLM to provide natural language feedback, helping the main LLM reflect on and correct mistakes (Wang and Li, 2023; Akyürek et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches involving LLMs. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 46.53708744049072,
      "citing_paper_id": "273654271",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, researchers have explored using Monte Carlo Tree Search (MCTS) to find the most promising reasoning paths (Liu et al., 2023b; Wang et al., 2024c; Hao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of Monte Carlo Tree Search (MCTS) for finding reasoning paths.",
      "processing_time": 46.92479610443115,
      "citing_paper_id": "273654271",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Recently, researchers have explored using Monte Carlo Tree Search (MCTS) to find the most promising reasoning paths (Liu et al., 2023b; Wang et al., 2024c; Hao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of Monte Carlo Tree Search (MCTS) for finding reasoning paths.",
      "processing_time": 46.92479610443115,
      "citing_paper_id": "273654271",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, research has shown that multiple LLM-based agents can enhance the capability of a single LLM through communication, especially in solving reasoning problems (Du et al., 2023; Hao et al., 2023; Zhu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 45.455358028411865,
      "citing_paper_id": "273654271",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "One approach trains a verifier or reward model to evaluate the reasoning steps generated by a separate LLM (Zhu et al., 2023; Lightman et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods for evaluating reasoning steps.",
      "processing_time": 43.049152135849,
      "citing_paper_id": "273654271",
      "cited_paper_id": 258987659
    },
    {
      "context_text": "Another line of work introduces multiple LLMs that debate with each other to improve reasoning (Du et al., 2023; Liang et al., 2023; Yin et al., 2023) or obtain better evaluations (Chern et al., 2024; Chan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving multiple LLMs debating or evaluating each other.",
      "processing_time": 45.77163863182068,
      "citing_paper_id": "273654271",
      "cited_paper_id": 260887105
    },
    {
      "context_text": "Another line of work introduces multiple LLMs that debate with each other to improve reasoning (Du et al., 2023; Liang et al., 2023; Yin et al., 2023) or obtain better evaluations (Chern et al., 2024; Chan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving multiple LLMs debating or evaluating each other.",
      "processing_time": 45.77163863182068,
      "citing_paper_id": "273654271",
      "cited_paper_id": 265609521
    },
    {
      "context_text": "Another line of work introduces multiple LLMs that debate with each other to improve reasoning (Du et al., 2023; Liang et al., 2023; Yin et al., 2023) or obtain better evaluations (Chern et al., 2024; Chan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving multiple LLMs debating or evaluating each other.",
      "processing_time": 45.77163863182068,
      "citing_paper_id": "273654271",
      "cited_paper_id": 267320303
    },
    {
      "context_text": "The main reason lies in the fact that these agents attempt to directly solve the complex problem independently and communicate with others after obtaining a solution, such as in LLM debate (Du et al., 2023) or ChatEval (Chan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or tools (LLM debate, ChatEval).",
      "processing_time": 44.18914580345154,
      "citing_paper_id": "273654271",
      "cited_paper_id": 260887105
    },
    {
      "context_text": "The Mistral-7B-based CoPlanner demonstrates comparable performance to the Tree of Thought (ToT) policy on BBH but achieves better results on LogiQA.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. No verifiable resources are identified.",
      "processing_time": 43.60332417488098,
      "citing_paper_id": "273654271",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "From the comparison between the random policy and the Chain-of-Thought (CoT) policy, we observe that randomly picking a meta-strategy sometimes performs better than using CoT prompting.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between policies. No verifiable resources are identified.",
      "processing_time": 44.172707080841064,
      "citing_paper_id": "273654271",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "2 The detailed implementation can be found in Appendix A.2 Chain-of-Thought Prompting Alone Cannot Help LLMs Pick the Right Meta-Strategy.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to an appendix in another paper.",
      "processing_time": 45.13001489639282,
      "citing_paper_id": "273654271",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "This finding is consistent with recent studies suggesting that most interactions in multi-agent frameworks only perturb the prompt instead of providing new ideas, making them easily surpassed by well-designed instructions or demonstrations (Huang et al., 2023; Wang et al., 2024b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses findings and research work.",
      "processing_time": 42.70898103713989,
      "citing_paper_id": "273654271",
      "cited_paper_id": 263609132
    },
    {
      "context_text": "Recent studies have found that these multi-agent frameworks do not exhibit significant advantages over elaborated instructions or demonstrations (Huang et al., 2023; Wang et al., 2024b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies comparing multi-agent frameworks with instructions or demonstrations.",
      "processing_time": 44.50901961326599,
      "citing_paper_id": "273654271",
      "cited_paper_id": 263609132
    },
    {
      "context_text": "Multi-agent frameworks have also been explored in domains such as games (Xu et al., 2023b,a), software development (Qian et al., 2023), and real-world simulations (Hua et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various domains where multi-agent frameworks have been explored. No verifiable resources are named.",
      "processing_time": 18.784158945083618,
      "citing_paper_id": "273654271",
      "cited_paper_id": 264590387
    },
    {
      "context_text": "2 (Jiang et al., 2023) and LLaMA 3 8B Instruct (Touvron et al., 2023; Jiang et al., 2023) as our backbone models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (LLaMA 3 8B Instruct) but does not reference any specific datasets. The context is focused on the models used, not on datasets.",
      "processing_time": 47.86408853530884,
      "citing_paper_id": "273654271",
      "cited_paper_id": null
    },
    {
      "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC",
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation.",
        "MATH": "Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations.",
        "AQUA-RAT": "Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems.",
        "HotpotQA": "Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions.",
        "StrategyQA": "Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
        "MultiRC": "Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences.",
        "ARC": "Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains.",
        "FOLIO": "Evaluates logical reasoning, focusing on the ability to solve first-order logic problems.",
        "P-FOLIO": "Tests advanced logical reasoning, focusing on more complex first-order logic problems.",
        "Game of 24": "Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations.",
        "MUSR": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks.",
        "BBH": "Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
      "processing_time": 111.4934401512146,
      "citing_paper_id": "277150776",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC",
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation.",
        "MATH": "Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations.",
        "AQUA-RAT": "Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems.",
        "HotpotQA": "Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions.",
        "StrategyQA": "Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
        "MultiRC": "Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences.",
        "ARC": "Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains.",
        "FOLIO": "Evaluates logical reasoning, focusing on the ability to solve first-order logic problems.",
        "P-FOLIO": "Tests advanced logical reasoning, focusing on more complex first-order logic problems.",
        "Game of 24": "Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations.",
        "MUSR": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks.",
        "BBH": "Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
      "processing_time": 111.4934401512146,
      "citing_paper_id": "277150776",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC",
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation.",
        "MATH": "Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations.",
        "AQUA-RAT": "Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems.",
        "HotpotQA": "Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions.",
        "StrategyQA": "Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
        "MultiRC": "Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences.",
        "ARC": "Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains.",
        "FOLIO": "Evaluates logical reasoning, focusing on the ability to solve first-order logic problems.",
        "P-FOLIO": "Tests advanced logical reasoning, focusing on more complex first-order logic problems.",
        "Game of 24": "Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations.",
        "MUSR": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks.",
        "BBH": "Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
      "processing_time": 111.4934401512146,
      "citing_paper_id": "277150776",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC",
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation.",
        "MATH": "Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations.",
        "AQUA-RAT": "Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems.",
        "HotpotQA": "Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions.",
        "StrategyQA": "Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
        "MultiRC": "Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences.",
        "ARC": "Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains.",
        "FOLIO": "Evaluates logical reasoning, focusing on the ability to solve first-order logic problems.",
        "P-FOLIO": "Tests advanced logical reasoning, focusing on more complex first-order logic problems.",
        "Game of 24": "Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations.",
        "MUSR": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks.",
        "BBH": "Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
      "processing_time": 111.4934401512146,
      "citing_paper_id": "277150776",
      "cited_paper_id": 264439655
    },
    {
      "context_text": "This foundational need for multi-step planning has led to the development of specialized benchmarks and evaluation frameworks that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC",
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess mathematical reasoning through algebraic word problems, focusing on step-by-step problem-solving and explanation generation.",
        "MATH": "Evaluates mathematical problem-solving skills, emphasizing the ability to solve complex math problems and generate explanations.",
        "AQUA-RAT": "Tests mathematical reasoning through algebraic word problems, focusing on the ability to understand and solve multi-step problems.",
        "HotpotQA": "Assesses multi-hop question answering, requiring models to reason over multiple pieces of evidence to answer questions.",
        "StrategyQA": "Evaluates the ability to answer questions that require strategic reasoning and multi-step inference.",
        "MultiRC": "Tests multi-hop reasoning in reading comprehension, focusing on the ability to answer questions that require understanding multiple sentences.",
        "ARC": "Assesses scientific reasoning, evaluating the ability to solve problems in various scientific domains.",
        "FOLIO": "Evaluates logical reasoning, focusing on the ability to solve first-order logic problems.",
        "P-FOLIO": "Tests advanced logical reasoning, focusing on more complex first-order logic problems.",
        "Game of 24": "Evaluates constraint satisfaction and arithmetic reasoning through a puzzle game that requires solving equations.",
        "MUSR": "Tests multistep soft reasoning, focusing on the ability to handle everyday common sense tasks.",
        "BBH": "Challenges models with difficult reasoning tasks, assessing their ability to solve complex problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets and benchmarks that are used to assess multi-step planning capabilities in various domains. These are specific, verifiable resources with clear identifiers.",
      "processing_time": 111.4934401512146,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 34953552
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 234093776
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 262053695
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263909014
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 264439655
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267199749
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268363855
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 269148675
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270620912
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270738094
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 272593197
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273507547
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275606655
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": 276421817
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "These include planning and multi-step reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023); LLM-Evolve (You et al., 2024); Reflection-Bench (Li et al., 2024) Memory (§2.4) NarrativeQA (Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON parse failed: ```json\n{\n    \"reasoning\": \"The context lists several datasets and benchmarks, many of which are dir",
      "processing_time": 197.9595115184784,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "…these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021), MultiRC (Khashabi et al., 2018)),…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA",
        "MultiRC"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate mathematical reasoning capabilities, focusing on solving algebraic word problems with step-by-step solutions.",
        "MATH": "Used to assess mathematical reasoning, particularly in solving complex math problems and providing detailed explanations.",
        "AQUA-RAT": "Used to test mathematical reasoning through multiple-choice questions, emphasizing problem-solving strategies and logical steps.",
        "HotpotQA": "Used to evaluate multi-hop question answering, requiring the model to integrate information from multiple paragraphs to answer questions.",
        "StrategyQA": "Used to assess the ability to answer questions that require strategic thinking and multi-step reasoning processes.",
        "MultiRC": "Used to evaluate reading comprehension and multi-hop reasoning, focusing on identifying correct answers from multiple-choice options based on given passages."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets used to evaluate the planning capabilities of LLMs in various domains, including mathematical reasoning and multi-hop question answering.",
      "processing_time": 78.30901384353638,
      "citing_paper_id": "277150776",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "…reasoning, tool use, self-reflection, Agent Evaluation Agent Capabilities Evaluation (§2) Planning and Multi-Step Reasoning (§2.1) AQUA-RAT (Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AQUA-RAT",
        "HotpotQA",
        "ARC",
        "StrategyQA",
        "GSM8K",
        "MATH"
      ],
      "dataset_descriptions": {
        "AQUA-RAT": "Used to evaluate multi-step reasoning and problem-solving skills in algebraic word problems, focusing on the ability to generate and follow logical rationales.",
        "HotpotQA": "Applied to assess complex question answering, emphasizing multi-hop reasoning and the ability to integrate information from multiple documents.",
        "ARC": "Utilized to test scientific reasoning and commonsense knowledge, specifically designed to challenge AI systems with questions that require deep understanding.",
        "StrategyQA": "Employed to evaluate strategic reasoning and planning, focusing on the ability to solve problems that require sequential decision-making and long-term planning.",
        "GSM8K": "Used to assess numerical reasoning and problem-solving in grade school math problems, emphasizing the ability to handle multi-step calculations and logical sequences.",
        "MATH": "Applied to evaluate advanced mathematical reasoning and problem-solving, focusing on complex problems that require deep understanding of mathematical concepts and principles."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets that are used for evaluating reasoning and planning capabilities of language models.",
      "processing_time": 82.00443005561829,
      "citing_paper_id": "277150776",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "Early benchmarks such as MiniWob (Shi et al., 2017) and MiniWoB++ (Liu et al., 2018) provided fundamental frameworks for assessing navigation and task automation capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MiniWob",
        "MiniWoB++"
      ],
      "dataset_descriptions": {
        "MiniWob": "Used to assess navigation and task automation capabilities in web-based agents, providing a foundational framework for evaluating planning and interaction skills.",
        "MiniWoB++": "Extended version of MiniWob, used to enhance and refine the assessment of navigation and task automation capabilities in web-based agents, focusing on more complex tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions MiniWob and MiniWoB++ as early benchmarks for assessing navigation and task automation capabilities, which are relevant to the planning capabilities of LLMs.",
      "processing_time": 61.53931260108948,
      "citing_paper_id": "277150776",
      "cited_paper_id": 34953552
    },
    {
      "context_text": "…StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al., 2024a) Application-SpecificAgentEvaluation(§3) Web Agents (§3.1) MiniWob (Shi et al., 2017); MiniWoB++ (Liu et al., 2018); Web-Shop (Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MiniWob",
        "MiniWoB++",
        "Web-Shop",
        "Mind2web",
        "WebVoyager"
      ],
      "dataset_descriptions": {
        "MiniWob": "Used to evaluate web-based agents' performance on a variety of web tasks, focusing on navigation and interaction capabilities.",
        "MiniWoB++": "Extended version of MiniWob, used to assess agents' ability to handle more complex web interactions and tasks.",
        "Web-Shop": "Evaluates agents' ability to complete e-commerce tasks, such as searching for products and making purchases.",
        "Mind2web": "Assesses agents' ability to understand and execute web-based instructions, focusing on natural language processing and task completion.",
        "WebVoyager": "Used to evaluate agents' performance in navigating and interacting with web pages, emphasizing exploration and task-solving skills."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks and challenges, but none of them are traditional datasets. They are primarily used for evaluating web-based agents and their capabilities.",
      "processing_time": 75.95562410354614,
      "citing_paper_id": "277150776",
      "cited_paper_id": 34953552
    },
    {
      "context_text": "In web agent evaluation, for example, we have moved from basic simulations like MiniWob to dynamic online environments like WebArena and VisualWebArena , and from LAB-Bench which is static and narrow to Discovery-World for scientific agents.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and environments, but none of them are clearly identified as datasets. They are more likely to be benchmarks, challenges, or environments for evaluating agents.",
      "processing_time": 47.73008489608765,
      "citing_paper_id": "277150776",
      "cited_paper_id": 34953552
    },
    {
      "context_text": "In web agent evaluation, for example, we have moved from basic simulations like MiniWob to dynamic online environments like WebArena and VisualWebArena , and from LAB-Bench which is static and narrow to Discovery-World for scientific agents.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and environments, but none of them are clearly identified as datasets. They are more likely to be benchmarks, challenges, or environments for evaluating agents.",
      "processing_time": 47.73008489608765,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267199749
    },
    {
      "context_text": "Additional examples of crowdsourced task-oriented dialogue benchmarks are MultiWOZ (Budzianowski et al., 2018) and SMCalFlow (Andreas et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used as a benchmark for task-oriented dialogue systems, specifically for evaluating multi-domain dialogue management and natural language understanding."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions MultiWOZ and SMCalFlow as examples of crowdsourced task-oriented dialogue benchmarks. MultiWOZ is a specific dataset, while SMCalFlow is excluded as it is not clearly identified as a dataset in the context.",
      "processing_time": 54.709986209869385,
      "citing_paper_id": "277150776",
      "cited_paper_id": 52897360
    },
    {
      "context_text": "…(Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several named resources, but only 'MultiWOZ' is confirmed as a dataset through the provided title. Other names are likely methods, benchmarks, or tools.",
      "processing_time": 52.70389723777771,
      "citing_paper_id": "277150776",
      "cited_paper_id": 52897360
    },
    {
      "context_text": "0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation (§5)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ",
        "SciCode"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
        "SciCode": "Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and frameworks, but only MultiWOZ and SciCode are clearly identified as datasets in the cited paper titles.",
      "processing_time": 57.582067251205444,
      "citing_paper_id": "277150776",
      "cited_paper_id": 52897360
    },
    {
      "context_text": "0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation (§5)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ",
        "SciCode"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
        "SciCode": "Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and frameworks, but only MultiWOZ and SciCode are clearly identified as datasets in the cited paper titles.",
      "processing_time": 57.582067251205444,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268031860
    },
    {
      "context_text": "0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation (§5)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ",
        "SciCode"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
        "SciCode": "Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and frameworks, but only MultiWOZ and SciCode are clearly identified as datasets in the cited paper titles.",
      "processing_time": 57.582067251205444,
      "citing_paper_id": "277150776",
      "cited_paper_id": 271270048
    },
    {
      "context_text": "0 (Lou et al., 2025); ScienceAgentBench (Chen et al., 2024); CORE-Bench (Siegel et al., 2024); SciCode (Tian et al., 2024b); MLGym-Bench (Nathani et al., 2025); DiscoveryWorld (Jansen et al., 2024); LAB-Bench (Laurent et al., 2024) (§3.4) ABCD (Chen et al., 2021a); MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation (§5)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ",
        "SciCode"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate task-oriented dialogue systems, focusing on multi-domain interactions and natural language understanding.",
        "SciCode": "Employed to assess the performance of AI systems in scientific coding tasks, emphasizing the ability to understand and execute complex research workflows."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several benchmarks and frameworks, but only MultiWOZ and SciCode are clearly identified as datasets in the cited paper titles.",
      "processing_time": 57.582067251205444,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275757481
    },
    {
      "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and efficiency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to track success rates in complex question-answering tasks, evaluating the performance of language agents in understanding and reasoning over multiple documents.",
        "ALFWorld": "Used to evaluate interactive learning in embodied environments, assessing the ability of agents to align text instructions with physical actions in a simulated world."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld', both of which are specific datasets or environments used for evaluating models. However, 'Reflexion' and 'RAISE' are methods or frameworks, not datasets.",
      "processing_time": 63.29358959197998,
      "citing_paper_id": "277150776",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and efficiency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to track success rates in complex question-answering tasks, evaluating the performance of language agents in understanding and reasoning over multiple documents.",
        "ALFWorld": "Used to evaluate interactive learning in embodied environments, assessing the ability of agents to align text instructions with physical actions in a simulated world."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld', both of which are specific datasets or environments used for evaluating models. However, 'Reflexion' and 'RAISE' are methods or frameworks, not datasets.",
      "processing_time": 63.29358959197998,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to track success rates in complex question-answering tasks, focusing on the ability of language agents to reason and retrieve information effectively.",
        "ALFWorld": "Employed to evaluate interactive learning in embodied environments, assessing the alignment between textual instructions and physical actions in a simulated world."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld' as specific datasets used for evaluating the performance of language agents. These are multi-word proper nouns and are clearly identifiable datasets.",
      "processing_time": 61.71349740028381,
      "citing_paper_id": "277150776",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For example, Reflexion (Shinn et al., 2023) tracks success rate on tasks like HotPotQA (Yang et al., 2018) and ALFWorld (Shridhar et al., 2021), while RAISE (Liu et al., 2024a) enhances the ReAct framework with a two-part memory system evaluated through human judgment on quality metrics and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to track success rates in complex question-answering tasks, focusing on the ability of language agents to reason and retrieve information effectively.",
        "ALFWorld": "Employed to evaluate interactive learning in embodied environments, assessing the alignment between textual instructions and physical actions in a simulated world."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HotPotQA' and 'ALFWorld' as specific datasets used for evaluating the performance of language agents. These are multi-word proper nouns and are clearly identifiable datasets.",
      "processing_time": 61.71349740028381,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "…were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALF-World"
      ],
      "dataset_descriptions": {
        "ALF-World": "Used to evaluate planning capabilities in interactive environments, focusing on aligning text and embodied actions in multi-turn feedback loops."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks and challenges, but only ALF-World is a specific dataset used for interactive learning and planning tasks.",
      "processing_time": 52.412298917770386,
      "citing_paper_id": "277150776",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGIEval",
        "MedMCQA"
      ],
      "dataset_descriptions": {
        "AGIEval": "Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops.",
        "MedMCQA": "Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks and challenges, but they are not specific datasets. However, 'AGIEval' and 'MedMCQA' are named resources that could be considered datasets in the context of evaluating LLMs.",
      "processing_time": 63.44788074493408,
      "citing_paper_id": "277150776",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGIEval",
        "MedMCQA"
      ],
      "dataset_descriptions": {
        "AGIEval": "Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops.",
        "MedMCQA": "Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks and challenges, but they are not specific datasets. However, 'AGIEval' and 'MedMCQA' are named resources that could be considered datasets in the context of evaluating LLMs.",
      "processing_time": 63.44788074493408,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258108259
    },
    {
      "context_text": "Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGIEval",
        "MedMCQA"
      ],
      "dataset_descriptions": {
        "AGIEval": "Used to evaluate foundation models on human-centric tasks, focusing on reasoning and planning capabilities through multi-turn feedback loops.",
        "MedMCQA": "Repurposed to assess LLMs' ability to recognize and correct errors in medical question-answering tasks, providing insights into self-reflection and error correction."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks and challenges, but they are not specific datasets. However, 'AGIEval' and 'MedMCQA' are named resources that could be considered datasets in the context of evaluating LLMs.",
      "processing_time": 63.44788074493408,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "Several of these benchmarks, particularly HotpotQA, ALF-Worlds, and Game of 24, have been specifically adapted for evaluating agent-based approaches like ReAct, where planning and calling the tools proposed by the agent are interleaved in interactive problem-solving settings.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific benchmarks but does not indicate they are datasets. They are likely challenges or leaderboards, which are excluded unless they refer to specific, downloadable datasets.",
      "processing_time": 48.13705539703369,
      "citing_paper_id": "277150776",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…that systematically assess these capabilities across diverse domains, including: mathematical reasoning (GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQUA-RAT (Ling et al., 2017)), multi-hop question answering (HotpotQA (Yang et al., 2018), StrategyQA (Geva et al., 2021),…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "AQUA-RAT",
        "HotpotQA",
        "StrategyQA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate mathematical reasoning capabilities, focusing on grade school math problems with step-by-step solutions.",
        "MATH": "Employed to measure mathematical problem-solving skills, covering a wide range of math topics and difficulty levels.",
        "AQUA-RAT": "Applied to assess reasoning abilities in solving algebraic word problems, emphasizing logical steps and explanations.",
        "HotpotQA": "Utilized to test multi-hop question answering, requiring the integration of information from multiple paragraphs.",
        "StrategyQA": "Used to evaluate the ability to answer questions that require strategic thinking and multi-step reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used to assess planning capabilities of LLMs, particularly in mathematical reasoning and multi-hop question answering.",
      "processing_time": 73.26102375984192,
      "citing_paper_id": "277150776",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "…(Ling et al., 2017); HotpotQA (Yang et al., 2018); ARC (Clark et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024);…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA",
        "ARC",
        "StrategyQA",
        "GSM8K",
        "MATH",
        "Game of 24",
        "MINT",
        "PlanBench",
        "FlowBench"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate multi-hop reasoning capabilities in question-answering systems, focusing on complex information retrieval and synthesis.",
        "ARC": "Applied to assess commonsense reasoning and knowledge integration in AI models, emphasizing challenging and diverse question types.",
        "StrategyQA": "Utilized to test strategic reasoning and planning in question-answering tasks, specifically designed to require multi-step logical thinking.",
        "GSM8K": "Employed to evaluate mathematical problem-solving skills, focusing on grade school-level arithmetic and algebraic problems.",
        "MATH": "Used to measure advanced mathematical problem-solving abilities, covering a wide range of topics and difficulty levels.",
        "Game of 24": "Applied to assess numerical reasoning and problem-solving skills through a card game, requiring quick and accurate calculations.",
        "MINT": "Utilized to evaluate multi-step reasoning and planning in natural language processing tasks, focusing on complex problem-solving scenarios.",
        "PlanBench": "Used to benchmark planning capabilities in AI models, specifically designed to test sequential decision-making and long-term planning.",
        "FlowBench": "Applied to evaluate flow-based reasoning and planning, focusing on tasks that require understanding and manipulating sequences of actions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation span mentions multiple datasets, all of which are specific and verifiable. The context suggests these datasets are used for evaluating or training models on various reasoning and problem-solving tasks.",
      "processing_time": 101.717294216156,
      "citing_paper_id": "277150776",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Others focused on the synthesis and contextualization of scientific literature, such as QASPER (Dasigi et al., 2021), QASA (Lee et al., 2023), and MS 2 (DeYoung et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions QASPER, QASA, and MS 2, but these are not datasets. They are methods or tools for synthesizing and contextualizing scientific literature. No specific datasets are mentioned.",
      "processing_time": 49.34435701370239,
      "citing_paper_id": "277150776",
      "cited_paper_id": 233231380
    },
    {
      "context_text": "Others focused on the synthesis and contextualization of scientific literature, such as QASPER (Dasigi et al., 2021), QASA (Lee et al., 2023), and MS 2 (DeYoung et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions QASPER, QASA, and MS 2, but these are not datasets. They are methods or tools for synthesizing and contextualizing scientific literature. No specific datasets are mentioned.",
      "processing_time": 49.34435701370239,
      "citing_paper_id": "277150776",
      "cited_paper_id": 234093776
    },
    {
      "context_text": "…et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SUPER"
      ],
      "dataset_descriptions": {
        "SUPER": "Used to evaluate agents on setting up and executing tasks from research repositories, focusing on the planning capabilities of LLMs in scientific contexts."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several named resources, but most are likely benchmarks or challenges rather than traditional datasets. Only 'SUPER' is confirmed as a dataset through the cited paper title.",
      "processing_time": 53.9226610660553,
      "citing_paper_id": "277150776",
      "cited_paper_id": 234093776
    },
    {
      "context_text": "…et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al., 2025); AAAR-1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SUPER"
      ],
      "dataset_descriptions": {
        "SUPER": "Used to evaluate agents on setting up and executing tasks from research repositories, focusing on the planning capabilities of LLMs in scientific contexts."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several named resources, but most are likely benchmarks or challenges rather than traditional datasets. Only 'SUPER' is confirmed as a dataset through the cited paper title.",
      "processing_time": 53.9226610660553,
      "citing_paper_id": "277150776",
      "cited_paper_id": 272593197
    },
    {
      "context_text": "…(Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021); ScienceWorld (Wang et al., 2022a); SUPER (Bogin et al., 2024); Ideation (Si et al.,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IT-Bench",
        "SWELancer",
        "ScienceQA",
        "QASPER",
        "MS 2",
        "ScienceWorld",
        "SUPER",
        "Ideation"
      ],
      "dataset_descriptions": {
        "IT-Bench": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "SWELancer": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "ScienceQA": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "QASPER": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "MS 2": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "ScienceWorld": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "SUPER": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context.",
        "Ideation": "Mentioned as a reusable resource, but specific usage details are not provided in the citation context."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several named datasets and benchmarks. However, the context does not specify how these resources are used in the research, making it difficult to provide detailed usage descriptions.",
      "processing_time": 90.64554166793823,
      "citing_paper_id": "277150776",
      "cited_paper_id": 234093776
    },
    {
      "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NaturalQuestions-Open",
        "multi-session chat datasets"
      ],
      "dataset_descriptions": {
        "NaturalQuestions-Open": "Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
        "multi-session chat datasets": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
      "processing_time": 62.000853538513184,
      "citing_paper_id": "277150776",
      "cited_paper_id": 236034497
    },
    {
      "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NaturalQuestions-Open",
        "multi-session chat datasets"
      ],
      "dataset_descriptions": {
        "NaturalQuestions-Open": "Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
        "multi-session chat datasets": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
      "processing_time": 62.000853538513184,
      "citing_paper_id": "277150776",
      "cited_paper_id": 259360665
    },
    {
      "context_text": "Similarly, A-MEM introduces an advanced memory architecture evaluated using the Lo-CoMo benchmark (Maharana et al., 2024), while MemGPT manages a tiered memory system tested on NaturalQuestions-Open (Liu et al., 2024b) and multi-session chat datasets (Xu et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NaturalQuestions-Open",
        "multi-session chat datasets"
      ],
      "dataset_descriptions": {
        "NaturalQuestions-Open": "Used to test the tiered memory system in MemGPT, focusing on open-domain question answering and long-term context management.",
        "multi-session chat datasets": "Used to evaluate the performance of MemGPT in managing multi-session conversations, assessing the system's ability to maintain context across multiple interactions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'NaturalQuestions-Open' and 'multi-session chat datasets', which are specific datasets used for evaluating memory systems in language models.",
      "processing_time": 62.000853538513184,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263909014
    },
    {
      "context_text": "…(Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024);…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AutoPlanBench",
        "ACPBench"
      ],
      "dataset_descriptions": {
        "AutoPlanBench": "Used to evaluate planning capabilities of LLMs, focusing on automated planning tasks and benchmarking performance across various planning scenarios.",
        "ACPBench": "Used to assess the planning abilities of LLMs, specifically targeting complex planning tasks and evaluating the effectiveness of chain-of-thought reasoning."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks and datasets, but most are excluded as they are primarily used for score comparison. Only 'AutoPlanBench' and 'ACPBench' are included as they are specifically designed for planning capabilities.",
      "processing_time": 60.820183992385864,
      "citing_paper_id": "277150776",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "…2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "ARC": "Used to assess scientific reasoning capabilities, focusing on abstract and concrete reasoning problems in a variety of domains.",
        "FOLIO": "Used to evaluate logical reasoning skills, specifically first-order logic inference and problem-solving tasks.",
        "P-FOLIO": "Used to assess logical reasoning, particularly in more complex and practical scenarios involving first-order logic.",
        "Game of 24": "Used to test constraint satisfaction and arithmetic reasoning, focusing on solving mathematical puzzles with specific constraints.",
        "MUSR": "Used to evaluate everyday common sense reasoning, particularly through multistep soft reasoning tasks.",
        "BBH": "Used to challenge reasoning abilities with difficult tasks, including logical, mathematical, and commonsense reasoning problems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and benchmarks, but only those that are specific and verifiable are included. The others are excluded as they are either benchmark suites or not clearly identified.",
      "processing_time": 79.21019220352173,
      "citing_paper_id": "277150776",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "…2018)), scientific reasoning (ARC (Clark et al., 2018a)), logical reasoning (FOLIO, P-FOLIO (Han et al., 2024, 2022)) constraint satisfaction puzzles (Game of 24 (Yao et al., 2023)), everyday common sense (MUSR (Sprague et al., 2023)), and challenging reasoning tasks (BBH (Suzgun et al., 2022)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ARC",
        "FOLIO",
        "P-FOLIO",
        "Game of 24",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "ARC": "Used to assess scientific reasoning capabilities, focusing on abstract and concrete reasoning problems in a variety of domains.",
        "FOLIO": "Used to evaluate logical reasoning skills, specifically first-order logic inference and problem-solving tasks.",
        "P-FOLIO": "Used to assess logical reasoning, particularly in more complex and practical scenarios involving first-order logic.",
        "Game of 24": "Used to test constraint satisfaction and arithmetic reasoning, focusing on solving mathematical puzzles with specific constraints.",
        "MUSR": "Used to evaluate everyday common sense reasoning, particularly through multistep soft reasoning tasks.",
        "BBH": "Used to challenge reasoning abilities with difficult tasks, including logical, mathematical, and commonsense reasoning problems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and benchmarks, but only those that are specific and verifiable are included. The others are excluded as they are either benchmark suites or not clearly identified.",
      "processing_time": 79.21019220352173,
      "citing_paper_id": "277150776",
      "cited_paper_id": 264439655
    },
    {
      "context_text": "Multi-step reasoning in LLMs typically involves executing sequential logical operations—typically requiring 3-10 intermediate steps—to arrive at solutions that cannot be derived through single-step inference (Cobbe et al., 2021; Yang et al., 2018; Suzgun et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the concept of multi-step reasoning in LLMs. No verifiable resources are identified.",
      "processing_time": 48.30470943450928,
      "citing_paper_id": "277150776",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "Early efforts to gauge LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGIEval"
      ],
      "dataset_descriptions": {
        "AGIEval": "Used to evaluate foundation models' reasoning and planning capabilities through multi-turn feedback loops, focusing on self-reflection and human-centric tasks."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions several benchmarks and challenges, but none of them are explicitly referred to as datasets. However, 'AGIEval' is mentioned and could be considered a dataset given its nature as a benchmark.",
      "processing_time": 55.97549772262573,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258108259
    },
    {
      "context_text": "…2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions several datasets/benchmarks but does not specify how they are used in the context of planning capabilities of LLMs. The context is too vague to determine specific usage.",
      "processing_time": 49.508052587509155,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "…2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could recognize or correct their own errors given external feedback in confined settings (Renze and Guven, 2024; Huang et al., 2024; Shinn et al., 2023; You et al., 2024; Sun et al., 2023; Liu et al., 2025).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions several datasets/benchmarks but does not specify how they are used in the context of planning capabilities of LLMs. The context is too vague to determine specific usage.",
      "processing_time": 49.508052587509155,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263609132
    },
    {
      "context_text": "Beyond context length optimization, memory mechanisms also enhance real-time decision-making and learning in agent settings, focusing on action optimization (Liu et al., 2024a; Shinn et al., 2023; Wang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses memory mechanisms and their impact on real-time decision-making and learning in agent settings.",
      "processing_time": 47.93809962272644,
      "citing_paper_id": "277150776",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "…et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions several benchmarks and tools, but none of them are clearly identified as datasets. They appear to be more related to methods, frameworks, or benchmarks for evaluating LLM capabilities.",
      "processing_time": 49.66128087043762,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "StreamBench (Wu et al., 2024a) represents a more challenging setting, evaluating how agents leverage external memory components—including the memory of previous interactions and external feed-5 back—to continuously improve performance over time, with quality and efficiency assessed across diverse datasets including text-to-SQL tasks (e.g., Spider (Yu et al., 2018)), ToolBench (Xu et al., 2023), and HotpotQA (Yang et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Spider",
        "ToolBench",
        "HotpotQA"
      ],
      "dataset_descriptions": {
        "Spider": "Used to evaluate text-to-SQL task performance, focusing on the ability of agents to generate SQL queries from natural language instructions.",
        "ToolBench": "Used to assess the capability of LLMs to interact with real-world APIs, emphasizing the integration and utilization of external tools.",
        "HotpotQA": "Used to evaluate multi-hop reasoning skills, focusing on the ability to answer complex questions requiring information from multiple sources."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used to evaluate the performance of agents in StreamBench, including Spider, ToolBench, and HotpotQA. These datasets are clearly identified and used for evaluating the planning capabilities of LLMs.",
      "processing_time": 68.55091094970703,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "StableToolBench (Guo et al., 2024) addresses the challenges of function-calling evaluation by introducing a virtual API server with caching and simulators to alleviate API status changes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called StableToolBench. The context focuses on the functionality and purpose of this tool, which is to address challenges in function-calling evaluation.",
      "processing_time": 50.58357644081116,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Benchmarks such as ToolAl-paca (Tang et al., 2023), APIBench (Patil et al., 2025), ToolBench (Qin et al., 2023), and the Berkeley Function Calling Leaderboard v1 (BFCL) (Yan et al., 2024) exemplify this phase, employing synthetic datasets and rule-based matching (e.g., via Abstract Syntax Trees) to establish baseline metrics like pass rates and structural accuracy.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as specific, reusable datasets. Instead, they are described as tools or leaderboards used to establish baseline metrics.",
      "processing_time": 50.302722215652466,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "The ability of LLMs to interact with external tools through function calling is fundamental for building intelligent agents capable of delivering real-time, contextually accurate responses (Qin et al., 2023; Tang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of LLMs to interact with external tools. No verifiable resources are identified.",
      "processing_time": 50.301263093948364,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions several benchmarks and tools, but none of them are clearly identified as datasets. The context does not provide specific details on how these resources are used in the research.",
      "processing_time": 51.47380185127258,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Benchmarks such as ToolAl-paca (Tang et al., 2023), APIBench (Patil et al., 2025), ToolBench (Qin et al., 2023), and the Berkeley Function Calling Leaderboard v1 (BFCL) (Yan et al., 2024) exemplify this phase, employing synthetic datasets and rule-based matching (e.g., via Abstract Syntax Trees) to…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. It describes their use in employing synthetic datasets and rule-based matching, but the benchmarks themselves are not datasets.",
      "processing_time": 52.5287344455719,
      "citing_paper_id": "277150776",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…(Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MINT"
      ],
      "dataset_descriptions": {
        "MINT": "Used to evaluate LLMs in multi-turn interactions with tools and language feedback, focusing on the planning capabilities of LLMs in complex, interactive scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks and datasets, but only 'MINT' is a specific dataset used for evaluating LLMs in multi-turn interactions. Others are excluded as they are either benchmarks or tools.",
      "processing_time": 61.66588234901428,
      "citing_paper_id": "277150776",
      "cited_paper_id": 262053695
    },
    {
      "context_text": "…et al., 2018a); StrategyQA (Geva et al., 2021); GSM8K (Cobbe et al., 2021); MATH (Hendrycks et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "StrategyQA",
        "GSM8K",
        "MATH",
        "Game of 24",
        "MINT",
        "PlanBench",
        "FlowBench",
        "FOLIO",
        "P-FOLIO"
      ],
      "dataset_descriptions": {
        "StrategyQA": "Used to evaluate LLMs' ability to solve strategic questions, focusing on multi-step reasoning and planning.",
        "GSM8K": "Used to assess LLMs' performance on grade school math problems, emphasizing step-by-step problem-solving skills.",
        "MATH": "Used to test LLMs' mathematical reasoning and problem-solving abilities, particularly in complex and multi-step problems.",
        "Game of 24": "Used to evaluate LLMs' numerical reasoning and strategic planning in solving arithmetic puzzles.",
        "MINT": "Used to assess LLMs' performance in multi-turn interactions with tools and language feedback, focusing on dynamic and interactive planning.",
        "PlanBench": "Used to evaluate LLMs' planning capabilities in various domains, focusing on task-oriented and goal-directed behavior.",
        "FlowBench": "Used to test LLMs' ability to follow and generate procedural instructions, emphasizing sequential and logical reasoning.",
        "FOLIO": "Used to evaluate LLMs' performance in financial and operational planning tasks, focusing on real-world application scenarios.",
        "P-FOLIO": "Used to assess LLMs' planning capabilities in personalized financial and operational contexts, emphasizing adaptability and user-specific strategies."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several datasets and benchmarks, but only those that are specific and verifiable are included. The context suggests these are used to evaluate the planning capabilities of LLMs.",
      "processing_time": 108.68282651901245,
      "citing_paper_id": "277150776",
      "cited_paper_id": 262053695
    },
    {
      "context_text": "The MINT benchmark (Wang et al., 2023) evaluates planning in interactive environments, showing that even advanced LLMs struggle with long-horizon tasks requiring multiple steps.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the MINT benchmark, which is used to evaluate planning capabilities in interactive environments. However, MINT is a benchmark/challenge and not a traditional dataset.",
      "processing_time": 53.71986436843872,
      "citing_paper_id": "277150776",
      "cited_paper_id": 262053695
    },
    {
      "context_text": "Improvement was typically measured by determining if the final answer was corrected, providing only a coarse evaluation and potentially ill-defined measurement, as observed improvements may depend on specific prompting techniques lacking proper standardization (Huang et al., 2024; Liu et al., 2025).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses issues with evaluation methods and prompting techniques.",
      "processing_time": 51.338730812072754,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263609132
    },
    {
      "context_text": "…(Koˇcisk`y et al., 2018); QMSum (Zhong et al., 2021); QUALITY (Pang et al., 2021); RAISE (Liu et al., 2024a); ReadA-gent (Lee et al., 2024); MemGPT (Packer et al., 2024); LoCoMo (Maharana et al., 2024); A-MEM (Xu et al., 2025); StreamBench (Wu et al., 2024a); LTMbenchmark (Castillo-Bolado et al.,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QMSum",
        "QUALITY"
      ],
      "dataset_descriptions": {
        "QMSum": "Used to evaluate summarization quality, focusing on multi-document summarization tasks and assessing coherence and informativeness.",
        "QUALITY": "Used to assess the quality of generated text, specifically evaluating fluency, coherence, and relevance in natural language generation tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation span mentions several names, but they are primarily models, methods, or benchmarks, not datasets. Only 'QMSum' and 'QUALITY' appear to be datasets based on their naming conventions.",
      "processing_time": 67.07044506072998,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263909014
    },
    {
      "context_text": "Recent works, such as ReadAgent (Lee et al., 2024), MemGPT (Packer et al., 2024), and A-MEM (Xu et al., 2025), investigate these methods and evaluate their efficacy through reasoning and retrieval metrics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several works but does not specify any datasets. It focuses on methods and their evaluation through metrics.",
      "processing_time": 52.36287713050842,
      "citing_paper_id": "277150776",
      "cited_paper_id": 263909014
    },
    {
      "context_text": "…et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlowBench",
        "FOLIO",
        "P-FOLIO",
        "MultiRC",
        "MUSR",
        "BBH",
        "ToolEmu",
        "MINT",
        "AutoPlanBench"
      ],
      "dataset_descriptions": {
        "FlowBench": "Used to evaluate the planning capabilities of LLMs, focusing on multi-step reasoning and task execution.",
        "FOLIO": "Used to assess the logical reasoning and inference abilities of LLMs in complex scenarios.",
        "P-FOLIO": "Used to evaluate the planning and logical reasoning capabilities of LLMs, with a focus on progressive complexity.",
        "MultiRC": "Used to test the ability of LLMs to reason about multiple correct answers in reading comprehension tasks.",
        "MUSR": "Used to evaluate the chain-of-thought reasoning capabilities of LLMs through multistep soft reasoning tasks.",
        "BBH": "Used to assess the robustness and generalization of LLMs in a variety of challenging reasoning tasks.",
        "ToolEmu": "Used to evaluate the ability of LLMs to emulate tools and perform complex tasks requiring planning.",
        "MINT": "Used to test the multi-step reasoning and planning capabilities of LLMs in natural language understanding tasks.",
        "AutoPlanBench": "Used to evaluate the automatic planning capabilities of LLMs in various domains and scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmark datasets and tools, but only those that are clearly identified as datasets are included. The context suggests these are used for evaluating planning capabilities.",
      "processing_time": 103.34426021575928,
      "citing_paper_id": "277150776",
      "cited_paper_id": 264439655
    },
    {
      "context_text": "…(Yao et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VisualWebArena"
      ],
      "dataset_descriptions": {
        "VisualWebArena": "Used to evaluate multimodal agents on realistic visual web tasks, focusing on the performance of agents in complex, real-world scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation span mentions several benchmarks and challenges, which are primarily used for score comparison rather than as specific, reusable datasets. However, 'VisualWebArena' is mentioned in a context that suggests it could be a specific dataset, especially given the cited paper's title.",
      "processing_time": 64.80457401275635,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267199749
    },
    {
      "context_text": "…et al., 2022); Mind2web (Deng et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al.,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VisualWebArena"
      ],
      "dataset_descriptions": {
        "VisualWebArena": "Used to evaluate multimodal agents on realistic visual web tasks, focusing on the performance of agents in complex, real-world scenarios."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation span mentions several benchmarks and challenges, which are primarily used for score comparison rather than as reusable datasets. However, 'VisualWebArena' is mentioned in a context that suggests it could be a specific, downloadable dataset beyond the leaderboard, especially given the cited paper's title.",
      "processing_time": 65.31920528411865,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267199749
    },
    {
      "context_text": "WebArena (Zhou et al., 2023) and its visual variant, Visual-WebArena (Koh et al., 2024), incorporate realistic user interface elements and visual cues, requiring agents to not only follow predefined workflows but also interpret and respond to visual information.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WebArena' and 'Visual-WebArena', which are environments for evaluating agents, but they are not datasets. They are described as incorporating realistic user interface elements and visual cues, which are used to test agent capabilities.",
      "processing_time": 57.74663996696472,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267199749
    },
    {
      "context_text": "API-Blend (Basu et al., 2024a) suggested a comprehensive approach focusing on identifying, curating, and transforming existing datasets into a large corpus for training and systematic testing of tool-augmented LLMs. API-Blend mimics real-world scenarios involving API tasks such as API/tool…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing datasets' but does not specify any particular dataset names. The focus is on the method (API-Blend) rather than a specific dataset.",
      "processing_time": 54.64295291900635,
      "citing_paper_id": "277150776",
      "cited_paper_id": 267897727
    },
    {
      "context_text": "…AgentBench (Liu et al., 2023b); Galileo’s Agent Leaderboard (Bhavsar, 2025); OSWorld (Xie et al., 2024); AppWorld (Trivedi et al., 2024); OmniACT (Kapoor et al., 2024a); TheAgentCompany (Xu et al., 2024); CR-MArena (Huang et al., 2025); HAL (Stroebl et al., 2025) Frameworks for Agent Evaluation…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OmniACT"
      ],
      "dataset_descriptions": {
        "OmniACT": "Used to enable multimodal generalist autonomous agents for desktop and web, focusing on evaluating planning capabilities through a diverse set of tasks and environments."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several frameworks and leaderboards, but only 'OmniACT' is identified as a dataset and benchmark. The other items are excluded as they are likely leaderboards or frameworks.",
      "processing_time": 24.753278255462646,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268031860
    },
    {
      "context_text": "Benchmarks like OS-World (Xie et al., 2024), OmniACT (Kapoor et al., 2024a), and AppWorld (Trivedi et al., 2024) test whether agents can navigate real-world computer systems, execute complex tasks, and coordinate actions across multiple applications.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OS-World",
        "OmniACT",
        "AppWorld"
      ],
      "dataset_descriptions": {
        "OS-World": "Used to test agents' ability to navigate and execute tasks in real-world computer systems, focusing on the coordination of actions across multiple applications.",
        "OmniACT": "Used to evaluate multimodal generalist autonomous agents in desktop and web environments, testing their ability to perform complex tasks and navigate diverse interfaces.",
        "AppWorld": "Used to assess agents' performance in executing complex tasks and coordinating actions across multiple applications, emphasizing real-world computer system navigation."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions specific benchmarks that are used to test the capabilities of agents in navigating and executing tasks in real-world computer systems. These benchmarks are likely to contain datasets or environments that are used for evaluation.",
      "processing_time": 39.252662658691406,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268031860
    },
    {
      "context_text": "In addition, WorkArena (Drouin et al., 2024) and WorkArena++ (Boisvert et al., 2025) simulate complex, multi-step tasks typical of office or enterprise environments, where coordinating several actions is necessary to achieve long-term objectives.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WorkArena' and 'WorkArena++' as simulations for complex, multi-step tasks in office environments. However, these are described as simulation environments rather than datasets.",
      "processing_time": 19.71761393547058,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268363855
    },
    {
      "context_text": "…MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et al., 2024); WorkArena++ (Boisvert et al., 2025) Software Engineering Agents (§3.2) HumanEval (Chen et al., 2021b); SWE-bench (Jimenez et…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks and challenges, but none of them are specific datasets used for training or evaluation. They are primarily used for evaluating the performance of models.",
      "processing_time": 18.289218425750732,
      "citing_paper_id": "277150776",
      "cited_paper_id": 268363855
    },
    {
      "context_text": "…et al., 2023); WebVoy-ager (He et al., 2024); WebLinX (Lù et al., 2024); WebArena (Zhou et al., 2023); VisualWebArena (Koh et al., 2024); MMInA (Zhang et al., 2024); AssistantBench (Yoran et al., 2024); Web-Canvas (Pan et al., 2024b); ST-WebAgentBench (Levy et al., 2024); WorkArena (Drouin et…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several benchmarks and challenges, which are primarily used for score comparison and not as specific, reusable datasets. None of the names meet the criteria for inclusion as datasets.",
      "processing_time": 19.493847846984863,
      "citing_paper_id": "277150776",
      "cited_paper_id": 269148675
    },
    {
      "context_text": "MMInA (Zhang et al., 2024) provides multimodal, Multi-hop, holistic evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMInA, which is described as a benchmark for evaluating multihop multimodal internet agents. However, it does not specify a dataset, but rather a benchmark or evaluation framework.",
      "processing_time": 21.376720905303955,
      "citing_paper_id": "277150776",
      "cited_paper_id": 269148675
    },
    {
      "context_text": "…Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SWT-Bench"
      ],
      "dataset_descriptions": {
        "SWT-Bench": "Used to test and validate real-world bug-fixes with code agents, focusing on the performance and effectiveness of code agents in fixing bugs."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions several benchmark names, but they are primarily used for score comparison rather than as specific, reusable datasets. However, 'SWT-Bench' is a specific dataset used for testing and validating real-world bug-fixes with code agents.",
      "processing_time": 40.152042865753174,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270620912
    },
    {
      "context_text": "TDD-Bench Verified (Ahmed et al., 2024) and SWT-Bench (Mündler et al., 2024) evaluate the agent’s ability to generate tests from user issues in real-world Github repositories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TDD-Bench",
        "SWT-Bench"
      ],
      "dataset_descriptions": {
        "TDD-Bench": "Used to evaluate the agent’s ability to generate tests from user issues in real-world Github repositories, focusing on test-driven development practices.",
        "SWT-Bench": "Used to evaluate the agent’s ability to generate tests from user issues in real-world Github repositories, specifically for validating real-world bug-fixes with code agents."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two benchmarks, TDD-Bench and SWT-Bench, which are used to evaluate the ability of agents to generate tests from user issues in real-world Github repositories.",
      "processing_time": 34.38026738166809,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270620912
    },
    {
      "context_text": "…2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024); NESTFUL (Basu et al., 2024b) Self-Reflection (§2.3) LLF-Bench (Cheng et al., 2023);…",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several benchmarks and tools, but none of them are identified as specific, reusable datasets. APIGen is mentioned, but it is a method for generating datasets, not a dataset itself.",
      "processing_time": 20.929888248443604,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270738094
    },
    {
      "context_text": "APIGen (Liu et al., 2024c) provides a comprehensive automated data generation pipeline that synthesizes high-quality function-calling datasets verified through hierarchical stages.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'function-calling datasets' which are synthesized and verified through hierarchical stages. However, the name 'APIGen' is a tool or method, not a dataset.",
      "processing_time": 31.523974180221558,
      "citing_paper_id": "277150776",
      "cited_paper_id": 270738094
    },
    {
      "context_text": "(3) Code Generation for Experiment Execution: Benchmarks such as SciCode (Tian et al., 2024a), ScienceAgentBench (Chen et al., 2025), SUPER (Bogin et al., 2024), and CORE-Bench (Siegel et al., 2024) are pivotal in verifying whether agents can produce accurate, executable scientific code.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SciCode",
        "SUPER"
      ],
      "dataset_descriptions": {
        "SciCode": "Used to verify agents' ability to produce accurate, executable scientific code, focusing on research coding tasks curated by scientists.",
        "SUPER": "Evaluates agents on setting up and executing tasks from research repositories, assessing their capability to handle scientific workflows."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks, but they are primarily used for score comparison rather than as specific, downloadable datasets. However, 'SciCode' and 'SUPER' are mentioned in the cited paper titles, which suggests they are specific resources.",
      "processing_time": 45.331743001937866,
      "citing_paper_id": "277150776",
      "cited_paper_id": 271270048
    },
    {
      "context_text": "(3) Code Generation for Experiment Execution: Benchmarks such as SciCode (Tian et al., 2024a), ScienceAgentBench (Chen et al., 2025), SUPER (Bogin et al., 2024), and CORE-Bench (Siegel et al., 2024) are pivotal in verifying whether agents can produce accurate, executable scientific code.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SciCode",
        "SUPER"
      ],
      "dataset_descriptions": {
        "SciCode": "Used to verify agents' ability to produce accurate, executable scientific code, focusing on research coding tasks curated by scientists.",
        "SUPER": "Evaluates agents on setting up and executing tasks from research repositories, assessing their capability to handle scientific workflows."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks, but they are primarily used for score comparison rather than as specific, downloadable datasets. However, 'SciCode' and 'SUPER' are mentioned in the cited paper titles, which suggests they are specific resources.",
      "processing_time": 45.331743001937866,
      "citing_paper_id": "277150776",
      "cited_paper_id": 272593197
    },
    {
      "context_text": "SWE-bench Multimodal (Yang et al., 2024) evaluates agents in visual software domains, targeting JavaScript-based applications with visual elements in problems and tests.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SWE-bench Multimodal"
      ],
      "dataset_descriptions": {
        "SWE-bench Multimodal": "Used to evaluate agents in visual software domains, specifically targeting JavaScript-based applications with visual elements in problems and tests."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions SWE-bench Multimodal, which is a specific dataset used to evaluate agents in visual software domains. The context indicates it is used for evaluating performance in JavaScript-based applications with visual elements.",
      "processing_time": 25.04140830039978,
      "citing_paper_id": "277150776",
      "cited_paper_id": 272593197
    },
    {
      "context_text": "…et al., 2023); SWE-bench Verified (OpenAI, 2024); SWE-bench Lite (SWE-bench Lite, 2024); SWE-bench+ (Aleithan et al., 2024); SWE-bench Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SWE-bench Verified",
        "TDD-Bench Verified"
      ],
      "dataset_descriptions": {
        "SWE-bench Verified": "Used to evaluate the planning capabilities of LLMs in executing tasks from research repositories, focusing on task setup and execution accuracy.",
        "TDD-Bench Verified": "Used to assess the test-driven development skills of LLMs, specifically their ability to write tests and implement corresponding code."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions several benchmark datasets, but they are primarily used for score comparison rather than as reusable datasets. However, 'SWE-bench Verified' and 'TDD-Bench Verified' are included as they seem to be specific, downloadable datasets.",
      "processing_time": 48.421050786972046,
      "citing_paper_id": "277150776",
      "cited_paper_id": 272593197
    },
    {
      "context_text": "ACPBench (Kokel et al., 2024) focuses on evaluating LLMs on core reasoning skills.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate LLMs on core reasoning skills, specifically focusing on action, change, and planning. The dataset provides tasks to assess the model's ability to reason about these aspects."
      },
      "confidence_score": 1.0,
      "reasoning": "ACPBench is mentioned as a benchmark for evaluating LLMs on core reasoning skills, which aligns with the research topic of planning capabilities of LLMs.",
      "processing_time": 24.578903198242188,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "…et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan et al., 2023); MINT (Wang et al., 2023); AutoPlanBench (Stein et al., 2023); ACPBench (Kokel et al., 2024); Natural Plan (Zheng et al., 2024) Function Calling & Tool Use (§2.2) BFCL (Yan et al., 2024); ToolBench (Qin et al., 2023);…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ACPBench"
      ],
      "dataset_descriptions": {
        "ACPBench": "Used to evaluate reasoning about action, change, and planning, focusing on the ability of models to understand and predict the outcomes of actions in dynamic environments."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions several benchmarks and challenges, which are typically not included unless they refer to specific, downloadable datasets. However, 'ACPBench' is mentioned in a context that suggests it might be a specific dataset or benchmark with a downloadable component.",
      "processing_time": 28.114959001541138,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273228858
    },
    {
      "context_text": "From a cognitive science perspective, Reflection-Bench (Li et al., 2024) was designed to assess LLMs’ cognitive reflection capabilities, breaking down reflection into components like perception of new information, memory usage, belief updating following surprise, decision-making adjustments,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reflection-Bench"
      ],
      "dataset_descriptions": {
        "Reflection-Bench": "Used to assess LLMs' cognitive reflection capabilities, focusing on components such as perception of new information, memory usage, belief updating, and decision-making adjustments."
      },
      "confidence_score": 0.9,
      "reasoning": "Reflection-Bench is mentioned as a tool to assess LLMs' cognitive reflection capabilities, which aligns with the research topic of planning capabilities of LLMs.",
      "processing_time": 24.048276901245117,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273507547
    },
    {
      "context_text": "From a cognitive science perspective, Reflection-Bench (Li et al., 2024) was designed to assess LLMs’ cognitive reflection capabilities, breaking down reflection into components like perception of new information, memory usage, belief updating following surprise, decision-making adjustments, counterfactual reasoning, and meta-reflection.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reflection-Bench"
      ],
      "dataset_descriptions": {
        "Reflection-Bench": "Used to assess LLMs' cognitive reflection capabilities, including perception, memory usage, belief updating, decision-making, counterfactual reasoning, and meta-reflection."
      },
      "confidence_score": 0.9,
      "reasoning": "Reflection-Bench is mentioned as a tool to assess LLMs' cognitive reflection capabilities, which aligns with the research topic of planning capabilities of LLMs.",
      "processing_time": 23.921558141708374,
      "citing_paper_id": "277150776",
      "cited_paper_id": 273507547
    },
    {
      "context_text": "Addressing the inherent complexity of multi-step interactions, ComplexFuncBench (Zhong et al., 2025) was specifically designed to assess scenarios requiring implicit parameter inference, adherence to user-defined constraints, and efficient long-context processing.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ComplexFuncBench, which is a benchmark or challenge, not a dataset. It does not meet the criteria for inclusion as a dataset.",
      "processing_time": 17.88527536392212,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275606655
    },
    {
      "context_text": "…et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023); APIGen (Liu et al., 2024c); StableToolBench (Guo et al., 2024);…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several names, but they are primarily tools, benchmarks, or methods rather than datasets. ComplexFuncBench is the only name that could potentially be a dataset, but it is more likely a benchmark or method based on the cited paper title.",
      "processing_time": 21.354273557662964,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275606655
    },
    {
      "context_text": "Similarly, the continuous refinement and variant creation within the SWE-bench family ( SWE-bench Lite , SWE-bench Verified , SWE-bench+ ) along with the development of IntellAgent based on τ -Bench , demonstrates an ongoing effort to enhance and adapt agent benchmarks to meet evolving evaluation needs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not specify their use as datasets. It focuses on the evolution and adaptation of benchmarks rather than their use in a specific research context.",
      "processing_time": 18.619998455047607,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275757481
    },
    {
      "context_text": "IntellAgent (Levi and Kadar, 2025a) provides an open-source framework for automatic benchmarking of conversational agents, taking a schema of the system database and a company policies document as input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions an open-source framework for benchmarking conversational agents but does not reference any specific datasets. The framework takes a schema and a company policies document as input, which are not considered datasets.",
      "processing_time": 21.1885769367218,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275757481
    },
    {
      "context_text": "…MultiWOZ (Budzianowski et al., 2018); SMCalFlow (Andreas et al., 2020); ALMITA (Arcad-inho et al., 2024); τ -Bench (Yao et al., 2024); IntellAgent (Levi and Kadar, 2025a); LTM (Castillo-Bolado et al., 2024b) Generalist Agents Evaluation (§4) GAIA (Mialon et al., 2023); AgentBench (Liu et al.,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MultiWOZ",
        "SMCalFlow"
      ],
      "dataset_descriptions": {
        "MultiWOZ": "Used to train and evaluate dialogue systems, focusing on multi-domain task-oriented conversations and natural language understanding.",
        "SMCalFlow": "Applied to assess the performance of conversational agents in handling complex calendar and scheduling tasks, emphasizing natural language processing and dialogue management."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several named resources, but most are models or benchmarks. Only 'MultiWOZ' and 'SMCalFlow' are datasets used for training and evaluation in dialogue systems.",
      "processing_time": 28.71096897125244,
      "citing_paper_id": "277150776",
      "cited_paper_id": 275757481
    },
    {
      "context_text": "…Multimodal (Yang et al., 2024); TDD-Bench Verified (Ahmed et al., 2024); SWT-Bench (Mündler et al., 2024); IT-Bench (Jha et al., 2025); SWELancer (Miserendino et al., 2025) Scientific Agents (§3.3) ScienceQA (Lu et al., 2022); QASPER (Dasigi et al., 2021); MS 2 (DeY-oung et al., 2021);…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks and challenges, which are typically excluded unless they refer to specific, downloadable datasets. None of the names meet the criteria for inclusion as datasets.",
      "processing_time": 18.985368967056274,
      "citing_paper_id": "277150776",
      "cited_paper_id": 276421817
    },
    {
      "context_text": "Finally, the introduction of SWE-Lancer (Miserendino et al., 2025) represents the latest trend in benchmark development.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SWE-Lancer but does not indicate it is a dataset. It appears to be a benchmark or a method, and the context does not provide enough information to classify it as a reusable dataset.",
      "processing_time": 21.061339378356934,
      "citing_paper_id": "277150776",
      "cited_paper_id": 276421817
    },
    {
      "context_text": "This is evident from benchmarks like SWE-bench and SWE-Lancer targeting complex coding tasks, CORE-Bench for scientific computational reproducibility, and intricate general agent benchmarks like GAIA and TheAgentCompany .",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not specify them as reusable datasets. They are likely used for evaluation or comparison rather than as primary data sources.",
      "processing_time": 30.887085914611816,
      "citing_paper_id": "277150776",
      "cited_paper_id": 276421817
    },
    {
      "context_text": "By targeting freelance coding tasks, SWELancer links agent performance to monetary value, underscoring the challenges in achieving long-term reasoning and decision-making in complex, real-world scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (SWELancer) and its application. No verifiable resources are identified.",
      "processing_time": 13.784414529800415,
      "citing_paper_id": "277150776",
      "cited_paper_id": 276421817
    },
    {
      "context_text": "…AI (Pa-tronus AI, Inc., 2023), LangChains’ AgentEvals (LangChain, 2025); Databricks Mosaic AI Agent Evaluation (Databricks, 2023) which is mostly designed for RAG like tasks, Botpress Multi-Agent Evaluation System (Kargwal, 2025) and AutoGen (Dibia et al., 2024) for multi-agent systems, and more.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several evaluation systems and tools, but none of them are specific datasets. They are primarily methods or tools for evaluating agents or multi-agent systems.",
      "processing_time": 30.12475824356079,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "There are many frameworks supporting the evaluation of a wide range of agent types, including LangSmith (LangChain, 2023), Langfuse (Lang-fuse, 2023), Google Vertex AI evaluation service (Google Cloud, 2025), Arize AI’s Evaluation Framework (Arize AI, Inc, 2025), Galileo Agen-tic Evaluation (Galileo, 2025), Patronus AI (Pa-tronus AI, Inc., 2023), LangChains’ AgentEvals (LangChain, 2025); Databricks Mosaic AI Agent Evaluation (Databricks, 2023) which is mostly designed for RAG like tasks, Botpress Multi-Agent Evaluation System (Kargwal, 2025) and AutoGen (Dibia et al., 2024) for multi-agent systems, and more.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various frameworks and tools but does not refer to any specific datasets. The context is about evaluation frameworks for agents, which are methods or tools rather than datasets.",
      "processing_time": 20.217499494552612,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "…LLM agent self-reflection were often indirect, repurposing existing reasoning or planning tasks, such as AGIEval (Zhong et al., 2023), MedMCQA (Pal et al., 2022), ALF-World (Shridhar et al., 2021), MiniWoB++ (Liu et al., 2018), etc., into multi-turn feedback loops, to see if models could…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks and challenges, which are typically excluded unless they refer to specific, downloadable datasets. None of the mentioned items are clearly identified as datasets.",
      "processing_time": 19.201020002365112,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "…et al., 2024); ToolBench (Qin et al., 2023); ToolAlpaca (Tang et al., 2023); APIBench (Patil et al., 2025); API-Bank (Li et al., 2023); NexusRaven (team, 2023); Seal-Tools (Wu et al., 2024b); ComplexFuncBench (Zhong et al., 2025); ToolSandbox (Lu et al., 2024); RestBench (Song et al., 2023);…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several names, but they are all models, methods, or tools, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 15.879069566726685,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "Frameworks like NexusRaven (team, 2023) further enrich this landscape by focusing on generalized tool-use scenarios that mirror the diverse challenges encountered in practice.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework called NexusRaven. No verifiable resources are identified.",
      "processing_time": 16.786561727523804,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "Future work should explore detailed, step-by-step assess-ments—similar to those emerging in benchmarks like WebCanvas (Pan et al., 2024b) and frameworks like LangSmith and Galileo Agentic Evaluations (Galileo, 2025) —to provide richer feedback and guide targeted improvements.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks and frameworks but does not refer to specific datasets. The names mentioned are likely benchmarks or evaluation frameworks rather than datasets.",
      "processing_time": 15.043296098709106,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, Galileo Agentic Evaluation (Galileo, 2025) introduces an action advancement metric , which measures whether each step successfully contributes to or advances toward a user-defined goal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Galileo Agentic Evaluation' but does not refer to it as a dataset. It appears to be a method or tool for evaluating action advancement in goal-directed tasks.",
      "processing_time": 20.389570713043213,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "…et al., 2021b); Game of 24 (Yao et al., 2023); MINT (Wang et al., 2023); PlanBench (Valmeekam et al., 2023); FlowBench (Xiao et al., 2024); FOLIO (Han et al., 2022); P-FOLIO (Han et al., 2024); MultiRC (Khashabi et al., 2018); MUSR (Sprague et al., 2023); BBH (Suzgun et al., 2022); ToolEmu (Ruan…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PlanBench",
        "FlowBench",
        "FOLIO",
        "P-FOLIO",
        "MultiRC",
        "MUSR",
        "BBH"
      ],
      "dataset_descriptions": {
        "PlanBench": "Used to evaluate planning capabilities of LLMs, focusing on task-oriented reasoning and problem-solving skills.",
        "FlowBench": "Applied to assess the ability of LLMs to handle complex workflows and procedural tasks.",
        "FOLIO": "Utilized to test the logical reasoning and inference capabilities of LLMs in various planning scenarios.",
        "P-FOLIO": "Used to evaluate the enhanced logical reasoning and planning abilities of LLMs with additional constraints.",
        "MultiRC": "Applied to assess the reading comprehension and multi-hop reasoning skills of LLMs in planning contexts.",
        "MUSR": "Used to evaluate the multi-step reasoning and planning capabilities of LLMs in real-world scenarios.",
        "BBH": "Utilized to test the robustness and generalization of LLMs in handling diverse and challenging planning tasks."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation span mentions several benchmark datasets and challenges, but none of them are traditional datasets with clear identifiers. They are primarily used for evaluating model performance.",
      "processing_time": 66.30409526824951,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "As observed by Kapoor et al. (2024b), current evaluations often prioritize accuracy while overlooking cost and efficiency measurements.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses a general observation about evaluation priorities.",
      "processing_time": 17.225377321243286,
      "citing_paper_id": "277150776",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent breakthroughs in large language models (LLMs) have significantly advanced natural language processing (Brown et al., 2020; Ouyang et al., 2022; Bubeck et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing advancements in LLMs.",
      "processing_time": 14.476535081863403,
      "citing_paper_id": "278905229",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "To ensure computational efficiency in processing the large-scale GrailQA dataset, our experimental framework adopts the same test samples established in ToG (Sun et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GrailQA"
      ],
      "dataset_descriptions": {
        "GrailQA": "Used to test computational efficiency in processing large-scale question answering tasks, focusing on the same test samples established in prior work."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the GrailQA dataset, which is a specific, verifiable resource used for testing computational efficiency in the research.",
      "processing_time": 22.268677711486816,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "• Extensive evaluations on WebQSP, CWQ, and GrailQA datasets demonstrate SRP’s superior performance compared to competitive base-lines, underscoring the reliability and accuracy of SRP.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQSP",
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQSP": "Used to evaluate SRP’s performance in complex question answering, demonstrating its reliability and accuracy through extensive testing.",
        "CWQ": "Used to assess SRP’s effectiveness in handling complex web questions, highlighting its superior performance over baselines.",
        "GrailQA": "Used to test SRP’s generalization capabilities in question answering, emphasizing its accuracy and reliability across diverse queries."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluation, which are relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 34.93695425987244,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "5 on WebQSP, CWQ, and GrailQA, which demonstrates that the reasoning paths generated by SRP through reliable planning and reflection are more effective in retrieving the knowledge needed to answer the question.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQSP",
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQSP": "Used to evaluate the effectiveness of reasoning paths generated by SRP in retrieving knowledge for question answering.",
        "CWQ": "Used to assess the performance of SRP in generating reliable planning and reflection for complex question answering.",
        "GrailQA": "Used to test the ability of SRP to effectively retrieve knowledge needed to answer questions through reliable planning and reflection."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating the effectiveness of reasoning paths generated by SRP.",
      "processing_time": 30.090221643447876,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "• GrailQA (Gu et al., 2021) is a large-scale KGQA dataset built on Freebase (Bollacker et al., 2008).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GrailQA"
      ],
      "dataset_descriptions": {
        "GrailQA": "Used to evaluate question answering systems on knowledge bases, focusing on generalization capabilities beyond i.i.d. settings. The dataset is built on Freebase and supports complex query structures."
      },
      "confidence_score": 1.0,
      "reasoning": "GrailQA is identified as a dataset in the context, and it is described as a large-scale KGQA dataset built on Freebase. The context indicates its use in question answering on knowledge bases.",
      "processing_time": 26.73279881477356,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "For number of few-shot demonstration, we utilize 3 shots in WebQSP and 4 shots in CWQ and GrailQA for path generation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQSP",
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQSP": "Used for few-shot learning experiments in question answering, specifically for path generation with 3 shots.",
        "CWQ": "Used for few-shot learning experiments in question answering, specifically for path generation with 4 shots.",
        "GrailQA": "Used for few-shot learning experiments in question answering, specifically for path generation with 4 shots."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for few-shot learning experiments in question answering.",
      "processing_time": 28.677003383636475,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "More importantly, the performance gap on GrailQA can be reasonably attributed to the fact that GPT-4.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance gap on GrailQA. GrailQA is a benchmark, not a dataset in the strict sense as defined.",
      "processing_time": 17.21321392059326,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "In path edit, we utilized 5 shots in WebQSP and CWQ and 4 shots in GrailQA.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQSP",
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQSP": "Used for few-shot learning in path editing, focusing on 5-shot examples to enhance question answering on knowledge bases.",
        "CWQ": "Used for few-shot learning in path editing, focusing on 5-shot examples to enhance question answering on knowledge bases.",
        "GrailQA": "Used for few-shot learning in path editing, focusing on 4-shot examples to enhance question answering on knowledge bases."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for few-shot learning in the context of question answering on knowledge bases.",
      "processing_time": 33.88441801071167,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "To evaluate the reasoning ability of SRP, we conduct experiments on three multi-hop KGQA datasets called WebQuestionsSP (WebQSP) (Yih et al., 2016), ComplexWebQuestions (CWQ) (Talmor and Be-rant, 2018) and GrailQA (Gu et al., 2021 statistics are illustrated in Table 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQuestionsSP",
        "ComplexWebQuestions",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQuestionsSP": "Used to evaluate the reasoning ability of SRP in multi-hop question answering over knowledge bases, focusing on complex query structures.",
        "ComplexWebQuestions": "Used to assess SRP's performance in handling complex, multi-hop questions over web-scale knowledge graphs, emphasizing reasoning complexity.",
        "GrailQA": "Used to test SRP's capability in answering complex, multi-hop questions over knowledge bases, with a focus on diverse and challenging query types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the reasoning ability of SRP in multi-hop KGQA tasks.",
      "processing_time": 35.50721621513367,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "The gap remains evident on CWQ and persists in GrailQA, where SRP maintains over 95% reliability.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "CWQ": "Used to evaluate the performance of SRP in question answering, focusing on the reliability of the system across different datasets.",
        "GrailQA": "Used to assess the generalization capabilities of SRP, maintaining over 95% reliability, highlighting the robustness of the system."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions CWQ and GrailQA, which are known datasets in the field of question answering. However, the context does not provide specific details on how these datasets are used beyond mentioning their names.",
      "processing_time": 31.20171046257019,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "When test Readi in GrailQA, we utilized 6 shots prompts for reasoning path generation and 4 shots demonstration for reasoning path edit.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GrailQA"
      ],
      "dataset_descriptions": {
        "GrailQA": "Used to test reasoning capabilities in a question-answering system, focusing on generating and editing reasoning paths with few-shot prompts."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions the use of GrailQA but does not specify it as a dataset. It focuses on the methodology of using shot prompts for reasoning path generation and editing.",
      "processing_time": 24.828911304473877,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "1-mini is slightly inferior to that of ToG-GPT4. in GrailQA, SRP-GPT4.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. No verifiable resources are identified.",
      "processing_time": 13.500490188598633,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "As shown in Table 2, SRP consistently demonstrates state-of-the-art (SOTA) performance on both the WebQSP and CWQ benchmarks, while achieving the second-highest overall performance on GrailQA, demonstrating its capacity to effectively leverage an external KG for robust retrieval and multi-hop reasoning.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as specific, downloadable datasets. It focuses on performance metrics rather than the datasets themselves.",
      "processing_time": 14.48020315170288,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "Furthermore, replacing the searched references with random-sampled references ( random reference ) also reduces the result from 71.2% to 70.9% on GrailQA and from 78.5% to 77.0% on WebQSP, implying that carefully searched references are superior to arbitrary references.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GrailQA",
        "WebQSP"
      ],
      "dataset_descriptions": {
        "GrailQA": "Used to evaluate the impact of reference quality on model performance, specifically comparing carefully searched references to random ones.",
        "WebQSP": "Used to assess the effect of reference quality on model performance, demonstrating the superiority of carefully searched references over random ones."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions GrailQA and WebQSP as datasets used to evaluate the impact of reference quality on performance.",
      "processing_time": 25.79999303817749,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "To further substantiate the reliability of our proposed SRP framework, we evaluate both the searching success rate and the reliable answering rate us-ing the WebQSP, CWQ, and GrailQA datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebQSP",
        "CWQ",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "WebQSP": "Used to evaluate the searching success rate and reliable answering rate of the SRP framework, focusing on complex question answering over knowledge bases.",
        "CWQ": "Used to evaluate the searching success rate and reliable answering rate of the SRP framework, focusing on complex question answering over knowledge bases.",
        "GrailQA": "Used to evaluate the searching success rate and reliable answering rate of the SRP framework, focusing on complex question answering over knowledge bases."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used to evaluate the SRP framework, which are relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 35.984190464019775,
      "citing_paper_id": "278905229",
      "cited_paper_id": 226965153
    },
    {
      "context_text": "Subsequently, various adaptations of CoT reasoning, such as Self-Consistency (Wang et al., 2022), Tree-of-Thought (Yao et al., 2023), Graph-of-Thought (Besta et al., 2024), were introduced to motivate the reasoning ability of LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions various adaptations of CoT reasoning but does not refer to any specific datasets. The focus is on methods and their impact on LLM reasoning.",
      "processing_time": 30.823854446411133,
      "citing_paper_id": "278905229",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "To promote reasoning ability of LLMs, numerous researchers have directed LLMs to incorporate their thought processes into their outputs rather than merely delivering direct answers (Wei et al., 2022; Hao et al., 2024; Kojima et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches to improve LLM reasoning.",
      "processing_time": 14.073122262954712,
      "citing_paper_id": "278905229",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Initially, Chain of Thought (CoT) (Wei et al., 2022) framework was developed to present several examples of intermediate reasoning steps in natural language as prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain of Thought) for improving reasoning in language models.",
      "processing_time": 27.37958002090454,
      "citing_paper_id": "278905229",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "To retrieve relation corresponding to ˆ r 0 , we first utilize BM25 and Contriever (Izacard et al., 2022) to obtain relations semantically similar to ˆ r 0 , denoted as R s 0 = { r s 0 , 0 , r s 0 , 1 , . . . } .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (BM25 and Contriever).",
      "processing_time": 14.44587779045105,
      "citing_paper_id": "278905229",
      "cited_paper_id": 249097975
    },
    {
      "context_text": "Following previous research (Cheng et al., 2024), we utilize a Pyserini as a hybrid searcher with BM25 and Contriver (Izacard et al., 2022).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods. Pyserini, BM25, and Contriever are tools or methods, not datasets.",
      "processing_time": 30.07675290107727,
      "citing_paper_id": "278905229",
      "cited_paper_id": 249097975
    },
    {
      "context_text": "Following previous research (Cheng et al., 2024), we utilize a Pyserini as a hybrid searcher with BM25 and Contriver (Izacard et al., 2022).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods. Pyserini, BM25, and Contriever are tools or methods, not datasets.",
      "processing_time": 30.07675290107727,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "To facilitate LLMs in achieving a cognitive process that parallels human thinking, many studies (Madaan et al., 2023; Wang et al., 2024; Guan et al., 2024; Shinn et al., 2023) have devised self-correction mechanisms using feedback to amend faulty reasoning and ensure precision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only self-correction mechanisms and feedback methods. No verifiable resources are identified.",
      "processing_time": 16.763996362686157,
      "citing_paper_id": "278905229",
      "cited_paper_id": 257900871
    },
    {
      "context_text": "Consistent with established methodologies in previous work (Cheng et al., 2024; Li et al., 2023a; Jiang et al., 2023), we employ exact match accuracy (Hits@1) as the primary evaluation metric for model performance assessment.",
      "catation_intent": "research work",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The context focuses on the methodology for assessing model performance.",
      "processing_time": 28.02919602394104,
      "citing_paper_id": "278905229",
      "cited_paper_id": 258714753
    },
    {
      "context_text": "Consistent with established methodologies in previous work (Cheng et al., 2024; Li et al., 2023a; Jiang et al., 2023), we employ exact match accuracy (Hits@1) as the primary evaluation metric for model performance assessment.",
      "catation_intent": "research work",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The context focuses on the methodology for assessing model performance.",
      "processing_time": 28.02919602394104,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "• StructGPT (Jiang et al., 2023) utilizes an interface for KG data to enable finite knowledge access and filtering, leveraging a LLM to repeatedly infer answers or subsequent planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It focuses on the methodology and capabilities of StructGPT.",
      "processing_time": 16.545810222625732,
      "citing_paper_id": "278905229",
      "cited_paper_id": 258714753
    },
    {
      "context_text": "1-mini (OpenAI, 2025); 3) Prompting LLM methods, including KB-BINDER (Li et al., 2023b), StructGPT (Jiang et al., 2023), ToG (Sun et al., 2023) Table 2: Performance comparison with different baselines on three KGQA dataets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'three KGQA datasets' but does not provide specific names. The cited paper title does not help disambiguate any dataset names.",
      "processing_time": 30.332433223724365,
      "citing_paper_id": "278905229",
      "cited_paper_id": 258714753
    },
    {
      "context_text": "Consequently, some approaches (Baek et al., 2023; Yang et al., 2024; Cheng et al., 2024) first extracted information from KGs and then directly supplied explicit knowledge to LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or approaches involving knowledge graphs and LLMs.",
      "processing_time": 15.471091747283936,
      "citing_paper_id": "278905229",
      "cited_paper_id": 259203671
    },
    {
      "context_text": "Consequently, some approaches (Baek et al., 2023; Yang et al., 2024; Cheng et al., 2024) first extracted information from KGs and then directly supplied explicit knowledge to LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or approaches involving knowledge graphs and LLMs.",
      "processing_time": 15.471091747283936,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "…to provide explicit and modifiable knowledge to LLMs. Earlier research incorporated KGs during the pre-training (Wang et al., 2021) or fine-tuning (Luo et al., 2023) phases, but these methods demand substantial computational resources and perform poorly when dealing with problem that were not…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods involving knowledge graphs (KGs) in the training of LLMs.",
      "processing_time": 28.908408403396606,
      "citing_paper_id": "278905229",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "• Readi (Cheng et al., 2024) is a novel framework that enables LLMs to efficiently and faithfully reason over structured environments by initially generating a reasoning path, in-stantiating it on the environment, and invoking targeted editing only when necessary.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a method or framework (Readi) rather than a dataset. There are no specific, verifiable datasets mentioned.",
      "processing_time": 13.724195957183838,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "Noted that the demonstration of prompts of relation check is from (Sun et al., 2023), and prompts of answering is following (Cheng et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing prompts and reasoning capabilities of LLMs.",
      "processing_time": 13.278382539749146,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "…(1) fine-tuning methods that equip models with the ability to utilize information in KGs (Shi et al., 2021; Zhang et al., 2022); and (2) guiding LLMs with prompts to make plans and decisions in order to search for the knowledge in KGs for answering questions (Li et al., 2023b; Cheng et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for enhancing LLM capabilities.",
      "processing_time": 12.573263168334961,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "Following method in Readi (Cheng et al., 2024), we conduct retrieval by comparing semantic similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for retrieval using semantic similarity.",
      "processing_time": 12.397911787033081,
      "citing_paper_id": "278905229",
      "cited_paper_id": 268379197
    },
    {
      "context_text": "In order to verify the performance of our SRP model, we compare SRP with the state-of-the-art KGQA methods: 1) Fine-Tuned model methods, including TransferNet (Shi et al., 2021), TIARA (Shu et al., 2022), SR+NSM+E2E (Zhang et al., 2022) and Flexkbqa (Li et al., 2024); 2) LLM-only methods, including GPT3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the SRP model with other state-of-the-art methods.",
      "processing_time": 16.747156143188477,
      "citing_paper_id": "278905229",
      "cited_paper_id": null
    },
    {
      "context_text": "…to verify the performance of our SRP model, we compare SRP with the state-of-the-art KGQA methods: 1) Fine-Tuned model methods, including TransferNet (Shi et al., 2021), TIARA (Shu et al., 2022), SR+NSM+E2E (Zhang et al., 2022) and Flexkbqa (Li et al., 2024); 2) LLM-only methods, including GPT3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the SRP model with other state-of-the-art methods.",
      "processing_time": 17.173378467559814,
      "citing_paper_id": "278905229",
      "cited_paper_id": null
    },
    {
      "context_text": "• TIARA (Shu et al., 2022) is a framework that leverages BERT for schema item retrieval and T5 for plan generation, employing constrained decoding to ensure grammatical correctness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 16.20353651046753,
      "citing_paper_id": "278905229",
      "cited_paper_id": null
    },
    {
      "context_text": "Pathfinding in grid-based environments has long been dominated by classical algorithms such as A* (Hart et al., 1968), Dijkstra (Dijkstra, 1959), and DFS, which rely on explicit search heuristics and guarantee optimality under predefined conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only classical pathfinding algorithms. No verifiable resources are identified.",
      "processing_time": 13.727054119110107,
      "citing_paper_id": "279070828",
      "cited_paper_id": 206799161
    },
    {
      "context_text": "Algorithms such as A* (Hart et al., 1968), Dijkstra’s (Dijkstra, 1959), and variants like Jump Point Search (Harabor and Grastien, 2014) are widely used for computing collision-free, cost-effective paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods for pathfinding. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 17.82476806640625,
      "citing_paper_id": "279070828",
      "cited_paper_id": 206799161
    },
    {
      "context_text": "In parallel, an emerging line of work explores hybrid approaches that integrate LLMs with traditional pathfinding algorithms such as A* and Dijkstra (Liu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach combining LLMs with pathfinding algorithms.",
      "processing_time": 15.452390432357788,
      "citing_paper_id": "279070828",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Early studies have demonstrated that LLMs can produce plausible paths or high-level plans in grid-based environments when provided with textual descriptions of the environment, along with specified start and goal positions (Valmeekam et al., 2023; Latif, 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs in planning tasks. No verifiable resources are identified.",
      "processing_time": 17.510005712509155,
      "citing_paper_id": "279070828",
      "cited_paper_id": 260440590
    },
    {
      "context_text": "Early studies have demonstrated that LLMs can produce plausible paths or high-level plans in grid-based environments when provided with textual descriptions of the environment, along with specified start and goal positions (Valmeekam et al., 2023; Latif, 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs in planning tasks. No verifiable resources are identified.",
      "processing_time": 17.510005712509155,
      "citing_paper_id": "279070828",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, the PPNL benchmark investigates the use of Chain-of-Thought (CoT) and ReAct prompting to encourage step-by-step reasoning in route planning tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the PPNL benchmark, which is a specific resource used to evaluate path planning capabilities of LLMs. However, it is a benchmark rather than a dataset.",
      "processing_time": 20.142974376678467,
      "citing_paper_id": "279070828",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "Specialized benchmarks, such as Path Planning from Natural Language (PPNL) (Aghzal et al., 2023) explored LLMs’ ability to perform end-to-end navigation while adhering to movement con-* straints and avoiding obstacles.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Path Planning from Natural Language (PPNL)"
      ],
      "dataset_descriptions": {
        "Path Planning from Natural Language (PPNL)": "Used to evaluate LLMs' path planning capabilities, focusing on end-to-end navigation while adhering to movement constraints and avoiding obstacles."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specialized benchmark called PPNL, which is used to evaluate LLMs' path planning capabilities. The context indicates that the dataset is used to test LLMs' ability to navigate while adhering to constraints and avoiding obstacles.",
      "processing_time": 28.623327255249023,
      "citing_paper_id": "279070828",
      "cited_paper_id": 263671594
    },
    {
      "context_text": "Unlike classical algorithms, LLMs leverage implicit knowledge and contextual understanding, allowing them to generate flexible solutions in unfamiliar scenarios (Wang et al., 2023; Li et al., 2025; Wen, 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities of LLMs compared to classical algorithms.",
      "processing_time": 16.19225549697876,
      "citing_paper_id": "279070828",
      "cited_paper_id": 274514455
    },
    {
      "context_text": "Building on this context, our study systematically compares the planning capabilities of nine LLMs and Fast Downward across five distinct PDDL domains: barman , blocks , elevator , satellite , and tidybot .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PDDL domains but does not specify them as datasets. They are likely problem domains or benchmarks, not datasets in the traditional sense.",
      "processing_time": 18.56825304031372,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "The plan lengths generated by Claude Sonnet 3.7 , Claude Sonnet 3.7 Thinking , GPT-o3 mini , and Gemini 2 Flash Thinking align most closely with the baseline established by the Fast Downward planner, which produces plans with an average length of 39.56 actions.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Fast Downward planner but does not refer to it as a dataset. It is used as a baseline for comparing plan lengths generated by other systems.",
      "processing_time": 19.753750562667847,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Fast Downward (FD).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation 'Fast Downward (FD)' refers to a planning system, not a dataset. The cited paper title confirms it is a planning system, not a dataset.",
      "processing_time": 20.126831531524658,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "In the tidybot domain, Fast Downward failed to find solutions for two problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a planning system and its performance in a domain.",
      "processing_time": 16.217896461486816,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "These findings reinforce the notion that although LLM based approaches Figure 2: Comparison of the planning capabilities of nine different LLMs against the classical planner Fast Downward , which serves as the baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Fast Downward as a baseline for comparing planning capabilities of LLMs, but it does not refer to a dataset. Fast Downward is a planning system, not a dataset.",
      "processing_time": 20.967902660369873,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "While methods like Fast Downward offer efficiency and formal verification, they struggle with uncertainty and require significant domain engineering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a planning system method. No dataset names are present in the citation span.",
      "processing_time": 25.59105896949768,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Traditionally, symbolic planners such as Fast Downward Helmert [2006] have been the primary tools for generating provably correct solutions from well-defined domains using the Planning Domain Definition Language (PDDL) McDermott et al. [1998].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only symbolic planners and a language for defining planning domains. No verifiable resources are identified.",
      "processing_time": 27.08395552635193,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Traditionally, symbolic planners such as Fast Downward Helmert [2006] have been the primary tools for generating provably correct solutions from well-defined domains using the Planning Domain Definition Language (PDDL) McDermott et al. [1998].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only symbolic planners and a language for defining planning domains. No verifiable resources are identified.",
      "processing_time": 27.08395552635193,
      "citing_paper_id": "280401757",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "This study examines the planning capabilities of various LLMs compared to Fast Downward, a well-established classical planner for task planning, using PDDL domain and problem descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Fast Downward, which is a classical planner, not a dataset. No specific datasets are mentioned.",
      "processing_time": 26.66137194633484,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Our study evaluates whether LLMs can function as general planners, similar to classical solvers like Fast Downward , given a domain and problem definition.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to a planning system. No verifiable resources are identified.",
      "processing_time": 27.068691730499268,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Fast Downward aligns with the ideal execution line, indicating that all planned actions are executable.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a planning system. There are no verifiable resources that meet the criteria.",
      "processing_time": 27.501386165618896,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "The shortest average plan length is observed in Claude Sonnet 3.5 , which generates plans with only 16.69 actions, less than half of the Fast Downward baseline.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only compares the performance of Claude Sonnet 3.5 to the Fast Downward baseline.",
      "processing_time": 29.704426527023315,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "By leveraging hierarchical decomposition and heuristic guidance, Fast Downward efficiently handles diverse domains, reinforcing its position as a reliable point of reference for evaluating emerging LLM-based planners.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a planning system. No verifiable resources are identified.",
      "processing_time": 27.07674789428711,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "Fast Downward is a domain-independent planning system that translates PDDL tasks into multi-valued variable representations, highlighting implicit constraints and dependencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a planning system. No verifiable resources are identified.",
      "processing_time": 27.330425262451172,
      "citing_paper_id": "280401757",
      "cited_paper_id": 17305
    },
    {
      "context_text": "LLMs often lack robust environment grounding Gramopadhye and Szafir [2023], Ahn et al. [2022] and are prone to generating incorrect or misleading outputs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with LLMs. No dataset names are present in the citation span.",
      "processing_time": 28.8788058757782,
      "citing_paper_id": "280401757",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "We set the global batch size to 48 and employ DeepSpeed ZeRO Stage3 (Rajbhandari et al., 2021) to speed up the fine-tuning process.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DeepSpeed ZeRO Stage3) for speeding up the fine-tuning process.",
      "processing_time": 29.4189453125,
      "citing_paper_id": "266999372",
      "cited_paper_id": 233289729
    },
    {
      "context_text": "…of external tools has been employed to enhance LLMs in various ways, including the improvement of real-time factual knowledge (Yang et al., 2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements to LLMs using external tools. No verifiable resources are identified.",
      "processing_time": 29.69715404510498,
      "citing_paper_id": "266999372",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "It is worth mentioning that ToolLLaMA only exhibits acceptable performance when the input length is 8192.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a performance characteristic of a model. No verifiable resources are identified.",
      "processing_time": 28.252550840377808,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "At an input length of 4096, ToolLLaMA shows deterioration across various metrics, particularly exhibiting a very high hallucination rate.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only performance metrics and a model (ToolLLaMA).",
      "processing_time": 28.268853902816772,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "We employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our α -UMi on several agent benchmarks (Qin et al., 2023b; Tang et al., 2023) focusing on tool use.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating the α-UMi on 'several agent benchmarks' but does not specify any particular datasets. The cited papers do not provide additional dataset names either.",
      "processing_time": 32.814204931259155,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "We employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our α -UMi on several agent benchmarks (Qin et al., 2023b; Tang et al., 2023) focusing on tool use.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating the α-UMi on 'several agent benchmarks' but does not specify any particular datasets. The cited papers do not provide additional dataset names either.",
      "processing_time": 32.814204931259155,
      "citing_paper_id": "266999372",
      "cited_paper_id": 259108190
    },
    {
      "context_text": "We employ LLaMA-2 (Touvron et al., 2023b) series to implement the LLM backbone and evaluate our α -UMi on several agent benchmarks (Qin et al., 2023b; Tang et al., 2023) focusing on tool use.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluating the α-UMi on 'several agent benchmarks' but does not specify any particular datasets. The cited papers do not provide additional dataset names either.",
      "processing_time": 32.814204931259155,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "The LLMs under consideration include Claude-2 (Anthropic, 2023), Chat-GPT, GPT-4, and ToolLLaMA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.013449907302856,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The LLMs under consideration include Claude-2 (Anthropic, 2023), Chat-GPT, GPT-4, and ToolLLaMA.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.013449907302856,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": 259108190
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "…proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 31.61955714225769,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Our foundation of proposing α -UMi and GLPFT is anchored in three main principles: Firstly, the limited ability and capacity of small LLMs, such as LLaMA-7B, pose challenges during fine-tuning in tool learning tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their limitations. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.53912925720215,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "We also evaluate the performance of ChatGPT and GPT-4 with 0-shot setting, and ToolLLaMA (Qin et al., 2023b), which is a 7B-parameter LLM fine-tuned by the ToolBench team from LLaMA-2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.271669149398804,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "We also evaluate the performance of ChatGPT and GPT-4 with 0-shot setting, and ToolLLaMA (Qin et al., 2023b), which is a 7B-parameter LLM fine-tuned by the ToolBench team from LLaMA-2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 30.271669149398804,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "We opt for LLaMA-2-chat-7B and LLaMA-2-chat-13B (Touvron et al., 2023b) as the backbone to implement our framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LLaMA-2-chat-7B and LLaMA-2-chat-13B, which are models, not datasets. No datasets are mentioned in the context.",
      "processing_time": 33.59184241294861,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "In addition to exploring proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The names mentioned (LLaMA, GPT-4) are models, not datasets.",
      "processing_time": 33.79245090484619,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "In addition to exploring proprietary LLMs like GPT-4, researchers are actively engaged in developing customizable agent systems by fine-tuning open-source LLMs, such as LLaMA (Touvron et al., 2023a), on diverse tool-use datasets (Patil et al., 2023; Tang et al., 2023; Qin et al., 2023b; Li et al., 2023; Gou et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'diverse tool-use datasets' but does not specify any particular dataset names. The names mentioned (LLaMA, GPT-4) are models, not datasets.",
      "processing_time": 33.79245090484619,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "…factual knowledge (Yang et al., 2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and research areas. No verifiable resources are identified.",
      "processing_time": 29.682177782058716,
      "citing_paper_id": "266999372",
      "cited_paper_id": 257637012
    },
    {
      "context_text": "These models must not only coma prehend user queries and generate answers accurately but also excel in task planning, tool selection and invocation, and result summarization (Yujia et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities required for models in task planning, tool selection, and invocation.",
      "processing_time": 30.26354217529297,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258179336
    },
    {
      "context_text": "Agents for tool learning are systems designed to assist users in completing tasks through a series of decision-making processes and tool use (Yujia et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of agents for tool learning. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 31.609244108200073,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258179336
    },
    {
      "context_text": "The ability of LLMs to use external tools has become a pivotal component in the development of AI agents, attracting rapidly growing attention (Qin et al., 2023b; Schick et al., 2023; Yang et al., 2023b; Shen et al., 2023; Patil et al., 2023; Qin et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the ability of LLMs to use external tools. No verifiable resources are identified.",
      "processing_time": 32.0467483997345,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "The ability of LLMs to use external tools has become a pivotal component in the development of AI agents, attracting rapidly growing attention (Qin et al., 2023b; Schick et al., 2023; Yang et al., 2023b; Shen et al., 2023; Patil et al., 2023; Qin et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the ability of LLMs to use external tools. No verifiable resources are identified.",
      "processing_time": 32.0467483997345,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "The ability of LLMs to use external tools has become a pivotal component in the development of AI agents, attracting rapidly growing attention (Qin et al., 2023b; Schick et al., 2023; Yang et al., 2023b; Shen et al., 2023; Patil et al., 2023; Qin et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the ability of LLMs to use external tools. No verifiable resources are identified.",
      "processing_time": 32.0467483997345,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "The ability of LLMs to use external tools has become a pivotal component in the development of AI agents, attracting rapidly growing attention (Qin et al., 2023b; Schick et al., 2023; Yang et al., 2023b; Shen et al., 2023; Patil et al., 2023; Qin et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the ability of LLMs to use external tools. No verifiable resources are identified.",
      "processing_time": 32.0467483997345,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, solutions like BabyAGI (Nakajima, 2023) and AutoGPT (Grav-itas, 2023) have been developed to address daily problems, while Voyager (Wang et al., 2023) and Ghost (Zhu et al., 2023) engage in free exploration within Minecraft games.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about the development and capabilities of various AI systems.",
      "processing_time": 31.106553554534912,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258959262
    },
    {
      "context_text": "…ways, including the improvement of real-time factual knowledge (Yang et al., 2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and improvements of large language models. No verifiable resources are identified.",
      "processing_time": 30.736417293548584,
      "citing_paper_id": "266999372",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "Single-LLM refers to the traditional single-LLM tool learning approach.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach. No verifiable resources are identified.",
      "processing_time": 29.671425819396973,
      "citing_paper_id": "266999372",
      "cited_paper_id": 259108190
    },
    {
      "context_text": "While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Tou-vron et al., 2023a,b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and their findings. No verifiable resources are identified.",
      "processing_time": 30.733598709106445,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Tou-vron et al., 2023a,b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and their findings. No verifiable resources are identified.",
      "processing_time": 30.733598709106445,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "While conventional approaches (Qin et al., 2023b; Gou et al., 2023; Zeng et al., 2023) focus on training a single open-source LLM with all these capabilities, notable performance limitations have been observed, especially with smaller open-source LLMs (Tou-vron et al., 2023a,b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and their findings. No verifiable resources are identified.",
      "processing_time": 30.733598709106445,
      "citing_paper_id": "266999372",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "To assess the performance of LLMs for solving real tasks via RapidAPI, we follow the ToolEval method (Qin et al., 2023b) proposed by the Tool-Bench team to conduct a real-time evaluation on the test set of ToolBench.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ToolBench"
      ],
      "dataset_descriptions": {
        "ToolBench": "Used to conduct real-time evaluations of LLMs on real tasks via RapidAPI, focusing on the test set to assess performance and capability."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ToolBench' as a dataset used for evaluating LLMs on real tasks via RapidAPI. The dataset is specifically used for real-time evaluation.",
      "processing_time": 38.74778628349304,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al., 2023; Li et al., 2023; Qin et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and research areas. No verifiable resources are identified.",
      "processing_time": 30.531768798828125,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al., 2023; Li et al., 2023; Qin et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and research areas. No verifiable resources are identified.",
      "processing_time": 30.531768798828125,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "…2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al., 2023; Li et al., 2023; Qin et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and research areas. No verifiable resources are identified.",
      "processing_time": 30.531768798828125,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "…2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al., 2023; Li et al., 2023; Qin et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various capabilities and research areas. No verifiable resources are identified.",
      "processing_time": 30.531768798828125,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "We evaluate the effectiveness of our framework on four tool learning benchmarks: ToolBench (Qin et al., 2023b), ToolAlpaca (Tang et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'ToolBench' and 'ToolAlpaca' as benchmarks, but these are not traditional datasets. They are likely suites of tasks or challenges rather than downloadable datasets.",
      "processing_time": 34.45289635658264,
      "citing_paper_id": "266999372",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "These tasks involve integrating API calls to accomplish tasks, where the agent must accurately select the appropriate API and compose necessary API requests.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the integration of API calls in tasks involving large language models.",
      "processing_time": 29.97303342819214,
      "citing_paper_id": "266999372",
      "cited_paper_id": 261241602
    },
    {
      "context_text": "Since ToRA has not released its training data, to facilitate the training of our framework, we utilize gpt-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to using gpt-3, which is a model, not a dataset.",
      "processing_time": 31.093693494796753,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "Notably, they lack domain specificity, real-time information, and face challenges in solving complex problems, especially in specialized areas such as mathematics (Gou et al., 2023) and program compilation (OpenAI, 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only challenges in specialized areas. No verifiable resources are identified.",
      "processing_time": 29.983524560928345,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "Following ToRA (Gou et al., 2023), we employ a program-aided agent to solve the mathematical problems presented in these datasets.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. The reference to ToRA is about a method, not a dataset.",
      "processing_time": 32.459261417388916,
      "citing_paper_id": "266999372",
      "cited_paper_id": 263310365
    },
    {
      "context_text": "Recent efforts have also emerged in fine-tuning open-source LLMs as agents, exemplified by works like FiREACT (Chen et al., 2023) and AgentTuning (Zeng et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or works. The context focuses on fine-tuning LLMs as agents, referencing specific works but not datasets.",
      "processing_time": 33.55887532234192,
      "citing_paper_id": "266999372",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "…a diverse array of external tools has been employed to enhance LLMs in various ways, including the improvement of real-time factual knowledge (Yang et al., 2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to external tools and enhancements for LLMs. No clear, verifiable datasets are identified.",
      "processing_time": 33.19944167137146,
      "citing_paper_id": "266999372",
      "cited_paper_id": 277276996
    },
    {
      "context_text": "Therefore, to make a relatively fair comparison, we adopt the idea of Modelscope-Agent (Li et al., 2023) to compare the predictions of our model with the annotated GPT-4 outputs on the step level.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison method using annotated GPT-4 outputs. No clear, verifiable dataset names are provided.",
      "processing_time": 33.352355003356934,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Therefore, to make a relatively fair comparison, we adopt the idea of Modelscope-Agent (Li et al., 2023) to compare the predictions of our model with the annotated GPT-4 outputs on the step level.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison method using annotated GPT-4 outputs. No clear, verifiable dataset names are provided.",
      "processing_time": 33.352355003356934,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "EM), Argument F1 (Aug. F1), and Rouge-L (R-L) as proposed by Li et al. (2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. There are no verifiable resources or datasets mentioned.",
      "processing_time": 31.08470106124878,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "To address this problem, for the overall results discussed in Section 4.1, we follow Li et al. (2023) to compare the output of agent with the annotated reference at each step 5 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison method involving annotated references.",
      "processing_time": 29.37309956550598,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "ToolBench (Qin et al., 2023b) is a benchmark for evaluating an agent’s ability to call APIs.",
      "catation_intent": "none",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ToolBench as a benchmark, which is primarily used for score comparison and not as a reusable dataset. Therefore, it does not meet the criteria for inclusion.",
      "processing_time": 34.20764708518982,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "ToolBench (Qin et al., 2023b) is a benchmark for evaluating an agent’s ability to call APIs.",
      "catation_intent": "none",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ToolBench as a benchmark, which is primarily used for score comparison and not as a reusable dataset. Therefore, it does not meet the criteria for inclusion.",
      "processing_time": 34.20764708518982,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": ", both evaluated by GPT-4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a model (GPT-4). No verifiable resources are identified.",
      "processing_time": 32.005622148513794,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "5-turbo-1106 ( OpenAI, 2022) and gpt-4 (OpenAI, 2023b) to collect execution trajectories in the training set of MATH and GSM8K and filter out the trajectories that do not lead to the correct final answer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH",
        "GSM8K"
      ],
      "dataset_descriptions": {
        "MATH": "Used to collect execution trajectories during training, specifically filtering out incorrect solution paths to improve model performance.",
        "GSM8K": "Used to collect execution trajectories during training, specifically filtering out incorrect solution paths to improve model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of MATH and GSM8K datasets for collecting execution trajectories and filtering incorrect ones. These datasets are relevant to the planning capabilities of LLMs.",
      "processing_time": 45.164613246917725,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "5-turbo-1106 ( OpenAI, 2022) and gpt-4 (OpenAI, 2023b) to collect execution trajectories in the training set of MATH and GSM8K and filter out the trajectories that do not lead to the correct final answer.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH",
        "GSM8K"
      ],
      "dataset_descriptions": {
        "MATH": "Used to collect execution trajectories during training, specifically filtering out incorrect solution paths to improve model performance.",
        "GSM8K": "Used to collect execution trajectories during training, specifically filtering out incorrect solution paths to improve model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of MATH and GSM8K datasets for collecting execution trajectories and filtering incorrect ones. These datasets are relevant to the planning capabilities of LLMs.",
      "processing_time": 45.164613246917725,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, we collect 5536 trajectories from ChatGPT, 573 trajectories from GPT-4 on MATH, and 6213 trajectories from ChatGPT on GSM8K.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH",
        "GSM8K"
      ],
      "dataset_descriptions": {
        "MATH": "Used to collect trajectories from GPT-4, focusing on mathematical problem-solving skills and planning capabilities.",
        "GSM8K": "Used to collect trajectories from ChatGPT, focusing on complex arithmetic and reasoning tasks to evaluate planning capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets (MATH, GSM8K) used for collecting trajectories from different models. These datasets are relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 45.68252205848694,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, for the t th step, we input the model with the previous trajectory of GPT-4, ask our framework to generate the rationale and action of this step, and then compare the generated rationale and action of this step with the output of GPT-4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a process involving GPT-4 but does not reference any named datasets.",
      "processing_time": 34.20167517662048,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, GPT-4 judges if the execution process of the agent is consistent with the reference process pre-generated by ChatGPT (Proc. correctness) and whether the final answer generated by the agent can solve the user instruction (Ans. correctness).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the processes and correctness checks performed by GPT-4.",
      "processing_time": 15.72549033164978,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Leveraging the capabilities of LLMs such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023b), AI agent systems have found application in diverse scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and applications of LLMs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 34.196709394454956,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Leveraging the capabilities of LLMs such as ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023b), AI agent systems have found application in diverse scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and applications of LLMs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 34.196709394454956,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "First, there are still some explorations that can be pursued, such as collaborating small LLMs with a strong close-source LLM such as GPT-4 to build a “large + small” collaborated multi-LLM tool learning agent.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential collaboration between LLMs.",
      "processing_time": 30.701112747192383,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "In addition, α -UMi also outperform the strong ChatGPT and GPT-4 in zero-shot setting.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (ChatGPT and GPT-4). No verifiable resources are identified.",
      "processing_time": 34.613308906555176,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Caller: Interacting with the environment such as code interpreters and APIs requires the LLM to generate legal and useful code or requests, which may cause collision with other abilities such as reasoning and general response generation during fine-tuning (Dong et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the interaction of LLMs with environments and potential conflicts during fine-tuning.",
      "processing_time": 34.94322085380554,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Subsequently, a diverse array of external tools has been employed to enhance LLMs in various ways, including the improvement of real-time factual knowledge (Yang et al., 2023a; Nakano et al., 2021), multimodal comprehension and generation (Yang et al., 2023b; Wu et al., 2023a; Yang et al., 2023c), code and math reasoning (Gou et al., 2023; OpenAI, 2023a), and domain knowledge of specific AI models and APIs (Shen et al., 2023; Li et al., 2023; Qin et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various tools and methods used to enhance LLMs. There are no clear identifiers for datasets.",
      "processing_time": 36.52808117866516,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "To complete tasks in ToolBench involves calling APIs through RapidAPI 4 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of APIs through RapidAPI. There are no verifiable resources that meet the criteria.",
      "processing_time": 37.649412393569946,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "Unlike ToolBench, the APIs and API calling results in ToolAlpaca are mocked from ChatGPT by imitating how the real APIs work.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between ToolBench and ToolAlpaca regarding API mocking.",
      "processing_time": 37.647674322128296,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "However, as introduced in Section 3.2, the APIs in RapidAPI update every day, which can cause network block, API breakdown, and exhausted quota.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses APIs in RapidAPI, which are not datasets but rather services or tools.",
      "processing_time": 38.283973932266235,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "The Tool-Bench team collects 16,464 real-world APIs from RapidAPI and a total of 125,387 execution trajectories as the training corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "execution trajectories"
      ],
      "dataset_descriptions": {
        "execution trajectories": "Used as a training corpus to develop models for API execution, focusing on real-world usage patterns and trajectories."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a specific corpus of execution trajectories collected by the Tool-Bench team, which is used as a training corpus.",
      "processing_time": 45.665332555770874,
      "citing_paper_id": "266999372",
      "cited_paper_id": null
    },
    {
      "context_text": "…benchmarks—such as AgentBench (Liu et al., 2023), VisualAgentBench (Sun et al., 2023) GAIA (Mialon et al., 2023), ToolBench (Qin et al., 2024) and HumanEval (Chen et al., 2021)—pri-marily focus on isolated reasoning and generation, overlooking the dynamics intrinsic to multi-agent interactions.",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. They are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 39.72468662261963,
      "citing_paper_id": "276766372",
      "cited_paper_id": 235755472
    },
    {
      "context_text": "Traditional single-agent benchmarks—such as AgentBench (Liu et al., 2023), VisualAgentBench (Sun et al., 2023) GAIA (Mialon et al., 2023), ToolBench (Qin et al., 2024) and HumanEval (Chen et al., 2021)—pri-marily focus on isolated reasoning and generation, overlooking the dynamics intrinsic to multi-agent interactions.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. They are primarily used for evaluating single-agent capabilities, which is not directly aligned with the research topic of planning capabilities in LLMs.",
      "processing_time": 43.26944851875305,
      "citing_paper_id": "276766372",
      "cited_paper_id": 235755472
    },
    {
      "context_text": "It maintains and updates a comprehensive internal state that includes each agent’s persona, inter-agent relationships, and reasoning strategies (e.g., Chain-of-Thought (Wei et al., 2023), ReACT (Yao et al., 2023)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies. No verifiable resources are identified.",
      "processing_time": 39.45541310310364,
      "citing_paper_id": "276766372",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "It maintains and updates a comprehensive internal state that includes each agent’s persona, inter-agent relationships, and reasoning strategies (e.g., Chain-of-Thought (Wei et al., 2023), ReACT (Yao et al., 2023)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and strategies. No verifiable resources are identified.",
      "processing_time": 39.45541310310364,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and other agents (Wang et al., 2023c; Park et al., 2023a; OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 40.38370680809021,
      "citing_paper_id": "276766372",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "…(Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and other agents (Wang et al., 2023c; Park et al., 2023a; OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 40.38370680809021,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "LLM-based multi-agent systems have enabled collaborative problem-solving across domains (Park et al., 2023a; Li et al., 2023b; Chen et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 40.61581611633301,
      "citing_paper_id": "276766372",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "…al., 2024), software engineering tasks (Huang et al., 2023; Wu et al., 2023a; Zhou et al., 2023a; Hong et al., 2024; Ishibashi and Nishimura, 2024; Islam et al., 2024; Wang et al., 2024a; Zhuge et al.) including code generation and maintenance (Bouzenia et al., 2024), and gaming applications…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications and tasks. The cited paper title 'MapCoder: Multi-Agent Code Generation for Competitive Problem Solving' does not provide additional context to identify a dataset.",
      "processing_time": 45.136810541152954,
      "citing_paper_id": "276766372",
      "cited_paper_id": 269921148
    },
    {
      "context_text": "Benchmark Curation Details The test cases of Minecraft environment are also adapted from VillagerAgent (Dong et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'VillagerAgent' but does not refer to it as a dataset. It is described as a framework, which is excluded according to the rules.",
      "processing_time": 42.3473174571991,
      "citing_paper_id": "276766372",
      "cited_paper_id": 270371956
    },
    {
      "context_text": "Environment Description The environment of Minecraft is adapted from the VillagerAgent (Dong et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'VillagerAgent' but does not refer to it as a dataset. It is described as a framework, which is not a dataset.",
      "processing_time": 41.83524465560913,
      "citing_paper_id": "276766372",
      "cited_paper_id": 270371956
    },
    {
      "context_text": "VillagerAgent has defined more than 40 tools.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to tools within the VillagerAgent framework.",
      "processing_time": 40.37698292732239,
      "citing_paper_id": "276766372",
      "cited_paper_id": 270371956
    },
    {
      "context_text": "We used the same 100 target structures to test, covering different levels of difficulties as VillagerAgent did.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific, verifiable dataset. It refers to '100 target structures' but does not provide a clear identifier or source.",
      "processing_time": 42.06552052497864,
      "citing_paper_id": "276766372",
      "cited_paper_id": 270371956
    },
    {
      "context_text": "In Minecraft, agents perform complex tasks from construction to navigation (Wang et al., 2023a; Chen et al., 2023b; Yu et al., 2024b; Dong et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks performed by agents in Minecraft. No clear identifiers for datasets are present.",
      "processing_time": 40.99078416824341,
      "citing_paper_id": "276766372",
      "cited_paper_id": 270371956
    },
    {
      "context_text": "In Minecraft, agents perform complex tasks from construction to navigation (Wang et al., 2023a; Chen et al., 2023b; Yu et al., 2024b; Dong et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks performed by agents in Minecraft. No clear identifiers for datasets are present.",
      "processing_time": 40.99078416824341,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Lastly, similar to the Reflexion (Shinn et al., 2023) method, our cognitive self-evolving planning method mirrors human learning by generating expected outcomes and progress for each task, storing these in memory, and then comparing actual performance against these expectations in subsequent…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Reflexion. No verifiable resources are identified.",
      "processing_time": 40.19990134239197,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Lastly, similar to the Reflexion (Shinn et al., 2023) method, our cognitive self-evolving planning method mirrors human learning by generating expected outcomes and progress for each task, storing these in memory, and then comparing actual performance against these expectations in subsequent iterations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Reflexion. No verifiable resources are identified.",
      "processing_time": 40.60427665710449,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "• Chain-of-Thought Reasoning.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a general concept of reasoning.",
      "processing_time": 40.985310554504395,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "These approaches enable complex applications ranging from geopolitical conflict simulation (Hua et al., 2024) to scientific discovery workflows (Zhou et al., 2024a; Zhang et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of certain approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 42.37443804740906,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "These approaches enable complex applications ranging from geopolitical conflict simulation (Hua et al., 2024) to scientific discovery workflows (Zhou et al., 2024a; Zhang et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of certain approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 42.37443804740906,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "These systems support scientific research through literature review and experimental design (Zhou et al., 2024a; Agarwal et al., 2024), software engineering tasks (Huang et al., 2023; Wu et al., 2023a; Zhou et al., 2023a; Hong et al., 2024; Ishibashi and Nishimura, 2024; Islam et al., 2024; Wang et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of systems in scientific research and software engineering tasks.",
      "processing_time": 40.98278474807739,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 42.34661102294922,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 42.34661102294922,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and other agents (Wang et al., 2023c; Park et al., 2023a; OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 19.25568151473999,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020), GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023) and Deepsek-R1 (Guo et al., 2025), now exhibit human-like language understanding and generation, enabling their use as autonomous agents interacting with environments, tools, and other agents (Wang et al., 2023c; Park et al., 2023a; OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 19.25568151473999,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "GameNGen enables real-time interaction in DOOM (Valevski et al., 2024), while CUISINEWORLD benchmarks multi-agent collaboration (Gong et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions GameNGen and CUISINEWORLD but does not indicate they are datasets. They appear to be systems or environments used for research.",
      "processing_time": 43.53528547286987,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "In our centralized coordination protocol, the planner supports four distinct planning approaches that reflect human decision-making processes: vanilla prompting, chain-of-thought (CoT) (Wei et al., 2022), group discussion, and cognitive self-evolving planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only planning approaches and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 42.36678910255432,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, multi-agent setups (Li et al., 2023a; Wang et al., 2023b, 2024b) leverage multiple LLM-based agents that collaborate, coordinate, and jointly plan to address these challenges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multi-agent setups using LLM-based agents. There are no clear identifiers for datasets.",
      "processing_time": 43.24029231071472,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Applications extend to social deduction games, game theory (Xu et al., 2023), healthcare (Ke et al., 2024; Kim et al., 2024), business (Chen et al., 2024), education (Gösling et al., 2024), and urban planning (Zhou et al., 2024b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various application areas of LLMs. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 43.54437756538391,
      "citing_paper_id": "276766372",
      "cited_paper_id": null
    },
    {
      "context_text": "Further research (Shi et al., 2024; Schick et al., 2023; Liu et al., 2024b) focuses on strengthening agents’ core skills.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to further research on strengthening agents' core skills. No verifiable resources are identified.",
      "processing_time": 42.92009949684143,
      "citing_paper_id": "277509805",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Further research (Shi et al., 2024; Schick et al., 2023; Liu et al., 2024b) focuses on strengthening agents’ core skills.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to further research on strengthening agents' core skills. No verifiable resources are identified.",
      "processing_time": 42.92009949684143,
      "citing_paper_id": "277509805",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "User mission automation is a significant research area for large LLMs. General (Achiam et al., 2023; Sun et al., 2024; Yang et al., 2024; Team et al., 2024; GLM et al., 2024; Srivastava and Dames, 2024) LLMs with larger scale primarily integrate it within multi-task learning process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to LLMs and their integration into multi-task learning processes.",
      "processing_time": 56.06623315811157,
      "citing_paper_id": "277509805",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "User mission automation is a significant research area for large LLMs. General (Achiam et al., 2023; Sun et al., 2024; Yang et al., 2024; Team et al., 2024; GLM et al., 2024; Srivastava and Dames, 2024) LLMs with larger scale primarily integrate it within multi-task learning process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to LLMs and their integration into multi-task learning processes.",
      "processing_time": 56.06623315811157,
      "citing_paper_id": "277509805",
      "cited_paper_id": 270562306
    },
    {
      "context_text": "User mission automation is a significant research area for large LLMs. General (Achiam et al., 2023; Sun et al., 2024; Yang et al., 2024; Team et al., 2024; GLM et al., 2024; Srivastava and Dames, 2024) LLMs with larger scale primarily integrate it within multi-task learning process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to LLMs and their integration into multi-task learning processes.",
      "processing_time": 56.06623315811157,
      "citing_paper_id": "277509805",
      "cited_paper_id": 271212406
    },
    {
      "context_text": "Specifically, the closed-source general models are: o1-2024-12-17(OpenAI), GPT-4o-2024-11-20(Achiam et al., 2023), Gemini-1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.84638810157776,
      "citing_paper_id": "277509805",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "The proposed benchmark simulates real-world application scenarios, and evaluates the core abilities of agents and tests various LLMs. BFCL v1(Patil et al., 2023) 0.0 0.9 BFCL v2(Charlie Cheng-Jie Ji, b) 0.0 0.9 ToolBench(Qin et al., 2024) 0.0 0.0 AnyToolBench(Du et al., 2024) 0.0 0.0 τ -bench(Yao…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. They are likely leaderboards or challenge suites, which are excluded unless they contain specific, downloadable datasets.",
      "processing_time": 58.0370934009552,
      "citing_paper_id": "277509805",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "…et al., 2024a) provide a comprehensive assessment of multiple agent abilities, others (Huang et al., 2024b; Tang et al., 2023; Qiao et al., 2024a) address specific issues like the illusion problem (Patil et al., 2023) and multistep execution capabilities (Shen et al., 2024a; Yao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and their contributions to the field of LLM capabilities.",
      "processing_time": 55.84541440010071,
      "citing_paper_id": "277509805",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "…the core abilities of agents and tests various LLMs. BFCL v1(Patil et al., 2023) 0.0 0.9 BFCL v2(Charlie Cheng-Jie Ji, b) 0.0 0.9 ToolBench(Qin et al., 2024) 0.0 0.0 AnyToolBench(Du et al., 2024) 0.0 0.0 τ -bench(Yao et al., 2024) 0.0 0.0 T-EVAL(Chen et al., 2024) 0.0 0.0 UltraTool(Huang…",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmark or challenge names, but none of them are specific, downloadable datasets. They are primarily used for score comparison and evaluation of LLMs.",
      "processing_time": 57.87667274475098,
      "citing_paper_id": "277509805",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Others(Du et al., 2024; Qin et al., 2024; Ye et al., 2024; Li et al., 2023) collected massive tools to investigate the impact of tool diversity on agent performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'massive tools' but does not specify any datasets. The context is about collecting tools to study the impact of tool diversity on agent performance, which is not a dataset.",
      "processing_time": 58.81644034385681,
      "citing_paper_id": "277509805",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "The proposed benchmark simulates real-world application scenarios, and evaluates the core abilities of agents and tests various LLMs. BFCL v1(Patil et al., 2023) 0.0 0.9 BFCL v2(Charlie Cheng-Jie Ji, b) 0.0 0.9 ToolBench(Qin et al., 2024) 0.0 0.0 AnyToolBench(Du et al., 2024) 0.0 0.0 τ -bench(Yao et al., 2024) 0.0 0.0 T-EVAL(Chen et al., 2024) 0.0 0.0 UltraTool(Huang et al., 2024a) 0.0 0.0 Table 1: Comparative Analysis of the Multi-Mission Tool Bench against other benchmarks in the field.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not refer to them as datasets. They are primarily used for score comparison and evaluation of LLMs, which excludes them from being labeled as datasets.",
      "processing_time": 59.02544331550598,
      "citing_paper_id": "277509805",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "While some works (Shen et al., 2024b; Chen et al., 2024; Huang et al., 2024a) provide a comprehensive assessment of multiple agent abilities, others (Huang et al., 2024b; Tang et al., 2023; Qiao et al., 2024a) address specific issues like the illusion problem (Patil et al., 2023) and multistep…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works and their contributions to the field of large language models and task automation.",
      "processing_time": 56.72678232192993,
      "citing_paper_id": "277509805",
      "cited_paper_id": 265506220
    },
    {
      "context_text": "Other studies, WorfBench and TaskBench (Qiao et al., 2024a; Shen et al., 2024b), also introduce a graph-based evaluation method for multi-tool invocation.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WorfBench' and 'TaskBench' but does not specify their use as datasets. They are likely benchmarks or methods, not datasets.",
      "processing_time": 58.40083050727844,
      "citing_paper_id": "277509805",
      "cited_paper_id": 265506220
    },
    {
      "context_text": "Other studies, WorfBench and TaskBench (Qiao et al., 2024a; Shen et al., 2024b), also introduce a graph-based evaluation method for multi-tool invocation.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WorfBench' and 'TaskBench' but does not specify their use as datasets. They are likely benchmarks or methods, not datasets.",
      "processing_time": 58.40083050727844,
      "citing_paper_id": "277509805",
      "cited_paper_id": null
    },
    {
      "context_text": "Certain research (Zhuang et al., 2023; Guo et al., 2024; Xie et al., 2024) examines agents within specific domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research examining agents within specific domains. No clear, verifiable datasets are identified.",
      "processing_time": 56.727630853652954,
      "citing_paper_id": "277509805",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "Certain research (Zhuang et al., 2023; Guo et al., 2024; Xie et al., 2024) examines agents within specific domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research examining agents within specific domains. No clear, verifiable datasets are identified.",
      "processing_time": 56.727630853652954,
      "citing_paper_id": "277509805",
      "cited_paper_id": 271953942
    },
    {
      "context_text": "Others (Zhang et al., 2024a; Han et al., 2024; Islam et al., 2024) introduce heuristic decision frameworks to solve complex problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only heuristic decision frameworks introduced by other researchers.",
      "processing_time": 54.586371183395386,
      "citing_paper_id": "277509805",
      "cited_paper_id": 269921148
    },
    {
      "context_text": "Others (Zhang et al., 2024a; Han et al., 2024; Islam et al., 2024) introduce heuristic decision frameworks to solve complex problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only heuristic decision frameworks introduced by other researchers.",
      "processing_time": 54.586371183395386,
      "citing_paper_id": "277509805",
      "cited_paper_id": 272424184
    },
    {
      "context_text": "5-Pro-002( Team et al., 2024), Mistral-Large-2411(Mistral), and doubao-1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets.",
      "processing_time": 55.55712151527405,
      "citing_paper_id": "277509805",
      "cited_paper_id": 270562306
    },
    {
      "context_text": "5-Instruction-Series( Yang et al., 2024), GLM-4-9B-Chat(GLM et al., 2024), DeepSeek-R1(Guo et al., 2025), DeepSeek-V3(Liu et al., 2024a), and Llama-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models and does not refer to any specific datasets. The context is focused on models and their versions, which are not considered datasets.",
      "processing_time": 57.315433979034424,
      "citing_paper_id": "277509805",
      "cited_paper_id": 270562306
    },
    {
      "context_text": "3-70B-Instruct( Dubey et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model. No dataset names are present in the citation span.",
      "processing_time": 55.56480002403259,
      "citing_paper_id": "277509805",
      "cited_paper_id": 271571434
    },
    {
      "context_text": "The specialized models are: Toolace (Liu et al., 2024b), Hammer2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Toolace' and 'Hammer2', but neither are datasets. They are likely models or methods, and there are no other specific, verifiable datasets mentioned.",
      "processing_time": 58.5612518787384,
      "citing_paper_id": "277509805",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "Concurrently, some work (meetkai; Lin et al., 2024; Liu et al., 2024b) generate more diverse training data with proposed frameworks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to generating more diverse training data, which is too generic.",
      "processing_time": 56.286975622177124,
      "citing_paper_id": "277509805",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "1-Series(Lin et al., 2024), watt-tool-8b(Shi et al., 2024), xLAM-7b-fc-r(Zhang et al., 2024a), and gorilla-openfunctions-v2(Charlie Cheng-Jie Ji, b).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.03393793106079,
      "citing_paper_id": "277509805",
      "cited_paper_id": 272424184
    },
    {
      "context_text": "Some studies (Xu et al., 2024; Qiao et al., 2024b; Zhang et al., 2024b) equip agents with self-reflection and self-correction capabilities to improve their understanding of environmental feedback.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of agents. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.64133024215698,
      "citing_paper_id": "277509805",
      "cited_paper_id": null
    },
    {
      "context_text": "…et al., 2024; Huang et al., 2024a) provide a comprehensive assessment of multiple agent abilities, others (Huang et al., 2024b; Tang et al., 2023; Qiao et al., 2024a) address specific issues like the illusion problem (Patil et al., 2023) and multistep execution capabilities (Shen et al., 2024a;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing various aspects of agent abilities and specific issues.",
      "processing_time": 55.42679047584534,
      "citing_paper_id": "277509805",
      "cited_paper_id": null
    },
    {
      "context_text": "It is not necessary for these models to offer perfect advice; even occasional correctness can reduce the agent’s exploration time [ Icarte et al. , 2018].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper discussing advice-based exploration in reinforcement learning.",
      "processing_time": 55.42394280433655,
      "citing_paper_id": "276558335",
      "cited_paper_id": 5082661
    },
    {
      "context_text": "They often rely on transformer architectures, which use self-attention to capture token dependencies [Vaswani et al. , 2017].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer architectures).",
      "processing_time": 54.32842016220093,
      "citing_paper_id": "276558335",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "When integrated with deep neural networks, deep RL has made breakthroughs in challenging domains such as games and robotics [Schulman et al. , 2017; Vinyals et al. , 2019].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and findings. The cited papers are about reinforcement learning algorithms and their applications, but do not specify datasets.",
      "processing_time": 57.69389843940735,
      "citing_paper_id": "276558335",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "When integrated with deep neural networks, deep RL has made breakthroughs in challenging domains such as games and robotics [Schulman et al. , 2017; Vinyals et al. , 2019].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and findings. The cited papers are about reinforcement learning algorithms and their applications, but do not specify datasets.",
      "processing_time": 57.69389843940735,
      "citing_paper_id": "276558335",
      "cited_paper_id": 204972004
    },
    {
      "context_text": "By training large architectures — often spanning billions or even trillions of parameters — on internet-scale datasets, LLMs such as GPT-3 [Brown et al. , 2020] have demonstrated emergent capabilities that smaller models could not achieve.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'internet-scale datasets' without naming any particular resource.",
      "processing_time": 56.0063693523407,
      "citing_paper_id": "276558335",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Leveraging large-scale, aligned image-text training, VLMs like CLIP [ Radford et al. , 2021 ] can perform a variety of tasks, including image-text retrieval and classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (CLIP) and its capabilities. No verifiable resource names are present in the context.",
      "processing_time": 56.946211099624634,
      "citing_paper_id": "276558335",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "A unique example is Text2Motion [Lin et al. , 2023], which combines both Comprehensive and Incremental Planning, ensuring efficiency and correctness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or system called Text2Motion. The citation is used to describe the capabilities of Text2Motion, not to reference a dataset.",
      "processing_time": 58.66886925697327,
      "citing_paper_id": "276558335",
      "cited_paper_id": 257663442
    },
    {
      "context_text": "Integration of LLMs into RL is hindered by the need to convert rich numeric signals, such as raw sensor data and actions, into sequences of textual tokens, losing the nuanced semantic information required for precise control [Du et al. , 2023; Hu and Sadigh, 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of integrating LLMs with RL.",
      "processing_time": 54.91067290306091,
      "citing_paper_id": "276558335",
      "cited_paper_id": 258179085
    },
    {
      "context_text": "Reflexion [ Shinn et al. , 2023] introduces verbal reinforcement, where LLMs generate and store self-reflective feedback in an episodic memory buffer to improve decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'verbal reinforcement'. No dataset names are present in the citation span.",
      "processing_time": 56.933616638183594,
      "citing_paper_id": "276558335",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Another approach is to carefully design the plan’s structure generated by LLMs to fit the real-world requirement [Tang et al. , 2023], which also faces the problem of lacking generalization in diverse tasks and environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem of lack of generalization in diverse tasks and environments.",
      "processing_time": 55.573832273483276,
      "citing_paper_id": "276558335",
      "cited_paper_id": 259144814
    },
    {
      "context_text": "For example, SayTap [ Tang et al. , 2023 ] uses foot contact patterns as a compact interface between language instructions and low-level quadruped control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SayTap) for translating language instructions into quadrupedal locomotion.",
      "processing_time": 56.41905879974365,
      "citing_paper_id": "276558335",
      "cited_paper_id": 259144814
    },
    {
      "context_text": "Similarly, REMEM-BERER [Zhang et al. , 2023a] incorporates a persistent experience memory, allowing LLMs to learn from past successes and failures in interactive environments without modifying parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (REMEM-BERER) and its capabilities. No verifiable resources are identified.",
      "processing_time": 56.92700958251953,
      "citing_paper_id": "276558335",
      "cited_paper_id": 259145016
    },
    {
      "context_text": "Leveraging these strengths, LLMs are now applied to tasks that extend beyond conventional Natural Language Processing (NLP), spanning domains from healthcare to robotics [ Ichter et al. , 2022; Thirunavukarasu et al. , 2023 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the application of LLMs across various domains. No verifiable resources are identified.",
      "processing_time": 56.412638902664185,
      "citing_paper_id": "276558335",
      "cited_paper_id": 259947046
    },
    {
      "context_text": "For example, in Text2Reward [Xie et al. , 2024], an LLM refines the reward function code until it executes successfully.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Text2Reward) for generating reward functions using LLMs.",
      "processing_time": 55.86837816238403,
      "citing_paper_id": "276558335",
      "cited_paper_id": 262053612
    },
    {
      "context_text": "For example, AdaRefiner [ Zhang and Lu, 2024 ] enhances the agent’s execution and understanding of LLM guidance by introducing a secondary LLM to evaluate the alignment of the agent’s execution process and the guidance of LLM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AdaRefiner) and its application to refine decisions of language models.",
      "processing_time": 56.114781856536865,
      "citing_paper_id": "276558335",
      "cited_paper_id": 263310497
    },
    {
      "context_text": "Similarly, BOSS [ Zhang et al. , 2023b ] learns from past trajectories but eliminates the need for a critic LLM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BOSS) and a reference to a paper. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.955052614212036,
      "citing_paper_id": "276558335",
      "cited_paper_id": 263708879
    },
    {
      "context_text": "To avoid querying LLMs after each failure execution and reduce the querying cost of LLMs, LgTS [Shukla et al. , 2024] uses LLMs to generate multiple candidate sub-goal sequences before execution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of LLMs for generating sub-goal sequences. No verifiable resources are identified.",
      "processing_time": 56.64436459541321,
      "citing_paper_id": "276558335",
      "cited_paper_id": 264146113
    },
    {
      "context_text": "PREDILECT [ Holk et al. , 2024 ] builds on preference-based RL, allowing human raters to specify both their preferred trajectory and the reasons for their choice.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called PREDILECT. The context focuses on the capabilities of PREDILECT in preference-based reinforcement learning.",
      "processing_time": 58.5523726940155,
      "citing_paper_id": "276558335",
      "cited_paper_id": 267897401
    },
    {
      "context_text": "MaestroMotif [Klissarov et al. , 2025] and LAST [Fu et al. , 2024] guide hierarchical RL by discovering and coordinating skills.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MaestroMotif and LAST, which are methods or tools, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 56.92088270187378,
      "citing_paper_id": "276558335",
      "cited_paper_id": 268032502
    },
    {
      "context_text": "Current works solving the grounding problem by applying a bridging layer or verification module between the high-level plan and the low-level controller [ Dalal et al. , 2024; Huang et al. , 2022 ] or by leveraging the value-function to ground the action [ Ichter et al. , 2022 ] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the methodologies used to solve the grounding problem in robotics tasks.",
      "processing_time": 57.098024129867554,
      "citing_paper_id": "276558335",
      "cited_paper_id": 269502645
    },
    {
      "context_text": "Different from LMA3, PSL [ Dalal et al. , 2024 ] leverages the LLM to decompose the long-horizon natural language task into specially formatted language sub-goals.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PSL) and a reference to a paper. The context focuses on the methodology and approach rather than a specific dataset.",
      "processing_time": 58.54502558708191,
      "citing_paper_id": "276558335",
      "cited_paper_id": 269502645
    },
    {
      "context_text": "LLMs demonstrate strong capabilities in generating high-level plans, but they lack real-world experience, so their plans may not be executable for embodied agents such as robots [ Ichter et al. , 2022; Dalal et al. , 2024 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing LLMs and their planning capabilities for robotics tasks.",
      "processing_time": 55.653217792510986,
      "citing_paper_id": "276558335",
      "cited_paper_id": 269502645
    },
    {
      "context_text": "Lai and Zang [2024] use an FM to identify and emphasize higher-quality trajectories.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (FM) for identifying and emphasizing higher-quality trajectories.",
      "processing_time": 56.59976601600647,
      "citing_paper_id": "276558335",
      "cited_paper_id": 269536190
    },
    {
      "context_text": "Recent work suggests FMs provide powerful priors for RL, though their planning ability remains heavily debated [Kambhampati et al. , 2024 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a debate about the planning abilities of LLMs in RL contexts.",
      "processing_time": 55.65456533432007,
      "citing_paper_id": "276558335",
      "cited_paper_id": 272330427
    },
    {
      "context_text": "Zeng et al. [2024] use an LLM to identify key behavioural features (to promote or discourage) and propose an initial reward function parameterization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of an LLM to identify key behavioral features and propose a reward function.",
      "processing_time": 56.01525163650513,
      "citing_paper_id": "276558335",
      "cited_paper_id": null
    },
    {
      "context_text": "RLingua [Chen et al. , 2024] improves sample efficiency in RL for robotic manipulation by leveraging LLM-generated rule-based controllers as priors and integrating prior knowledge into policy learning through prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RLingua) and its application in improving sample efficiency in reinforcement learning for robotic manipulation.",
      "processing_time": 57.16930794715881,
      "citing_paper_id": "276558335",
      "cited_paper_id": null
    },
    {
      "context_text": "While fuzzy search [25] (identifying cache hits based on similar but not identical keywords) could handle approximate key similarities and is feasible to integrate, we opted against it and leave it for future exploration for two main reasons: (1) Determining fuzzy matches based on semantic or…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (fuzzy search) which is not used in the current research.",
      "processing_time": 56.01235032081604,
      "citing_paper_id": "279447720",
      "cited_paper_id": 1767517
    },
    {
      "context_text": "Unlike single-model tasks like chatbots [15], math [22], or coding [14], these applications coordinate multiple models and queries to solve complex tasks, like data-intensive reasoning [35], software engineering [55, 45], web navigation [60], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 56.55326509475708,
      "citing_paper_id": "279447720",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Unlike single-model tasks like chatbots [15], math [22], or coding [14], these applications coordinate multiple models and queries to solve complex tasks, like data-intensive reasoning [35], software engineering [55, 45], web navigation [60], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 56.55326509475708,
      "citing_paper_id": "279447720",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "Unlike single-model tasks like chatbots [15], math [22], or coding [14], these applications coordinate multiple models and queries to solve complex tasks, like data-intensive reasoning [35], software engineering [55, 45], web navigation [60], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 56.55326509475708,
      "citing_paper_id": "279447720",
      "cited_paper_id": 273502415
    },
    {
      "context_text": "Unlike single-model tasks like chatbots [15], math [22], or coding [14], these applications coordinate multiple models and queries to solve complex tasks, like data-intensive reasoning [35], software engineering [55, 45], web navigation [60], etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. The cited paper titles do not introduce any specific datasets either.",
      "processing_time": 56.55326509475708,
      "citing_paper_id": "279447720",
      "cited_paper_id": 276107438
    },
    {
      "context_text": "This could hinder adaptation to similar queries with minor differences ( e.g., numeric values or variable names in mathematical reasoning [16], coding tasks [24]), a common challenge in agentic AI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general challenges in agentic AI. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 57.17186498641968,
      "citing_paper_id": "279447720",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "This could hinder adaptation to similar queries with minor differences ( e.g., numeric values or variable names in mathematical reasoning [16], coding tasks [24]), a common challenge in agentic AI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general challenges in agentic AI. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 57.17186498641968,
      "citing_paper_id": "279447720",
      "cited_paper_id": 268379413
    },
    {
      "context_text": "While effective, these agents incur significant costs due to the complexity of workflows executed [29, 57] and need to interact with external tools and environments [38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing the complexity of workflows and interactions with external tools.",
      "processing_time": 55.720415353775024,
      "citing_paper_id": "279447720",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Agentic applications based on Large Language Models (LLMs) have shown early promise in replicating human performance on a broad range of workflows, from coding [24, 26, 52] to web navigation [17, 21, 60] to open-ended research [3, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 57.79344463348389,
      "citing_paper_id": "279447720",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "Agentic applications based on Large Language Models (LLMs) have shown early promise in replicating human performance on a broad range of workflows, from coding [24, 26, 52] to web navigation [17, 21, 60] to open-ended research [3, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 57.79344463348389,
      "citing_paper_id": "279447720",
      "cited_paper_id": 267211622
    },
    {
      "context_text": "Agentic applications based on Large Language Models (LLMs) have shown early promise in replicating human performance on a broad range of workflows, from coding [24, 26, 52] to web navigation [17, 21, 60] to open-ended research [3, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 57.79344463348389,
      "citing_paper_id": "279447720",
      "cited_paper_id": 268379413
    },
    {
      "context_text": "While this holds for chatbots, many agentic AI applications are data-dependent : Outputs depend not only on input queries but also on external data ( e.g., data-intensive reasoning [35]) or dynamic environments ( e.g., web or GUI agents [60, 48, 47]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'external data' and 'dynamic environments'. No verifiable resources are identified.",
      "processing_time": 56.715458393096924,
      "citing_paper_id": "279447720",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "…particular, caching has emerged as a popular approach, with two primary implementations: Context caching ( e.g., KV cache reuse and prompt caching [18, 53, 51]) stores internal model states to speed up subsequent generations, while semantic caching [10, 40, 2] stores and reuses (input, output)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for caching in large language models.",
      "processing_time": 54.859970808029175,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265067391
    },
    {
      "context_text": "Context caching [18, 53, 51], also known as KV cache reuse or prompt caching, involves storing and reusing the key-value pairs generated during the prefill phase of LLM inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques related to context caching in LLMs.",
      "processing_time": 55.273462772369385,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265067391
    },
    {
      "context_text": "Context caching [18, 53, 51], also known as KV cache reuse or prompt caching, involves storing and reusing the key-value pairs generated during the prefill phase of LLM inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques related to context caching in LLMs.",
      "processing_time": 55.273462772369385,
      "citing_paper_id": "279447720",
      "cited_paper_id": 270062853
    },
    {
      "context_text": "Context caching relies on KV cache as the medium for storing and reusing knowledge [32, 18, 51].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of KV cache for context caching. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.144936323165894,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265067391
    },
    {
      "context_text": "Context caching relies on KV cache as the medium for storing and reusing knowledge [32, 18, 51].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of KV cache for context caching. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.144936323165894,
      "citing_paper_id": "279447720",
      "cited_paper_id": 271515809
    },
    {
      "context_text": "We evaluate our system on two data-intensive reasoning benchmarks: FinanceBench [23] (financial and long-context data reasoning) and Tabular Math Word Problems [34] (tabular and mathematical data reasoning).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FinanceBench",
        "Tabular Math Word Problems"
      ],
      "dataset_descriptions": {
        "FinanceBench": "Used to evaluate financial and long-context data reasoning capabilities, focusing on complex financial questions and contextual understanding.",
        "Tabular Math Word Problems": "Used to assess tabular and mathematical data reasoning, focusing on solving math problems presented in tabular formats."
      },
      "confidence_score": 1.0,
      "reasoning": "FinanceBench and Tabular Math Word Problems are mentioned as benchmarks used for evaluating the system. FinanceBench is specifically noted for financial and long-context data reasoning, while Tabular Math Word Problems is for tabular and mathematical data reasoning.",
      "processing_time": 72.32183790206909,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265294665
    },
    {
      "context_text": "We evaluate our agentic plan caching framework across two datasets focused on data-intensive reasoning, covering long-context financial data (FinanceBench [23]) as well as tabular mathematical data (Tabular Math Word Problems [34]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FinanceBench",
        "Tabular Math Word Problems"
      ],
      "dataset_descriptions": {
        "FinanceBench": "Used to evaluate the agentic plan caching framework on long-context financial data, focusing on data-intensive reasoning tasks.",
        "Tabular Math Word Problems": "Used to evaluate the agentic plan caching framework on tabular mathematical data, focusing on data-intensive reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FinanceBench and Tabular Math Word Problems, which are used to evaluate an agentic plan caching framework.",
      "processing_time": 69.41418194770813,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265294665
    },
    {
      "context_text": "• Semantic Caching : We implement a query-level semantic caching method based on previous work [10, 40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for semantic caching. No dataset names are present in the citation span.",
      "processing_time": 55.95039391517639,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "• Semantic Caching : We implement a query-level semantic caching method based on previous work [10, 40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for semantic caching. No dataset names are present in the citation span.",
      "processing_time": 55.95039391517639,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "Following the approach of GPTCache [10], 1 we cache and reuse responses to individual queries, determining cache hits based on query-level similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for caching and reusing responses in LLM applications.",
      "processing_time": 55.198389530181885,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "A common method for identifying similar queries in a cache is to assess semantic or textual similarity, as seen in frameworks like GPTCache [10] which use embeddings for similarity searches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method or framework (GPTCache) and its use of embeddings for similarity searches.",
      "processing_time": 58.07926058769226,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "Semantic caching [10, 40, 2], on the other hand, stores input-output pairs of previous LLM invocations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method or technique called 'semantic caching'.",
      "processing_time": 55.941540241241455,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "Semantic caching [10, 40, 2], on the other hand, stores input-output pairs of previous LLM invocations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method or technique called 'semantic caching'.",
      "processing_time": 55.941540241241455,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "…Context caching ( e.g., KV cache reuse and prompt caching [18, 53, 51]) stores internal model states to speed up subsequent generations, while semantic caching [10, 40, 2] stores and reuses (input, output) pairs to accelerate the serving of queries that are similar to historical queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses caching techniques, which are methods, not datasets.",
      "processing_time": 55.92250680923462,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "…Context caching ( e.g., KV cache reuse and prompt caching [18, 53, 51]) stores internal model states to speed up subsequent generations, while semantic caching [10, 40, 2] stores and reuses (input, output) pairs to accelerate the serving of queries that are similar to historical queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses caching techniques, which are methods, not datasets.",
      "processing_time": 55.92250680923462,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "Semantic caching stores input-output pairs from previous LLM calls, assuming outputs depend solely on input prompts [10, 40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for caching input-output pairs from LLM calls.",
      "processing_time": 54.923253297805786,
      "citing_paper_id": "279447720",
      "cited_paper_id": 265607979
    },
    {
      "context_text": "Semantic caching stores input-output pairs from previous LLM calls, assuming outputs depend solely on input prompts [10, 40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for caching input-output pairs from LLM calls.",
      "processing_time": 54.923253297805786,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "Many such agentic AI applications, like multi-agent systems [43, 20] and cloud-edge LLM systems [56, 35] , follow a two-stage pipeline loop (similar to the ReAct-agent loop [54]), as shown in Figure 1a: (1) Plan and (2) Act.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to multi-agent systems and cloud-edge LLM systems. No verifiable resources are identified.",
      "processing_time": 56.825684547424316,
      "citing_paper_id": "279447720",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "This results in substantial costs for executing agentic workflows via APIs [13, 35] or locally [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing execution costs of workflows. No verifiable resources are identified.",
      "processing_time": 56.159503698349,
      "citing_paper_id": "279447720",
      "cited_paper_id": 267898017
    },
    {
      "context_text": "• Full-History Caching (discussed in §3.2): Inspired by knowledge caching in retrieval-augmented generation [53, 27], this baseline caches the complete agent execution log, including inputs and outputs of all LLM agent components.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and concepts related to knowledge caching and retrieval-augmented generation.",
      "processing_time": 55.6554913520813,
      "citing_paper_id": "279447720",
      "cited_paper_id": 269283058
    },
    {
      "context_text": "• Full-History Caching (discussed in §3.2): Inspired by knowledge caching in retrieval-augmented generation [53, 27], this baseline caches the complete agent execution log, including inputs and outputs of all LLM agent components.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and concepts related to knowledge caching and retrieval-augmented generation.",
      "processing_time": 55.6554913520813,
      "citing_paper_id": "279447720",
      "cited_paper_id": 270062853
    },
    {
      "context_text": "…caching has emerged as a popular approach, with two primary implementations: Context caching ( e.g., KV cache reuse and prompt caching [18, 53, 51]) stores internal model states to speed up subsequent generations, while semantic caching [10, 40, 2] stores and reuses (input, output) pairs…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches related to caching in large language models.",
      "processing_time": 55.12701392173767,
      "citing_paper_id": "279447720",
      "cited_paper_id": 270062853
    },
    {
      "context_text": "Specifically, the Plan stage is often implemented via test-time compute techniques [11, 41] like chain-of-thought reasoning [46], which can require numerous LLM queries and access to expensive LLMs ( e.g., reasoning models).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and models. No verifiable resources are identified.",
      "processing_time": 55.12403702735901,
      "citing_paper_id": "279447720",
      "cited_paper_id": 271571035
    },
    {
      "context_text": "We assess application-level performance using GPT-4o as the evaluation model, as LLM-based evaluation is more effective than exact matches or F1 scores for numeric evaluation and long-form responses [12, 19, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of GPT-4o as an evaluation model. No verifiable datasets are referenced.",
      "processing_time": 57.2130331993103,
      "citing_paper_id": "279447720",
      "cited_paper_id": 277780948
    },
    {
      "context_text": "Many of these LLM-based agents follow a two-stage pipeline that alternates between [54, 39]: (1) Plan – reasoning about what to do next, and (2) Act – executing those plans.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general process involving planning and acting in LLM-based agents.",
      "processing_time": 55.645336627960205,
      "citing_paper_id": "279447720",
      "cited_paper_id": 278237252
    },
    {
      "context_text": "This is because it might overemphasize context-specific details ( e.g., names of individuals or companies) rather than the broader intent of queries, which makes it difficult to establish an effective similarity threshold [40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses a general issue related to context-specific details in queries.",
      "processing_time": 56.58039307594299,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "This relies on the fact that many prompts share similar underlying intents and thus expected outputs despite having different wording [40].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general concept about prompts sharing similar underlying intents.",
      "processing_time": 56.997092723846436,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "Some studies focus on defining memory formats [54] and managing memory efficiently [36, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies focusing on memory formats and management. No verifiable resources are identified.",
      "processing_time": 56.99404525756836,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "Although several memory architectures have been proposed to help agents store and learn from past experiences [42, 50, 44, 36], these efforts primarily focus on using such memories to improve the agent’s accuracy on completing workflows ( e.g., with fewer hallucinations [9] or with higher task…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about memory architectures and their use in improving agent accuracy.",
      "processing_time": 56.133206844329834,
      "citing_paper_id": "279447720",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, Overcooked (Carroll et al., 2019; Liu et al., 2023a) simulates a cooperative cooking environment where agents must collaborate efficiently under time constraints and task dependencies, testing planning and communication abilities.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Overcooked' as a simulation environment but does not specify it as a dataset. It is described as a cooperative cooking environment used to test planning and communication abilities.",
      "processing_time": 58.54186701774597,
      "citing_paper_id": "274150091",
      "cited_paper_id": 202770731
    },
    {
      "context_text": "…game environments into a unified testbed for research on long-context LLMs. Games have historically served as highly effective metrics for evaluating progress in deep reinforcement learning research (Bellemare et al., 2013; Silver et al., 2018; Schrittwieser et al., 2020; Vinyals et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to games as environments for evaluating reinforcement learning models.",
      "processing_time": 55.626407623291016,
      "citing_paper_id": "274150091",
      "cited_paper_id": 208158225
    },
    {
      "context_text": "Sub-selecting only the relevant parts of demonstrations via retrieval-augmented few-shot prompting strategies (Lewis et al., 2020) might offer a way to circumvent these challenges.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or strategy for few-shot prompting.",
      "processing_time": 54.87605047225952,
      "citing_paper_id": "274150091",
      "cited_paper_id": 218869575
    },
    {
      "context_text": "Benchmarks such as SuperGLUE (Wang et al., 2019), which tests general-purpose language understanding and MMLU (Hendrycks et al., 2020), which measures massive multitask language understanding, have been instrumental in advancing LLM research.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as specific, reusable datasets. It focuses on their role in advancing LLM research rather than using them as datasets.",
      "processing_time": 58.04544019699097,
      "citing_paper_id": "274150091",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "LLMs possess vast knowledge across domains (Brown, 2020; Hendrycks et al., 2020), can reason in specific scenarios (Wei et al., 2022a; Shinn et al., 2023; Rein et al., 2023), and can reliably follow human instructions in simple settings (Ouyang et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 56.56057167053223,
      "citing_paper_id": "274150091",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "LLMs possess vast knowledge across domains (Brown, 2020; Hendrycks et al., 2020), can reason in specific scenarios (Wei et al., 2022a; Shinn et al., 2023; Rein et al., 2023), and can reliably follow human instructions in simple settings (Ouyang et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 56.56057167053223,
      "citing_paper_id": "274150091",
      "cited_paper_id": 249674500
    },
    {
      "context_text": "LLMs possess vast knowledge across domains (Brown, 2020; Hendrycks et al., 2020), can reason in specific scenarios (Wei et al., 2022a; Shinn et al., 2023; Rein et al., 2023), and can reliably follow human instructions in simple settings (Ouyang et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 56.56057167053223,
      "citing_paper_id": "274150091",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "In the domain of code understanding and generation, benchmarks such as HumanEval (Chen et al., 2021) and CodeXGLUE (Lu et al., 2021) evaluate models capabilities in programming tasks.",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as reusable datasets. It focuses on evaluating model capabilities in programming tasks.",
      "processing_time": 55.936171531677246,
      "citing_paper_id": "274150091",
      "cited_paper_id": 235755472
    },
    {
      "context_text": "( Hafner, 2021) A Minecraft-inspired grid environment where the player has to explore, gather resources and craft items to ensure their survival.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes a Minecraft-inspired environment but does not mention any specific, verifiable dataset. The environment is described as a method or tool rather than a dataset.",
      "processing_time": 57.47579884529114,
      "citing_paper_id": "274150091",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Crafter (Hafner, 2021) is an open-source 2D survival game designed specifically for research on strong generalization, deep exploration, and long-term reasoning in reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a game environment used for research, which is not a dataset.",
      "processing_time": 56.549710273742676,
      "citing_paper_id": "274150091",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Specifically BALROG enables seamless running of LLM and VLM agents on BabyAI, Crafter, TextWorld, Baba Is AI, MiniHack, and NetHack (Chevalier-Boisvert et al., 2019; Hafner, 2021; Cˆot´e et al., 2019; Cloos et al., 2024; Samvelyan et al., 2021; K¨uttler et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several environments but does not specify any datasets. The environments are likely used for evaluating LLM and VLM agents, but they are not datasets.",
      "processing_time": 57.80676198005676,
      "citing_paper_id": "274150091",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021) assess models’ abilities to solve grade-school and competition-level math problems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess models' ability to solve grade-school math problems, focusing on step-by-step reasoning and problem-solving skills.",
        "MATH": "Used to evaluate models' performance on competition-level math problems, emphasizing complex reasoning and problem-solving strategies."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GSM8K and MATH, which are used to assess models' mathematical reasoning capabilities. These datasets are directly relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 71.18056678771973,
      "citing_paper_id": "274150091",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "Mathematical reasoning datasets like GSM8K and MATH (Cobbe et al., 2021; Hendrycks et al., 2021) assess models’ abilities to solve grade-school and competition-level math problems, while Shi et al. (2022) explore multilingual chain-of-thought reasoning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to assess models' ability to solve grade-school math problems, focusing on step-by-step reasoning and problem-solving skills.",
        "MATH": "Used to evaluate models' performance on competition-level math problems, emphasizing complex reasoning and problem-solving strategies."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, GSM8K and MATH, which are used to assess models' mathematical reasoning capabilities. These datasets are directly relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 71.18056678771973,
      "citing_paper_id": "274150091",
      "cited_paper_id": 252735112
    },
    {
      "context_text": "Many existing works on decision-making have studied model performance on this environment (Reed et al., 2022; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to model performance in an unspecified environment. No clear, verifiable datasets are identified.",
      "processing_time": 56.93415570259094,
      "citing_paper_id": "274150091",
      "cited_paper_id": 246485514
    },
    {
      "context_text": "Many existing works on decision-making have studied model performance on this environment (Reed et al., 2022; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to model performance in an unspecified environment. No clear, verifiable datasets are identified.",
      "processing_time": 56.93415570259094,
      "citing_paper_id": "274150091",
      "cited_paper_id": 248722148
    },
    {
      "context_text": "The computational expressiveness of LLMs is fundamentally linked with the ability to solve complex reasoning problems (Wei et al., 2022a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the computational expressiveness of LLMs and their ability to solve complex reasoning problems.",
      "processing_time": 56.55268597602844,
      "citing_paper_id": "274150091",
      "cited_paper_id": 249674500
    },
    {
      "context_text": "Other environments such as MineDojo (Fan et al., 2022) and MineRL (Guss et al., 2019) also present open-ended challenges for agentic capabilities, their steep computational requirements and reliance on multimodal inputs make them less practical for accessible, large-scale benchmarks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MineDojo and MineRL as environments presenting open-ended challenges for agentic capabilities, but does not refer to them as datasets. They are described as environments with computational requirements and multimodal inputs.",
      "processing_time": 59.06647562980652,
      "citing_paper_id": "274150091",
      "cited_paper_id": 249848263
    },
    {
      "context_text": "The benchmark codebase also supports the study of In-Context Reinforcement Learning (Lee et al., 2024; Laskin et al., 2022; Lin et al., 2023), where agents learn to improve from mistakes during inference.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of In-Context Reinforcement Learning. No verifiable resources are identified.",
      "processing_time": 56.59647607803345,
      "citing_paper_id": "274150091",
      "cited_paper_id": 253107613
    },
    {
      "context_text": "The benchmark codebase also supports the study of In-Context Reinforcement Learning (Lee et al., 2024; Laskin et al., 2022; Lin et al., 2023), where agents learn to improve from mistakes during inference.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of In-Context Reinforcement Learning. No verifiable resources are identified.",
      "processing_time": 56.59647607803345,
      "citing_paper_id": "274150091",
      "cited_paper_id": 259262142
    },
    {
      "context_text": "The benchmark codebase also supports the study of In-Context Reinforcement Learning (Lee et al., 2024; Laskin et al., 2022; Lin et al., 2023), where agents learn to improve from mistakes during inference.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of In-Context Reinforcement Learning. No verifiable resources are identified.",
      "processing_time": 56.59647607803345,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263909278
    },
    {
      "context_text": "For example, model performance on the tasks in BALROG might be improved by integrating multi-agent collaboration (Chang, 2023; Khan et al., 2024; Yao et al., 2024) and tool usage (Shen et al., 2024; Ruan et al., 2023; Schick et al., 2024; Qin et al., 2023) in decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.79620337486267,
      "citing_paper_id": "274150091",
      "cited_paper_id": 257532801
    },
    {
      "context_text": "For example, model performance on the tasks in BALROG might be improved by integrating multi-agent collaboration (Chang, 2023; Khan et al., 2024; Yao et al., 2024) and tool usage (Shen et al., 2024; Ruan et al., 2023; Schick et al., 2024; Qin et al., 2023) in decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.79620337486267,
      "citing_paper_id": "274150091",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "For example, model performance on the tasks in BALROG might be improved by integrating multi-agent collaboration (Chang, 2023; Khan et al., 2024; Yao et al., 2024) and tool usage (Shen et al., 2024; Ruan et al., 2023; Schick et al., 2024; Qin et al., 2023) in decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.79620337486267,
      "citing_paper_id": "274150091",
      "cited_paper_id": 267627652
    },
    {
      "context_text": "Advanced Reasoning Strategies Beyond simply prompting LLMs and VLMs to directly predict the next action of game-play, BALROG also supports the study of more advanced reasoning techniques like chain-of-thought (Wei et al., 2022b), self-refinement (Madaan et al., 2024), and basic planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only advanced reasoning techniques. The context focuses on methodologies rather than data sources.",
      "processing_time": 56.008424043655396,
      "citing_paper_id": "274150091",
      "cited_paper_id": 257900871
    },
    {
      "context_text": "…have recently investigated how LLMs use these skills to solve practical tasks, including using computer interfaces to perform office-related chores (Wang et al., 2024; Qin et al., 2024), navigating web pages (Yao et al., 2022; Zhou et al., 2023), and solve GitHub issues (Jimenez et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and activities performed by LLMs. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.579482316970825,
      "citing_paper_id": "274150091",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "…have recently investigated how LLMs use these skills to solve practical tasks, including using computer interfaces to perform office-related chores (Wang et al., 2024; Qin et al., 2024), navigating web pages (Yao et al., 2022; Zhou et al., 2023), and solve GitHub issues (Jimenez et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and activities performed by LLMs. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.579482316970825,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263829697
    },
    {
      "context_text": "The env wrapper.py file standard-izes interaction across settings, and the evaluator executes agents and collects performance metrics. tasks (Wang et al., 2024), navigating the Internet (Zhou et al., 2023), and resolving GitHub issues (Jimenez et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'tasks' but does not specify a dataset name. The cited papers do not provide additional dataset names. No specific, verifiable datasets are mentioned.",
      "processing_time": 57.75924849510193,
      "citing_paper_id": "274150091",
      "cited_paper_id": 260164780
    },
    {
      "context_text": "The env wrapper.py file standard-izes interaction across settings, and the evaluator executes agents and collects performance metrics. tasks (Wang et al., 2024), navigating the Internet (Zhou et al., 2023), and resolving GitHub issues (Jimenez et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'tasks' but does not specify a dataset name. The cited papers do not provide additional dataset names. No specific, verifiable datasets are mentioned.",
      "processing_time": 57.75924849510193,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263829697
    },
    {
      "context_text": "…from tasks where we see fair zero-shot performance by state-of-the-art long-context models (BabyAI) to those where even specialized neural models trained on billions of in-domain datapoints make very limited progress (NetHack) (Piterbarg et al., 2024; Klissarov et al., 2023; Wołczyk et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BabyAI",
        "NetHack"
      ],
      "dataset_descriptions": {
        "BabyAI": "Used to evaluate zero-shot performance of long-context models, focusing on tasks where these models show fair performance without additional training.",
        "NetHack": "Used to assess the limitations of specialized neural models, highlighting tasks where even extensive in-domain training results in limited progress."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'BabyAI' and 'NetHack' as specific environments or datasets used to evaluate model performance. These are multi-word proper nouns that fit the criteria for inclusion.",
      "processing_time": 69.90321898460388,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263334319
    },
    {
      "context_text": "…from tasks where we see fair zero-shot performance by state-of-the-art long-context models (BabyAI) to those where even specialized neural models trained on billions of in-domain datapoints make very limited progress (NetHack) (Piterbarg et al., 2024; Klissarov et al., 2023; Wołczyk et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BabyAI",
        "NetHack"
      ],
      "dataset_descriptions": {
        "BabyAI": "Used to evaluate zero-shot performance of long-context models, focusing on tasks where these models show fair performance without additional training.",
        "NetHack": "Used to assess the limitations of specialized neural models, highlighting tasks where even extensive in-domain training results in limited progress."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'BabyAI' and 'NetHack' as specific environments or datasets used to evaluate model performance. These are multi-word proper nouns that fit the criteria for inclusion.",
      "processing_time": 69.90321898460388,
      "citing_paper_id": "274150091",
      "cited_paper_id": 266174368
    },
    {
      "context_text": "Recent successes of Large Language Models (LLMs) have renewed interest in building general-purpose agents capable of autonomously achieving complex goals Yang et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general interest in the capabilities of LLMs.",
      "processing_time": 55.3427734375,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263829697
    },
    {
      "context_text": "These limitations are further exemplified in our benchmark, where grid-based image observations differ significantly from the natural image-text pairs on which many VLMs are trained (Yu et al., 2023; Rahmanzadehgervi et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to 'natural image-text pairs' but does not provide a specific dataset name.",
      "processing_time": 57.10097932815552,
      "citing_paper_id": "274150091",
      "cited_paper_id": 263909278
    },
    {
      "context_text": "These limitations are further exemplified in our benchmark, where grid-based image observations differ significantly from the natural image-text pairs on which many VLMs are trained (Yu et al., 2023; Rahmanzadehgervi et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to 'natural image-text pairs' but does not provide a specific dataset name.",
      "processing_time": 57.10097932815552,
      "citing_paper_id": "274150091",
      "cited_paper_id": 271064506
    },
    {
      "context_text": "NetHack has already been used extensively as a testbed for RL agents (Wołczyk et al., 2024; Piterbarg et al., 2024; Hambro et al., 2022b); tabula-rasa RL agents particularly struggle due to sparse reward, complex credit assignment, extremely long-time-horizon, and high stochasticity of the game.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NetHack as a testbed for RL agents but does not refer to it as a dataset. It is described as a game environment, which is not a dataset.",
      "processing_time": 58.29095125198364,
      "citing_paper_id": "274150091",
      "cited_paper_id": 266174368
    },
    {
      "context_text": "Several works studied the multi-agent capabilities of LLMs to see if they can co-operate (Gong et al., 2023; Piatti et al., 2024) or effectively play against other agents (Jin et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on multi-agent capabilities of LLMs. No clear identifiers for datasets are present.",
      "processing_time": 57.56385540962219,
      "citing_paper_id": "274150091",
      "cited_paper_id": 267413027
    },
    {
      "context_text": "For example, LLMs fail to act robustly in dynamic environments, and they cannot reliably learn from mistakes, reason about space and time, or plan over long time horizons (Xing et al., 2024; Yamada et al., 2023; Kambhampati et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general limitations of LLMs. No verifiable resources are identified.",
      "processing_time": 56.56728672981262,
      "citing_paper_id": "274150091",
      "cited_paper_id": 267617061
    },
    {
      "context_text": "…contrast, BAL-ROG fills an important gap by providing a wide range of games at varying difficulties-including the NetHack Learning Environment (K¨uttler et al., 2020), which takes humans years to master, and where zero-shot LLMs struggle greatly, as also seen in prior work (Jeurissen et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NetHack Learning Environment"
      ],
      "dataset_descriptions": {
        "NetHack Learning Environment": "Used to evaluate zero-shot capabilities of LLMs in complex game environments, highlighting the challenges and limitations of current models in mastering human-level tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions the NetHack Learning Environment, which is a specific environment used for evaluating LLMs in game-playing scenarios. It is not a traditional dataset but a benchmark environment.",
      "processing_time": 65.63394284248352,
      "citing_paper_id": "274150091",
      "cited_paper_id": 268230464
    },
    {
      "context_text": "This could be due to the models’ inability to retrieve relevant computational circuits (Olah et al., 2020), limitations to inference-time budget (Snell et al., 2024), or representational expressivity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their limitations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.194031715393066,
      "citing_paper_id": "274150091",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "Previously, some related works employed games to benchmark LLMs (Liu et al., 2023b; Todd et al., 2024; Wu et al., 2023; Ruoss et al., 2024), highlighting their emphasis on problem-solving, spatial reasoning, and well-defined rules and objectives.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions games used to benchmark LLMs but does not specify any particular datasets. The context focuses on the use of games for benchmarking, which are not considered datasets.",
      "processing_time": 58.346436977386475,
      "citing_paper_id": "274150091",
      "cited_paper_id": 274437383
    },
    {
      "context_text": "For example, Sonnet 3.5’s superior performance can be attributed in part to its training on tasks involving computer usage (Anthropic, 2024), which inherently require integrating visual and textual inputs for action-based reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to tasks involving computer usage, but does not name a dataset.",
      "processing_time": 56.89122247695923,
      "citing_paper_id": "274150091",
      "cited_paper_id": null
    },
    {
      "context_text": "The second variant utilizes a more conventional cross-attention mechanism [52].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (cross-attention mechanism).",
      "processing_time": 55.18204355239868,
      "citing_paper_id": "273025962",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Significant progress has been made in the development of robot simulators such as RLBench [26], AI2-THOR [27] and Isaac Sim [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions robot simulators but does not refer to any specific datasets. The names mentioned are tools or platforms, not datasets.",
      "processing_time": 56.88576102256775,
      "citing_paper_id": "273025962",
      "cited_paper_id": 28328610
    },
    {
      "context_text": "Significant progress has been made in the development of robot simulators such as RLBench [26], AI2-THOR [27] and Isaac Sim [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions robot simulators but does not refer to any specific datasets. The names mentioned are tools or platforms, not datasets.",
      "processing_time": 56.88576102256775,
      "citing_paper_id": "273025962",
      "cited_paper_id": 237277983
    },
    {
      "context_text": "We follow PolarNet [2] to project multi-view RGB-D images into a unified point cloud in world coordinates, then downsample this point cloud to one point per 1cm 3 voxel [48].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for processing 3D data using PolarNet and Open3D, but does not reference any dataset by name.",
      "processing_time": 58.999236822128296,
      "citing_paper_id": "273025962",
      "cited_paper_id": 34198369
    },
    {
      "context_text": "The high dimensionality of manipulation action space makes it challenging to directly use reinforcement learning (RL) in training [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a challenge related to reinforcement learning in robotic manipulation.",
      "processing_time": 55.79371690750122,
      "citing_paper_id": "273025962",
      "cited_paper_id": 49470584
    },
    {
      "context_text": "During inference, for each new instruction, we use SentenceBert [ ?",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (SentenceBert). The citation is used to reference the method, not a dataset.",
      "processing_time": 57.50560450553894,
      "citing_paper_id": "273025962",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "Learning robotic manipulation conditioned on vision and language has received increasing attention [29], [30], [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 56.18057870864868,
      "citing_paper_id": "273025962",
      "cited_paper_id": 225062560
    },
    {
      "context_text": "To address this, more recent benchmarks [19], [20], [21], [22], [23], [25], [24] have introduced generalization evaluations on new compositions of objects and colors, new object shapes, or even long-horizon tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context mentions 'benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional clarity on specific datasets.",
      "processing_time": 57.85906744003296,
      "citing_paper_id": "273025962",
      "cited_paper_id": 225076003
    },
    {
      "context_text": "To address this, more recent benchmarks [19], [20], [21], [22], [23], [25], [24] have introduced generalization evaluations on new compositions of objects and colors, new object shapes, or even long-horizon tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context mentions 'benchmarks' but does not specify any particular dataset names. The cited papers do not provide additional clarity on specific datasets.",
      "processing_time": 57.85906744003296,
      "citing_paper_id": "273025962",
      "cited_paper_id": 261681849
    },
    {
      "context_text": "Existing LLMs [14] and VLMs [15], [39], [40], are able to generalize to unseen scenarios due to the training on massive data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only that existing LLMs and VLMs generalize well due to training on massive data. No specific dataset names are provided.",
      "processing_time": 58.51570963859558,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Existing LLMs [14] and VLMs [15], [39], [40], are able to generalize to unseen scenarios due to the training on massive data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only that existing LLMs and VLMs generalize well due to training on massive data. No specific dataset names are provided.",
      "processing_time": 58.51570963859558,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "Existing LLMs [14] and VLMs [15], [39], [40], are able to generalize to unseen scenarios due to the training on massive data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only that existing LLMs and VLMs generalize well due to training on massive data. No specific dataset names are provided.",
      "processing_time": 58.51570963859558,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259187664
    },
    {
      "context_text": "OWLv2 also generates a semantic embedding for each bounding box, which is aligned with text embeddings from the CLIP text encoder [39].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) for generating text embeddings. The context is about aligning visual and text embeddings, not using a dataset.",
      "processing_time": 58.66869854927063,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In this work, we take advantage of rich spatial information of 3D point cloud for motion planning, while leaving object grounding in 2D to benefit from the generalization strength of pretrained 2D models [15], [39], [40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to 2D models and 3D point cloud data. No verifiable resource names are provided.",
      "processing_time": 57.88635277748108,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In this work, we take advantage of rich spatial information of 3D point cloud for motion planning, while leaving object grounding in 2D to benefit from the generalization strength of pretrained 2D models [15], [39], [40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to 2D models and 3D point cloud data. No verifiable resource names are provided.",
      "processing_time": 57.88635277748108,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "In this work, we take advantage of rich spatial information of 3D point cloud for motion planning, while leaving object grounding in 2D to benefit from the generalization strength of pretrained 2D models [15], [39], [40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to 2D models and 3D point cloud data. No verifiable resource names are provided.",
      "processing_time": 57.88635277748108,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259187664
    },
    {
      "context_text": "Assuming the language instruction L is encoded by a CLIP text encoder [39] and represented as a sequence of word embeddings The first variant employs the adaptive normalization approach [51].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on the use of CLIP for encoding language instructions and adaptive normalization.",
      "processing_time": 58.01823568344116,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Inspired by the remarkable generalization capabilities of foundation models [39], [15], [42], recent work explores how to leverage these models for planning, perception and control in robotics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general capabilities of foundation models. No dataset names are present in the text.",
      "processing_time": 57.19021821022034,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Inspired by the remarkable generalization capabilities of foundation models [39], [15], [42], recent work explores how to leverage these models for planning, perception and control in robotics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general capabilities of foundation models. No dataset names are present in the text.",
      "processing_time": 57.19021821022034,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Inspired by the remarkable generalization capabilities of foundation models [39], [15], [42], recent work explores how to leverage these models for planning, perception and control in robotics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general capabilities of foundation models. No dataset names are present in the text.",
      "processing_time": 57.19021821022034,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "All the baselines use CLIP text encoder [39], while only 3D diffuser actor employs visual representations pretrained on large-scale datasets.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the use of a text encoder and visual representations pretrained on large-scale datasets without naming them.",
      "processing_time": 58.36345648765564,
      "citing_paper_id": "273025962",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We utilize waypoint representation [38], [47], [17], [18] for action sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of waypoint representation in action sequences. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.815080642700195,
      "citing_paper_id": "273025962",
      "cited_paper_id": 235606348
    },
    {
      "context_text": "We utilize waypoint representation [38], [47], [17], [18] for action sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of waypoint representation in action sequences. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.815080642700195,
      "citing_paper_id": "273025962",
      "cited_paper_id": 246634577
    },
    {
      "context_text": "We utilize waypoint representation [38], [47], [17], [18] for action sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of waypoint representation in action sequences. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.815080642700195,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "Existing works [33], [1], [36], [17], [34], [37] mainly use 2D images to predict actions, though recent works have begun exploring 3D representations [38], [18], [2], [4], [35], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 2D images and 3D representations. No verifiable resources are identified.",
      "processing_time": 58.014761447906494,
      "citing_paper_id": "273025962",
      "cited_paper_id": 235606348
    },
    {
      "context_text": "Existing works [33], [1], [36], [17], [34], [37] mainly use 2D images to predict actions, though recent works have begun exploring 3D representations [38], [18], [2], [4], [35], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 2D images and 3D representations. No verifiable resources are identified.",
      "processing_time": 58.014761447906494,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "Existing works [33], [1], [36], [17], [34], [37] mainly use 2D images to predict actions, though recent works have begun exploring 3D representations [38], [18], [2], [4], [35], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 2D images and 3D representations. No verifiable resources are identified.",
      "processing_time": 58.014761447906494,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259308821
    },
    {
      "context_text": "Existing works [33], [1], [36], [17], [34], [37] mainly use 2D images to predict actions, though recent works have begun exploring 3D representations [38], [18], [2], [4], [35], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to 2D images and 3D representations. No verifiable resources are identified.",
      "processing_time": 58.014761447906494,
      "citing_paper_id": "273025962",
      "cited_paper_id": 268856732
    },
    {
      "context_text": "Huang et al. [43] use LLMs to decompose high-level tasks into sub-steps.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for task decomposition. No verifiable resources are identified.",
      "processing_time": 57.33524227142334,
      "citing_paper_id": "273025962",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "One prominent direction focuses on pretraining visual and language representations on large-scale web data for robotics [5], [6], [7], [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'large-scale web data'. No specific, verifiable datasets are named.",
      "processing_time": 57.794766902923584,
      "citing_paper_id": "273025962",
      "cited_paper_id": 247618840
    },
    {
      "context_text": "One prominent direction focuses on pretraining visual and language representations on large-scale web data for robotics [5], [6], [7], [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'large-scale web data'. No specific, verifiable datasets are named.",
      "processing_time": 57.794766902923584,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252718704
    },
    {
      "context_text": "One prominent direction focuses on pretraining visual and language representations on large-scale web data for robotics [5], [6], [7], [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'large-scale web data'. No specific, verifiable datasets are named.",
      "processing_time": 57.794766902923584,
      "citing_paper_id": "273025962",
      "cited_paper_id": 268856732
    },
    {
      "context_text": "Early benchmarks [17], [18] train and test policies on the same task set, overlooking the critical aspect of generalization to unseen scenarios.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general criticism of early benchmarks. No verifiable resources are identified.",
      "processing_time": 57.372700214385986,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "We propose a new classification-based approach for action prediction, in contrast to the regression-based approach [17], [2], [8] or inefficient position classification over the whole 3D workspace [18], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to different approaches for action prediction. No verifiable resources are identified.",
      "processing_time": 57.33529305458069,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "We propose a new classification-based approach for action prediction, in contrast to the regression-based approach [17], [2], [8] or inefficient position classification over the whole 3D workspace [18], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to different approaches for action prediction. No verifiable resources are identified.",
      "processing_time": 57.33529305458069,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259308821
    },
    {
      "context_text": "We propose a new classification-based approach for action prediction, in contrast to the regression-based approach [17], [2], [8] or inefficient position classification over the whole 3D workspace [18], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to different approaches for action prediction. No verifiable resources are identified.",
      "processing_time": 57.33529305458069,
      "citing_paper_id": "273025962",
      "cited_paper_id": 268856732
    },
    {
      "context_text": "Therefore, most works rely on imitation learning (IL) [33], [1], [17], [18], [34], [2], [8], [4], [35] using scripted trajectories [26] or tele-operation data [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'scripted trajectories' and 'tele-operation data', which could be considered datasets, but they lack specific identifiers. No other specific datasets are mentioned.",
      "processing_time": 58.614137172698975,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "Therefore, most works rely on imitation learning (IL) [33], [1], [17], [18], [34], [2], [8], [4], [35] using scripted trajectories [26] or tele-operation data [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'scripted trajectories' and 'tele-operation data', which could be considered datasets, but they lack specific identifiers. No other specific datasets are mentioned.",
      "processing_time": 58.614137172698975,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259308821
    },
    {
      "context_text": "Therefore, most works rely on imitation learning (IL) [33], [1], [17], [18], [34], [2], [8], [4], [35] using scripted trajectories [26] or tele-operation data [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'scripted trajectories' and 'tele-operation data', which could be considered datasets, but they lack specific identifiers. No other specific datasets are mentioned.",
      "processing_time": 58.614137172698975,
      "citing_paper_id": "273025962",
      "cited_paper_id": 263626099
    },
    {
      "context_text": "Therefore, most works rely on imitation learning (IL) [33], [1], [17], [18], [34], [2], [8], [4], [35] using scripted trajectories [26] or tele-operation data [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'scripted trajectories' and 'tele-operation data', which could be considered datasets, but they lack specific identifiers. No other specific datasets are mentioned.",
      "processing_time": 58.614137172698975,
      "citing_paper_id": "273025962",
      "cited_paper_id": 268856732
    },
    {
      "context_text": "We run four state-of-the-art methods on Gem-Bench, including two 2D image based models 2 (Hive-former [17] and RVT-2 [37]), and two 3D-based models (Po-larnet [2] and 3D diffuser actor [35]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Gem-Bench but does not specify it as a dataset. It is likely a benchmark or leaderboard, which is excluded according to the instructions.",
      "processing_time": 58.152583599090576,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252200013
    },
    {
      "context_text": "For example, CaP [16] uses LLMs to generate codes that executes primitive actions, and VoxPoser [3] composes value maps with foundation models for action execution with classic motion planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CaP' and 'VoxPoser' but does not refer to them as datasets. They are described as systems or methods using LLMs for specific tasks.",
      "processing_time": 59.261858224868774,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "For example, CaP [16] uses LLMs to generate codes that executes primitive actions, and VoxPoser [3] composes value maps with foundation models for action execution with classic motion planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CaP' and 'VoxPoser' but does not refer to them as datasets. They are described as systems or methods using LLMs for specific tasks.",
      "processing_time": 59.261858224868774,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259837330
    },
    {
      "context_text": "CaP [16] instructs LLMs to write code which call tools for perception and control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for instructing LLMs to write code for embodied control.",
      "processing_time": 57.35311436653137,
      "citing_paper_id": "273025962",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Learning-based robotic policies often struggle to generalize to new scenarios [41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in robot learning.",
      "processing_time": 56.1697199344635,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257079001
    },
    {
      "context_text": "Another approach [1], [9], [10], [11], [12] pretrains entire robotic policies by combining robot data with web data, but these methods remain constrained by the limited size and diversity of available robot datasets [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'robot datasets' but does not specify any particular dataset names. The term 'robot datasets' is too generic and lacks specific identifiers.",
      "processing_time": 57.978057861328125,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257364842
    },
    {
      "context_text": "Another approach [1], [9], [10], [11], [12] pretrains entire robotic policies by combining robot data with web data, but these methods remain constrained by the limited size and diversity of available robot datasets [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'robot datasets' but does not specify any particular dataset names. The term 'robot datasets' is too generic and lacks specific identifiers.",
      "processing_time": 57.978057861328125,
      "citing_paper_id": "273025962",
      "cited_paper_id": 263626099
    },
    {
      "context_text": "Another approach [1], [9], [10], [11], [12] pretrains entire robotic policies by combining robot data with web data, but these methods remain constrained by the limited size and diversity of available robot datasets [13].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'robot datasets' but does not specify any particular dataset names. The term 'robot datasets' is too generic and lacks specific identifiers.",
      "processing_time": 57.978057861328125,
      "citing_paper_id": "273025962",
      "cited_paper_id": 270440391
    },
    {
      "context_text": "Next, we use the Segment Anything Model (SAM) [15] to segment the object within each bounding box.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (Segment Anything Model) but does not refer to any specific dataset. The citation is used to describe a method for object segmentation.",
      "processing_time": 57.90137577056885,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "More recently, foundation models such as large language models (LLMs) [14] and vision-language models (VLMs) [15] have been employed to enhance generalization of robot policies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their application in enhancing robot policies.",
      "processing_time": 56.23971915245056,
      "citing_paper_id": "273025962",
      "cited_paper_id": 257952310
    },
    {
      "context_text": "First, we employ the open-vocabulary object detector OWLv2 [40] to detect bounding boxes with high objectiveness scores for each RGB image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using OWLv2, which is a method or model, not a dataset. No specific dataset is mentioned or used in the described research context.",
      "processing_time": 58.19796371459961,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259187664
    },
    {
      "context_text": "45.3 6.9 - PolarNet [2] 46.4 6.4 8.9 PerAct [18] 49.4 6.2 128.0 RVT [34] 62.9 4.4 8.0 Act3D [4] 65.0 4.3 40.0 RVT2 [37] 81.4 2.4 6.6 3D diffuser actor [35] 81.3 Baselines.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 57.23821520805359,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259308821
    },
    {
      "context_text": "Vision-language robotic manipulation aims to train policies performing complex tasks based on visual inputs and language instructions [1], [2], [3], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals of vision-language robotic manipulation.",
      "processing_time": 56.557533740997314,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259308821
    },
    {
      "context_text": "Vision-language robotic manipulation aims to train policies performing complex tasks based on visual inputs and language instructions [1], [2], [3], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals of vision-language robotic manipulation.",
      "processing_time": 56.557533740997314,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259837330
    },
    {
      "context_text": "To address this, VoxPoser [3] use LLMs to construct 3D voxel maps of affordance, constraint, rotation and velocity, which are fed into traditional motion planing algorithms to plan a trajectory.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs and 3D voxel maps. No clear, verifiable dataset names are present.",
      "processing_time": 58.43556547164917,
      "citing_paper_id": "273025962",
      "cited_paper_id": 259837330
    },
    {
      "context_text": "ViLa [45] replaces LLMs with a multi-modal LLM GPT-4V [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about replacing LLMs with a multi-modal LLM, which does not involve the use of a dataset.",
      "processing_time": 59.46192479133606,
      "citing_paper_id": "273025962",
      "cited_paper_id": 263218031
    },
    {
      "context_text": "…learning experiments within the Android ecosystem, successfully tested various RL-based agents like DDPG (Zhang and Van Huynh, 2023), D4PG (Barth-Maron et al., 2018), MPO (Abdolmaleki et al., 2018), DQN (Mnih et al., 2015), IMPALA (Espeholt et al., 2018) and R2D2 (Kapturowski et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only reinforcement learning algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.30638885498047,
      "citing_paper_id": "270869659",
      "cited_paper_id": 3517962
    },
    {
      "context_text": "This open-source platform tailored for reinforcement learning experiments within the Android ecosystem, successfully tested various RL-based agents like DDPG (Zhang and Van Huynh, 2023), D4PG (Barth-Maron et al., 2018), MPO (Abdolmaleki et al., 2018), DQN (Mnih et al., 2015), IMPALA (Espeholt et al., 2018) and R2D2 (Kapturowski et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reinforcement learning algorithms and methods. The context is focused on the testing of RL-based agents within an Android ecosystem.",
      "processing_time": 58.835508823394775,
      "citing_paper_id": "270869659",
      "cited_paper_id": 3517962
    },
    {
      "context_text": "Interacting with mobile devices using natural language is a long-standing pursuit in human-computer interaction (Bolt, 1980; Karat et al., 2002; Følstad and Brandtzæg, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior work on conversational interfaces.",
      "processing_time": 57.09996247291565,
      "citing_paper_id": "270869659",
      "cited_paper_id": 59788741
    },
    {
      "context_text": "ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on comparing model performance against various language models.",
      "processing_time": 57.86420655250549,
      "citing_paper_id": "270869659",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on comparing model performance against various language models.",
      "processing_time": 57.86420655250549,
      "citing_paper_id": "270869659",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on comparing model performance against various language models.",
      "processing_time": 57.86420655250549,
      "citing_paper_id": "270869659",
      "cited_paper_id": 248496292
    },
    {
      "context_text": "ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on comparing model performance against various language models.",
      "processing_time": 57.86420655250549,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa (Liu et al., 2019), GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2023), OPT (Zhang et al., 2022), ChatGPT (Bubeck et al., 2023) and GPT-4 (OpenAI, 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on comparing model performance against various language models.",
      "processing_time": 57.86420655250549,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "We evaluate four popular LLMs on the proposed Mobile-Bench task set: GPT-3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions evaluating LLMs on a task set called Mobile-Bench, which appears to be a specific benchmark or dataset. However, since the name 'Mobile-Bench' does not follow the required naming conventions (no multi-word proper noun, uppercase acronym, or hyphenated name with digits), it is excluded.",
      "processing_time": 64.27737498283386,
      "citing_paper_id": "270869659",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Owing to the limit of budget, only GPT-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (GPT-3). No verifiable resources are identified.",
      "processing_time": 57.694371461868286,
      "citing_paper_id": "270869659",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "As observed in Table 4, it can be observed that GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a model (GPT-3). No verifiable resources are identified.",
      "processing_time": 58.088219165802,
      "citing_paper_id": "270869659",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "The widely used RICO dataset (Deka et al., 2017) is commonly employed for this purpose, with Screen2Vec (Li et al., 2021) utilizing it to evaluate agent performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RICO"
      ],
      "dataset_descriptions": {
        "RICO": "Used to evaluate agent performance in the context of GUI screens, specifically focusing on the semantic embedding of GUI components and screens."
      },
      "confidence_score": 1.0,
      "reasoning": "The RICO dataset is mentioned as a specific, verifiable resource used for evaluating agent performance in the context of GUI screens.",
      "processing_time": 63.94448947906494,
      "citing_paper_id": "270869659",
      "cited_paper_id": 231718747
    },
    {
      "context_text": "…(Wang et al., 2023) in UI interface representation, LLM agent’s understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only platforms and environments. The context focuses on the use of these platforms for defining tasks within games or search engines.",
      "processing_time": 59.28724670410156,
      "citing_paper_id": "270869659",
      "cited_paper_id": 235212182
    },
    {
      "context_text": "Due to Google’s breakthrough (Wang et al., 2023) in UI interface representation, LLM agent’s understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only platforms and environments. The citation is more about the development of UI platforms for LLM agents.",
      "processing_time": 58.79981207847595,
      "citing_paper_id": "270869659",
      "cited_paper_id": 235212182
    },
    {
      "context_text": "Due to Google’s breakthrough (Wang et al., 2023) in UI interface representation, LLM agent’s understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only platforms and environments. The citation is more about the development of UI platforms for LLM agents.",
      "processing_time": 58.79981207847595,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Prior to the emphasis on LLM-based agents, research efforts were directed towards RL-based agents, exemplified by the Android-Env platform (Toyama et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Android-Env as a platform for RL-based agents, but does not refer to it as a dataset. It is used to illustrate a shift in research focus.",
      "processing_time": 59.80006504058838,
      "citing_paper_id": "270869659",
      "cited_paper_id": 235212182
    },
    {
      "context_text": "…text fails to capture Platform&BenchMark InfoUI API&UI Real APP Real Query Multi-APP World of Bits (Shi et al., 2017) ✓ ✗ ✗ ✗ ✗ WebShop (Yao et al., 2022) AndroidEnv (Toyama et al., 2021) MobileEnv (Zhang et al., 2023) Table 1: Comparison of Mobile-Bench with existing LLM-based agent platforms.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'World of Bits' and 'WebShop', which are platforms or benchmarks, but not specific datasets. 'AndroidEnv' and 'MobileEnv' are platforms, not datasets.",
      "processing_time": 60.15619921684265,
      "citing_paper_id": "270869659",
      "cited_paper_id": 235212182
    },
    {
      "context_text": "With the remarkable advancements in large language models (LLM) (Bai et al., 2022; Chowdhery et al., 2022; Du et al., 2021; Touvron et al., 2023; Ouyang et al., 2022), LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to advancements in large language models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.162312507629395,
      "citing_paper_id": "270869659",
      "cited_paper_id": 254823489
    },
    {
      "context_text": "5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023 Step + + ; 7: Html ← Appium ( Emulator ) ; 8: API ← LLM ( Task, L API , AH, Tho, Plan, Html ) 9: if API then 10: Action ( API ) 11: AH.append ( API ) 12: else 13: UI ← LLM ( Task, AH, Tho, Plan, Html ) 14: Action ( UI ) 15: AH.append ( UI )…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.67576026916504,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "Provide these API as an example to GPT-4 for query generation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. It only mentions 'API' which is not a dataset.",
      "processing_time": 58.51952028274536,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "Provide these API as an example to GPT-4 for query generation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. It only mentions 'API' which is not a dataset.",
      "processing_time": 58.51952028274536,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "For MAMT, we randomly sample 6 applications from the entire application collection, and then provide some examples of real multi-APP data to prompt GPT-4 to select 2-4 applications to generate tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for sampling applications and generating tasks using GPT-4, which is a model, not a dataset.",
      "processing_time": 59.98489022254944,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "For MAMT, we randomly sample 6 applications from the entire application collection, and then provide some examples of real multi-APP data to prompt GPT-4 to select 2-4 applications to generate tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for sampling applications and generating tasks using GPT-4, which is a model, not a dataset.",
      "processing_time": 59.98489022254944,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "5 outperforms GPT-4 in PassRate on SAMT(64%>63%), and it requires fewer steps to complete the task(12.06 13.94).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics comparing two models. No verifiable resources are identified.",
      "processing_time": 58.20358085632324,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "5 outperforms GPT-4 in PassRate on SAMT(64%>63%), and it requires fewer steps to complete the task(12.06 13.94).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics comparing two models. No verifiable resources are identified.",
      "processing_time": 58.20358085632324,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "5 utilizes an interface with a context length of 16K. GPT-4 uses a standard interface, which necessitated compression and trimming of actions history.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the interfaces and capabilities of GPT-4 and another model.",
      "processing_time": 58.996671199798584,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "5 utilizes an interface with a context length of 16K. GPT-4 uses a standard interface, which necessitated compression and trimming of actions history.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the interfaces and capabilities of GPT-4 and another model.",
      "processing_time": 58.996671199798584,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Subsequently, GPT-4 evaluates the task completion status.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of task completion status by GPT-4.",
      "processing_time": 58.18806457519531,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "Subsequently, GPT-4 evaluates the task completion status.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of task completion status by GPT-4.",
      "processing_time": 58.18806457519531,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023 Step + + ; 7: Html ← Appium ( Emulator ) ; 8: API ← LLM ( Task, L API , AH, Tho, Plan, Html ) 9: if API then 10: Action ( API ) 11: AH.append ( API ) 12: else 13: UI ← LLM ( Task, AH, Tho, Plan, Html ) 14: Action ( UI ) 15: AH.append ( UI ) 16: end if 17: Html ← Appium ( Emulator ) ; 18: Tho ← LLM ( Task, AH, Plan, Html ) 19: Finish ← LLM ( Task, AH, Tho, Html ) 20: end while online APIs of OpenAI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific, verifiable datasets. It describes a process involving LLMs and APIs but does not reference any named datasets.",
      "processing_time": 59.97281312942505,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023 Step + + ; 7: Html ← Appium ( Emulator ) ; 8: API ← LLM ( Task, L API , AH, Tho, Plan, Html ) 9: if API then 10: Action ( API ) 11: AH.append ( API ) 12: else 13: UI ← LLM ( Task, AH, Tho, Plan, Html ) 14: Action ( UI ) 15: AH.append ( UI ) 16: end if 17: Html ← Appium ( Emulator ) ; 18: Tho ← LLM ( Task, AH, Plan, Html ) 19: Finish ← LLM ( Task, AH, Tho, Html ) 20: end while online APIs of OpenAI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific, verifiable datasets. It describes a process involving LLMs and APIs but does not reference any named datasets.",
      "processing_time": 59.97281312942505,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Since there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama (Qin et al., 2023) method, we use GPT-4 to construct SAMT and MAMT data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using GPT-4 to construct SAMT and MAMT data, but does not specify a dataset. 'Toollama' is a method, not a dataset.",
      "processing_time": 60.52849793434143,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "Since there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama (Qin et al., 2023) method, we use GPT-4 to construct SAMT and MAMT data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using GPT-4 to construct SAMT and MAMT data, but does not specify a dataset. 'Toollama' is a method, not a dataset.",
      "processing_time": 60.52849793434143,
      "citing_paper_id": "270869659",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Since there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama (Qin et al., 2023) method, we use GPT-4 to construct SAMT and MAMT data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using GPT-4 to construct SAMT and MAMT data, but does not specify a dataset. 'Toollama' is a method, not a dataset.",
      "processing_time": 60.52849793434143,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "(Bolotova-Baranova et al., 2023) The initial test data originates from software automation tests, but some complex data points are generated by GPT-4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'software automation tests' as the source of initial test data, which does not qualify as a specific, verifiable dataset. 'GPT-4' is mentioned as a tool for generating complex data points, but it is not a dataset.",
      "processing_time": 63.04079461097717,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "(Bolotova-Baranova et al., 2023) The initial test data originates from software automation tests, but some complex data points are generated by GPT-4.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'software automation tests' as the source of initial test data, which does not qualify as a specific, verifiable dataset. 'GPT-4' is mentioned as a tool for generating complex data points, but it is not a dataset.",
      "processing_time": 63.04079461097717,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Therefore, in Table 5, we evaluate and analyze the impact of introducing APIs on task completion based on GPT-4.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to evaluating the impact of APIs on task completion using GPT-4, which is a model, not a dataset.",
      "processing_time": 61.08663892745972,
      "citing_paper_id": "270869659",
      "cited_paper_id": 257687695
    },
    {
      "context_text": "Therefore, in Table 5, we evaluate and analyze the impact of introducing APIs on task completion based on GPT-4.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to evaluating the impact of APIs on task completion using GPT-4, which is a model, not a dataset.",
      "processing_time": 61.08663892745972,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "DroidTask (Wen et al., 2023a) and various un-named datasets (Liu et al., 2023b; Wen et al., 2023b) covering various mobile applications have also been established.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DroidTask"
      ],
      "dataset_descriptions": {
        "DroidTask": "Used to evaluate the planning capabilities of LLMs in automating Android UI interactions, focusing on task completion and efficiency."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'DroidTask' and 'various un-named datasets', but only 'DroidTask' is a specific, verifiable dataset. The other datasets are not named and thus do not meet the criteria.",
      "processing_time": 69.20492196083069,
      "citing_paper_id": "270869659",
      "cited_paper_id": 258170084
    },
    {
      "context_text": "(Qin et al., 2023) We assess an agent’s human-computer interaction capabilities by calculating the proportion of queries successfully completed within the specified step limits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for assessing an agent's capabilities.",
      "processing_time": 58.31705355644226,
      "citing_paper_id": "270869659",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Since observation-thought-action is already a standardized process in the agent direction(Qin et al., 2023), and verified by experimental results, planning and thought before action are essential.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a standardized process in agent direction. No verifiable resources are identified.",
      "processing_time": 59.37059664726257,
      "citing_paper_id": "270869659",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "In fact, voice assistants on mobile phones can meet most of the users’ daily needs, yet they do not interact directly with UI interfaces but operate by invoking the APIs (Qin et al., 2023) behind applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the interaction capabilities of voice assistants with APIs.",
      "processing_time": 58.909586906433105,
      "citing_paper_id": "270869659",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Researchers developed Tool-lama (Qin et al., 2023) to evaluate the capabilities to use tools and API calls.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a tool called Tool-lama. The cited paper title suggests the tool is used to evaluate LLMs' capabilities with APIs, but no datasets are explicitly mentioned.",
      "processing_time": 62.2422456741333,
      "citing_paper_id": "270869659",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…2022; Chowdhery et al., 2022; Du et al., 2021; Touvron et al., 2023; Ouyang et al., 2022), LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind their proficiency with web pages on PCs (Yao et al., 2022; Sun et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works discussing LLM capabilities.",
      "processing_time": 58.45480418205261,
      "citing_paper_id": "270869659",
      "cited_paper_id": 264590280
    },
    {
      "context_text": "PPTC Benchmark (Guo et al., 2023) proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PPTC Benchmark"
      ],
      "dataset_descriptions": {
        "PPTC Benchmark": "Used to evaluate the performance of LLM-based agents on PowerPoint tasks, focusing on task completion capabilities and assessing the planning and execution skills of the models."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions the 'PPTC Benchmark' which is a specific benchmark designed to evaluate LLM-based agents on PowerPoint tasks. However, it is primarily a benchmark or challenge rather than a traditional dataset.",
      "processing_time": 71.02816820144653,
      "citing_paper_id": "270869659",
      "cited_paper_id": 265019477
    },
    {
      "context_text": "PPTC Benchmark (Guo et al., 2023) devised 279 multi-round dialogue tasks for PPT file operations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PPTC Benchmark"
      ],
      "dataset_descriptions": {
        "PPTC Benchmark": "Used to evaluate large language models on 279 multi-round dialogue tasks for PPT file operations, focusing on the models' ability to complete specific PowerPoint tasks."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the PPTC Benchmark, which is a specific benchmark with a clear name and purpose. However, it is primarily a suite of tasks rather than a downloadable dataset. Since the task suite is used to evaluate LLMs, it is included with a lower confidence.",
      "processing_time": 73.31288409233093,
      "citing_paper_id": "270869659",
      "cited_paper_id": 265019477
    },
    {
      "context_text": "PPTC Benchmark (Guo et al., 2023) uses Turn-based and Session-based accuracy.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the PPTC Benchmark, which is a benchmark rather than a dataset. It does not refer to a specific, downloadable dataset.",
      "processing_time": 60.400227785110474,
      "citing_paper_id": "270869659",
      "cited_paper_id": 265019477
    },
    {
      "context_text": "Although Fan (Fan et al., 2024) found that the cost of inference can be reduced by using only the necessary layers for inference, it is still expensive to calculate the win rate.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about reducing inference costs in LLMs.",
      "processing_time": 59.01600623130798,
      "citing_paper_id": "270869659",
      "cited_paper_id": 268248810
    },
    {
      "context_text": "More significant research has focused on LLM-based agents (Liu et al., 2024; Sun et al., 2024b,a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.158998012542725,
      "citing_paper_id": "270869659",
      "cited_paper_id": 268297061
    },
    {
      "context_text": "AgentBench (Liu et al., 2023a) presents a standardized Agent task evaluation architecture with strong decoupling and scalability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AgentBench, which is described as an evaluation architecture rather than a dataset. It does not fit the criteria for a dataset as defined in the instructions.",
      "processing_time": 61.10289430618286,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "To assess agents’ proficiency in understanding user interfaces, a diverse dataset covering various tasks is crucial (Liu et al., 2023a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions the need for a diverse dataset but does not specify a named dataset. The reference is too generic and lacks a specific, identifiable dataset name.",
      "processing_time": 60.50323724746704,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, Screen2Words used a sampling method to sample screens from the RICO-SCA (Li et al., 2020) dataset and hired professional annotators to generate English summaries for these screens (Wang et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RICO-SCA"
      ],
      "dataset_descriptions": {
        "RICO-SCA": "Used to sample screens for generating English summaries, focusing on the visual elements and their textual representations in user interfaces."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RICO-SCA dataset, which is a specific, verifiable resource used for sampling screens. The dataset is used to generate English summaries for the sampled screens.",
      "processing_time": 68.68143486976624,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Following the method described by Wang (Wang et al., 2023), we convert XML to HTML, as the training data for LLMs is predominantly sourced from the Internet, which includes numerous HTML files.",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'training data for LLMs' but does not specify a named dataset. It only refers to the general nature of the data source (Internet, HTML files).",
      "processing_time": 61.08209753036499,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Due to Google’s breakthrough (Wang et al., 2023) in UI interface representation, LLM agent’s understanding of UI pages becomes easier, leading to the creation of UI platforms such as Android-Env (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only platforms and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 59.79062867164612,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Toolformer (Schick et al., 2023) and HuggingGPT (Shen et al., 2023) evaluate LLM’s capability to master tools.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods (Toolformer, HuggingGPT).",
      "processing_time": 59.53623867034912,
      "citing_paper_id": "270869659",
      "cited_paper_id": null
    },
    {
      "context_text": "Early methods including heuristic attacks [3, 38] and gradient-based attacks [5, 27] have demonstrated a high rate of success in attacking white-box recommendation models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and attacks. There are no clear identifiers for datasets in the text.",
      "processing_time": 60.016626834869385,
      "citing_paper_id": "271961798",
      "cited_paper_id": 14588414
    },
    {
      "context_text": "Allexperimentsareconductedonthree commonly-used datasets in RecSys: Movielens-1M ( ML1M ) [19], Taobao [51], and LastFM [43] datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Movielens-1M",
        "Taobao",
        "LastFM"
      ],
      "dataset_descriptions": {
        "Movielens-1M": "Used to evaluate recommendation algorithms, focusing on user-movie interactions and rating predictions.",
        "Taobao": "Applied to study user behavior in e-commerce, analyzing purchase history and browsing patterns.",
        "LastFM": "Utilized to assess music recommendation systems, examining user listening habits and preferences."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used in recommender systems experiments. These datasets are clearly identified and are relevant to the topic of planning capabilities in LLMs, as they involve user interaction and decision-making processes.",
      "processing_time": 78.79723978042603,
      "citing_paper_id": "271961798",
      "cited_paper_id": 25648541
    },
    {
      "context_text": "The recommendation model employed by APRec is SASRec [24], which is not a large language model and lacks the ability to comprehend textual language in LLM-based recommendations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a recommendation model (SASRec). The context is about the capabilities of the model, not the use of a dataset.",
      "processing_time": 61.84401559829712,
      "citing_paper_id": "271961798",
      "cited_paper_id": 52127932
    },
    {
      "context_text": "Recommender Systems ( RecSys ) play a vital role in capturing users’ interests and preferences across various fields [11], such as e-commerce (e.g., Amazon, Taobao), social media (e.g., Twitter, Facebook), etc. Traditional RecSys typically rely on users’ historical interactions to analyze user…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of recommender systems. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 61.454702615737915,
      "citing_paper_id": "271961798",
      "cited_paper_id": 67769538
    },
    {
      "context_text": "It proposes several item indexing strategies, introduces the whole-word embedding to represent items, and fine-tunes the T5 [31] to improve the recommendation performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (T5) and a general approach to improving recommendation performance.",
      "processing_time": 60.161407709121704,
      "citing_paper_id": "271961798",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "For P5 , we set 𝑘 = 10 and 𝑛 = 10 as defaults, and for TALLRec , we set 𝑘 = 6 and 𝑛 = 12.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters for P5 and TALLRec. No verifiable resources are identified.",
      "processing_time": 60.46270942687988,
      "citing_paper_id": "271961798",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "For instance, P5 [18] leverages LLM’s (i.e. T5 [31]) capabilities to significantly enhance recommendation performance by understanding nuanced user preferences and item descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 60.47165107727051,
      "citing_paper_id": "271961798",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "For instance, P5 [18] leverages LLM’s (i.e. T5 [31]) capabilities to significantly enhance recommendation performance by understanding nuanced user preferences and item descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 60.47165107727051,
      "citing_paper_id": "271961798",
      "cited_paper_id": 247749019
    },
    {
      "context_text": "3) Regarding methods such as GA and BAE, which utilize the genetic algorithm and BERT for perturbation generation, they are faster than the proposed method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing the speed of different methods.",
      "processing_time": 60.468334674835205,
      "citing_paper_id": "271961798",
      "cited_paper_id": 214802269
    },
    {
      "context_text": "• BAE [17] masks the crucial words within the input prompt and exploits the language model, i.e., BERT [25], to predict the contextually appropriate perturbations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BAE) and a model (BERT).",
      "processing_time": 60.15036988258362,
      "citing_paper_id": "271961798",
      "cited_paper_id": 214802269
    },
    {
      "context_text": "As the impact of each token within the prompt can vary significantly, positioning the insertion tokens is crucial for conducting effective attacks [16, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of token positioning in adversarial attacks.",
      "processing_time": 59.44196057319641,
      "citing_paper_id": "271961798",
      "cited_paper_id": 214802269
    },
    {
      "context_text": "Bert [25] is used to generate 50 candidates, and BAE selects the perturbation that is most effective in undermining the recommendation performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and BAE, but neither is a dataset. BERT is a model, and BAE is a method for generating adversarial examples. No specific, verifiable datasets are mentioned.",
      "processing_time": 63.991599321365356,
      "citing_paper_id": "271961798",
      "cited_paper_id": 214802269
    },
    {
      "context_text": "For example, most existing solutions, such as KGAttack [4], PoisonRec [33], and CopyAttack [9], develop RL-based agents to obtain malicious user profiles (i.e., a series of items) and inject them into the victim RecSys for manipulating system’s decision.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and frameworks. The context focuses on describing existing solutions and their approaches, which do not involve the use of datasets.",
      "processing_time": 62.55563831329346,
      "citing_paper_id": "271961798",
      "cited_paper_id": 218908860
    },
    {
      "context_text": "PoisonRec is the first black-box attack framework, which leverages the reinforcement learning architecture to automatically learn effective attack strategies [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called PoisonRec. The context focuses on the method's capabilities and novelty.",
      "processing_time": 61.29393482208252,
      "citing_paper_id": "271961798",
      "cited_paper_id": 218908860
    },
    {
      "context_text": "H@ 𝑟 and N@ 𝑟 are Top-𝑟 Hit Ratio and Normalized Discounted Cumulative Gain [4, 18], which are two widely-used metrics for evaluating the performance of LLM-empowered RecSys.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (H@r and N@r) but does not reference any specific datasets. The context is focused on the metrics used to evaluate LLM-empowered recommendation systems.",
      "processing_time": 63.47443866729736,
      "citing_paper_id": "271961798",
      "cited_paper_id": 247749019
    },
    {
      "context_text": "Recently, due to the powerful language understanding and in-context learning capabilities, Large Language Models (LLMs) have provided great potential to revo-lutionize RecSys [2, 18, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general capabilities of LLMs in revolutionizing recommendation systems.",
      "processing_time": 60.123897552490234,
      "citing_paper_id": "271961798",
      "cited_paper_id": 247749019
    },
    {
      "context_text": "P5 [18] and TALLRec [2] are exploited to investigate the safety vulnerability of LLM-empowered recommender systems: • P5 first converts all data, including user-item interactions, user descriptions, etc., to natural language sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'P5' and 'TALLRec', but neither are datasets. P5 is described as a method for converting data to natural language sequences, which is not a dataset.",
      "processing_time": 63.48512029647827,
      "citing_paper_id": "271961798",
      "cited_paper_id": 247749019
    },
    {
      "context_text": "(cid:154) H@ 𝑟 and (cid:154) N@ 𝑟 are the Top-𝑟 Hit Ratio and Normalized Discounted Cumulative Gain when the victim model is under attack.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No dataset names are present in the citation span.",
      "processing_time": 60.11653280258179,
      "citing_paper_id": "271961798",
      "cited_paper_id": 247749019
    },
    {
      "context_text": "More recently, Large Language Models (LLMs) have achieved great success in various fields, such as psychology [1], drug discovery [28], and health [46], demonstrating their remarkable potential in approximating human-level intelligence.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 61.2762348651886,
      "citing_paper_id": "271961798",
      "cited_paper_id": 251719353
    },
    {
      "context_text": "Due to the space limitation, some studies about the LLM-empowered RecSys and vulnerabilities of LLM are reviewed in Appendix C. Generally, adversarial attacks for recommender systems are broadly divided into two categories [14]: 1) Evasion Attack happens during the inference phase.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general categorization of adversarial attacks in recommender systems.",
      "processing_time": 60.35196113586426,
      "citing_paper_id": "271961798",
      "cited_paper_id": 252407646
    },
    {
      "context_text": "Please generate a few letters or words that can change your prediction after inserting it after inserting it at the end of the input sentence. harmful content [23, 37], which is time-consuming and ineffectively with the development of the LLMs. 2) Optimization-based Methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and issues related to LLMs. No verifiable resources are identified.",
      "processing_time": 61.283891677856445,
      "citing_paper_id": "271961798",
      "cited_paper_id": 256827239
    },
    {
      "context_text": "Please generate a few letters or words that can change your prediction after inserting it after inserting it at the end of the input sentence. harmful content [23, 37], which is time-consuming and ineffectively with the development of the LLMs. 2) Optimization-based Methods.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and issues related to LLMs. No verifiable resources are identified.",
      "processing_time": 61.283891677856445,
      "citing_paper_id": "271961798",
      "cited_paper_id": 259342528
    },
    {
      "context_text": "Although DL-based methods effectively model the representations of users and items, they struggle with encoding textual information (e.g., item titles, user reviews) for reasoning on user’s prediction [29, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general challenge in deep learning methods for recommender systems. No verifiable resources are identified.",
      "processing_time": 61.72862410545349,
      "citing_paper_id": "271961798",
      "cited_paper_id": 259129651
    },
    {
      "context_text": "Although DL-based methods effectively model the representations of users and items, they struggle with encoding textual information (e.g., item titles, user reviews) for reasoning on user’s prediction [29, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general challenge in deep learning methods for recommender systems. No verifiable resources are identified.",
      "processing_time": 61.72862410545349,
      "citing_paper_id": "271961798",
      "cited_paper_id": 259342486
    },
    {
      "context_text": "This impressive capability is attributed to the training on vast textual corpora (i.e., open-world knowledge) with a huge amount of model parameters [49, 50].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only refers to vast textual corpora in a general sense.",
      "processing_time": 59.686074018478394,
      "citing_paper_id": "271961798",
      "cited_paper_id": 259342486
    },
    {
      "context_text": "These methods exploit diverse optimization strategies, such as genetic algorithm [26], gradient-based search [52, 53], reinforcement learning [45], to find the optimal perturbation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization strategies and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 61.7528920173645,
      "citing_paper_id": "271961798",
      "cited_paper_id": 260202961
    },
    {
      "context_text": "These methods exploit diverse optimization strategies, such as genetic algorithm [26], gradient-based search [52, 53], reinforcement learning [45], to find the optimal perturbation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization strategies and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 61.7528920173645,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, Zou et al. [53] create the desired adversarial postfix by generating a candidate set according to the gradient and replacing the word from a candidate randomly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for creating adversarial postfixes.",
      "processing_time": 59.866129875183105,
      "citing_paper_id": "271961798",
      "cited_paper_id": 260202961
    },
    {
      "context_text": "The semantic similarity is computed by introducing an additional embedding model bge-large-en [41].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only an embedding model. The context is about computing semantic similarity using an embedding model, which is not a dataset.",
      "processing_time": 62.08459186553955,
      "citing_paper_id": "271961798",
      "cited_paper_id": 261823330
    },
    {
      "context_text": "H@5 ↓ H@10 ↓ N@5 ↓ N@10 ↓ ASR-H@5 ↑ ASR-H@10 ↑ ASR-N@5 ↑ ASR-N@10 ↑ We use the bge-large-en model [41] to map the adversarial and benign prompt to a 512-dimension vector.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model. The context focuses on using the bge-large-en model for mapping prompts to vectors.",
      "processing_time": 61.74390625953674,
      "citing_paper_id": "271961798",
      "cited_paper_id": 261823330
    },
    {
      "context_text": "Poor initialization can lead the agent to get stuck in local optimal when learning the attack policy [6], bringing difficulties in effectively attacking the target sys-tem.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with initialization in neural networks.",
      "processing_time": 59.84718203544617,
      "citing_paper_id": "271961798",
      "cited_paper_id": 263787277
    },
    {
      "context_text": "• LLMBA [44] directly utilizes large language models to generate adversarial perturbations and insert them to the end of the benign input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of large language models for generating adversarial perturbations.",
      "processing_time": 60.81136107444763,
      "citing_paper_id": "271961798",
      "cited_paper_id": 264406064
    },
    {
      "context_text": "In other words, the attackers can devise adversarial perturbations by solely querying the target system and observing the resulting output probabilities, similar to the soft-label black-box setting in [22, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for adversarial attacks.",
      "processing_time": 59.64708638191223,
      "citing_paper_id": "271961798",
      "cited_paper_id": 267412137
    },
    {
      "context_text": "LLM is employed to generate adversarial samples automatically, which is more efficient and diverse [7, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs to generate adversarial samples. No verifiable resources are identified.",
      "processing_time": 61.8513548374176,
      "citing_paper_id": "271961798",
      "cited_paper_id": 270441137
    },
    {
      "context_text": "LLM is employed to generate adversarial samples automatically, which is more efficient and diverse [7, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs to generate adversarial samples. No verifiable resources are identified.",
      "processing_time": 61.8513548374176,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "Deng et al. [7] propose to exploit the time-based characteristics intrinsic to deconstruct the defense mechanism of LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to deconstruct the defense mechanism of LLMs.",
      "processing_time": 61.26388740539551,
      "citing_paper_id": "271961798",
      "cited_paper_id": 270441137
    },
    {
      "context_text": "• RL [13] uses the Proximal Policy Optimization (PPO) [32] to train the attack policy to generate adversarial perturbations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PPO) used for training an attack policy. No verifiable resources are identified.",
      "processing_time": 62.27342772483826,
      "citing_paper_id": "271961798",
      "cited_paper_id": 276248853
    },
    {
      "context_text": "MultiAttack [13] also considers utilizing social relationships to degrade the performance of RecSys.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for attacking recommendation systems.",
      "processing_time": 60.03464984893799,
      "citing_paper_id": "271961798",
      "cited_paper_id": 276248853
    },
    {
      "context_text": "Given that the task performance of large language models is significantly influenced by the quality of the input prompts [47], freezing the parameters of the LLM-based agent results in the attack policy being highly dependent on the input instruction provided by attackers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the influence of input prompts on LLM performance.",
      "processing_time": 60.43548655509949,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "An automatic method for the generation of adversarial prompts is also presented by fine-tuning the LLM. Xu et al. [42] leverage the LLM to generate poisoned instructions and insert the backdoor into LLMs via instruction tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and approaches but does not reference any named datasets.",
      "processing_time": 61.51263928413391,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "Lapid et al. [26] propose to exploit the genetic algorithm to iteratively generate the universal adversarial prompt.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (genetic algorithm) for generating universal adversarial prompts.",
      "processing_time": 61.07945537567139,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "• GA [26]employs thegeneticalgorithm tofind theadversarial perturbation and insert them to the end of the benign input.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (genetic algorithm).",
      "processing_time": 60.0224130153656,
      "citing_paper_id": "271961798",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) have shown impressive reasoning and decision-making capabilities in complex tasks like mathematical reasoning and creative writing by processing and generating thoughts based on token-level predictions [Huang et al., 2022a, Zhang et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 61.49235510826111,
      "citing_paper_id": "270257760",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Large Language Models (LLMs) have shown impressive reasoning and decision-making capabilities in complex tasks like mathematical reasoning and creative writing by processing and generating thoughts based on token-level predictions [Huang et al., 2022a, Zhang et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 61.49235510826111,
      "citing_paper_id": "270257760",
      "cited_paper_id": 265050948
    },
    {
      "context_text": "Among these methods, Chain of Thought (CoT) [Wei et al., 2022] Wang et al. [2022] introduce the Self-consistency with CoT (CoT-SC) method, which enhances reasoning accuracy and stability by independently generating multiple thought chains and selecting the most reliable answer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the introduction and improvement of reasoning methods in language models.",
      "processing_time": 62.414138078689575,
      "citing_paper_id": "270257760",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For baselines, we compare our approach with several prompting methods, including IO, CoT [Wei et al., 2022], CoT-SC [Wang et al., 2022], ToT [Gomez], and RAT [CraftJarvis].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various prompting methods and models. There are no clear identifiers for datasets.",
      "processing_time": 61.67400240898132,
      "citing_paper_id": "270257760",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…directly responds to the prompt, the basic idea of thought structures like Chain-of-Thought (CoT) [Wei et al., 2022] and Self-consistency with CoT (CoT-SC) [Wang et al., 2022] is to guide the reasoning process by generating a sequential and coherent reasoning framework according to the given task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses methods for improving reasoning in language models but does not mention any specific datasets.",
      "processing_time": 59.89122724533081,
      "citing_paper_id": "270257760",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…[Zhang et al., 2024] prompting, where the model directly responds to the prompt, the basic idea of thought structures like Chain-of-Thought (CoT) [Wei et al., 2022] and Self-consistency with CoT (CoT-SC) [Wang et al., 2022] is to guide the reasoning process by generating a sequential and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts related to reasoning processes in language models.",
      "processing_time": 60.58014631271362,
      "citing_paper_id": "270257760",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "This approach helps the LLM to produce responses that are more accurate, contextually relevant, and with fewer hallucinations [Yang et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general improvement in LLM responses. No verifiable resources are identified.",
      "processing_time": 61.65608310699463,
      "citing_paper_id": "270257760",
      "cited_paper_id": 259262077
    },
    {
      "context_text": "Additionally, the selection mechanism of the optimal branch of ToT is not effective and efficient enough, which could result in generating excessive texts and branches [Ding et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the limitations of the selection mechanism in the ToT framework.",
      "processing_time": 62.42612338066101,
      "citing_paper_id": "270257760",
      "cited_paper_id": 265050948
    },
    {
      "context_text": "This task is a benchmark for LLM mathematical reasoning [Kim et al., 2023, Ding et al., 2023], as it requires arithmetic operations and strategic planning abilities to explore various combinations and operation sequences for the optimal solution.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a task that serves as a benchmark for LLM mathematical reasoning. No verifiable resources are identified.",
      "processing_time": 62.75111770629883,
      "citing_paper_id": "270257760",
      "cited_paper_id": 265050948
    },
    {
      "context_text": "This task is a benchmark for LLM mathematical reasoning [Kim et al., 2023, Ding et al., 2023], as it requires arithmetic operations and strategic planning abilities to explore various combinations and operation sequences for the optimal solution.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a task that serves as a benchmark for LLM mathematical reasoning. No verifiable resources are identified.",
      "processing_time": 62.75111770629883,
      "citing_paper_id": "270257760",
      "cited_paper_id": 266044180
    },
    {
      "context_text": "This task is crucial for evaluating the reliability of LLMs’ text generation, as we target on generating content that is accurate and trustworthy [Gao et al., 2023, Rawte et al., 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of evaluating LLMs' text generation reliability.",
      "processing_time": 60.80660343170166,
      "citing_paper_id": "270257760",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "Following the method introduced in [Ding et al., 2024], we perform the standard hallucination detection.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for hallucination detection.",
      "processing_time": 59.43679404258728,
      "citing_paper_id": "270257760",
      "cited_paper_id": 267740648
    },
    {
      "context_text": "Also, CoT often fails to adjust its reasoning strategy flexibly in tasks with multiple solutions [Chen et al., 2024].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to the flexibility of Chain-of-Thought (CoT) reasoning.",
      "processing_time": 62.087249994277954,
      "citing_paper_id": "270257760",
      "cited_paper_id": 268249090
    },
    {
      "context_text": "Retrieval Augmented Thoughts (RAT) [Wang et al., 2024] combining CoT and Retrieval-Augmented Generation (RAG) [Lewis et al., 2020] is capable of fact-checking, and makes remarkable improvements in reducing factual errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 61.56707000732422,
      "citing_paper_id": "270257760",
      "cited_paper_id": 268249090
    },
    {
      "context_text": "• Encyclopedic Content is a comprehensive knowledge base sourced from Wikipedia 2 and WikiQA (Yang et al., 2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WikiQA"
      ],
      "dataset_descriptions": {
        "WikiQA": "Used to evaluate open-domain question answering systems, focusing on the ability to find answers in a large, unstructured text corpus."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Encyclopedic Content' as a knowledge base sourced from Wikipedia and WikiQA. WikiQA is a specific dataset used for open-domain question answering.",
      "processing_time": 69.13867950439453,
      "citing_paper_id": "276250494",
      "cited_paper_id": 1373518
    },
    {
      "context_text": "…agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used to evaluate multi-step reasoning capabilities in question answering, focusing on diverse and explainable multi-hop questions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Hot-potQA' as an example of a dataset used for multi-step reasoning tasks. The cited paper title confirms it is a dataset.",
      "processing_time": 68.64599680900574,
      "citing_paper_id": "276250494",
      "cited_paper_id": 52822214
    },
    {
      "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve interaction with and feedback from the environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
        "ALFWorld": "Used for sequential decision-making tasks, involving interaction with and feedback from the environment to assess the planning capabilities of LLMs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively.",
      "processing_time": 75.63840293884277,
      "citing_paper_id": "276250494",
      "cited_paper_id": 52822214
    },
    {
      "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve interaction with and feedback from the environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used for multi-step reasoning tasks, focusing on diverse, explainable multi-hop question answering to evaluate the planning capabilities of LLMs.",
        "ALFWorld": "Used for sequential decision-making tasks, involving interaction with and feedback from the environment to assess the planning capabilities of LLMs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively.",
      "processing_time": 75.63840293884277,
      "citing_paper_id": "276250494",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Large language models (LLMs) are rapidly evolving beyond traditional natural language processing tasks (Ouyang et al., 2022; Brown et al., 2020; Achiam et al., 2023), demonstrating increasing intelligence and autonomy by exhibiting capabilities in perception, reasoning, planning, and action within…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 61.11814522743225,
      "citing_paper_id": "276250494",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "…al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action trajectories that involve interaction with and feedback from the environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used to evaluate multi-step reasoning capabilities in language models, focusing on complex question answering that requires multiple pieces of evidence."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as examples of datasets used in multi-step reasoning and sequential decision-making tasks. However, 'ALFWorld' is a method or environment, not a dataset.",
      "processing_time": 71.1868987083435,
      "citing_paper_id": "276250494",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Due to the lack of agent-specific pre-training corpus, existing LLM agents rely on either prompt engineering (Hsieh et al., 2023; Lu et al., 2024; Yao et al., 2023; Wang et al., 2023) or instruction fine-tuning (Chen et al., 2023; Zeng et al., 2023) to understand human instructions, decompose…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers are used to support the discussion on how LLM agents are currently trained.",
      "processing_time": 61.30888748168945,
      "citing_paper_id": "276250494",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMLU as a benchmark, which is excluded as per instructions. No other specific datasets are mentioned.",
      "processing_time": 59.835456132888794,
      "citing_paper_id": "276250494",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMLU as a benchmark, which is excluded as per instructions. No other specific datasets are mentioned.",
      "processing_time": 59.835456132888794,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865718
    },
    {
      "context_text": "…benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MMLU as a benchmark, which is excluded as per instructions. No other specific datasets are mentioned.",
      "processing_time": 59.835456132888794,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering scenarios.",
        "ALFWorld": "Used for sequential decision-making tasks to assess the capabilities of agent frameworks in interactive environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
      "processing_time": 72.91538858413696,
      "citing_paper_id": "276250494",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "We execute official codes from agent frameworks (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2024a; Shinn et al., 2024) in multi-step reasoning tasks ( e.g. , Hot-potQA (Yang et al., 2018)) and sequential decision-making tasks ( e.g. , ALFWorld (Shridhar et al., 2021)) to collect action…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Hot-potQA",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "Hot-potQA": "Used for multi-step reasoning tasks to evaluate the performance of agent frameworks in complex question-answering scenarios.",
        "ALFWorld": "Used for sequential decision-making tasks to assess the capabilities of agent frameworks in interactive environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Hot-potQA' and 'ALFWorld' as specific datasets used for multi-step reasoning and sequential decision-making tasks, respectively. These are clearly identifiable datasets.",
      "processing_time": 72.91538858413696,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "…tasks (Ouyang et al., 2022; Brown et al., 2020; Achiam et al., 2023), demonstrating increasing intelligence and autonomy by exhibiting capabilities in perception, reasoning, planning, and action within complex real-world environments (Yao et al., 2023; Lu et al., 2024; Sun et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to tasks and capabilities of language models. No verifiable resources are identified.",
      "processing_time": 60.13526678085327,
      "citing_paper_id": "276250494",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "The use of closed-source LLMs incurs substantial financial costs and raises safety concerns (Li et al., 2023a; Zhuang et al., 2024b; Yuan et al., 2024b; Sun et al., 2024b; Shi et al., 2024a), limiting their wider deployment.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses issues related to the use of closed-source LLMs.",
      "processing_time": 58.865901470184326,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258060250
    },
    {
      "context_text": "The use of closed-source LLMs incurs substantial financial costs and raises safety concerns (Li et al., 2023a; Zhuang et al., 2024b; Yuan et al., 2024b; Sun et al., 2024b; Shi et al., 2024a), limiting their wider deployment.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses issues related to the use of closed-source LLMs.",
      "processing_time": 58.865901470184326,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260887189
    },
    {
      "context_text": "Following Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot demonstrations (Lu et al., 2024), environmental feedback (Yao et al., 2023; Sun et al., 2024a; Wang et al., 2023), and tree-like reasoning procedures (Yao et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMLU"
      ],
      "dataset_descriptions": {
        "MMLU": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks, but only 'MMLU' is a specific, identifiable dataset. The others are excluded as they are primarily used for score comparison or are not clearly datasets.",
      "processing_time": 66.97696447372437,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "Following Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMLU"
      ],
      "dataset_descriptions": {
        "MMLU": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of LLMs across various tasks and domains."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks, but only 'MMLU' is a specific, identifiable dataset. The others are excluded as they are primarily used for score comparison.",
      "processing_time": 66.36879324913025,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "To this end, we evaluate Hephaestus -Base on three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020), reporting the benchmark loss in Figure 5.",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions benchmarks but does not specify them as reusable datasets. MMLU is excluded as it is primarily a leaderboard.",
      "processing_time": 58.78387427330017,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "To this end, we evaluate Hephaestus -Base on three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020), reporting the benchmark loss in Figure 5.",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions benchmarks but does not specify them as reusable datasets. MMLU is excluded as it is primarily a leaderboard.",
      "processing_time": 58.78387427330017,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Moreover, the superficial alignment hypothesis (Zhou et al., 2024) suggests that a model’s fundamental knowledge and capabilities are acquired almost entirely during pre-training.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hypothesis about model capabilities.",
      "processing_time": 56.389620780944824,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258822910
    },
    {
      "context_text": "Nevertheless, the superficial alignment hypothesis (Zhou et al., 2024) suggests that a model’s fundamental knowledge and capabilities are predominantly acquired during pre-training.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hypothesis about model capabilities.",
      "processing_time": 56.239184617996216,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258822910
    },
    {
      "context_text": "The Superficial Alignment Hypothesis (Zhou et al., 2024; Gudibande et al., 2024; Lin et al., 2024b) posits that LLMs acquire most of their knowledge during pre-training, which is more important than instruction fine-tuning in terms of obtaining generalizable fundamental capabilities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hypotheses and theories about LLMs. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.063905000686646,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258822910
    },
    {
      "context_text": "The Superficial Alignment Hypothesis (Zhou et al., 2024; Gudibande et al., 2024; Lin et al., 2024b) posits that LLMs acquire most of their knowledge during pre-training, which is more important than instruction fine-tuning in terms of obtaining generalizable fundamental capabilities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only hypotheses and theories about LLMs. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.063905000686646,
      "citing_paper_id": "276250494",
      "cited_paper_id": 271532761
    },
    {
      "context_text": "ToolkenGPT (Hao et al., 2024) incorporated tools as special tokens into the model’s vocabulary, while ToolLLaMA (Qin et al., 2024) built datasets rich in various tools.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets rich in various tools' in relation to ToolLLaMA, but does not specify a named dataset. The mention is too generic and lacks a clear identifier.",
      "processing_time": 60.45854306221008,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258823133
    },
    {
      "context_text": "ToolkenGPT (Hao et al., 2024) incorporated tools as special tokens into the model’s vocabulary, while ToolLLaMA (Qin et al., 2024) built datasets rich in various tools.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets rich in various tools' in relation to ToolLLaMA, but does not specify a named dataset. The mention is too generic and lacks a clear identifier.",
      "processing_time": 60.45854306221008,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Others have leveraged domain-specific data to learn tool embeddings or modify the decoding process (Schick et al., 2024; Hao et al., 2024; Zhang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'domain-specific data' but does not specify any particular dataset names. The context is too generic to identify specific datasets.",
      "processing_time": 58.47500538825989,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258823133
    },
    {
      "context_text": "Post-training techniques focus more on instruction following and aligning output with specific formats (Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024 works (Nijkamp et al., 2023; Roziere et al., 2023; Xu et al., 2024; Patil et al., 2023) have primarily focused on…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only post-training techniques and methods. No verifiable resources are identified.",
      "processing_time": 57.80074954032898,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258823133
    },
    {
      "context_text": "Post-training techniques focus more on instruction following and aligning output with specific formats (Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024 works (Nijkamp et al., 2023; Roziere et al., 2023; Xu et al., 2024; Patil et al., 2023) have primarily focused on…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only post-training techniques and methods. No verifiable resources are identified.",
      "processing_time": 57.80074954032898,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Post-training techniques focus more on instruction following and aligning output with specific formats (Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024 works (Nijkamp et al., 2023; Roziere et al., 2023; Xu et al., 2024; Patil et al., 2023) have primarily focused on…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only post-training techniques and methods. No verifiable resources are identified.",
      "processing_time": 57.80074954032898,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "The most closely related work to our proposed model is OpenFunctions-v2 (Patil et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'OpenFunctions-v2' but does not indicate it is a dataset. It is likely a model or method, and there are no other specific datasets mentioned.",
      "processing_time": 59.50758767127991,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "…(Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024 works (Nijkamp et al., 2023; Roziere et al., 2023; Xu et al., 2024; Patil et al., 2023) have primarily focused on improving task-specific capabilities ( e.g. , code generation) instead of general-domain LLM agents,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to improving task-specific capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 58.26634240150452,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "For example, Gorilla (Patil et al., 2023) fine-tuned a LLaMA-based model using API documentation and demonstrations from Huggingface, TorchHub, and TensorFlowHub.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions fine-tuning a model using API documentation and demonstrations from Huggingface, TorchHub, and TensorFlowHub, but these are not datasets. They are platforms or repositories of models and APIs.",
      "processing_time": 60.46926665306091,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "By fitting these parameters using a collection of small et al., 2023) to monitor the agent capabilities, and MMLU (Hendrycks et al., 2020) to monitor the general capabilities of LLMs. Optimal Data Mixing Ratio.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'MMLU' which is a known benchmark, but it does not refer to a specific, downloadable dataset. The other reference to 'a collection of small et al., 2023' is too vague and lacks a specific identifier.",
      "processing_time": 62.92907929420471,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "• Berkeley Function Calling Leaderboard (BFCL) (Patil et al., 2023) provides a rigorous framework for assessing the function-calling proficiencies of diverse LLM agents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Berkeley Function Calling Leaderboard (BFCL)"
      ],
      "dataset_descriptions": {
        "Berkeley Function Calling Leaderboard (BFCL)": "Used to assess function-calling proficiencies of LLM agents, providing a rigorous framework for evaluation and benchmarking."
      },
      "confidence_score": 0.7,
      "reasoning": "BFCL is mentioned as a leaderboard, which is typically used for score comparison rather than as a reusable dataset. However, since it provides a framework for assessment, it could be considered a resource.",
      "processing_time": 68.44349551200867,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "…Dubey et al. (2024), we leverage additional three agent benchmarks (Nexus (Srinivasan et al., 2023), API-Bank (Li et al., 2023b), and API-Bench (Patil et al., 2023)) and one general benchmark (MMLU) (Hendrycks et al., 2020) for benchmark loss in the scaling law experiments. few-shot…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMLU"
      ],
      "dataset_descriptions": {
        "MMLU": "Used to evaluate benchmark loss in scaling law experiments, focusing on the performance of large language models across various tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks, but only MMLU is a specific, downloadable dataset. The others are excluded as they are primarily used for score comparison.",
      "processing_time": 64.71040511131287,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "The BFCL’s evaluation protocol incorporates varying degrees of complexity, ranging from single-function selection tasks to scenarios necessitating the concurrent execution of multiple-function calls.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an evaluation protocol. No clear identifiers for datasets are present.",
      "processing_time": 57.14267420768738,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "We leverage BFCL-v2 and -v3 to evaluate the function-calling capability of LLM agents.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BFCL-v2 and -v3 but does not specify them as datasets. They are likely versions of a method or tool used to evaluate LLMs.",
      "processing_time": 59.639206409454346,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "In contrast, Hephaestus exhibits significantly better performance on BFCL-v3, suggesting that its improvements in core agentic capabilities and generalization stem from pre-training on our large-scale, diverse agent-oriented corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BFCL-v3"
      ],
      "dataset_descriptions": {
        "BFCL-v3": "Used to evaluate the performance of Hephaestus, focusing on core agentic capabilities and generalization, highlighting improvements from pre-training."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'BFCL-v3' as a dataset used to evaluate performance, but does not provide enough detail about its specific use or characteristics. 'Our large-scale, diverse agent-oriented corpus' is too generic and lacks a specific identifier.",
      "processing_time": 68.54790568351746,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Datasets ( ) AST Exec BFCL-v2 Models ( ) OA Simple Python Java JS MF PF PM OA Simple Python REST MF PF PM OA",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide any specific, verifiable datasets. It mentions 'Datasets' and 'Models' but does not specify any particular dataset names.",
      "processing_time": 58.43010091781616,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Existing tool documentation data for agent training either lacks diverse real-world APIs (Patil et al., 2023; Tang et al., 2023) or is constrained to single-tool or single-round tool execution.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with existing tool documentation data. No clear, verifiable resource names are provided.",
      "processing_time": 57.61462640762329,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Existing tool documentation data for agent training either lacks diverse real-world APIs (Patil et al., 2023; Tang et al., 2023) or is constrained to single-tool or single-round tool execution.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with existing tool documentation data. No clear, verifiable resource names are provided.",
      "processing_time": 57.61462640762329,
      "citing_paper_id": "276250494",
      "cited_paper_id": 259108190
    },
    {
      "context_text": "Notably, the latest iteration, BFCL-v3, represents a significant methodological advancement over its predecessor by introducing a novel category that evaluates multi-turn and multi-step function invocation, more closely simulating real-world tool usage scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological advancement in a model. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.037400245666504,
      "citing_paper_id": "276250494",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Specifically, we systematically explore publicly accessible resources to gather high-quality API documentation and associated action trajectories.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'publicly accessible resources' and 'API documentation', which are too generic.",
      "processing_time": 58.39966869354248,
      "citing_paper_id": "276250494",
      "cited_paper_id": 259243960
    },
    {
      "context_text": "Specifically, we systematically explore publicly accessible resources to gather high-quality API documentation and associated action trajectories.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'publicly accessible resources' and 'API documentation', which are too generic.",
      "processing_time": 58.39966869354248,
      "citing_paper_id": "276250494",
      "cited_paper_id": 273654757
    },
    {
      "context_text": "Some have fine-tuned LLMs with specific API documentation and function call sequences (Qin et al., 2024; Gou et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions fine-tuning LLMs with specific API documentation and function call sequences, but does not name any specific datasets.",
      "processing_time": 57.607001066207886,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Considering the extensive diversity of APIs and the complexity of multi-tool instructions, tool learning inherently presents greater challenges than natu-ral language tasks, such as text generation (Qin et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general comparison between tool learning and natural language tasks.",
      "processing_time": 56.62347388267517,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…well-crafted prompting or extensive post-training, LLM-based autonomous agents augmented with external tools ( e.g. , APIs) have demonstrated exceptional instruction-following capabilities in a wide range of tasks (Schick et al., 2024; Qin et al., 2024; Srinivasan et al., 2023; Zeng et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs and their use with external tools. No verifiable resources are named.",
      "processing_time": 58.56070137023926,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…well-crafted prompting or extensive post-training, LLM-based autonomous agents augmented with external tools ( e.g. , APIs) have demonstrated exceptional instruction-following capabilities in a wide range of tasks (Schick et al., 2024; Qin et al., 2024; Srinivasan et al., 2023; Zeng et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs and their use with external tools. No verifiable resources are named.",
      "processing_time": 58.56070137023926,
      "citing_paper_id": "276250494",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "Furthermore, trajectory data mostly imitate expert behavior or follow function-calling rules with inferior planning and reasoning, failing to fully elicit LLMs’ capabilities and handle complex instructions (Qin et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the limitations of trajectory data in eliciting LLM capabilities.",
      "processing_time": 57.6572368144989,
      "citing_paper_id": "276250494",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…LLM agents rely on either prompt engineering (Hsieh et al., 2023; Lu et al., 2024; Yao et al., 2023; Wang et al., 2023) or instruction fine-tuning (Chen et al., 2023; Zeng et al., 2023) to understand human instructions, decompose high-level tasks, generate grounded plans, and execute multi-step…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for improving LLM agents. The cited paper title 'FireAct: Toward Language Agent Fine-tuning' does not provide additional context to identify a dataset.",
      "processing_time": 60.931204319000244,
      "citing_paper_id": "276250494",
      "cited_paper_id": 263829338
    },
    {
      "context_text": "To address this limitation, recent instruction tuning methods (Achiam et al., 2023; Srinivasan et al., 2023; Zeng et al., 2023; Chen et al., 2024b) have expanded to include a diverse range of API function call data and tasks, aiming to equip models with broader generalization capabilities across…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a range of API function call data and tasks. No clear, verifiable dataset names are provided.",
      "processing_time": 58.008150815963745,
      "citing_paper_id": "276250494",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "While these models excel in natural language processing tasks, they still underperform when serving as the core of LLM agents (Zeng et al., 2023; Liu et al., 2024d).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs underperform as agents. No verifiable resources are identified.",
      "processing_time": 57.21654915809631,
      "citing_paper_id": "276250494",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "While these models excel in natural language processing tasks, they still underperform when serving as the core of LLM agents (Zeng et al., 2023; Liu et al., 2024d).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs underperform as agents. No verifiable resources are identified.",
      "processing_time": 57.21654915809631,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "…rely on either prompt engineering (Hsieh et al., 2023; Lu et al., 2024; Yao et al., 2023; Wang et al., 2023) or instruction fine-tuning (Chen et al., 2023; Zeng et al., 2023) to understand human instructions, decompose high-level tasks, generate grounded plans, and execute multi-step actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The context focuses on techniques for improving LLMs' planning capabilities through prompt engineering and instruction fine-tuning.",
      "processing_time": 59.04916334152222,
      "citing_paper_id": "276250494",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "While models fine-tuned on task-specific data excel in corresponding tasks (Groq, 2024; Zeng et al., 2023; Liu et al., 2024c), they struggle to generalize across different agent benchmarks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general performance across benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 56.848716259002686,
      "citing_paper_id": "276250494",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "While models fine-tuned on task-specific data excel in corresponding tasks (Groq, 2024; Zeng et al., 2023; Liu et al., 2024c), they struggle to generalize across different agent benchmarks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general performance across benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 56.848716259002686,
      "citing_paper_id": "276250494",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "…specific formats (Patil et al., 2023; Hao et al., 2024; Qin et al., 2024; Schick et al., 2024 works (Nijkamp et al., 2023; Roziere et al., 2023; Xu et al., 2024; Patil et al., 2023) have primarily focused on improving task-specific capabilities ( e.g. , code generation) instead of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works that focus on improving task-specific capabilities in language models.",
      "processing_time": 57.27066659927368,
      "citing_paper_id": "276250494",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "Several notable examples have emerged in this domain: CodeGen (Nijkamp et al., 2023) and CodeLLaMA (Roziere et al., 2023) enhance the coding skills of LLMs. Building on the success of these code LLMs, LEMUR (Xu et al., 2024) further instruction tunes a code LLM with additional assistant and tool-related data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models (CodeGen, CodeLLaMA, LEMUR) but does not refer to any specific datasets. The context focuses on the capabilities and enhancements of these models rather than the datasets used.",
      "processing_time": 60.34405755996704,
      "citing_paper_id": "276250494",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "…examples have emerged in this domain: CodeGen (Nijkamp et al., 2023) and CodeLLaMA (Roziere et al., 2023) enhance the coding skills of LLMs. Building on the success of these code LLMs, LEMUR (Xu et al., 2024) further instruction tunes a code LLM with additional assistant and tool-related data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the enhancement of coding skills in LLMs and the introduction of LEMUR, which uses additional assistant and tool-related data, but no specific dataset names are provided.",
      "processing_time": 61.859212160110474,
      "citing_paper_id": "276250494",
      "cited_paper_id": 267412607
    },
    {
      "context_text": "Moreover, heavy fine-tuning prevents generalization and degrades performance in general use cases, potentially suppressing the original base model capabilities (Ghosh et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the limitations of instruction tuning and its impact on model generalization.",
      "processing_time": 57.2712721824646,
      "citing_paper_id": "276250494",
      "cited_paper_id": 267548105
    },
    {
      "context_text": "9% over Claude-3-Haiku and 4 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only performance metrics over a model variant. No verifiable resources are identified.",
      "processing_time": 57.00709414482117,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "LLM agents often struggle to generalize to new scenarios ( e.g. , from single to multiple tools) that differ from their original fine-tuning data distributions (Qin et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'fine-tuning data distributions'. No specific, verifiable datasets are named.",
      "processing_time": 57.960331439971924,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "To evaluate the precision of the fastText classifier in filtering general text from web retrieval data, we leverage Claude-3-Sonnet to annotate 20K samples.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Claude-3-Sonnet' as a tool used for annotation, which is not a dataset but a method or model. No other specific datasets are mentioned.",
      "processing_time": 59.367940187454224,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "For efficiency, we utilize multiple LLMs from Amazon Bedrock 11 for data synthesis, including Claude-3-Sonnet , Claude-3-Haiku (Anthropic, 2024), Mistral-Large (Mistral, 2024), LLaMA-3-70B-Instruct (Dubey et al., 2024), and Command-R-Plus (Cohere, 2024).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several LLMs but does not refer to any specific datasets. The context is focused on the use of models for data synthesis, which does not align with the criteria for extracting datasets.",
      "processing_time": 59.554630517959595,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "First, we employ Claude-3-Sonnet (Anthropic, 2024) as the data annotator to identify whether the sample belongs to agent data or a general web corpus.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Claude-3-Sonnet' but it is described as a data annotator, which is a method or tool, not a dataset. No other datasets are mentioned.",
      "processing_time": 59.35143518447876,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "First, we employ Claude-3-Sonnet (Anthropic, 2024) as the data annotator to annotate a total of 71 , 473 samples from the retrieved data, identifying 37 , 714 as agent-relevant and 33 , 767 as general text para-graphs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Claude-3-Sonnet' but it is described as a data annotator, which is a method or tool, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 59.376081466674805,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "To scale and diversify the pre-training corpus for LLM agents, we introduce a three-stage construction process (Figure 7) for Hephaestus-Forge in § 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for constructing a pre-training corpus for LLM agents.",
      "processing_time": 57.40201139450073,
      "citing_paper_id": "276250494",
      "cited_paper_id": 268297180
    },
    {
      "context_text": "Pandora (Xiang et al., 2024) represents a pre-trained world model that incorporates visual encoders to process a wide array of multi-modal data, including videos and textual actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pre-trained model called Pandora. The context focuses on the capabilities of the model rather than the datasets used.",
      "processing_time": 58.14711618423462,
      "citing_paper_id": "276250494",
      "cited_paper_id": 270521777
    },
    {
      "context_text": "In addition, ToolACE accommodates complex nested parameters, manages both parallel and dependent function calls, and encompasses a wide variety of tool-related data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool called ToolACE. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.28674936294556,
      "citing_paper_id": "276250494",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "To further improve its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ShareGPT",
        "ToolACE",
        "AgentFlan"
      ],
      "dataset_descriptions": {
        "ShareGPT": "Used for instruction fine-tuning to enhance instruction-following capabilities, focusing on high-quality instruction-completion pairs.",
        "ToolACE": "Utilized for instruction fine-tuning to improve function calling in LLMs, enhancing interaction with complex agent environments.",
        "AgentFlan": "Applied for instruction fine-tuning to align with complex agent environments, providing diverse and high-quality instruction-completion data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for instruction fine-tuning of a model called Hephaestus. These datasets are named and appear to be relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 77.47682166099548,
      "citing_paper_id": "276250494",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "To further improve its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ShareGPT",
        "ToolACE",
        "AgentFlan"
      ],
      "dataset_descriptions": {
        "ShareGPT": "Used for instruction fine-tuning to enhance instruction-following capabilities, focusing on high-quality instruction-completion pairs.",
        "ToolACE": "Utilized for instruction fine-tuning to improve function calling in LLMs, enhancing interaction with complex agent environments.",
        "AgentFlan": "Applied for instruction fine-tuning to align with complex agent environments, providing diverse and high-quality instruction-completion data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for instruction fine-tuning of a model called Hephaestus. These datasets are named and appear to be relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 77.47682166099548,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "• ToolACE (Liu et al., 2024c) is a single-tool conversation dataset designed to enhance the function-calling capabilities of LLM agents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ToolACE"
      ],
      "dataset_descriptions": {
        "ToolACE": "Used to enhance function-calling capabilities of LLM agents, focusing on single-tool conversations to improve planning and execution in LLMs."
      },
      "confidence_score": 1.0,
      "reasoning": "ToolACE is identified as a dataset designed for enhancing function-calling capabilities in LLM agents, which is directly relevant to the research topic of planning capabilities of LLMs.",
      "processing_time": 64.83442902565002,
      "citing_paper_id": "276250494",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "…its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ShareGPT",
        "ToolACE",
        "AgentFlan"
      ],
      "dataset_descriptions": {
        "ShareGPT": "Used for instruction fine-tuning to enhance Hephaestus's alignment with complex agent environments, focusing on high-quality instruction-completion tasks.",
        "ToolACE": "Used for instruction fine-tuning to improve Hephaestus's function calling capabilities, enhancing its performance in complex agent environments.",
        "AgentFlan": "Used for instruction fine-tuning to refine Hephaestus's planning and execution capabilities in complex agent environments, focusing on high-quality instruction-completion tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for instruction fine-tuning of a model called Hephaestus. These datasets are clearly named and relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 79.49735593795776,
      "citing_paper_id": "276250494",
      "cited_paper_id": 272368347
    },
    {
      "context_text": "…its instruction-following capabilities to align with complex agent environments, Hephaestus undergoes instruction fine-tuning on a blend of high-quality instruction-completion datasets, including ShareGPT (Chiang et al., 2023), ToolACE (Liu et al., 2024c), and AgentFlan (Chen et al., 2024b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ShareGPT",
        "ToolACE",
        "AgentFlan"
      ],
      "dataset_descriptions": {
        "ShareGPT": "Used for instruction fine-tuning to enhance Hephaestus's alignment with complex agent environments, focusing on high-quality instruction-completion tasks.",
        "ToolACE": "Used for instruction fine-tuning to improve Hephaestus's function calling capabilities, enhancing its performance in complex agent environments.",
        "AgentFlan": "Used for instruction fine-tuning to refine Hephaestus's planning and execution capabilities in complex agent environments, focusing on high-quality instruction-completion tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for instruction fine-tuning of a model called Hephaestus. These datasets are clearly named and relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 79.49735593795776,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "By leveraging real user data, ShareGPT allows models to better generalize across diverse tasks and navigate increasingly complex instructions, closely mimicking real-world conversational scenarios.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'real user data' without providing a specific name or identifier.",
      "processing_time": 57.341556549072266,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "• ShareGPT (Chiang et al., 2023) is a general dataset comprising real-world conversations from 70K user data, designed to fine-tune models for enhanced instruction-following capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ShareGPT"
      ],
      "dataset_descriptions": {
        "ShareGPT": "Used to fine-tune models for enhanced instruction-following capabilities, focusing on real-world conversations from 70K user data."
      },
      "confidence_score": 1.0,
      "reasoning": "ShareGPT is mentioned as a dataset used for fine-tuning models to enhance instruction-following capabilities. It is a specific, named dataset with a clear purpose.",
      "processing_time": 64.58471488952637,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "We conduct the main experiments of Hephaestus on three widely used LLM agent benchmarks across a wide range of scenarios, including: • AgentBench (Liu et al., 2024d) presents six distinct environments in a multi-turn, open-ended generation setting: Operating System (OS), Database (DB), Knowledge Graph (KG), House-Holding (HH), Web Shopping (WS), and Web Browsing (WB).",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AgentBench' as a benchmark used for evaluating LLM agents in various environments. However, it is described as a benchmark rather than a specific, downloadable dataset.",
      "processing_time": 59.11590600013733,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "We conduct the main experiments of Hephaestus on three widely used LLM agent benchmarks across a wide range of scenarios, including: • AgentBench (Liu et al., 2024d) presents six distinct environments in a multi-turn, open-ended generation setting: Operating System (OS), Database (DB), Knowledge…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AgentBench"
      ],
      "dataset_descriptions": {
        "AgentBench": "Used to evaluate LLM agents in six distinct environments, focusing on multi-turn, open-ended generation tasks to assess planning capabilities."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AgentBench' as a benchmark used for evaluating LLM agents in various environments. However, it is not clear if 'AgentBench' is a specific, downloadable dataset or just a suite of benchmarks. Given the context, it is included with a lower confidence.",
      "processing_time": 68.76946187019348,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "We mainly evaluate our Hephaestus on the following benchmarks: (1) AgentBench (Liu et al., 2024d) Baselines.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AgentBench' but does not specify it as a dataset. It is listed under 'benchmarks', which typically refers to evaluation frameworks rather than reusable datasets.",
      "processing_time": 58.94256663322449,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "We leverage AgentBench to evaluate intrinsic reasoning and adaptation to environmental feedback.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "AgentBench is mentioned as a tool for evaluation, but it does not fit the criteria for a dataset. It is likely a benchmark or challenge, which is excluded unless it refers to a specific, downloadable dataset.",
      "processing_time": 60.20754408836365,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "…of agent-specific pre-training corpus, existing LLM agents rely on either prompt engineering (Hsieh et al., 2023; Lu et al., 2024; Yao et al., 2023; Wang et al., 2023) or instruction fine-tuning (Chen et al., 2023; Zeng et al., 2023) to understand human instructions, decompose high-level tasks,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It focuses on methods and approaches such as prompt engineering and instruction fine-tuning.",
      "processing_time": 57.53842854499817,
      "citing_paper_id": "276250494",
      "cited_paper_id": null
    },
    {
      "context_text": "They used BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) metrics to compared prediction and ground truth instances.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (BLEU-4, ROUGE-L, METEOR) but does not refer to any specific datasets. These metrics are used for evaluation, not as datasets.",
      "processing_time": 59.11005139350891,
      "citing_paper_id": "270216385",
      "cited_paper_id": 964287
    },
    {
      "context_text": "They used BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), and METEOR (Banerjee and Lavie, 2005) metrics to compared prediction and ground truth instances.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics (BLEU-4, ROUGE-L, METEOR) but does not refer to any specific datasets. These metrics are used for evaluation, not as datasets.",
      "processing_time": 59.11005139350891,
      "citing_paper_id": "270216385",
      "cited_paper_id": 7164502
    },
    {
      "context_text": "Evaluation metrics involve accuracy of API calls and ROUGE-L for post-call responses, with task planning efficacy measured by the successful completion of planned tasks through model-driven API calls.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. ROUGE-L is a metric, not a dataset.",
      "processing_time": 56.596126317977905,
      "citing_paper_id": "270216385",
      "cited_paper_id": 964287
    },
    {
      "context_text": "The early works testing model’s intelligence referred to as the Turing Test, raising the question of whether machines could imitate human intelligence and made people fail to differentiate (Pinar Saygin et al., 2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to the Turing Test. The context is about the concept of the Turing Test and its implications on machine intelligence.",
      "processing_time": 58.29827356338501,
      "citing_paper_id": "270216385",
      "cited_paper_id": 990084
    },
    {
      "context_text": "Recently, the Transformer architecture (Vaswani et al., 2017) was proposed, allowing word embed-dings to be context-dependent, and model training to be scaled up (Min et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 55.94558548927307,
      "citing_paper_id": "270216385",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Regarding the finance-specific tasks, FPB (Malo et al., 2014), FiQA SA (Maia et al., 2018), Headline (Sinha and Khandait, 2021), NER (Alvarado et al., 2015), and ConvFinQA (Chen et al., 2022) were used.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FPB",
        "FiQA SA",
        "Headline",
        "NER",
        "ConvFinQA"
      ],
      "dataset_descriptions": {
        "FPB": "Used to evaluate financial prediction benchmarks, focusing on stock price prediction using news articles and financial reports.",
        "FiQA SA": "Applied to assess sentiment analysis in financial news, specifically measuring the impact of news sentiment on market trends.",
        "Headline": "Utilized to analyze the impact of financial headlines on market movements, focusing on the relationship between news and stock prices.",
        "NER": "Employed for named entity recognition in financial texts, identifying key entities such as companies, financial instruments, and economic indicators.",
        "ConvFinQA": "Used to evaluate conversational finance question answering, focusing on numerical reasoning and complex financial queries."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets by name, all of which are specific and verifiable. Each dataset is used for evaluating or training models on finance-specific tasks.",
      "processing_time": 85.50863695144653,
      "citing_paper_id": "270216385",
      "cited_paper_id": 13866508
    },
    {
      "context_text": "Regarding the finance-specific tasks, FPB (Malo et al., 2014), FiQA SA (Maia et al., 2018), Headline (Sinha and Khandait, 2021), NER (Alvarado et al., 2015), and ConvFinQA (Chen et al., 2022) were used.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FPB",
        "FiQA SA",
        "Headline",
        "NER",
        "ConvFinQA"
      ],
      "dataset_descriptions": {
        "FPB": "Used to evaluate financial prediction benchmarks, focusing on stock price prediction using news articles and financial reports.",
        "FiQA SA": "Applied to assess sentiment analysis in financial news, specifically measuring the impact of news sentiment on market trends.",
        "Headline": "Utilized to analyze the impact of financial headlines on market movements, focusing on the relationship between news and stock prices.",
        "NER": "Employed for named entity recognition in financial texts, identifying key entities such as companies, financial instruments, and economic indicators.",
        "ConvFinQA": "Used to evaluate conversational finance question answering, focusing on numerical reasoning and complex financial queries."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets by name, all of which are specific and verifiable. Each dataset is used for evaluating or training models on finance-specific tasks.",
      "processing_time": 85.50863695144653,
      "citing_paper_id": "270216385",
      "cited_paper_id": 221555387
    },
    {
      "context_text": "Regarding the finance-specific tasks, FPB (Malo et al., 2014), FiQA SA (Maia et al., 2018), Headline (Sinha and Khandait, 2021), NER (Alvarado et al., 2015), and ConvFinQA (Chen et al., 2022) were used.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FPB",
        "FiQA SA",
        "Headline",
        "NER",
        "ConvFinQA"
      ],
      "dataset_descriptions": {
        "FPB": "Used to evaluate financial prediction benchmarks, focusing on stock price prediction using news articles and financial reports.",
        "FiQA SA": "Applied to assess sentiment analysis in financial news, specifically measuring the impact of news sentiment on market trends.",
        "Headline": "Utilized to analyze the impact of financial headlines on market movements, focusing on the relationship between news and stock prices.",
        "NER": "Employed for named entity recognition in financial texts, identifying key entities such as companies, financial instruments, and economic indicators.",
        "ConvFinQA": "Used to evaluate conversational finance question answering, focusing on numerical reasoning and complex financial queries."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions several datasets by name, all of which are specific and verifiable. Each dataset is used for evaluating or training models on finance-specific tasks.",
      "processing_time": 85.50863695144653,
      "citing_paper_id": "270216385",
      "cited_paper_id": 252780839
    },
    {
      "context_text": "…to action unit detection, RAF-DB dataset (Shan and Deng, 2018) for facial expression and compound emotion recognition (Du et al., 2014), CASME2 dataset (Yan et al., 2014) for Micro-expression Recognition (Zhao et al., 2023), and iMiGUE dataset (Liu et al., 2021a) for Micro-gesture Recognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RAF-DB",
        "CASME2",
        "iMiGUE"
      ],
      "dataset_descriptions": {
        "RAF-DB": "Used for facial expression and compound emotion recognition, providing labeled images for training and evaluating models in emotion detection.",
        "CASME2": "Utilized for spontaneous micro-expression recognition, offering high-resolution video sequences to study subtle facial movements and their temporal dynamics.",
        "iMiGUE": "Applied for micro-gesture recognition, providing a dataset of fine-grained hand and finger movements to enhance gesture-based interaction systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions specific datasets used for various facial and micro-expression recognition tasks, which are relevant to the broader domain of human behavior analysis, potentially useful for understanding planning capabilities in LLMs.",
      "processing_time": 78.19378852844238,
      "citing_paper_id": "270216385",
      "cited_paper_id": 17756042
    },
    {
      "context_text": "In the financial sector, Daniel et al. (2008) tackles the \"look-ahead benchmark bias\" in the evaluation of investment managers, which identifies significant discrepancies in performance metrics due to timing differences in benchmark composition.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses a methodological issue in evaluating investment managers, but does not mention any specific dataset.",
      "processing_time": 55.32379388809204,
      "citing_paper_id": "270216385",
      "cited_paper_id": 18506861
    },
    {
      "context_text": "Using the Generative Pre-trained Transformer (GPT) series as an illustration, the progression in complexity and models’ capability is marked by a significant increase in the number of parameters: GPT-1 (Radford et al., 2018) has 117 million parameters, GPT-2 (Radford et al., 2019) expands this to 1.5 billion parameters, and GPT-3 (Mann et al., 2020) further escalates to 175 billion parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the progression of GPT models in terms of parameter count but does not mention any specific datasets used for training or evaluation.",
      "processing_time": 57.3053879737854,
      "citing_paper_id": "270216385",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "…as an illustration, the progression in complexity and models’ capability is marked by a significant increase in the number of parameters: GPT-1 (Radford et al., 2018) has 117 million parameters, GPT-2 (Radford et al., 2019) expands this to 1.5 billion parameters, and GPT-3 (Mann et al., 2020)…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their parameter counts. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.763023376464844,
      "citing_paper_id": "270216385",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Similarly, OpenBookQA (Mi-haylov et al., 2018) contains elementary-level questions designed to assess understanding of basic scientific facts and their application in novel scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenBookQA"
      ],
      "dataset_descriptions": {
        "OpenBookQA": "Used to assess understanding of basic scientific facts and their application in novel scenarios through elementary-level questions."
      },
      "confidence_score": 1.0,
      "reasoning": "OpenBookQA is a specific dataset mentioned in the context, which is used to assess understanding of basic scientific facts and their application in novel scenarios.",
      "processing_time": 62.80519890785217,
      "citing_paper_id": "270216385",
      "cited_paper_id": 52183757
    },
    {
      "context_text": "Bian et al. (2024) shows that instruction tuning models have superior performance on several commonsense QA dataset including CommonsenseQA (Talmor et al., 2018) and OpenBookQA (Mihaylov et al., 2018), which illustrates that commonsense ability can be improved by with human alignment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CommonsenseQA",
        "OpenBookQA"
      ],
      "dataset_descriptions": {
        "CommonsenseQA": "Used to evaluate the performance of instruction-tuned models on commonsense question answering, focusing on human alignment and improvement in commonsense reasoning.",
        "OpenBookQA": "Used to assess the effectiveness of instruction-tuned models in open book question answering, emphasizing the enhancement of commonsense abilities through human alignment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, CommonsenseQA and OpenBookQA, which are used to evaluate the performance of instruction-tuned models on commonsense question answering tasks.",
      "processing_time": 72.36710596084595,
      "citing_paper_id": "270216385",
      "cited_paper_id": 52183757
    },
    {
      "context_text": "Bian et al. (2024) shows that instruction tuning models have superior performance on several commonsense QA dataset including CommonsenseQA (Talmor et al., 2018) and OpenBookQA (Mihaylov et al., 2018), which illustrates that commonsense ability can be improved by with human alignment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CommonsenseQA",
        "OpenBookQA"
      ],
      "dataset_descriptions": {
        "CommonsenseQA": "Used to evaluate the performance of instruction-tuned models on commonsense question answering, focusing on human alignment and improvement in commonsense reasoning.",
        "OpenBookQA": "Used to assess the effectiveness of instruction-tuned models in open book question answering, emphasizing the enhancement of commonsense abilities through human alignment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, CommonsenseQA and OpenBookQA, which are used to evaluate the performance of instruction-tuned models on commonsense question answering tasks.",
      "processing_time": 72.36710596084595,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "HotpotQA (Yang et al., 2018) is requires finding and reasoning over multiple supporting documents to formulate responses.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate multi-hop reasoning capabilities, specifically requiring the integration of information from multiple documents to answer questions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions HotpotQA as a dataset requiring reasoning over multiple documents, which aligns with the topic of planning capabilities in LLMs.",
      "processing_time": 62.800363540649414,
      "citing_paper_id": "270216385",
      "cited_paper_id": 52822214
    },
    {
      "context_text": "The first models of AI tried to simulate the function of a single neuron with feedforward, simple input-output functions (Muthukrishnan et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a historical reference to early AI models. No verifiable resources are identified.",
      "processing_time": 56.83234190940857,
      "citing_paper_id": "270216385",
      "cited_paper_id": 62039799
    },
    {
      "context_text": "Raina and Gales (2022) use the RACE++ dataset (Liang et al., 2019) to train a deep learning model to explicitly class a multiple-choice question in the complexity levels of easy, medium and hard, which could make the process of assessing multiple-choice question generation automatic.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RACE++"
      ],
      "dataset_descriptions": {
        "RACE++": "Used to train a deep learning model to classify multiple-choice questions into difficulty levels, aiming to automate the assessment of question generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the RACE++ dataset, which is used to train a deep learning model for classifying multiple-choice questions by difficulty levels. This aligns with the research topic on planning capabilities of LLMs.",
      "processing_time": 65.49927258491516,
      "citing_paper_id": "270216385",
      "cited_paper_id": 204854503
    },
    {
      "context_text": "Bang et al. (2023) shows that ChatGPT has commonsense reasoning capability over several com-monsence benchmark over general knowledge (Tal-mor et al., 2018) and physical concepts (Bisk et al., 2020; Wang et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions commonsense benchmarks but does not specify any dataset names. The focus is on the findings of ChatGPT's capabilities.",
      "processing_time": 57.385518312454224,
      "citing_paper_id": "270216385",
      "cited_paper_id": 208290939
    },
    {
      "context_text": "Spider Re-alistic (Deng et al., 2020) presents a SQL-based QA dataset, necessitating models to engage in text-to-SQL generation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Spider Re-alistic"
      ],
      "dataset_descriptions": {
        "Spider Re-alistic": "Used to train models for text-to-SQL generation, focusing on realistic SQL queries and complex database schemas."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'SQL-based QA dataset' which is a specific type of dataset. The name 'Spider Re-alistic' is a plausible multi-word proper noun that fits the criteria.",
      "processing_time": 64.98223996162415,
      "citing_paper_id": "270216385",
      "cited_paper_id": 225066679
    },
    {
      "context_text": "HoVer (Jiang et al., 2020) requires models to gather facts from multiple Wikipedia articles which are related to a claim and determine if these facts substantiate the claim.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HoVer"
      ],
      "dataset_descriptions": {
        "HoVer": "Used to train models for fact extraction and claim verification from multiple Wikipedia articles, focusing on the ability to gather and substantiate claims with relevant facts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HoVer' as a dataset used for fact extraction and claim verification from multiple Wikipedia articles. It fits the criteria for a specific, verifiable dataset.",
      "processing_time": 64.16878628730774,
      "citing_paper_id": "270216385",
      "cited_paper_id": 226278099
    },
    {
      "context_text": "StrategyQA (Geva et al., 2021) requires models to generate several implicit reasoning steps to devise a strategy leading to a final decision for the question.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions StrategyQA, which is a benchmark for evaluating implicit reasoning strategies in question answering. However, it does not refer to a specific dataset but rather a task or challenge.",
      "processing_time": 59.112711668014526,
      "citing_paper_id": "270216385",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "Hort et al. (2021) uses a model behavior mutation approach for benchmarking ML bias mitigation methods.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for benchmarking bias mitigation methods.",
      "processing_time": 55.833908796310425,
      "citing_paper_id": "270216385",
      "cited_paper_id": 237205744
    },
    {
      "context_text": "Besides the widely known Truth-fulQA benchmark (Lin et al., 2022) , we also focus on the following topics: Hallucination, Bias Mitigation .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Truth-fulQA benchmark but does not indicate it is a dataset. It is likely a benchmark or leaderboard, which is excluded.",
      "processing_time": 57.407437801361084,
      "citing_paper_id": "270216385",
      "cited_paper_id": 237532606
    },
    {
      "context_text": "Jia et al. (2021) had fluent English speakers to annotated data from a peer-assessment platform, Expertiza and make sure enough inter-annotator agreement to test the accuracy of the BERT model for evaluating peer assessments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Expertiza"
      ],
      "dataset_descriptions": {
        "Expertiza": "Used to test the accuracy of the BERT model in evaluating peer assessments, ensuring sufficient inter-annotator agreement through fluent English speakers' annotations."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'annotated data from a peer-assessment platform, Expertiza' which is a specific dataset used for testing the accuracy of the BERT model in evaluating peer assessments.",
      "processing_time": 65.40935754776001,
      "citing_paper_id": "270216385",
      "cited_paper_id": 238531697
    },
    {
      "context_text": "BBQ (Parrish et al., 2021) is a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BBQ"
      ],
      "dataset_descriptions": {
        "BBQ": "Used to construct question sets highlighting social biases against protected classes, focusing on U.S. English-speaking contexts and nine social dimensions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly identifies BBQ as a dataset, and the cited paper title confirms it is a bias benchmark for question answering.",
      "processing_time": 62.32461762428284,
      "citing_paper_id": "270216385",
      "cited_paper_id": 239010011
    },
    {
      "context_text": "Self-Check (Miao et al., 2023) offers a zero-shot mechanism that empowers LLMs to autonomously verify their multi-step reasoning in math problem-solving, which significantly boosts accuracy on benchmarks including GSM8K, MathQA, and MATH by filtering out low-confidence solutions.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as datasets. The focus is on the method (Self-Check) and its impact on accuracy.",
      "processing_time": 57.57940721511841,
      "citing_paper_id": "270216385",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "Notable examples include GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to train and evaluate models on solving grade school math word problems, focusing on step-by-step reasoning and verification.",
        "MATH": "Used to assess the ability of models to solve advanced mathematical problems, emphasizing problem-solving skills and accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, GSM8K and MATH, which are used in the context of solving math word problems.",
      "processing_time": 68.78808832168579,
      "citing_paper_id": "270216385",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "In this context, several innovative approaches, such as (Huang et al., 2022a; Singh et al., 2023; Song et al., 2023a) harness the extensive commonsense knowledge available through LLMs, enabling these models to efficiently segment tasks into manageable subtasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods or findings. No clear identifiers for datasets are present.",
      "processing_time": 57.22351622581482,
      "citing_paper_id": "270216385",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "To enable nuanced control in robots for complex real-world tasks, the Code as Policies (Liang et al., 2023) paradigm uses LLMs to generate policy code for spatial reasoning and adapting to new instructions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using LLMs for generating policy code.",
      "processing_time": 56.54224920272827,
      "citing_paper_id": "270216385",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Regarding the general purpose tasks, standard LLM benchmarks were utilized for evaluation, such as BIG-bench Hard (Suzgun et al., 2022), and several datasets about Knowledge As-sessments, Reading Comprehension, and Linguistic Tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'BIG-bench Hard' but does not specify it as a dataset. It is likely a benchmark or leaderboard, which is excluded. No other specific datasets are mentioned.",
      "processing_time": 59.11643981933594,
      "citing_paper_id": "270216385",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "Stolfo et al. (2023) found that instruction-tuned LLM have a remarkable improvement in both sensitivity and robustness on mathematical problem compared to non-instruction-tuned models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between instruction-tuned and non-instruction-tuned LLMs.",
      "processing_time": 56.916282415390015,
      "citing_paper_id": "270216385",
      "cited_paper_id": 253080612
    },
    {
      "context_text": "Moreover, several works like DEPS (Wang et al., 2023b), AdaPlan-ner (Sun et al., 2023), and Robots That Ask For Help (Ren et al., 2023), introduce dynamic elements of interactive re-planning, adaptive strategies, and the ability to seek assistance when faced with uncertainties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 57.427908182144165,
      "citing_paper_id": "270216385",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Moreover, several works like DEPS (Wang et al., 2023b), AdaPlan-ner (Sun et al., 2023), and Robots That Ask For Help (Ren et al., 2023), introduce dynamic elements of interactive re-planning, adaptive strategies, and the ability to seek assistance when faced with uncertainties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 57.427908182144165,
      "citing_paper_id": "270216385",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "Benoit (2023) showed when presented with 45 simplified standardized vignettes (Semigran et al., 2015), Chat-GPT identified illnesses with 75.6% first-pass diagnostic accuracy and 57.8% triage accuracy, which performed similarly to physicians’ 72.1% on the same set of 45 vignettes.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions '45 simplified standardized vignettes' from Semigran et al. (2015), but does not provide a specific dataset name. The reference is used to compare Chat-GPT's performance to physicians.",
      "processing_time": 60.58301377296448,
      "citing_paper_id": "270216385",
      "cited_paper_id": 256661805
    },
    {
      "context_text": "LLMs’ ability in legislation area has also attracted attention because GPT-4 scored approximately 297 points on the uniform bar examination, passing the threshold for all jurisdiction (Katz et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a score on the uniform bar examination. No verifiable dataset names are present.",
      "processing_time": 57.07716679573059,
      "citing_paper_id": "270216385",
      "cited_paper_id": 257572753
    },
    {
      "context_text": "Yuan et al. (2023) compare the arithemtic capability of 13 models on each operation types and found that GPT-4 is the only model that have excellent performance in every of them.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of models on arithmetic operations.",
      "processing_time": 54.880242586135864,
      "citing_paper_id": "270216385",
      "cited_paper_id": 257952500
    },
    {
      "context_text": "Han et al. (2023) and Liu et al. (2023) include GPT-4 in their evaluation and found that its performance qualitatively matches that of humans in some scenarios.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of GPT-4. No verifiable resources are identified.",
      "processing_time": 57.25968956947327,
      "citing_paper_id": "270216385",
      "cited_paper_id": 258041354
    },
    {
      "context_text": "For instance, Karinshak et al. (2023) used impact evaluation to measure participants’ attitude to GPT-3-generated pro-vaccination messages.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method of evaluating the impact of AI-generated messages.",
      "processing_time": 57.413912296295166,
      "citing_paper_id": "270216385",
      "cited_paper_id": 258171641
    },
    {
      "context_text": "Turpin et al. (2023) demonstrate that Chain-of-Thought (CoT) explanations can systematically misrepre-sent the true reasoning behind a model’s predictions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought explanations).",
      "processing_time": 56.165632486343384,
      "citing_paper_id": "270216385",
      "cited_paper_id": 258556812
    },
    {
      "context_text": "LATM (Cai et al., 2024) utilizes GPT-4 to develop tools, demonstrating that more cost-effective models can achieve comparable performance to larger models in these applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of GPT-4 for developing tools. No verifiable resources are identified.",
      "processing_time": 57.946364641189575,
      "citing_paper_id": "270216385",
      "cited_paper_id": 258947222
    },
    {
      "context_text": "Feldman et al. (2023) helps recognize and flag instances when LLMs operate outside their domain knowledge, ensuring that users receive accurate information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to recognizing and flagging instances of LLMs operating outside their domain knowledge.",
      "processing_time": 58.13725566864014,
      "citing_paper_id": "270216385",
      "cited_paper_id": 259129531
    },
    {
      "context_text": "Conditionally, Xie et al. (2023) proposed PIXIU, a framework including the financial LLM based on fine-tuning LLaMA, a instruction data with 136K data samples to support the fine-tuning, and an evaluation benchmark with 5 tasks and 9 datasets, giving LLMs in financial area a benchmark to assess…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'instruction data' and 'evaluation benchmark', but does not specify named datasets. The 'instruction data' is described generically, and the 'evaluation benchmark' is part of a suite of tasks, which is excluded.",
      "processing_time": 61.213635206222534,
      "citing_paper_id": "270216385",
      "cited_paper_id": 259129602
    },
    {
      "context_text": "Similarly, SayPlan (Rana et al., 2023) enhances task planning capabilities of LLMs by grounding them with 3D Scene Graphs to facilitate extensive environmental interactions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (SayPlan) and its application. No verifiable resources are identified.",
      "processing_time": 57.36262035369873,
      "citing_paper_id": "270216385",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "CValues (Xu et al., 2023b), the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria.",
      "catation_intent": "research work",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'CValues' as a benchmark, but it is described as a tool for measuring the alignment ability of LLMs rather than a reusable dataset. It does not fit the criteria for a traditional dataset.",
      "processing_time": 60.292274475097656,
      "citing_paper_id": "270216385",
      "cited_paper_id": 259983087
    },
    {
      "context_text": "Ethical Consideration Ethical evaluation in sensitive areas such as medical ethics (Balas et al., 2024) and moral decision-making (Scherrer et al., 2023) is challenging.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of ethical evaluation. No dataset names are present in the text.",
      "processing_time": 57.3570442199707,
      "citing_paper_id": "270216385",
      "cited_paper_id": 260164518
    },
    {
      "context_text": "Ethical Consideration Ethical evaluation in sensitive areas such as medical ethics (Balas et al., 2024) and moral decision-making (Scherrer et al., 2023) is challenging.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of ethical evaluation. No dataset names are present in the text.",
      "processing_time": 57.3570442199707,
      "citing_paper_id": "270216385",
      "cited_paper_id": 265104049
    },
    {
      "context_text": "In the MoralChoice datasets (Scherrer et al., 2023), one example of a moral dilemma involves a scenario where the respondent is a driver approaching a pedestrian crossing the street.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MoralChoice datasets"
      ],
      "dataset_descriptions": {
        "MoralChoice datasets": "Used to present moral dilemmas involving pedestrians to evaluate the moral beliefs encoded in LLMs, focusing on decision-making scenarios."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'MoralChoice datasets' which is a specific, multi-word proper noun. It is used to present moral dilemmas for evaluating moral beliefs in LLMs.",
      "processing_time": 65.54063749313354,
      "citing_paper_id": "270216385",
      "cited_paper_id": 260164518
    },
    {
      "context_text": "SafetyBench (Scherrer et al., 2023), a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple-choice questions spanning seven distinct categories of safety concerns.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SafetyBench, a benchmark for evaluating the safety of LLMs, which includes multiple-choice questions across various safety categories. However, it is a benchmark rather than a traditional dataset.",
      "processing_time": 59.87834310531616,
      "citing_paper_id": "270216385",
      "cited_paper_id": 260164518
    },
    {
      "context_text": "Scherrer et al. (2023) introduces a novel statistical method to examine the moral beliefs of LLMs and quantifies how likely LLMs are to make decisions in various moral scenarios, analyzing their responses across 680 high-ambiguity and 687 low-ambiguity dilemmas.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a novel statistical method and the analysis of LLM responses in moral scenarios, but does not specify a dataset name. The focus is on the method and findings.",
      "processing_time": 59.016634702682495,
      "citing_paper_id": "270216385",
      "cited_paper_id": 260164518
    },
    {
      "context_text": "Demszky et al. (2023) additionally proposed that in assessing the capability of LLMs for psychological tasks, initial assessment could be conducted using expert evaluation for a manipulation check or a measure of construct validity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving expert evaluation.",
      "processing_time": 56.333802700042725,
      "citing_paper_id": "270216385",
      "cited_paper_id": 264107446
    },
    {
      "context_text": "Yang et al. (2023) introduces a self-check approach for detecting factual errors in LLMs during critical tasks, using reverse validation in a zero-resource setting.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for detecting factual errors in LLMs.",
      "processing_time": 56.52323818206787,
      "citing_paper_id": "270216385",
      "cited_paper_id": 264107446
    },
    {
      "context_text": "Regarding psychological measurement, Demszky et al. (2023) proposed 2 methods to evaluate the effects of features on human thought and behaviour: 1) Expert evaluation means trained research assistants and LLMs score the same texts for particular psychological construct, and then compute agreement…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for evaluating psychological constructs using LLMs and human experts.",
      "processing_time": 56.85531949996948,
      "citing_paper_id": "270216385",
      "cited_paper_id": 264107446
    },
    {
      "context_text": "Existing papers on LLM evaluation meth-ods, including Guo et al. (2023) and Chang et al. (2023), provide a thorough review of evaluation approaches for various aspects of LLMs, yet no study has offered a phased framework to explore the usability of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a review of evaluation methods for LLMs. No verifiable resources are identified.",
      "processing_time": 58.193376541137695,
      "citing_paper_id": "270216385",
      "cited_paper_id": 264825354
    },
    {
      "context_text": "Existing papers on LLM evaluation meth-ods, including Guo et al. (2023) and Chang et al. (2023), provide a thorough review of evaluation approaches for various aspects of LLMs, yet no study has offered a phased framework to explore the usability of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a review of evaluation methods for LLMs. No verifiable resources are identified.",
      "processing_time": 58.193376541137695,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "A few models were even designed specifically for financial use, such as Fin-BERT (Liu et al., 2021b), XuanYuan 2.0 (Zhang and Yang, 2023), and BloombergGPT (Wu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is about models designed for financial use, not datasets.",
      "processing_time": 57.59209156036377,
      "citing_paper_id": "270216385",
      "cited_paper_id": 265038146
    },
    {
      "context_text": "Recent research (Huang and Chang, 2023; Sun et al., 2024) has increasingly emphasized the augmentation of reasoning capacities in LLMs, aiming to attain human-level or even sur-pass human-level reasoning prowess within specialized domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research trends and goals.",
      "processing_time": 55.95173239707947,
      "citing_paper_id": "270216385",
      "cited_paper_id": 266362535
    },
    {
      "context_text": "…the comprehensive abilities of LLMs are automatically evaluated through tasks spanning multiple domains such as HELM (Liang et al., 2022) and BIG-Bench (Sri-vastava et al., 2022), or by generating human feed-back automatically like AlpacaFarm (Dubois et al., 2024) and MT-bench (Zheng et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks and challenges, which are excluded as they are primarily used for score comparison. No specific, reusable datasets are mentioned.",
      "processing_time": 58.14571690559387,
      "citing_paper_id": "270216385",
      "cited_paper_id": 266362535
    },
    {
      "context_text": "In current benchmarks, the comprehensive abilities of LLMs are automatically evaluated through tasks spanning multiple domains such as HELM (Liang et al., 2022) and BIG-Bench (Sri-vastava et al., 2022), or by generating human feed-back automatically like AlpacaFarm (Dubois et al., 2024) and MT-bench (Zheng et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks and evaluation methods but does not specify any datasets. The names mentioned are benchmark suites or methods, not datasets.",
      "processing_time": 57.778913497924805,
      "citing_paper_id": "270216385",
      "cited_paper_id": 266362535
    },
    {
      "context_text": "Li et al. (2024b) has reviewed the current methods of using LLMs as scorers and has also identified potential issues, such as a preference for content generated by the same model or specific biases in evaluation order.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of methods and potential issues. No verifiable resources are named.",
      "processing_time": 57.78376221656799,
      "citing_paper_id": "270216385",
      "cited_paper_id": 266999586
    },
    {
      "context_text": "Because of the inexplicability of LLMs, we need various evaluation methods to understand their capabilities, and this is the driving force behind the progress of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for evaluation methods to understand LLM capabilities.",
      "processing_time": 56.810909032821655,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "From the validation process of BloombergGPT, we can understand the evaluation methods of financial LLMs. Wu et al. (2023) evaluated BloombergGPT on two broad categories of tasks: finance-specific and general purpose.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only broad categories of tasks. There are no clear identifiers for datasets.",
      "processing_time": 57.12829327583313,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "…with Kim et al. (2024b) introducing ProPILE, a probing tool that enables data subjects to detect potential PII leakage in services based on LLMs. Das et al. (2024) examines these vulnerabilities in depth, highlighting the urgent need for improved security protocols and the exploration of…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. The focus is on security and privacy challenges, particularly PII leakage in LLMs.",
      "processing_time": 58.63127946853638,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "In this section, our attention is directed towards evaluating the various reasoning abilities of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general evaluation of reasoning abilities of LLMs.",
      "processing_time": 57.31710362434387,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Addressing this issue, they underscore the significance of knowledge memorization and recall for LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of LLMs. No verifiable resources are identified.",
      "processing_time": 57.680137157440186,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "The models mentioned above, due to their tremendous size, are referred to as LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only refers to large language models (LLMs). There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.60818910598755,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Self-Alignment for Factuality (Zhang et al., 2024b) uses self-evaluation to improve factual accuracy within LLMs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving factual accuracy in LLMs.",
      "processing_time": 56.91037178039551,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "The discussion extends with Kim et al. (2024b) introducing ProPILE, a probing tool that enables data subjects to detect potential PII leakage in services based on LLMs. Das et al. (2024) examines these vulnerabilities in depth, highlighting the urgent need for improved security protocols and the exploration of effective defenses, while Yan et al. (2024a) focuses on clarifying the data privacy concerns associated with LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and methods. The focus is on security and privacy challenges of LLMs.",
      "processing_time": 58.4741268157959,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "This study introduced the two-stage framework: from core ability to agent to evaluate the usability of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a framework for evaluating LLMs. No verifiable resources are identified.",
      "processing_time": 58.31916356086731,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "When mentioning LLMs for financial use, Li et al. (2023b) argued that two major challenges are the production of dis-information and the manifestation of biases, such as racial, gender, and religious biases, in LLMs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges related to LLMs. No verifiable resources are identified.",
      "processing_time": 57.995410203933716,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Through exploring these dimensions, we aim to understand the broader societal implications of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the aim of understanding societal implications of LLMs.",
      "processing_time": 58.59578561782837,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "In this section, benchmarks and datasets play a vital role in evaluating the safety alignment of LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets or benchmarks, only a general statement about their importance.",
      "processing_time": 57.10848140716553,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "These methodologies emphasize a critical shift towards more sophisticated and reliable assessments of adversarial threat land-scapes in LLMs. RigorLLM (Yuan et al., 2024), a framework employing techniques like energy-based data generation and minimax optimization to enhance the moderation of harmful content and improve resilience against complex adversarial attacks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological framework called RigorLLM. The context focuses on the methodologies and techniques used within RigorLLM, which are not datasets.",
      "processing_time": 60.162882566452026,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Because LLMs have the capability to understand and utilize multiple language, emotion detection and psychological measurement can be done by LLMs. Plenty of researches evaluated whether LLMs could complete these tasks with enough quality.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and research areas. No clear, verifiable resources are identified.",
      "processing_time": 58.013755798339844,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Lastly, we give our point of view on the usability of LLMs and suggest future directions and challenges in evaluating LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the usability of LLMs and suggests future directions.",
      "processing_time": 58.632246255874634,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "By exploring these dimensions, we aim to comprehend the broader societal impacts of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the aim of comprehending societal impacts of LLMs.",
      "processing_time": 58.7742223739624,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Therefore, it is essential to understand the implications of LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only states a general need to understand the implications of LLMs.",
      "processing_time": 58.61743783950806,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "In this section, we explore essential safety mechanisms required to protect users when interacting with LLMs. Ensuring that these models generate only safe content is crucial, Oviedo-Trespalacios et al. (2023) found that ChatGPT sometimes made incorrect or harmful statements, emphasizing the need for expert verification.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only findings about the behavior of ChatGPT. No verifiable resources are identified.",
      "processing_time": 58.296082496643066,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267406814
    },
    {
      "context_text": "Also, the primary challenge in evaluation was incorporating domain knowledge from financial experts to validate the model’s performance based on financial NLP tasks (Lee et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenge of incorporating domain knowledge from financial experts.",
      "processing_time": 57.33923101425171,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267412025
    },
    {
      "context_text": "SALAD-Bench (Li et al., 2024a), designed to estimate LLMs, includes evaluations of attack and defense methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SALAD-Bench"
      ],
      "dataset_descriptions": {
        "SALAD-Bench": "Used to evaluate the safety of LLMs, focusing on attack and defense methods through a hierarchical and comprehensive benchmark."
      },
      "confidence_score": 1.0,
      "reasoning": "SALAD-Bench is mentioned as a benchmark for evaluating LLMs, specifically focusing on safety aspects including attack and defense methods.",
      "processing_time": 64.9702570438385,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267523467
    },
    {
      "context_text": "For example, the framework proposed by (Wang et al., 2024c) can manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extends existing benchmarks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for manipulating contexts or questions in existing benchmarks. No clear, verifiable dataset names are provided.",
      "processing_time": 59.14060020446777,
      "citing_paper_id": "270216385",
      "cited_paper_id": 267750939
    },
    {
      "context_text": "Additionally, it is crucial to ensure that LLMs do not produce adult content accessible by minors (Cifuentes et al., 2022; Karamizadeh et al., 2023), mitigate any harmful content that could affect children, guarantee that outputs do not encourage illegal activities (Nayerifard et al., 2023; Casino…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing ethical considerations in LLMs.",
      "processing_time": 57.592729330062866,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "On the resilience against adversarial attacks, Yip et al. (2024) introduces a framework that quantifies the resilience of applications against prompt inject attacks using innovative techniques for robust and interoperable evaluations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for evaluating resilience against adversarial attacks.",
      "processing_time": 57.59041094779968,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "Liu et al. (2024b); Jin et al. (2024) both proposes for the use of gradient-based method to enhance the evaluation of adversarial resilience in LLM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for evaluating adversarial resilience in LLMs.",
      "processing_time": 57.410229444503784,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "They found that Chat 3.5 Turbo give the similar result to 2,800 English speakers’ response (Tobia, 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between Chat 3.5 Turbo and English speakers' responses.",
      "processing_time": 58.22393226623535,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "Sorensen et al. (2024) emphasizes value plu-ralism in decision-making.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a concept (value pluralism) in decision-making.",
      "processing_time": 59.05709981918335,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "NavGPT (Zhou et al., 2023a) utilizes LLMs to perform explicit reasoning and planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for reasoning and planning.",
      "processing_time": 58.02616214752197,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "…by minors (Cifuentes et al., 2022; Karamizadeh et al., 2023), mitigate any harmful content that could affect children, guarantee that outputs do not encourage illegal activities (Nayerifard et al., 2023; Casino et al., 2022), and avoid the generation of content that could incite violence.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concerns about content moderation and safety. No verifiable resources are identified.",
      "processing_time": 58.618449449539185,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "Engel and Mcadams (2024) asked Chat 3.5 Turbo whether the statutory term “vehicle” includes a list of candidate objects to assessment LLMs’ understanding of statutory meaning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method of assessing LLMs' understanding of statutory meaning.",
      "processing_time": 58.340450048446655,
      "citing_paper_id": "270216385",
      "cited_paper_id": null
    },
    {
      "context_text": "ArigGraph (GPT-4o-mini) showed weaker performance on Musique, but outperformed GraphRAG on HotpotQA.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Musique and HotpotQA, which are known datasets in the field of question answering. However, they are primarily used as benchmarks or leaderboards, not as reusable datasets in the traditional sense.",
      "processing_time": 61.480926275253296,
      "citing_paper_id": "271039035",
      "cited_paper_id": 204578308
    },
    {
      "context_text": "Such games have long been benchmarks for researching agents capable of effectively remembering information and establishing long-term dependencies [ Parisotto et al. , 2020; Pleines et al. , 2022; Sorokin et al. , 2022].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers. The context is about using games as benchmarks for research, but no specific datasets are named.",
      "processing_time": 59.98704195022583,
      "citing_paper_id": "271039035",
      "cited_paper_id": 204578308
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 222140924
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 259950380
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 260063238
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 265019383
    },
    {
      "context_text": "However, these approaches lack mechanisms for updates in dynamic environments, a key advantage of AriGraph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or system called 'AriGraph'. No verifiable resources are identified.",
      "processing_time": 59.163435220718384,
      "citing_paper_id": "271039035",
      "cited_paper_id": 269302548
    },
    {
      "context_text": "Text-based environments [ C ˆ ot ´ e et al. , 2018; Hausknecht et al. , 2019; Shridhar et al. , 2021; Wang et al. , 2022 ] were originally designed to evaluate reinforcement learning (RL) agents [ Guo et al. , 2020; Yao et al. , 2020; Ammanabrolu et al. , 2020; Ammanabrolu and Hausknecht, 2020;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to text-based environments and reinforcement learning agents. No clear, verifiable datasets are identified.",
      "processing_time": 59.31776475906372,
      "citing_paper_id": "271039035",
      "cited_paper_id": 247451124
    },
    {
      "context_text": "This module adheres to the ReAct [ Yao et al. , 2023 ] framework, requiring the agent to articulate the rationale behind an action before execution.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or framework called ReAct. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.32342505455017,
      "citing_paper_id": "271039035",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Research into alternative frameworks like Recurrent Memory Transformer [ Bulatov et al. , 2022; Bulatov et al. , 2024 ] and MAMBA [ Gu and Dao, 2023 ] seeks to provide long-term memory solutions, though these models are still in their infancy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the development of long-term memory solutions in models.",
      "processing_time": 59.31977128982544,
      "citing_paper_id": "271039035",
      "cited_paper_id": 258291566
    },
    {
      "context_text": "It extends its action space with “go to location” type commands and infers an optimal route to a target location using spatial relations stored in a semantic graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method or capability of a model, not the use of a dataset.",
      "processing_time": 59.524741888046265,
      "citing_paper_id": "271039035",
      "cited_paper_id": 259950380
    },
    {
      "context_text": "It extends its action space with “go to location” type commands and infers an optimal route to a target location using spatial relations stored in a semantic graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method or capability of a model, not the use of a dataset.",
      "processing_time": 59.524741888046265,
      "citing_paper_id": "271039035",
      "cited_paper_id": null
    },
    {
      "context_text": "Over the past year, the research community has explored general architectures and core modules for such LLM agents [Wang et al. , 2024; Sumers et al. , 2024; Cheng et al. , 2024 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers discussing architectures and core modules for LLM agents.",
      "processing_time": 59.03977823257446,
      "citing_paper_id": "271039035",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Voyager [ Wang et al. , 2023a ] , Ghost in the Minecraft [ Zhu et al. , 2023] and Jarvis-1 [Wang et al. , 2023b] are advanced, open-ended LLM agents that show significantly better performance in Minecraft compared to earlier techniques.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions advanced LLM agents but does not refer to any specific datasets. The focus is on the performance of these agents in Minecraft.",
      "processing_time": 59.51998233795166,
      "citing_paper_id": "271039035",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "Multiple experiments have already explored the potential of LLMs in these complex scenarios [ Tsai et al. , 2023; Tan et al. , 2023; Momennejad et al. , 2023; Ding et al. , 2024a ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works exploring the potential of LLMs in complex scenarios.",
      "processing_time": 59.507084369659424,
      "citing_paper_id": "271039035",
      "cited_paper_id": 267770308
    },
    {
      "context_text": "Multiple experiments have already explored the potential of LLMs in these complex scenarios [ Tsai et al. , 2023; Tan et al. , 2023; Momennejad et al. , 2023; Ding et al. , 2024a ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works exploring the potential of LLMs in complex scenarios.",
      "processing_time": 59.507084369659424,
      "citing_paper_id": "271039035",
      "cited_paper_id": null
    },
    {
      "context_text": "Despite the constraints inherent in transformer architectures, contemporary methods enable LLMs to manage contexts encompassing millions of tokens [ Ding et al. , 2024b ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities of LLMs in managing large contexts.",
      "processing_time": 58.831106424331665,
      "citing_paper_id": "271039035",
      "cited_paper_id": 267770308
    },
    {
      "context_text": "We based our experiments on NetPlay [Jeurissen et al. , 2024 ] agent, which demonstrates state-of-the-art performance among LLM agents that do not rely on finetuning or RL.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NetPlay agent). The context focuses on the performance of the agent, not on any dataset.",
      "processing_time": 60.578527212142944,
      "citing_paper_id": "271039035",
      "cited_paper_id": 268230464
    },
    {
      "context_text": "For RL baselines, we collect the best results reported by [ Adhikari et al. , 2021; Tuli et al. , 2022; Basu et al. , 2024 ] for the GATA, LTL-GATA, and EXPLORER architectures on the Cooking task with four difficulties levels from [ Adhikari et al. , 2021 ] .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific architectures and a task but does not refer to any named datasets. The focus is on comparing performance of different models on a specific task.",
      "processing_time": 60.00305438041687,
      "citing_paper_id": "271039035",
      "cited_paper_id": 268417347
    },
    {
      "context_text": "The latest research demonstrating best performance in Q&A tasks includes Graphreader [Li et al. , 2024a], HOLMES [Panda et al. , 2024], HippoRAG [Guti´errez et al. , 2024], GraphRAG [ Edge et al. , 2024 ] which all employ the technique of building knowledge graphs from texts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The focus is on the techniques and models used for Q&A tasks.",
      "processing_time": 60.85247182846069,
      "citing_paper_id": "271039035",
      "cited_paper_id": 270371213
    },
    {
      "context_text": "The latest research demonstrating best performance in Q&A tasks includes Graphreader [Li et al. , 2024a], HOLMES [Panda et al. , 2024], HippoRAG [Guti´errez et al. , 2024], GraphRAG [ Edge et al. , 2024 ] which all employ the technique of building knowledge graphs from texts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The focus is on the techniques and models used for Q&A tasks.",
      "processing_time": 60.85247182846069,
      "citing_paper_id": "271039035",
      "cited_paper_id": 270620354
    },
    {
      "context_text": "Considerable research is dedicated to leveraging established knowledge graphs for enhancing Q&A [Baek et al. , 2023; Li et al. , 2024b] systems to address the factual knowledge deficiency observed in LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of knowledge graphs in general to enhance Q&A systems. No verifiable dataset names are provided.",
      "processing_time": 60.17198181152344,
      "citing_paper_id": "271039035",
      "cited_paper_id": 270620354
    },
    {
      "context_text": "We used 200 random samples from both datasets similar to [Li et al. , 2024a].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'both datasets' but does not specify their names. The cited paper title does not help disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 62.51339936256409,
      "citing_paper_id": "271039035",
      "cited_paper_id": 270620354
    },
    {
      "context_text": "We compared the performance of our approach against Graphreader [Li et al. Compared to the Treasure Hunt, the Cleaning game possesses a slightly different challenge as it is more important to properly filter outdated information about object locations, than not to lose any information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two games (Treasure Hunt and Cleaning) and a reference to a method (GraphReader).",
      "processing_time": 60.44115328788757,
      "citing_paper_id": "271039035",
      "cited_paper_id": 270620354
    },
    {
      "context_text": "Mirroring this complexity, the board game Diplomacy[59] involves seven players to control European powers, presenting a complex strategic challenge that requires both sophisticated negotiation and strategic planning to triumph.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the complexity of the board game Diplomacy, which is not a dataset.",
      "processing_time": 59.815197706222534,
      "citing_paper_id": "271065021",
      "cited_paper_id": 46997949
    },
    {
      "context_text": "The Diplomacy game [59, 8] is set in pre-World War I Europe and involves each player (agent) representing one of the seven Great Powers of Europe, such as Germany, France, England, Italy, Austria-Hungary, Russia, and Turkey.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the setup of the Diplomacy game, which is not a dataset.",
      "processing_time": 60.28520107269287,
      "citing_paper_id": "271065021",
      "cited_paper_id": 46997949
    },
    {
      "context_text": "[33, 69] After training, when Richelieu is faced with a certain state, it can draw on a larger pool of similar historical experiences.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a capability of the system after training.",
      "processing_time": 59.649996280670166,
      "citing_paper_id": "271065021",
      "cited_paper_id": 53533716
    },
    {
      "context_text": "The AI community has shown an increasing interest in the deployment of AI agents to master such games [45, 26, 29, 15, 36, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in AI agents mastering games. No verifiable resources are identified.",
      "processing_time": 60.50881600379944,
      "citing_paper_id": "271065021",
      "cited_paper_id": 153257084
    },
    {
      "context_text": "The AI community has shown an increasing interest in the deployment of AI agents to master such games [45, 26, 29, 15, 36, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in AI agents mastering games. No verifiable resources are identified.",
      "processing_time": 60.50881600379944,
      "citing_paper_id": "271065021",
      "cited_paper_id": 254277011
    },
    {
      "context_text": "The AI community has shown an increasing interest in the deployment of AI agents to master such games [45, 26, 29, 15, 36, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in AI agents mastering games. No verifiable resources are identified.",
      "processing_time": 60.50881600379944,
      "citing_paper_id": "271065021",
      "cited_paper_id": null
    },
    {
      "context_text": "[39, 22] 4 Self-Evolving LLM-based Diplomat We have constructed a comprehensive framework with modules for memory management, social reasoning, strategic planning, negotiation, decision-making, memory update, and self-evolving to fully leverage the capabilities of LLMs. Richelieu starts by setting…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a framework and modules for leveraging LLM capabilities. There are no clear identifiers for datasets.",
      "processing_time": 60.97106456756592,
      "citing_paper_id": "271065021",
      "cited_paper_id": 202538486
    },
    {
      "context_text": "Among the methods, one typical research is DipNet [39] which uses supervised and reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DipNet. The context is about using supervised and reinforcement learning, but no dataset names are provided.",
      "processing_time": 61.62868332862854,
      "citing_paper_id": "271065021",
      "cited_paper_id": 202538486
    },
    {
      "context_text": "The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet [39], the BRPI [3], the SearchBot [18], and the DORA[5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (SL-DipNet, RL-DipNet, BRPI, SearchBot, DORA) but does not refer to any specific datasets. These models are used for no-press diplomacy in multi-agent gameplay, which is not directly related to the planning capabilities of LLMs.",
      "processing_time": 67.14879775047302,
      "citing_paper_id": "271065021",
      "cited_paper_id": 202538486
    },
    {
      "context_text": "The other five are no-press diplomacy models, including the SL-DipNet and RL-DipNet [39], the BRPI [3], the SearchBot [18], and the DORA[5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (SL-DipNet, RL-DipNet, BRPI, SearchBot, DORA) but does not refer to any specific datasets. These models are used for no-press diplomacy in multi-agent gameplay, which is not directly related to the planning capabilities of LLMs.",
      "processing_time": 67.14879775047302,
      "citing_paper_id": "271065021",
      "cited_paper_id": 238408283
    },
    {
      "context_text": "We let Richelieu compete with the other six models including Cicero[6], SL-DipNet and RL-DipNet [39], BRPI [3], SearchBot [18], and DORA[5] on No-Press Diplomacy, in which players make moves without communication.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and a game environment. No-Press Diplomacy is a game environment, not a dataset.",
      "processing_time": 61.63558030128479,
      "citing_paper_id": "271065021",
      "cited_paper_id": 202538486
    },
    {
      "context_text": "We let Richelieu compete with the other six models including Cicero[6], SL-DipNet and RL-DipNet [39], BRPI [3], SearchBot [18], and DORA[5] on No-Press Diplomacy, in which players make moves without communication.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and a game environment. No-Press Diplomacy is a game environment, not a dataset.",
      "processing_time": 61.63558030128479,
      "citing_paper_id": "271065021",
      "cited_paper_id": 238408283
    },
    {
      "context_text": "We let Richelieu compete with the other six models including Cicero[6], SL-DipNet and RL-DipNet [39], BRPI [3], SearchBot [18], and DORA[5] on No-Press Diplomacy, in which players make moves without communication.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and a game environment. No-Press Diplomacy is a game environment, not a dataset.",
      "processing_time": 61.63558030128479,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "The widely-used open source Diplomacy game platform introduced by [39] is adopted for evaluating Richelieu against other models.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a game platform but does not specify a dataset. The platform is used for evaluation, but it is not a dataset.",
      "processing_time": 61.13398218154907,
      "citing_paper_id": "271065021",
      "cited_paper_id": 202538486
    },
    {
      "context_text": "We will also extend the framework to other multi-agent scenarios, including embodied interactions [75, 11, 9], sensor networks [54, 61, 38, 31], and video games [49, 34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.45841336250305,
      "citing_paper_id": "271065021",
      "cited_paper_id": 225066700
    },
    {
      "context_text": "We will also extend the framework to other multi-agent scenarios, including embodied interactions [75, 11, 9], sensor networks [54, 61, 38, 31], and video games [49, 34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.45841336250305,
      "citing_paper_id": "271065021",
      "cited_paper_id": 258041253
    },
    {
      "context_text": "We will also extend the framework to other multi-agent scenarios, including embodied interactions [75, 11, 9], sensor networks [54, 61, 38, 31], and video games [49, 34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.45841336250305,
      "citing_paper_id": "271065021",
      "cited_paper_id": 258509586
    },
    {
      "context_text": "We will also extend the framework to other multi-agent scenarios, including embodied interactions [75, 11, 9], sensor networks [54, 61, 38, 31], and video games [49, 34].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 61.45841336250305,
      "citing_paper_id": "271065021",
      "cited_paper_id": 267081115
    },
    {
      "context_text": "Based on DipNet, BRPI [3], SearchBot [18], DORA [5], and KL-Regularized search (Diplodocus) [24] were conducted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 60.95428204536438,
      "citing_paper_id": "271065021",
      "cited_paper_id": 238408283
    },
    {
      "context_text": "[74, 58, 53] At each turn, the agent will run in the following steps: 1) Social Reasoning: First of all, the agent undergoes a comprehensive analysis of the game state s t to build the social belief, including the intention of other players and their relationship ⃗ϕ t ∈ Φ n .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a process for social reasoning in a game context. No verifiable resources are identified.",
      "processing_time": 61.28342938423157,
      "citing_paper_id": "271065021",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "In each turn,[58, 53] the agent i gets the current state s t ∈ S , the actions of other players from the previous turn ⃗a − i t − 1 , and the messages ⃗m − i,i t from other players during this turn’s negotiations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the state and action information used by agents in a multi-agent system.",
      "processing_time": 60.53433966636658,
      "citing_paper_id": "271065021",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Besides [55][50][56][76][62] apply LLM agents to the complex planning tasks in the well-known open-world game Minecraft[16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'complex planning tasks in the well-known open-world game Minecraft' but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 62.6183135509491,
      "citing_paper_id": "271065021",
      "cited_paper_id": 249848263
    },
    {
      "context_text": "Besides [55][50][56][76][62] apply LLM agents to the complex planning tasks in the well-known open-world game Minecraft[16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'complex planning tasks in the well-known open-world game Minecraft' but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 62.6183135509491,
      "citing_paper_id": "271065021",
      "cited_paper_id": 266690872
    },
    {
      "context_text": "Besides [55][50][56][76][62] apply LLM agents to the complex planning tasks in the well-known open-world game Minecraft[16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'complex planning tasks in the well-known open-world game Minecraft' but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 62.6183135509491,
      "citing_paper_id": "271065021",
      "cited_paper_id": 268042457
    },
    {
      "context_text": "And Meta AI’s work, instead of one unified architecture, Cicero [6] integrates a language model for negotiation and an RL model for planning respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of a language model and an RL model for planning and negotiation.",
      "processing_time": 62.28918814659119,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "The recent breakthrough [6] has turned into press diplomacy, which allows communication between players.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a breakthrough in the game of Diplomacy involving language models and strategic reasoning.",
      "processing_time": 61.20258808135986,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "However, the previous methods [6] heavily rely on domain-specific human data, leading to its poor generalization to other scenarios/ applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reliance on domain-specific human data. No clear, verifiable dataset names are provided.",
      "processing_time": 61.591126441955566,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "As shown in Figure 5, Richelieu’s performance against Cicero [6] becomes better with increasing training iterations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models (Richelieu and Cicero). No verifiable resources are identified.",
      "processing_time": 61.89814329147339,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "2) We demonstrate the superior performance of our agent playing against the SOTA method, e.g., Cicero [6], that relies on a large-scale human demonstration for training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Cicero) that uses large-scale human demonstrations, which is not a specific dataset.",
      "processing_time": 62.80565142631531,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "Among them, Cicero[6] by Meta is a diplomacy model with a negotiation module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Cicero, which is a model, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 61.14280152320862,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "By integrating these three capabilities, the agent can operate at the highest level of diplomatic sophistication, outperforming the state-of-the-art AI diplomats [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper that combines language models with strategic reasoning for playing Diplomacy.",
      "processing_time": 61.509434938430786,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "Such studies [13][6][25][29] mainly benefit from the recent thriving language models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'language models'. No specific, verifiable datasets are named.",
      "processing_time": 61.513723850250244,
      "citing_paper_id": "271065021",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "Such studies [13][6][25][29] mainly benefit from the recent thriving language models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'language models'. No specific, verifiable datasets are named.",
      "processing_time": 61.513723850250244,
      "citing_paper_id": "271065021",
      "cited_paper_id": 254277011
    },
    {
      "context_text": "Such studies [13][6][25][29] mainly benefit from the recent thriving language models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'language models'. No specific, verifiable datasets are named.",
      "processing_time": 61.513723850250244,
      "citing_paper_id": "271065021",
      "cited_paper_id": 265213232
    },
    {
      "context_text": "However, Deepmind propose to learn negotiation agents based on predefined contracts/protocols [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for learning negotiation agents. The context is about the methodology rather than a reusable dataset.",
      "processing_time": 61.84976315498352,
      "citing_paper_id": "271065021",
      "cited_paper_id": 254277011
    },
    {
      "context_text": "Related work including AutoGPT, AgentGPT, BabyAGl [47], Toolformer [43], and Visual ChatGPT aim to improve LLM capabilities in task automation and tool usage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 60.446125745773315,
      "citing_paper_id": "271065021",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Related work including AutoGPT, AgentGPT, BabyAGl [47], Toolformer [43], and Visual ChatGPT aim to improve LLM capabilities in task automation and tool usage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 60.446125745773315,
      "citing_paper_id": "271065021",
      "cited_paper_id": 259088724
    },
    {
      "context_text": "Such systems include HuggingGPT [44], GPT4Tools [63] and ToT [65], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions systems but does not refer to any specific datasets, models, or methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 62.36510109901428,
      "citing_paper_id": "271065021",
      "cited_paper_id": 257833781
    },
    {
      "context_text": "Such systems include HuggingGPT [44], GPT4Tools [63] and ToT [65], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions systems but does not refer to any specific datasets, models, or methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 62.36510109901428,
      "citing_paper_id": "271065021",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Such systems include HuggingGPT [44], GPT4Tools [63] and ToT [65], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions systems but does not refer to any specific datasets, models, or methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 62.36510109901428,
      "citing_paper_id": "271065021",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "With the emergence and growth of large language models (LLM), there is a growing trend in utilizing LLMs as fundamental controllers for autonomous agents[52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in using LLMs for autonomous agents.",
      "processing_time": 60.45101761817932,
      "citing_paper_id": "271065021",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "The previous applications on personal assistants [32], robotics [10, 64], and video games [48] have shown the surprising ability of LLM-based agents in communication and planning, benefiting from the emergent ability of common sense reasoning, in-context/ few-shot Preprint.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based agents. No clear identifiers for datasets are present.",
      "processing_time": 61.493237257003784,
      "citing_paper_id": "271065021",
      "cited_paper_id": 262044464
    },
    {
      "context_text": "The previous applications on personal assistants [32], robotics [10, 64], and video games [48] have shown the surprising ability of LLM-based agents in communication and planning, benefiting from the emergent ability of common sense reasoning, in-context/ few-shot Preprint.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based agents. No clear identifiers for datasets are present.",
      "processing_time": 61.493237257003784,
      "citing_paper_id": "271065021",
      "cited_paper_id": 269745421
    },
    {
      "context_text": "Specifically, notable advancements include policy iteration methods from DeepMind and Facebook AI Research’s equilibrium search agent [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only policy iteration methods and an equilibrium search agent. No verifiable resources are identified.",
      "processing_time": 61.118321895599365,
      "citing_paper_id": "271065021",
      "cited_paper_id": 265213232
    },
    {
      "context_text": "It not only requires planning long-horizon strategic [40] and communicating with natural language, but also reasoning and adopting the complex social dynamics with partial observations, including gaining trust and reputation, building rapport, detecting deception, and assessing the reliability of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities required for agents in a game-like environment.",
      "processing_time": 60.20632982254028,
      "citing_paper_id": "271065021",
      "cited_paper_id": 267061073
    },
    {
      "context_text": "[ 60, 35] To counteract the challenge of non-binding agreements and potential deception, we incorporate a discrete module dedicated to the assessment of the veracity of statements made by other players during negotiations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to assess the veracity of statements in negotiations.",
      "processing_time": 60.70219039916992,
      "citing_paper_id": "271065021",
      "cited_paper_id": 267938212
    },
    {
      "context_text": "[51, 57] 3) Negotiator: To achieve the sub-goals, the negotiator will start a dialogue session with some players, and evaluate their trueness ⃗ψ − i t by referring to their words ⃗m − i,i t , the current state s t , their sincerity ⃗γ − i t and the experience ⃗ξ t .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for evaluating player trueness in a dialogue session, which is part of a larger system involving a negotiator.",
      "processing_time": 63.45196080207825,
      "citing_paper_id": "271065021",
      "cited_paper_id": 268681670
    },
    {
      "context_text": "[51, 57] 3) Negotiator: To achieve the sub-goals, the negotiator will start a dialogue session with some players, and evaluate their trueness ⃗ψ − i t by referring to their words ⃗m − i,i t , the current state s t , their sincerity ⃗γ − i t and the experience ⃗ξ t .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for evaluating player trueness in a dialogue session, which is part of a larger system involving a negotiator.",
      "processing_time": 63.45196080207825,
      "citing_paper_id": "271065021",
      "cited_paper_id": 270062501
    },
    {
      "context_text": "[71, 19] 2) Planner with Reflection: Then, the agent proposes sub-goals χ i t ∈ X that is strategically aligned with the long-term goals Υ , with the social belief and refining the proposed goal with experience ⃗η t ∈ H m abstract from the memory M via self-reflection.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concepts related to strategic reasoning and social simulations. No verifiable resources are identified.",
      "processing_time": 61.23241114616394,
      "citing_paper_id": "271065021",
      "cited_paper_id": 268819803
    },
    {
      "context_text": "[71, 19] 2) Planner with Reflection: Then, the agent proposes sub-goals χ i t ∈ X that is strategically aligned with the long-term goals Υ , with the social belief and refining the proposed goal with experience ⃗η t ∈ H m abstract from the memory M via self-reflection.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concepts related to strategic reasoning and social simulations. No verifiable resources are identified.",
      "processing_time": 61.23241114616394,
      "citing_paper_id": "271065021",
      "cited_paper_id": 269757934
    },
    {
      "context_text": "Consequently, we need to identify the intention and relationship of the current state by social reasoning to shape the social belief [71, 19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for social reasoning to shape social belief. No verifiable resources are identified.",
      "processing_time": 61.40447163581848,
      "citing_paper_id": "271065021",
      "cited_paper_id": 268819803
    },
    {
      "context_text": "Consequently, we need to identify the intention and relationship of the current state by social reasoning to shape the social belief [71, 19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for social reasoning to shape social belief. No verifiable resources are identified.",
      "processing_time": 61.40447163581848,
      "citing_paper_id": "271065021",
      "cited_paper_id": 269757934
    },
    {
      "context_text": "And Reflexion, a framework that improves LLMs through linguistic feedback and episodic memory [68], facilitating better decision-making across diverse tasks is proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework called Reflexion. No verifiable resources are identified.",
      "processing_time": 60.90346336364746,
      "citing_paper_id": "271065021",
      "cited_paper_id": 268876709
    },
    {
      "context_text": "[41, 70]For example, it is common for a non-neighboring country to become too powerful.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It discusses a general observation about countries becoming powerful, which is not relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 63.44146656990051,
      "citing_paper_id": "271065021",
      "cited_paper_id": 269757480
    },
    {
      "context_text": "[12] To this end, Richelieu can also adopt a strategy of attacking distant countries while allying with nearby ones, sacrificing short-term benefits to avoid the emergence of future hegemonic states that could threaten his own survival.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It describes a strategic approach in a geopolitical context, which is not directly related to planning capabilities of LLMs.",
      "processing_time": 62.35783672332764,
      "citing_paper_id": "271065021",
      "cited_paper_id": null
    },
    {
      "context_text": "Others may lie and deceive [27]; their words in negotiations are not binding.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is a general statement about human behavior.",
      "processing_time": 60.26055884361267,
      "citing_paper_id": "271065021",
      "cited_paper_id": null
    },
    {
      "context_text": "3) Architecture and Baselines: We compare LLM-Personalize with state-of-the-art LLM planning baseline methods: LLM-Planner [1], SayPlan [3] and SayCan [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. No verifiable resources are identified.",
      "processing_time": 60.26927638053894,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "3) Architecture and Baselines: We compare LLM-Personalize with state-of-the-art LLM planning baseline methods: LLM-Planner [1], SayPlan [3] and SayCan [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. No verifiable resources are identified.",
      "processing_time": 60.26927638053894,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Closely related to our work are [1], [2], [3]: [2] performs grounding of LLM planners with affordance functions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'grounding of LLM planners with affordance functions' which aligns with the title of Cited Paper 1. However, no specific dataset is mentioned in the context.",
      "processing_time": 63.278751850128174,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Closely related to our work are [1], [2], [3]: [2] performs grounding of LLM planners with affordance functions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'grounding of LLM planners with affordance functions' which aligns with the title of Cited Paper 1. However, no specific dataset is mentioned in the context.",
      "processing_time": 63.278751850128174,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "The linguistic abilities of LLMs enable a wide range of applications, e.g., in task planning, LLMs can leverage their capabilities to interpret complex context and generate detailed, coherent plans [1], [2], [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs in task planning. No verifiable resources are identified.",
      "processing_time": 61.49182415008545,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "The linguistic abilities of LLMs enable a wide range of applications, e.g., in task planning, LLMs can leverage their capabilities to interpret complex context and generate detailed, coherent plans [1], [2], [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs in task planning. No verifiable resources are identified.",
      "processing_time": 61.49182415008545,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Prior works in task planning have effectively utilised pre-trained LLMs for generating executable plans for robotic agents [1], [2], [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works using pre-trained LLMs for task planning in robotics.",
      "processing_time": 61.1822235584259,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Prior works in task planning have effectively utilised pre-trained LLMs for generating executable plans for robotic agents [1], [2], [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works using pre-trained LLMs for task planning in robotics.",
      "processing_time": 61.1822235584259,
      "citing_paper_id": "269293529",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Prior works in task planning have effectively utilised pre-trained LLMs for generating executable plans for robotic agents [1], [2], [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works using pre-trained LLMs for task planning in robotics.",
      "processing_time": 61.1822235584259,
      "citing_paper_id": "269293529",
      "cited_paper_id": 258212642
    },
    {
      "context_text": "Prior works in task planning have effectively utilised pre-trained LLMs for generating executable plans for robotic agents [1], [2], [3], [4], [5], [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to prior works using pre-trained LLMs for task planning in robotics.",
      "processing_time": 61.1822235584259,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "We show in our experiments that LLM-Personalize outperforms state-of-the-art baseline LLM-based planners [1], [2], [3] with more than a 30 percent increase in success rate, as a result of improved understanding and alignment with human preferences.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison of performance between different models. No verifiable resources are identified.",
      "processing_time": 61.014458894729614,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "We show in our experiments that LLM-Personalize outperforms state-of-the-art baseline LLM-based planners [1], [2], [3] with more than a 30 percent increase in success rate, as a result of improved understanding and alignment with human preferences.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison of performance between different models. No verifiable resources are identified.",
      "processing_time": 61.014458894729614,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "The application of large language models (LLMs) to the robotics domain has demonstrated substantial potential, especially in the realm of task planning [1], [2], [3], [4], [5], [6], [7], [8], by leveraging their advanced language comprehension and text generation capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general application of LLMs in robotics and task planning. No verifiable resources are identified.",
      "processing_time": 61.83589839935303,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "The application of large language models (LLMs) to the robotics domain has demonstrated substantial potential, especially in the realm of task planning [1], [2], [3], [4], [5], [6], [7], [8], by leveraging their advanced language comprehension and text generation capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general application of LLMs in robotics and task planning. No verifiable resources are identified.",
      "processing_time": 61.83589839935303,
      "citing_paper_id": "269293529",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "The application of large language models (LLMs) to the robotics domain has demonstrated substantial potential, especially in the realm of task planning [1], [2], [3], [4], [5], [6], [7], [8], by leveraging their advanced language comprehension and text generation capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general application of LLMs in robotics and task planning. No verifiable resources are identified.",
      "processing_time": 61.83589839935303,
      "citing_paper_id": "269293529",
      "cited_paper_id": 258212642
    },
    {
      "context_text": "The application of large language models (LLMs) to the robotics domain has demonstrated substantial potential, especially in the realm of task planning [1], [2], [3], [4], [5], [6], [7], [8], by leveraging their advanced language comprehension and text generation capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general application of LLMs in robotics and task planning. No verifiable resources are identified.",
      "processing_time": 61.83589839935303,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Prior works on LLM grounding have proposed to align LLMs with the tasks’ physical context through methods such as translating generated plans to executable actions [4], integrating contextual information such as affordance [2], [8], scene graph [3] or environmental feedback [3], [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for grounding LLMs but does not mention any specific datasets. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 62.16209816932678,
      "citing_paper_id": "269293529",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Prior works on LLM grounding have proposed to align LLMs with the tasks’ physical context through methods such as translating generated plans to executable actions [4], integrating contextual information such as affordance [2], [8], scene graph [3] or environmental feedback [3], [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for grounding LLMs but does not mention any specific datasets. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 62.16209816932678,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Substantial progress have been made in LLM alignment including reinforcement learning (RL) based methods such as [13], [14], [9], [15], [16] and supervised learning (SL) based methods such as [17], [18], [19], [9], [20], and self-training [9] uses either an RL or SL objective.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methods and approaches. No verifiable resources are identified.",
      "processing_time": 60.85074520111084,
      "citing_paper_id": "269293529",
      "cited_paper_id": 252596089
    },
    {
      "context_text": "Substantial progress have been made in LLM alignment including reinforcement learning (RL) based methods such as [13], [14], [9], [15], [16] and supervised learning (SL) based methods such as [17], [18], [19], [9], [20], and self-training [9] uses either an RL or SL objective.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methods and approaches. No verifiable resources are identified.",
      "processing_time": 60.85074520111084,
      "citing_paper_id": "269293529",
      "cited_paper_id": 258685337
    },
    {
      "context_text": "[11] used LLMs for inferring rules summarizing personalized user preferences, in contrast, our work studies optimization and personalization of LLM planners for complex plan/action sequencing in multi-room household scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of research approaches.",
      "processing_time": 59.05327391624451,
      "citing_paper_id": "269293529",
      "cited_paper_id": 258564887
    },
    {
      "context_text": "[3] addresses the scalability problem with a static 3D scene graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for addressing scalability with 3D scene graphs.",
      "processing_time": 60.80740928649902,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "For SayPlan, we additionally provide the state and affordance of the receptacles and objects (e.g., pick up) as in [3] and allow 10 LLM semantic search steps and 5 revision (re-plan) retries for each plan iteration where the final revised plan is executed, and the revision feedback is provided by a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (SayPlan) and its usage but does not reference any dataset by name.",
      "processing_time": 61.976330280303955,
      "citing_paper_id": "269293529",
      "cited_paper_id": 259837542
    },
    {
      "context_text": "Inspired by classical planning literature (Bonet and Geffner, 2001; Hoffmann and Nebel, 2001; Chitnis et al., 2016; Gehring et al., 2022) that uses heuristic functions as dense reward generators to perform informed search, recent works (Lin et al., 2023; Hao et al., 2023) proposed to use the LLM as…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planning literature and recent works. No verifiable resources are identified.",
      "processing_time": 61.028977394104004,
      "citing_paper_id": "267938891",
      "cited_paper_id": 983645
    },
    {
      "context_text": "Inspired by classical planning literature (Bonet and Geffner, 2001; Hoffmann and Nebel, 2001; Chitnis et al., 2016; Gehring et al., 2022) that uses heuristic functions as dense reward generators to perform informed search, recent works (Lin et al., 2023; Hao et al., 2023) proposed to use the LLM as…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to classical planning literature and recent works. No verifiable resources are identified.",
      "processing_time": 61.028977394104004,
      "citing_paper_id": "267938891",
      "cited_paper_id": 11337237
    },
    {
      "context_text": "Additionally, research has ventured into harnessing more expansive language applications to model the dynamics and reward mechanisms of environments, employing planning algorithms to guide decision-making processes (Bialystok, 1978; Liu et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 60.24159336090088,
      "citing_paper_id": "267938891",
      "cited_paper_id": 144110805
    },
    {
      "context_text": "Additionally, research has ventured into harnessing more expansive language applications to model the dynamics and reward mechanisms of environments, employing planning algorithms to guide decision-making processes (Bialystok, 1978; Liu et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 60.24159336090088,
      "citing_paper_id": "267938891",
      "cited_paper_id": 263310943
    },
    {
      "context_text": "…al., 2019; Li et al., 2019; Radford et al., 2021; Zellers et al., 2021b) by learning additional downstream networks on top of the pre-trained LLM (Lynch and Sermanet, 2020a; Akakzia et al., 2020; Zellers et al., 2021a) or finetuning in the environment (Reid et al., 2022; Li et al., 2022; Chen et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. The context is about training and fine-tuning LLMs, but no datasets are explicitly named.",
      "processing_time": 62.862717628479004,
      "citing_paper_id": "267938891",
      "cited_paper_id": 218665356
    },
    {
      "context_text": "…al., 1999; Mnih et al., 2013; Zhang, 2022; Zhang et al., 2024a), enabling them to learn from high-level specifications of goals (Jiang et al., 2019; Lynch and Sermanet, 2020b; Hejna et al., 2023) or to benefit from the step-by-step instructions provided by large language models (LLMs) (Ahn et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and models. There are no clear identifiers for datasets within the given context.",
      "processing_time": 61.44054555892944,
      "citing_paper_id": "267938891",
      "cited_paper_id": 218665356
    },
    {
      "context_text": "For the baselines, ReAct (Yao et al., 2022), AdaPlanner (Sun et al., 2023), and Reflexion (Shinn et al., 2023) all use GPT-3 as their LLM agents, while BUTLER (Shridhar et al., 2020) is a RL-style imitation learning algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the LLM agents and algorithms used in various systems.",
      "processing_time": 61.44251990318298,
      "citing_paper_id": "267938891",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "In this section, we conduct empirical studies in several text-based benchmarks, including the embodied environment ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the standard planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld' and 'InterCode' as environments used for empirical studies, but these are not datasets. 'BlocksWorld' is a standard planning benchmark, which could be considered a dataset, but it lacks a specific identifier.",
      "processing_time": 65.48790216445923,
      "citing_paper_id": "267938891",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "In this section, we conduct empirical studies in several text-based benchmarks, including the embodied environment ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the standard planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld' and 'InterCode' as environments used for empirical studies, but these are not datasets. 'BlocksWorld' is a standard planning benchmark, which could be considered a dataset, but it lacks a specific identifier.",
      "processing_time": 65.48790216445923,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "Notably, the inferior performance of BUTLER indicates that RL algorithms may have difficulties understanding the task and generalize beyond.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of an algorithm. There are no clear identifiers for datasets in the text.",
      "processing_time": 60.83826565742493,
      "citing_paper_id": "267938891",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We further present a practical algorithm called SLINVIT and empirically validate it in various benchmarks, including ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFWorld",
        "InterCode",
        "BlocksWorld"
      ],
      "dataset_descriptions": {
        "ALFWorld": "Used to validate the SLINVIT algorithm in an embodied environment, focusing on aligning text and interactive learning scenarios.",
        "InterCode": "Used to validate the SLINVIT algorithm in an interactive coding environment, focusing on standardizing and benchmarking with execution feedback.",
        "BlocksWorld": "Used to validate the SLINVIT algorithm in a planning benchmark, focusing on evaluating planning capabilities in a controlled environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three benchmarks: ALFWorld, InterCode, and BlocksWorld. ALFWorld and InterCode are specific environments used for empirical validation, while BlocksWorld is a planning benchmark. These are used to validate the SLINVIT algorithm.",
      "processing_time": 84.10737562179565,
      "citing_paper_id": "267938891",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We further present a practical algorithm called SLINVIT and empirically validate it in various benchmarks, including ALFWorld (Shridhar et al., 2020), the interactive coding environment InterCode (Yang et al., 2023a), and the planning benchmark BlocksWorld (Valmeekam et al., 2023b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFWorld",
        "InterCode",
        "BlocksWorld"
      ],
      "dataset_descriptions": {
        "ALFWorld": "Used to validate the SLINVIT algorithm in an embodied environment, focusing on aligning text and interactive learning scenarios.",
        "InterCode": "Used to validate the SLINVIT algorithm in an interactive coding environment, focusing on standardizing and benchmarking with execution feedback.",
        "BlocksWorld": "Used to validate the SLINVIT algorithm in a planning benchmark, focusing on evaluating planning capabilities in a controlled environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three benchmarks: ALFWorld, InterCode, and BlocksWorld. ALFWorld and InterCode are specific environments used for empirical validation, while BlocksWorld is a planning benchmark. These are used to validate the SLINVIT algorithm.",
      "processing_time": 84.10737562179565,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "ALFWorld (Shridhar et al., 2020) is an interactive text-based environment with aligned embodied simulators.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFWorld, which is an interactive text-based environment, not a dataset. No specific dataset is mentioned.",
      "processing_time": 60.46198487281799,
      "citing_paper_id": "267938891",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…data (Suglia et al., 2021; Sharma et al., 2021; Mezghani et al., 2023; Driess et al., 2023) or multi-modal data (Lu et al., 2019; Li et al., 2019; Radford et al., 2021; Zellers et al., 2021b) by learning additional downstream networks on top of the pre-trained LLM (Lynch and Sermanet, 2020a;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It refers to data in a generic sense and cites papers that discuss methods or models.",
      "processing_time": 61.551515102386475,
      "citing_paper_id": "267938891",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "…Li et al., 2019; Radford et al., 2021; Zellers et al., 2021b) by learning additional downstream networks on top of the pre-trained LLM (Lynch and Sermanet, 2020a; Akakzia et al., 2020; Zellers et al., 2021a) or finetuning in the environment (Reid et al., 2022; Li et al., 2022; Chen et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited paper title 'PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World' does not provide additional context to identify a dataset.",
      "processing_time": 65.98802256584167,
      "citing_paper_id": "267938891",
      "cited_paper_id": 235266260
    },
    {
      "context_text": "…2021; Sharma et al., 2021; Mezghani et al., 2023; Driess et al., 2023) or multi-modal data (Lu et al., 2019; Li et al., 2019; Radford et al., 2021; Zellers et al., 2021b) by learning additional downstream networks on top of the pre-trained LLM (Lynch and Sermanet, 2020a; Akakzia et al., 2020;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 61.208361864089966,
      "citing_paper_id": "267938891",
      "cited_paper_id": 235266260
    },
    {
      "context_text": "…on the web-scale corpora, Large Language Models (LLMs) have exhibited emergent capabilities and seen tremendous success across various fields, such as code development (Chen et al., 2021; Roziere et al., 2023; Li et al., 2023) and theorem proving (Yang et al., 2023b; Romera-Paredes et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to web-scale corpora and success in code development and theorem proving.",
      "processing_time": 60.66790962219238,
      "citing_paper_id": "267938891",
      "cited_paper_id": 235755472
    },
    {
      "context_text": "…on the web-scale corpora, Large Language Models (LLMs) have exhibited emergent capabilities and seen tremendous success across various fields, such as code development (Chen et al., 2021; Roziere et al., 2023; Li et al., 2023) and theorem proving (Yang et al., 2023b; Romera-Paredes et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to web-scale corpora and success in code development and theorem proving.",
      "processing_time": 60.66790962219238,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262077
    },
    {
      "context_text": "…on the web-scale corpora, Large Language Models (LLMs) have exhibited emergent capabilities and seen tremendous success across various fields, such as code development (Chen et al., 2021; Roziere et al., 2023; Li et al., 2023) and theorem proving (Yang et al., 2023b; Romera-Paredes et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to web-scale corpora and success in code development and theorem proving.",
      "processing_time": 60.66790962219238,
      "citing_paper_id": "267938891",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Besides, other works directly train the model on embodied decision-making data (Suglia et al., 2021; Sharma et al., 2021; Mezghani et al., 2023; Driess et al., 2023) or multi-modal data (Lu et al., 2019; Li et al., 2019; Radford et al., 2021; Zellers et al., 2021b) by learning additional downstream…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'embodied decision-making data' and 'multi-modal data' but does not provide specific, identifiable dataset names. These are generic references to types of data rather than specific datasets.",
      "processing_time": 63.332043170928955,
      "citing_paper_id": "267938891",
      "cited_paper_id": 238259902
    },
    {
      "context_text": "…Zhang, 2022; Zhang et al., 2024a), enabling them to learn from high-level specifications of goals (Jiang et al., 2019; Lynch and Sermanet, 2020b; Hejna et al., 2023) or to benefit from the step-by-step instructions provided by large language models (LLMs) (Ahn et al., 2022; Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 61.35905885696411,
      "citing_paper_id": "267938891",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "…Zhang, 2022; Zhang et al., 2024a), enabling them to learn from high-level specifications of goals (Jiang et al., 2019; Lynch and Sermanet, 2020b; Hejna et al., 2023) or to benefit from the step-by-step instructions provided by large language models (LLMs) (Ahn et al., 2022; Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 61.35905885696411,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259224572
    },
    {
      "context_text": "Similar to our work, Ahn et al. (2022); Hu and Sadigh (2023); Lin et al. (2023); Hao et al. (2023) also use value functions to ground the LLM agent, but the sub-problem horizon is set to 1 and the executed actions are one-step greedy without backtracking, which still suffers from the curse of the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing methodologies rather than using specific datasets.",
      "processing_time": 60.41752815246582,
      "citing_paper_id": "267938891",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "…by the LLM have also enabled converting natural language instructions into planning languages and then adopting the classical planner (Liu et al., 2023a; Liang et al., 2023; Silver et al., 2023; Xie et al., 2023), which, however, are constrained in narrowed domains and predefined environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches using LLMs for planning. No verifiable resources are identified.",
      "processing_time": 61.269909143447876,
      "citing_paper_id": "267938891",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "…by the LLM have also enabled converting natural language instructions into planning languages and then adopting the classical planner (Liu et al., 2023a; Liang et al., 2023; Silver et al., 2023; Xie et al., 2023), which, however, are constrained in narrowed domains and predefined environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches using LLMs for planning. No verifiable resources are identified.",
      "processing_time": 61.269909143447876,
      "citing_paper_id": "267938891",
      "cited_paper_id": 256808659
    },
    {
      "context_text": "…by the LLM have also enabled converting natural language instructions into planning languages and then adopting the classical planner (Liu et al., 2023a; Liang et al., 2023; Silver et al., 2023; Xie et al., 2023), which, however, are constrained in narrowed domains and predefined environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches using LLMs for planning. No verifiable resources are identified.",
      "processing_time": 61.269909143447876,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258762760
    },
    {
      "context_text": "The recent advances in robotics (Huang et al., 2023b; Liang et al., 2023) and games (Wang et al., 2023a; Yuan et al., 2023; Wang et al., 2023c; Liu et al., 2023b) further highlight the potential of LLMs to build effective agents in well-designed interactive environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in robotics and games. No verifiable resources are identified.",
      "processing_time": 60.184879302978516,
      "citing_paper_id": "267938891",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "The recent advances in robotics (Huang et al., 2023b; Liang et al., 2023) and games (Wang et al., 2023a; Yuan et al., 2023; Wang et al., 2023c; Liu et al., 2023b) further highlight the potential of LLMs to build effective agents in well-designed interactive environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in robotics and games. No verifiable resources are identified.",
      "processing_time": 60.184879302978516,
      "citing_paper_id": "267938891",
      "cited_paper_id": 257805102
    },
    {
      "context_text": "The recent advances in robotics (Huang et al., 2023b; Liang et al., 2023) and games (Wang et al., 2023a; Yuan et al., 2023; Wang et al., 2023c; Liu et al., 2023b) further highlight the potential of LLMs to build effective agents in well-designed interactive environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in robotics and games. No verifiable resources are identified.",
      "processing_time": 60.184879302978516,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259837330
    },
    {
      "context_text": "The recent advances in robotics (Huang et al., 2023b; Liang et al., 2023) and games (Wang et al., 2023a; Yuan et al., 2023; Wang et al., 2023c; Liu et al., 2023b) further highlight the potential of LLMs to build effective agents in well-designed interactive environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in robotics and games. No verifiable resources are identified.",
      "processing_time": 60.184879302978516,
      "citing_paper_id": "267938891",
      "cited_paper_id": 263310943
    },
    {
      "context_text": "Following RAP Hao et al. (2023), we group the task instances in Valmeekam et al. (2023b) by the minimum number of actions required, leading to 57 cases solvable within 4 steps, and 114 cases within 6 steps.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only task instances grouped by the number of actions required. No clear, verifiable dataset names are provided.",
      "processing_time": 61.255574464797974,
      "citing_paper_id": "267938891",
      "cited_paper_id": 256846992
    },
    {
      "context_text": "However, the reasoning and planning abilities of LLMs, which are important metrics for intelligent agents, have been found to be inconsistent and often unreliable (Valmeekam et al., 2023b; Mahowald et al., 2023; Huang et al., 2023a; Pallagani et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that discuss the planning and reasoning abilities of LLMs.",
      "processing_time": 60.741774797439575,
      "citing_paper_id": "267938891",
      "cited_paper_id": 256846992
    },
    {
      "context_text": "However, the reasoning and planning abilities of LLMs, which are important metrics for intelligent agents, have been found to be inconsistent and often unreliable (Valmeekam et al., 2023b; Mahowald et al., 2023; Huang et al., 2023a; Pallagani et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that discuss the planning and reasoning abilities of LLMs.",
      "processing_time": 60.741774797439575,
      "citing_paper_id": "267938891",
      "cited_paper_id": 263609132
    },
    {
      "context_text": "BlocksWorld (Valmeekam et al., 2023b; Liu et al., 2023a) is another planning benchmark that contains various tasks to arrange blocks in specific configurations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BlocksWorld"
      ],
      "dataset_descriptions": {
        "BlocksWorld": "Used as a planning benchmark to evaluate the ability of large language models to arrange blocks in specific configurations, focusing on their planning proficiency."
      },
      "confidence_score": 0.9,
      "reasoning": "The term 'BlocksWorld' is mentioned as a planning benchmark containing various tasks. It is specific and relevant to the research topic of planning capabilities of LLMs.",
      "processing_time": 69.21826934814453,
      "citing_paper_id": "267938891",
      "cited_paper_id": 256846992
    },
    {
      "context_text": "BlocksWorld (Valmeekam et al., 2023b; Liu et al., 2023a) is another planning benchmark that contains various tasks to arrange blocks in specific configurations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BlocksWorld"
      ],
      "dataset_descriptions": {
        "BlocksWorld": "Used as a planning benchmark to evaluate the ability of large language models to arrange blocks in specific configurations, focusing on their planning proficiency."
      },
      "confidence_score": 0.9,
      "reasoning": "The term 'BlocksWorld' is mentioned as a planning benchmark containing various tasks. It is specific and relevant to the research topic of planning capabilities of LLMs.",
      "processing_time": 69.21826934814453,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Moreover, a recent line of work (Yao et al., 2023a,b; Liu et al., 2023b; Sel et al., 2023; Zhang et al., 2023) has developed various prompting schemes to enhance LLM reasoning, though these approaches generally do not integrate feedback from the environment into the decision-making process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only prompting schemes and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 61.43441724777222,
      "citing_paper_id": "267938891",
      "cited_paper_id": 257805102
    },
    {
      "context_text": "Moreover, a recent line of work (Yao et al., 2023a,b; Liu et al., 2023b; Sel et al., 2023; Zhang et al., 2023) has developed various prompting schemes to enhance LLM reasoning, though these approaches generally do not integrate feedback from the environment into the decision-making process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only prompting schemes and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 61.43441724777222,
      "citing_paper_id": "267938891",
      "cited_paper_id": 261049794
    },
    {
      "context_text": "Moreover, a recent line of work (Yao et al., 2023a,b; Liu et al., 2023b; Sel et al., 2023; Zhang et al., 2023) has developed various prompting schemes to enhance LLM reasoning, though these approaches generally do not integrate feedback from the environment into the decision-making process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only prompting schemes and methods. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 61.43441724777222,
      "citing_paper_id": "267938891",
      "cited_paper_id": 263310943
    },
    {
      "context_text": "…exhibited by the LLM have also enabled converting natural language instructions into planning languages and then adopting the classical planner (Liu et al., 2023a; Liang et al., 2023; Silver et al., 2023; Xie et al., 2023), which, however, are constrained in narrowed domains and predefined…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches. The context focuses on the capabilities of LLMs in planning and their limitations.",
      "processing_time": 62.01457142829895,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Among them, the ReAct (Yao et al., 2022) agent generates both reasoning traces and task-specific actions in an interleaved manner, Plan-and-Solve (Wang et al., 2023b) improves the Chain-of-Thought (Wei et al., 2022) prompt to devise a fixed high-level plan before taking actions in the environment, and Huang et al. (2022) prompt the LLM to extract temporally extended plans in a zero-shot manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on describing different agents and prompting techniques for improving planning capabilities in LLMs.",
      "processing_time": 62.231019258499146,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "The baselines we compare include ReAct (Yao et al., 2022), Plan & Solve (Wang et al., 2023b), and Try Again (Yang et al., 2023a), which is a vanilla LLM-based planning algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and algorithms. No verifiable resources are identified.",
      "processing_time": 59.99048852920532,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "The baselines we compare include ReAct (Yao et al., 2022), Plan & Solve (Wang et al., 2023b), and Try Again (Yang et al., 2023a), which is a vanilla LLM-based planning algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and algorithms. No verifiable resources are identified.",
      "processing_time": 59.99048852920532,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "Among them, the ReAct (Yao et al., 2022) agent generates both reasoning traces and task-specific actions in an interleaved manner, Plan-and-Solve (Wang et al., 2023b) improves the Chain-of-Thought (Wei et al., 2022) prompt to devise a fixed high-level plan before taking actions in the environment,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing different agents and their capabilities in planning and solving tasks.",
      "processing_time": 61.2347526550293,
      "citing_paper_id": "267938891",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "This result is further evidenced through experiments in interactive environments such as ALFWorld, InterCode, and BlocksWorld, underscoring our method’s improved sample efficiency.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions interactive environments but does not specify them as datasets. They are likely environments or platforms rather than datasets.",
      "processing_time": 60.15083885192871,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "%) comparison in the InterCode-Bash environment.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison in an environment. No clear, verifiable datasets are identified.",
      "processing_time": 60.34444832801819,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "InterCode-SQL",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific information about datasets used in the research. 'InterCode-SQL' appears to be a method or tool rather than a dataset.",
      "processing_time": 61.892139196395874,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "In Table 4, we investigate the sample efficiency of the proposed algorithm in the InterCode environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "InterCode"
      ],
      "dataset_descriptions": {
        "InterCode": "Used to investigate the sample efficiency of the proposed algorithm in an interactive coding environment with execution feedback, focusing on performance and learning dynamics."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'InterCode environment' which is likely a specific environment or dataset used for interactive coding with execution feedback. However, it is not clear if 'InterCode' is a dataset or a benchmark environment.",
      "processing_time": 70.52191758155823,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "Unlike the ALFWorld environment where the task goals can be described by preconditions, there is no straightforward way to directly measure the value of the current state with explicit and simple rules in the InterCode environment.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only environments (ALFWorld and InterCode). These are not datasets but rather simulation environments used for testing AI systems.",
      "processing_time": 61.6315598487854,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "The success rates in the InterCode-SQL and InterCode-Bash environments are reported in Table 2 and 3, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'InterCode-SQL' and 'InterCode-Bash' environments, which are likely part of the InterCode benchmark. However, these are not datasets but rather environments or benchmarks. No specific datasets are mentioned.",
      "processing_time": 64.0504994392395,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "Besides, we use the original dense reward as proposed in (Yang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to a method or approach. The context is too vague to infer the use of a dataset.",
      "processing_time": 60.82090663909912,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "We observe that SLINVIT achieves the highest success rate in all the hardness modes of the InterCode-SQL benchmark, all the file systems of the InterCode-Bash benchmark, and has the best overall performance.",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not refer to them as specific, downloadable datasets. It focuses on the performance of SLINVIT on these benchmarks.",
      "processing_time": 60.12527322769165,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "InterCode (Yang et al., 2023a) is an interactive coding environment with code as actions and execution feedback as observations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions InterCode, which is described as an interactive coding environment, not a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 60.134443521499634,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "It provides two benchmarks to evaluate the planning abilities of the large language models, namely InterCode-SQL and InterCode-Bash which use SQL and Bash commands as action spaces, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "InterCode-SQL",
        "InterCode-Bash"
      ],
      "dataset_descriptions": {
        "InterCode-SQL": "Used to evaluate the planning abilities of large language models using SQL commands as action spaces, focusing on interactive coding tasks with execution feedback.",
        "InterCode-Bash": "Used to evaluate the planning abilities of large language models using Bash commands as action spaces, focusing on interactive coding tasks with execution feedback."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two benchmarks, InterCode-SQL and InterCode-Bash, which are used to evaluate the planning abilities of large language models. These are specific datasets designed for this purpose.",
      "processing_time": 76.39768815040588,
      "citing_paper_id": "267938891",
      "cited_paper_id": 259262186
    },
    {
      "context_text": "Works building on the self-reflection abilities of LLMs (Shinn et al., 2023; Sun et al., 2023; Ma et al., 2023) also demonstrate limitations in refining initial strategies based on feedback, as shown in our experiments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works that build on the self-reflection abilities of LLMs. There is no indication of a reusable dataset.",
      "processing_time": 61.06747627258301,
      "citing_paper_id": "267938891",
      "cited_paper_id": 264306288
    },
    {
      "context_text": "…LLMs are not specifically attuned to, resulting in significant difficulties in refining their strategies based on environmental feedback by reasoning the environment feedback based solely on its inherent capabilities (Valmeekam et al., 2023a; Shinn et al., 2023; Ivanova, 2023; Zhang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing limitations of LLMs in reasoning about environmental feedback.",
      "processing_time": 58.72346043586731,
      "citing_paper_id": "267938891",
      "cited_paper_id": 265610018
    },
    {
      "context_text": "Third, we consider an important class of bilateral bargaining games, where a seller and a buyer negotiate on the price of a good over a finite number of time steps under complete or incomplete information (Rubinstein, 1982; Fudenberg and Tirole, 1991).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts from game theory.",
      "processing_time": 57.25184893608093,
      "citing_paper_id": "270063284",
      "cited_paper_id": 547566
    },
    {
      "context_text": "Third, we consider an important class of bilateral bargaining games, where a seller and a buyer negotiate on the price of a good over a finite number of time steps under complete or incomplete information (Rubinstein, 1982; Fudenberg and Tirole, 1991).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts from game theory.",
      "processing_time": 57.25184893608093,
      "citing_paper_id": "270063284",
      "cited_paper_id": 14827857
    },
    {
      "context_text": "The optimal decision for either agent, assuming his/her opponent is also acting optimally, i.e., being rational, is to play the Subgame Perfect Equilibrium (SPE) strategy, which, in this setting, is unique and can be computed using backward induction (Fudenberg and Tirole, 1991).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept from game theory.",
      "processing_time": 56.58703255653381,
      "citing_paper_id": "270063284",
      "cited_paper_id": 547566
    },
    {
      "context_text": "Second, we consider a dynamic mechanism design environment, which offers a multi-agent generalization of MDP where the mechanism designer seeks to maximize the cumulative sum of rewards over multiple agents based on agents’ reported reward functions (Bergemann and V¨alim¨aki, 2010, 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework and concepts related to dynamic mechanism design and multi-agent systems.",
      "processing_time": 58.7029972076416,
      "citing_paper_id": "270063284",
      "cited_paper_id": 2548749
    },
    {
      "context_text": "In addition to STRIDE and the aforementioned baselines, we also include UCB-VI algorithm in the experiments, which serves as a reference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and baselines. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.35819983482361,
      "citing_paper_id": "270063284",
      "cited_paper_id": 7559418
    },
    {
      "context_text": "STRIDE reliably implements the behavior of UCB-VI algorithm using the provided operations, and thus converges to the optimal policy at a similar rate as UCB-VI .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and their performance. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.7767813205719,
      "citing_paper_id": "270063284",
      "cited_paper_id": 7559418
    },
    {
      "context_text": "A classic solution to this problem is UCB-VI (Azar et al., 2017), which is used as the reference algorithm for STRIDE.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UCB-VI) used as a reference algorithm.",
      "processing_time": 57.575199604034424,
      "citing_paper_id": "270063284",
      "cited_paper_id": 7559418
    },
    {
      "context_text": "In classic economics literature (Sobel and Takahashi, 1983; Cramton, 1984), this sequence is obtained by: (i) backward induction from time T to time 1, which results in b 0 expressed as a function of b T − 1 ; (ii) as the initial belief b 0 = 1, we can solve this equation to obtain the value of b T…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and methods from economics literature.",
      "processing_time": 57.00082850456238,
      "citing_paper_id": "270063284",
      "cited_paper_id": 17055038
    },
    {
      "context_text": "…e.g., let multiple LLM agents communicate and collaborate through natural language to complete the software development life cycle (Qian et al., 2023), and robotics, e.g., equip LLM with a wide range of manipulation and navigation skills to control a mobile manipulator robot (Ahn et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs in software development and robotics. No verifiable resources are identified.",
      "processing_time": 59.242563009262085,
      "citing_paper_id": "270063284",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "…et al., 2024), such as social simulation, e.g., building virtual environment with LLM-based agents to simulate social phenomena (Park et al., 2023; Aher et al., 2023), scientific research assistant, e.g., utilize LLMs for automating the design, planning, and execution of scientific experiments…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of LLMs. No clear, verifiable datasets are referenced.",
      "processing_time": 58.380645513534546,
      "citing_paper_id": "270063284",
      "cited_paper_id": 251719353
    },
    {
      "context_text": "These models not only exhibit strategic thinking akin to humans (Aher et al., 2022; Kwon et al., 2023) but also demonstrate a remarkable ability to reason flexibly, adeptly handling nuanced and context-specific information (Suzgun et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.32834768295288,
      "citing_paper_id": "270063284",
      "cited_paper_id": 252917648
    },
    {
      "context_text": "These models not only exhibit strategic thinking akin to humans (Aher et al., 2022; Kwon et al., 2023) but also demonstrate a remarkable ability to reason flexibly, adeptly handling nuanced and context-specific information (Suzgun et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.32834768295288,
      "citing_paper_id": "270063284",
      "cited_paper_id": 257255456
    },
    {
      "context_text": "These models not only exhibit strategic thinking akin to humans (Aher et al., 2022; Kwon et al., 2023) but also demonstrate a remarkable ability to reason flexibly, adeptly handling nuanced and context-specific information (Suzgun et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and their capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.32834768295288,
      "citing_paper_id": "270063284",
      "cited_paper_id": 260881158
    },
    {
      "context_text": "Large language models (LLMs) such as GPT-4 have demonstrated exceptional proficiency in generating coherent natural language from textual inputs (Bubeck et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs like GPT-4.",
      "processing_time": 57.12993597984314,
      "citing_paper_id": "270063284",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "However, due to their inherent limitations in basic mathematics (Bubeck et al., 2023), instruction following (Jang et al., 2022), and susceptibility to hallucinations (Chen et al., 2023), the following challenges exist: (i) LLMs may fail to accurately interpret game rules and objectives expressed…",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only limitations and challenges of LLMs. No verifiable resources are identified.",
      "processing_time": 58.28413796424866,
      "citing_paper_id": "270063284",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "…tasks (Wang et al., 2024), such as social simulation, e.g., building virtual environment with LLM-based agents to simulate social phenomena (Park et al., 2023; Aher et al., 2023), scientific research assistant, e.g., utilize LLMs for automating the design, planning, and execution of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks and applications of LLMs. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.31091666221619,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Compared with popular frameworks like ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2024), whose Thought step typically involves a single step of textual reasoning before interacting with the environment, which is depicted as the green region of Figure 1, our design is tailored to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only frameworks and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.24259805679321,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "What sets our work apart from previous LLM agents, such as ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2024), is the sophisticated integration of these operational tools by the Thought sequence to execute complex calculations that traditionally pose challenges for LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other works and their methodologies. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.96682262420654,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Compared with popular frameworks like ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2024), whose Thought step typically involves a single step of textual reasoning before interacting with the environment, which is depicted as the green region of Figure 1, our design is tailored to multi-step reasoning assisted with tools and memory, as shown in the blue part of Figure 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only frameworks and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.216731548309326,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "…most prevalent approach in this domain is the integration of LLMs into Monte Carlo tree search (MCTS) algorithms, where they typically serve as tree traversal policy (Zhao et al., 2024), action pruner (Liu et al., 2023), world model (Hao et al., 2023), and evaluation function (Liu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the integration of LLMs into MCTS algorithms.",
      "processing_time": 58.816102504730225,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "…research, emerging primarily from the reinforcement learning community, instead integrates LLMs into traditional reinforcement learning algorithms to leverage the common sense knowledge that LLMs acquire during pretraining (Hao et al., 2023; Liu et al., 2023; Zhou et al., 2023; Zhao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and their contributions to integrating LLMs with reinforcement learning.",
      "processing_time": 58.255151987075806,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "…research, emerging primarily from the reinforcement learning community, instead integrates LLMs into traditional reinforcement learning algorithms to leverage the common sense knowledge that LLMs acquire during pretraining (Hao et al., 2023; Liu et al., 2023; Zhou et al., 2023; Zhao et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and their contributions to integrating LLMs with reinforcement learning.",
      "processing_time": 58.255151987075806,
      "citing_paper_id": "270063284",
      "cited_paper_id": 263829963
    },
    {
      "context_text": "…design of the LLM agent architecture to enhance its capability in the specific domain, but typically, an LLM agent architecture features memory (Zhu et al., 2023) and planning (Yao et al., 2022) modules that enable LLMs to recall past behaviors and plan future actions in a dynamic environment,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about LLM agent architecture. No verifiable resources are identified.",
      "processing_time": 58.019291162490845,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258959262
    },
    {
      "context_text": "…design of the LLM agent architecture to enhance its capability in the specific domain, but typically, an LLM agent architecture features memory (Zhu et al., 2023) and planning (Yao et al., 2022) modules that enable LLMs to recall past behaviors and plan future actions in a dynamic environment,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about LLM agent architecture. No verifiable resources are identified.",
      "processing_time": 58.019291162490845,
      "citing_paper_id": "270063284",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…to simply prompting the LLM with a description of the problem setup and potentially some chain-of-thought examples (Brookins and DeBacker, 2023; Gandhi et al., 2023), STRIDE can effectively address the aforementioned challenges and enhance the LLM’s reasoning capability in strategic settings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches to enhance LLM reasoning capabilities.",
      "processing_time": 56.589237213134766,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258968043
    },
    {
      "context_text": "Another pivotal study by Gandhi et al. (2023) proposed to enhance the strategic reasoning of LLMs by providing few-shot chain-of-thought (CoT) examples for matrix games and multi-turn bargaining games, and showed that LLMs are capable of generalizing the demonstration to new game instances, but…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The focus is on the strategic reasoning capabilities of LLMs using few-shot chain-of-thought examples.",
      "processing_time": 59.995502948760986,
      "citing_paper_id": "270063284",
      "cited_paper_id": 258968043
    },
    {
      "context_text": "Compared to simply prompting the LLM with a description of the problem setup and potentially some chain-of-thought examples (Brookins and DeBacker, 2023; Gandhi et al., 2023), STRIDE can effectively address the aforementioned challenges and enhance the LLM’s reasoning capability in strategic…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers discussing methods or findings. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.97772407531738,
      "citing_paper_id": "270063284",
      "cited_paper_id": 259714625
    },
    {
      "context_text": "There have been several contributions recently studying the behavior of LLMs in strategic settings, e.g., matrix games like Dictator and Prisoner’s Dilemma (Brookins and DeBacker, 2023; Lor`e and Heydari, 2023; Fan et al., 2023; Akata et al., 2023; Guo et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions studies on LLMs in strategic settings but does not specify any datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.1512405872345,
      "citing_paper_id": "270063284",
      "cited_paper_id": 259714625
    },
    {
      "context_text": "There have been several contributions recently studying the behavior of LLMs in strategic settings, e.g., matrix games like Dictator and Prisoner’s Dilemma (Brookins and DeBacker, 2023; Lor`e and Heydari, 2023; Fan et al., 2023; Akata et al., 2023; Guo et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions studies on LLMs in strategic settings but does not specify any datasets. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.1512405872345,
      "citing_paper_id": "270063284",
      "cited_paper_id": 266163085
    },
    {
      "context_text": "…al., 2023) and planning (Yao et al., 2022) modules that enable LLMs to recall past behaviors and plan future actions in a dynamic environment, and a set of tools (Qin et al., 2023) that facilitate mathematical computation, accessing internal or external memory, and interacting with the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modules and tools. The context focuses on the capabilities and functionalities of LLMs rather than the use of datasets.",
      "processing_time": 59.414777755737305,
      "citing_paper_id": "270063284",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Recent work by Davidson et al. (2024) and Bianchi et al. (2024) also used bargaining games to evaluate the strategic reasoning of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'bargaining games' but does not specify any named datasets. The context focuses on evaluating the strategic reasoning of LLMs using bargaining games, which are likely part of a platform or tool rather than a specific dataset.",
      "processing_time": 62.32998776435852,
      "citing_paper_id": "270063284",
      "cited_paper_id": 267548095
    },
    {
      "context_text": "To effectively leverage the operational tools for solving complex problems, we propose a unique design for the reasoning module, which is empowered by a pretrained LLM like GPT-4 (Achiam et al., 2023) or Claude (Anthropic, 2024), in the STRIDE framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.09543251991272,
      "citing_paper_id": "270063284",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) [5, 29] are used in tasks beyond traditional NLP, such as using tools [34, 19, 46] or solving reasoning problems [37, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 58.26961588859558,
      "citing_paper_id": "263310628",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Large Language Models (LLMs) [5, 29] are used in tasks beyond traditional NLP, such as using tools [34, 19, 46] or solving reasoning problems [37, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 58.26961588859558,
      "citing_paper_id": "263310628",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Large Language Models (LLMs) [5, 29] are used in tasks beyond traditional NLP, such as using tools [34, 19, 46] or solving reasoning problems [37, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 58.26961588859558,
      "citing_paper_id": "263310628",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "Large Language Models (LLMs) [5, 29] are used in tasks beyond traditional NLP, such as using tools [34, 19, 46] or solving reasoning problems [37, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 58.26961588859558,
      "citing_paper_id": "263310628",
      "cited_paper_id": 258212542
    },
    {
      "context_text": "5 [5] ones when assigned the same role in a mixed population simulation, hinting at potential fairness and disparity considerations when users use models with varying capabilities as assistants.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the behavior of language models in simulations.",
      "processing_time": 56.55463123321533,
      "citing_paper_id": "263310628",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "These attacks are broadly useful for AI safety research to study AI manipulation and deception [31], alignment of multi-agent systems, and actions driven by an assigned persona [2, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 57.46182155609131,
      "citing_paper_id": "263310628",
      "cited_paper_id": 254246305
    },
    {
      "context_text": "These attacks are broadly useful for AI safety research to study AI manipulation and deception [31], alignment of multi-agent systems, and actions driven by an assigned persona [2, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and concepts. No verifiable resources are identified.",
      "processing_time": 57.46182155609131,
      "citing_paper_id": "263310628",
      "cited_paper_id": 258947657
    },
    {
      "context_text": "Our findings show that GPT-4 [29] (the best-evaluated model) still underperforms when increasing games’ difficulty.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (GPT-4). The context focuses on the performance of the model in games, which does not indicate the use of a specific dataset.",
      "processing_time": 60.69916868209839,
      "citing_paper_id": "263310628",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "In addition, [1, 9, 8, 6] used negotiation games to evaluate LLMs either non-interactively or with only two players.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of negotiation games. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 58.87293577194214,
      "citing_paper_id": "263310628",
      "cited_paper_id": 258740978
    },
    {
      "context_text": "In addition, [1, 9, 8, 6] used negotiation games to evaluate LLMs either non-interactively or with only two players.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of negotiation games. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 58.87293577194214,
      "citing_paper_id": "263310628",
      "cited_paper_id": 258968043
    },
    {
      "context_text": "They also span commonsense reasoning [40, 32] and Theory-of-Mind (ToM) capabilities [35, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only capabilities such as commonsense reasoning and Theory-of-Mind. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.07339668273926,
      "citing_paper_id": "263310628",
      "cited_paper_id": 258999153
    },
    {
      "context_text": "They also span commonsense reasoning [40, 32] and Theory-of-Mind (ToM) capabilities [35, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only capabilities such as commonsense reasoning and Theory-of-Mind. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.07339668273926,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "This also broadly measures the trustworthiness of models in following instructions and keeping in-context confidential information [7], a task that is also related to ToM [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task related to trustworthiness and privacy. No clear, verifiable resource names are present.",
      "processing_time": 58.87203240394592,
      "citing_paper_id": "263310628",
      "cited_paper_id": 264555202
    },
    {
      "context_text": "More results are in Appendices F and G. 5 Related Work Previous work used and evaluated LLM agents in tasks such as web browsing or synthesizing knowledge [17, 16, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks and previous work. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.03489422798157,
      "citing_paper_id": "263310628",
      "cited_paper_id": 268042527
    },
    {
      "context_text": "5 and the latest version of Gemini [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to Gemini, which is likely a model or system, not a dataset.",
      "processing_time": 58.23872113227844,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "This should ideally apply to AI and LLM agents, which are increasingly relied on as personal [25, 28] and negotiation assistants [11, 20, 30, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the role of AI and LLM agents as personal and negotiation assistants.",
      "processing_time": 59.49714493751526,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "This should ideally apply to AI and LLM agents, which are increasingly relied on as personal [25, 28] and negotiation assistants [11, 20, 30, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the role of AI and LLM agents as personal and negotiation assistants.",
      "processing_time": 59.49714493751526,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "This should ideally apply to AI and LLM agents, which are increasingly relied on as personal [25, 28] and negotiation assistants [11, 20, 30, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the role of AI and LLM agents as personal and negotiation assistants.",
      "processing_time": 59.49714493751526,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "This should ideally apply to AI and LLM agents, which are increasingly relied on as personal [25, 28] and negotiation assistants [11, 20, 30, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the role of AI and LLM agents as personal and negotiation assistants.",
      "processing_time": 59.49714493751526,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "Planning is integral to how humans negotiate [18].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, methods, or other resources. It only states a general fact about human planning.",
      "processing_time": 58.23943495750427,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "The base game is adapted, with our own descriptions, from a negotiation exercise [38, 39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a negotiation exercise, which is likely a method or tool rather than a dataset.",
      "processing_time": 59.030094146728516,
      "citing_paper_id": "263310628",
      "cited_paper_id": null
    },
    {
      "context_text": "To apply our framework, we define the state as a fact we are focusing on, analogous to the human’s working memory (Baddeley, 1992) for inference.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept from cognitive psychology. No verifiable resources are identified.",
      "processing_time": 57.59216070175171,
      "citing_paper_id": "258865812",
      "cited_paper_id": 1916803
    },
    {
      "context_text": "Humans possess an internal world model , a mental representation of the environment (Johnson-Laird, 1983, 2010; Gentner and Stevens, 2014), which enables humans to simulate actions and their effects on the world’s state for deliberate planning for complex tasks of motor control, imagery, inference,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts about mental models and human reasoning.",
      "processing_time": 56.86525869369507,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15080556
    },
    {
      "context_text": "Humans possess an internal world model, a mental representation of the environment [28, 27, 15], which enables humans to simulate actions and their effects on the world’s state for deliberate planning ∗equal contribution",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to mental models and human reasoning. No verifiable resources are identified.",
      "processing_time": 57.99473547935486,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15080556
    },
    {
      "context_text": "Classical planning methods have been widely adopted in robots and embodied environments [9, 42, 8, 61, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to classical planning methods in robotics and embodied environments.",
      "processing_time": 57.22554564476013,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15117338
    },
    {
      "context_text": "This is similar in spirit to model predictive control (Camacho and Alba, 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach.",
      "processing_time": 55.969794034957886,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15117338
    },
    {
      "context_text": "Combining world model and planning, our framework is similar to model predictive control [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (model predictive control).",
      "processing_time": 56.25567293167114,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15117338
    },
    {
      "context_text": "Classical planning methods have been widely adopted in robots and embod-ied environments (Camacho and Alba, 2013; Jiang et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only classical planning methods in robotics and embodied environments.",
      "processing_time": 56.66233968734741,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15117338
    },
    {
      "context_text": "We adopt Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006; Coulom, 2007), a powerful planning algorithm that strategically explores the space of reasoning trees and strikes a proper balance between exploration and exploitation to find high-reward reasoning traces efficiently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Monte Carlo Tree Search).",
      "processing_time": 56.83893823623657,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15184765
    },
    {
      "context_text": "To balance between exploration (of less-visited nodes) and exploitation (of high-value nodes), we use the wellknown Upper Confidence bounds applied to Trees (UCT) algorithm [31] to select each child node.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the UCT algorithm. The context is about using UCT for balancing exploration and exploitation in planning.",
      "processing_time": 58.623584508895874,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15184765
    },
    {
      "context_text": "2), we can enable LLMs to reason with advanced planning algorithms, where we adopt Monte Carlo Tree Search (MCTS) [31, 12], a powerful planning algorithm that strategically explores the space of reasoning trees and strikes a proper balance between exploration and exploitation to find high-reward reasoning traces efficiently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Monte Carlo Tree Search).",
      "processing_time": 56.661717891693115,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15184765
    },
    {
      "context_text": "To balance between exploration (of less-visited nodes) and exploitation (of high-value nodes), we use the well-known Upper Confidence bounds applied to Trees (UCT) (Kocsis and Szepesvári, 2006) to select each child node.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UCT) for selecting child nodes in a tree structure.",
      "processing_time": 57.424450635910034,
      "citing_paper_id": "258865812",
      "cited_paper_id": 15184765
    },
    {
      "context_text": "The ability to generate plans is important for intelligent embodied agents, e.g. household robots (Puig et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper about simulating household activities. No verifiable resources are identified.",
      "processing_time": 58.41241693496704,
      "citing_paper_id": "258865812",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "Planning, a central ability in intelligent agents, involves generating a series of actions to achieve a specific goal [40, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the concept of planning in intelligent agents.",
      "processing_time": 58.23160433769226,
      "citing_paper_id": "258865812",
      "cited_paper_id": 118922379
    },
    {
      "context_text": "Planning with LLMs. Planning, a central ability in intelligent agents, involves generating a series of actions to achieve a specific goal (McCarthy, 1963; Bylander, 1994).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about planning in intelligent agents. No verifiable resources are identified.",
      "processing_time": 58.01749396324158,
      "citing_paper_id": "258865812",
      "cited_paper_id": 118922379
    },
    {
      "context_text": "Planning with LLMs. Planning, a central ability in intelligent agents, involves generating a series of actions to achieve a specific goal (McCarthy, 1963; Bylander, 1994).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about planning in intelligent agents. No verifiable resources are identified.",
      "processing_time": 58.01749396324158,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "Recent years have witnessed successful applications of planning algorithms in RL [50], such as AlphaZero [52], MuZero [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions planning algorithms in RL, specifically referencing AlphaZero and MuZero. However, these are models or methods, not datasets.",
      "processing_time": 58.20898699760437,
      "citing_paper_id": "258865812",
      "cited_paper_id": 208158225
    },
    {
      "context_text": "For example, to make an action plan towards a goal, planning with the world model involves exploring various alternative courses of actions, assessing the likely outcomes by rolling out possible future scenarios, and iteratively refining the plan based on the assessment [25, 14, 52, 19, 48, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about planning and learned models. No verifiable resources are identified.",
      "processing_time": 58.01957440376282,
      "citing_paper_id": "258865812",
      "cited_paper_id": 208158225
    },
    {
      "context_text": "For example, to make an action plan towards a goal, planning with the world model involves exploring various alternative courses of actions, assessing the likely outcomes by rolling out possible future scenarios, and iteratively refining the plan based on the assessment [25, 14, 52, 19, 48, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about planning and learned models. No verifiable resources are identified.",
      "processing_time": 58.01957440376282,
      "citing_paper_id": "258865812",
      "cited_paper_id": 234680400
    },
    {
      "context_text": "Recent years have witnessed successful applications of planning algorithms (Sekar et al., 2020), such as AlphaZero (Silver et al., 2017), and MuZero (Schrittwieser et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions planning algorithms and specific models (AlphaZero, MuZero) but does not refer to any specific datasets. The context is about the success of these algorithms in various games.",
      "processing_time": 60.25900983810425,
      "citing_paper_id": "258865812",
      "cited_paper_id": 208158225
    },
    {
      "context_text": "With latent imagination in a world model, RL agents can be trained to solve long-horizon tasks (Hafner et al., 2019, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research work on training RL agents with world models.",
      "processing_time": 57.19784665107727,
      "citing_paper_id": "258865812",
      "cited_paper_id": 222133157
    },
    {
      "context_text": "With latent imagination in a world model, an RL agent can be trained to solve long-horizon tasks [18, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers about training RL agents with world models.",
      "processing_time": 56.95995569229126,
      "citing_paper_id": "258865812",
      "cited_paper_id": 222133157
    },
    {
      "context_text": "…towards a goal, planning with the world model involves exploring various alternative courses of actions, assessing the likely outcomes by rolling out possible future scenarios, and iteratively refining the plan based on the assessment (Huys et al., 2012; Gasparski and Orel, 2014; Ho et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to planning and mental representations.",
      "processing_time": 57.190669536590576,
      "citing_paper_id": "258865812",
      "cited_paper_id": 234680400
    },
    {
      "context_text": "SayCan [1], for instance, combines LLMs with affordance functions to generate feasible plans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining LLMs with affordance functions.",
      "processing_time": 56.955528259277344,
      "citing_paper_id": "258865812",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Large language models (LLMs) have exhibited emergent reasoning abilities in a wide range of tasks (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of large language models.",
      "processing_time": 57.17875337600708,
      "citing_paper_id": "258865812",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "Reasoning with LLMs. LLM reasoning typically involves decomposing complex questions into sequential intermediate steps (a.k.a. chains) before producing the final answer, exemplified by Chain-of-Thought (CoT) prompting and its variants (Wei et al., 2022; Kojima et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for reasoning with LLMs.",
      "processing_time": 56.995840549468994,
      "citing_paper_id": "258865812",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Recently, prompting LLMs to do planning directly has gained attention and shown potential (Huang et al., 2022; Singh et al., 2022; Ding et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss planning capabilities of LLMs.",
      "processing_time": 57.16025471687317,
      "citing_paper_id": "258865812",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "…(Johnson-Laird, 1983, 2010; Gentner and Stevens, 2014), which enables humans to simulate actions and their effects on the world’s state for deliberate planning for complex tasks of motor control, imagery, inference, and decision making (Tolman, 1948; Briscoe, 2011; Schulkin, 2012; LeCun, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature discussing human cognitive abilities and planning. No verifiable resources are identified.",
      "processing_time": 58.6125864982605,
      "citing_paper_id": "258865812",
      "cited_paper_id": 251881108
    },
    {
      "context_text": "during complex tasks of motor control, imagery, inference, and decision making [54, 55, 4, 49, 17, 33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to complex tasks. No verifiable resources are identified.",
      "processing_time": 57.14728403091431,
      "citing_paper_id": "258865812",
      "cited_paper_id": 251881108
    },
    {
      "context_text": "Recently, prompting LLMs to do planning direcly has gained attention and shown potential [24, 23, 53, 13, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss prompting LLMs for planning. No verifiable resources are identified.",
      "processing_time": 58.77789783477783,
      "citing_paper_id": "258865812",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Recently, prompting LLMs to do planning direcly has gained attention and shown potential [24, 23, 53, 13, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss prompting LLMs for planning. No verifiable resources are identified.",
      "processing_time": 58.77789783477783,
      "citing_paper_id": "258865812",
      "cited_paper_id": 257663442
    },
    {
      "context_text": "[34] incorporates latent “situations” into LLMs, referring to the state of entities from the context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept of incorporating latent 'situations' into LLMs.",
      "processing_time": 57.8244948387146,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877723
    },
    {
      "context_text": "More related works on world models and planning are discussed in the Appendix D. 3 Reasoning via Planning (RAP) In this section, we present the Reasoning via Planning (RAP) framework that enables LLMs to strategically plan a coherent reasoning trace for solving a wide range of reasoning tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework and related works.",
      "processing_time": 56.135475158691406,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "In this paper, we present Reasoning via Planning (RAP), a novel LLM reasoning framework that equips LLMs with an ability to reason akin to human-like strategic planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a novel framework called Reasoning via Planning (RAP).",
      "processing_time": 57.56691074371338,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "Reasoning via Planning (RAP) Chain-of-Thought",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or approach called 'Reasoning via Planning (RAP) Chain-of-Thought'.",
      "processing_time": 58.99900960922241,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "…to a target state, GPT-3 (Brown et al., 2020) achieves a success rate of only 1%, compared to 78% for humans (Valmeekam et al., 2022); these models also struggle with complex tasks that require multiple steps of math, logical, or commonsense reasoning (Huang and Chang, 2022; Mialon et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons between models and humans. No verifiable resources are identified.",
      "processing_time": 57.80432486534119,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "Moreover, based on LLMs’ powerful programming ability (Lyu et al., 2023; Jojic et al., 2023; Liu et al., 2023), recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning algorithms, such as LLM+P (Liu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the translation of natural language instructions into executable programming languages and the use of classical planning algorithms.",
      "processing_time": 60.13461136817932,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "To address these limitations, this paper proposes a new framework, Reasoning via Planning (RAP) , that enables LLMs to reason in a manner close to humans’ conscious planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a new framework called Reasoning via Planning (RAP).",
      "processing_time": 57.54435849189758,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "Figure 1: An overview of Reasoning via Planning (RAP).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a figure overviewing a concept.",
      "processing_time": 56.54314088821411,
      "citing_paper_id": "258865812",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "Moreover, based on LLMs’ powerful programming ability [37, 29, 36], some recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning algorithms, such as LLM+P [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on translating natural language instructions into programming languages and running planning algorithms.",
      "processing_time": 58.880502223968506,
      "citing_paper_id": "258865812",
      "cited_paper_id": 256416127
    },
    {
      "context_text": "By default, we use the LLaMA-33B model (Touvron et al., 2023a) as the base LLM for both our methods and baselines, with a sampling temperature of 0.8.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (LLaMA-33B). There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.66964244842529,
      "citing_paper_id": "258865812",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "Moreover, based on LLMs’ powerful programming ability (Lyu et al., 2023; Jojic et al., 2023; Liu et al., 2023), recent works first translate natural language instructions into the executable programming languages, such as Planning Domain Description Language (PDDL), and runs classical planning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for translating natural language instructions into executable programming languages like PDDL.",
      "processing_time": 58.864898443222046,
      "citing_paper_id": "258865812",
      "cited_paper_id": 257496672
    },
    {
      "context_text": "Even more, our framework allows LLaMA-33B to outperform GPT-4 by 33% relative gain, which is known to have much stronger reasoning ability (Bubeck et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and performance comparisons.",
      "processing_time": 56.23772192001343,
      "citing_paper_id": "258865812",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "Large language models (LLMs) have exhibited emergent reasoning abilities in a wide range of tasks [5, 10, 44, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 58.69072890281677,
      "citing_paper_id": "258865812",
      "cited_paper_id": 258740735
    },
    {
      "context_text": "In the future, it is worth exploring how to fine-tune LLMs to better reason and serve as a world model (Xiang et al., 2023), as well as how to combine external tools (Hao et al., 2023a; Schick et al., 2023) with RAP to solve more complex real-world problems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only future research directions and combinations of methods.",
      "processing_time": 56.63838481903076,
      "citing_paper_id": "258865812",
      "cited_paper_id": 258762577
    },
    {
      "context_text": "…an internal world model , a mental representation of the environment (Johnson-Laird, 1983, 2010; Gentner and Stevens, 2014), which enables humans to simulate actions and their effects on the world’s state for deliberate planning for complex tasks of motor control, imagery, inference, and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.84577250480652,
      "citing_paper_id": "258865812",
      "cited_paper_id": 263098481
    },
    {
      "context_text": "More recently, CoRe [67] proposes to fine-tune both the reasoning step generator and verifier for solving math word problems, also using MCTS for reasoning decoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'CoRe' and 'MCTS', but does not provide enough information to determine if these are datasets, methods, or other types of resources. No specific dataset names are mentioned.",
      "processing_time": 60.89016246795654,
      "citing_paper_id": "258865812",
      "cited_paper_id": 263868305
    },
    {
      "context_text": "Recent efforts have been made to investigate non-linear reasoning structures by sampling more reasoning steps efficiently guided by some search algorithms [30, 67, 63, 64].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss methods or findings.",
      "processing_time": 57.051836252212524,
      "citing_paper_id": "258865812",
      "cited_paper_id": 263868305
    },
    {
      "context_text": "For instance, CoRe (Zhu et al., 2022) fine-tunes reasoning step generator and verifier for math word problems with MCTS for decoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CoRe' which is a method, and 'math word problems' which suggests a domain but does not specify a dataset. No clear dataset name is provided.",
      "processing_time": 60.47562575340271,
      "citing_paper_id": "258865812",
      "cited_paper_id": 263868305
    },
    {
      "context_text": "The term Large Language Models (LLMs) typically refers to transformer-based models [11] of billions of parameters that are trained to perform tasks related to Natural Language Processing (NLP).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to a method (transformer-based models).",
      "processing_time": 57.696630001068115,
      "citing_paper_id": "272694440",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "In addition, we conducted some baseline comparisons using a random agent and a tabular Q-learning agent [40] similar to [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (random agent and tabular Q-learning agent).",
      "processing_time": 57.8763542175293,
      "citing_paper_id": "272694440",
      "cited_paper_id": 208910339
    },
    {
      "context_text": "In addition, we conducted some baseline comparisons using a random agent and a tabular Q-learning agent [40] similar to [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (random agent and tabular Q-learning agent).",
      "processing_time": 57.8763542175293,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "Secondly, pre-trained language models exhibit remarkable adaptability, as they can be easily adapted to new tasks or domains with relatively minimal additional training, offering a significant advantage over other machine learning models that may require extensive retraining [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the adaptability of pre-trained language models.",
      "processing_time": 57.69746685028076,
      "citing_paper_id": "272694440",
      "cited_paper_id": 254823156
    },
    {
      "context_text": "LLMs have achieved notable successes in recent years, demonstrating significant potential in mimicking human-like behavior [4, 5].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general achievements of LLMs. No verifiable resources are identified.",
      "processing_time": 58.135509729385376,
      "citing_paper_id": "272694440",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "In addition to planning tasks, LLM-based agents have been successful in exploration, as demonstrated by Du et al.[6] and Wang et al.[7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers demonstrating LLM-based agents in exploration tasks.",
      "processing_time": 57.74945068359375,
      "citing_paper_id": "272694440",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Building upon this capability, a growing research area has employed LLMs as central controllers to construct autonomous agents to obtain human-like decision-making capabilities [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general research area using LLMs for autonomous agents.",
      "processing_time": 57.96710133552551,
      "citing_paper_id": "272694440",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "More recently, the PentestGPT framework[32] seeks to automate penetration testing more comprehensively, with evaluations conducted across various machines on platforms like HacktheBox.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a framework and platform for evaluation, which do not qualify as datasets.",
      "processing_time": 59.49552655220032,
      "citing_paper_id": "272694440",
      "cited_paper_id": 260887370
    },
    {
      "context_text": "Following the taxonomy proposed by [8], the LLM agent architecture comprises the following components: Profile, Memory, Planning, and Action (Figure 1).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a taxonomy of LLM agent architecture components.",
      "processing_time": 57.376270055770874,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Compared to RL approaches, LLM-based agents encapsulate a vast spectrum of human knowledge and a richer understanding of the world that enables them to act without needing additional or specific training [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between RL approaches and LLM-based agents. No verifiable resources are identified.",
      "processing_time": 59.16114163398743,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "A more detailed description of both scenarios is presented in [10].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a more detailed description of scenarios, which is not sufficient to identify a verifiable resource.",
      "processing_time": 60.554884910583496,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "The LLM agent used for all the experiments was introduced in [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an LLM agent. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 59.10402750968933,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "Prior work has explored the intersection of LLMs, cybersecurity, and sequential decision-making [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general exploration of LLMs, cybersecurity, and sequential decision-making.",
      "processing_time": 58.73970079421997,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "It comprises 113 questions and answers generated from previous game runs using GPT-4 in the small scenario [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a set of questions and answers but does not specify a named dataset. The reference to 'previous game runs' suggests a custom or internal dataset, which is not specific enough to be included.",
      "processing_time": 61.51552891731262,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "As part of this work, we re-used the prompts and structure of the agent to directly compare the fine-tuned LLMs introduced in this work with the results of the commercial pre-trained models introduced in [10].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between fine-tuned LLMs and commercial pre-trained models. No verifiable resources are identified.",
      "processing_time": 60.148595094680786,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "NetSecGame is a network security simulation environment that follows a reinforcement learning (RL) architecture [10, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions NetSecGame as a network security simulation environment but does not refer to it as a dataset. It is described as an environment for reinforcement learning, which is more aligned with a method or tool.",
      "processing_time": 61.50969862937927,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "In this work, we fine-tuned a 7 billion pre-trained LLM (Zephyr-7b-β ) to be used as a red team agent in the NetSecGame environment [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pre-trained model and an environment. No verifiable datasets are referenced.",
      "processing_time": 58.727564096450806,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "For the small and full scenarios, the results were taken from [10].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that results were taken from another paper. There is no indication of a reusable dataset.",
      "processing_time": 59.415284156799316,
      "citing_paper_id": "272694440",
      "cited_paper_id": 261076387
    },
    {
      "context_text": "Future work will focus on optimizing our prompts using techniques such as DSPy [41] to adapt them to the specific strengths of each model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DSPy) for optimizing prompts. The context is about future work and does not describe the use of a dataset.",
      "processing_time": 60.56359624862671,
      "citing_paper_id": "272694440",
      "cited_paper_id": 263671701
    },
    {
      "context_text": "The base model used was Zephyr-7b-β [15] which is an instruction-tuned model based on Mistral-7B-v0.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (Zephyr-7b-β) but does not refer to any specific dataset. The citation is about the model used, not a dataset.",
      "processing_time": 60.55401849746704,
      "citing_paper_id": "272694440",
      "cited_paper_id": 264490502
    },
    {
      "context_text": "This method is also called distilled SFT (dSFT) [14, 15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called distilled SFT (dSFT).",
      "processing_time": 58.26309323310852,
      "citing_paper_id": "272694440",
      "cited_paper_id": 264490502
    },
    {
      "context_text": "Other works have also explored using LLMs for planning tasks, such as Spring [26], which uses an LLM to ”study” a paper describing the Crafter game environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Crafter game environment' but does not specify a dataset. The reference is to a method or tool (LLM) used to study a game environment, not a dataset.",
      "processing_time": 61.20034217834473,
      "citing_paper_id": "272694440",
      "cited_paper_id": 268042522
    },
    {
      "context_text": "Raman et al. (2024) investigate the performance of commercial LLMs in the context of Certified Ethical Hacking (CEH) certification questions[33].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of commercial LLMs on CEH certification questions.",
      "processing_time": 58.42911648750305,
      "citing_paper_id": "272694440",
      "cited_paper_id": 268307249
    },
    {
      "context_text": "However, in intricate settings such as theme parks, due to unfamiliarity with the space and spatial anxiety [11], humans’ guidance can be vague or uncertain, affected not just by the choice of words but also by the subtleties in their voice [12], [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about spatial navigation and human guidance. No verifiable resources are identified.",
      "processing_time": 59.28817534446716,
      "citing_paper_id": "271709656",
      "cited_paper_id": 1543962
    },
    {
      "context_text": "Restrictions apply.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. The cited papers are about speech recognition and hesitation disfluencies, which do not introduce specific datasets.",
      "processing_time": 60.876280546188354,
      "citing_paper_id": "271709656",
      "cited_paper_id": 5182891
    },
    {
      "context_text": "Restrictions apply.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. The cited papers are about speech recognition and hesitation disfluencies, which do not introduce specific datasets.",
      "processing_time": 60.876280546188354,
      "citing_paper_id": "271709656",
      "cited_paper_id": 31204656
    },
    {
      "context_text": "These attacks often involve simple token operations such as synonym replacement and misleading models into making errors [49], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only adversarial attacks and methods. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 60.243598222732544,
      "citing_paper_id": "271709656",
      "cited_paper_id": 7038773
    },
    {
      "context_text": "These attacks often involve simple token operations such as synonym replacement and misleading models into making errors [49], [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only adversarial attacks and methods. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 60.243598222732544,
      "citing_paper_id": "271709656",
      "cited_paper_id": 220714040
    },
    {
      "context_text": "With the progression of language models, concerns like adversarial attacks and prompt manipulation have gained prominence [47], [48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concerns about adversarial attacks and prompt manipulation in language models.",
      "processing_time": 58.40643620491028,
      "citing_paper_id": "271709656",
      "cited_paper_id": 21740766
    },
    {
      "context_text": "This framework is adapted by seminal works in robot social navigation [30], [31], [32], [33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to seminal works in robot social navigation. No verifiable resources are identified.",
      "processing_time": 59.58988070487976,
      "citing_paper_id": "271709656",
      "cited_paper_id": 52198622
    },
    {
      "context_text": "A growing body of research investigated quantifying uncertainty due to LLM’s hallucinations [22], [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general area of research. No verifiable resources are identified.",
      "processing_time": 59.084819316864014,
      "citing_paper_id": "271709656",
      "cited_paper_id": 53715584
    },
    {
      "context_text": "It is crucial for robots to emulate how humans interact and form opinions about each other, including assessments of credibility and trust, and understand human uncertainty to ensure safe and efficient actions [9], [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of trust in human-robot interactions. No verifiable resources are identified.",
      "processing_time": 59.543418407440186,
      "citing_paper_id": "271709656",
      "cited_paper_id": 64316736
    },
    {
      "context_text": "In terms of speech rate, people might either slow down as they ponder their words or accelerate their speaking speed due to nervousness [40], [39].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general observations about speech rate. No verifiable resources are referenced.",
      "processing_time": 59.000523805618286,
      "citing_paper_id": "271709656",
      "cited_paper_id": 205223625
    },
    {
      "context_text": "3) We conduct experiments on a large-scale Disfluent Nav-igational Instruction Audio Dataset [14], RoboTHOR simulation environment [15], and also real-world setup, to show that TrustNavGPT significantly surpasses existing LLM-based navigation techniques, by a 55% improvement in achieving successful…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Disfluent Nav-igational Instruction Audio Dataset"
      ],
      "dataset_descriptions": {
        "Disfluent Nav-igational Instruction Audio Dataset": "Used to conduct experiments evaluating TrustNavGPT's performance in navigation tasks, specifically focusing on disfluent navigational instructions in audio format."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset 'Disfluent Nav-igational Instruction Audio Dataset' and a simulation environment 'RoboTHOR'. The dataset is used for experiments to evaluate TrustNavGPT's performance in navigation tasks.",
      "processing_time": 73.56708240509033,
      "citing_paper_id": "271709656",
      "cited_paper_id": 215768690
    },
    {
      "context_text": "Recent studies [44] have explored the use of foundation models to directly generate executable action codes from linguistic instructions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using foundation models to generate executable action codes. No verifiable resources are identified.",
      "processing_time": 60.28346252441406,
      "citing_paper_id": "271709656",
      "cited_paper_id": 215768690
    },
    {
      "context_text": "…and subgoals mentioned in navigational commands [7], [16], the application of LLMs in facilitating sequential decision-making for zero-shot robot navigation [17], [8], and also the investigation of LLMs for the semantic prediction of object locations, thereby enhancing navigational efficiency [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of LLMs in various navigational tasks. No clear, verifiable datasets are identified.",
      "processing_time": 60.70939493179321,
      "citing_paper_id": "271709656",
      "cited_paper_id": 257378363
    },
    {
      "context_text": "…and subgoals mentioned in navigational commands [7], [16], the application of LLMs in facilitating sequential decision-making for zero-shot robot navigation [17], [8], and also the investigation of LLMs for the semantic prediction of object locations, thereby enhancing navigational efficiency [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of LLMs in various navigational tasks. No clear, verifiable datasets are identified.",
      "processing_time": 60.70939493179321,
      "citing_paper_id": "271709656",
      "cited_paper_id": 264146047
    },
    {
      "context_text": "Recent scholarly work has explored the integration of LLMs with visual inputs to map landmarks and subgoals mentioned in navigational commands [7], [16], the application of LLMs in facilitating sequential decision-making for zero-shot robot navigation [17], [8], and also the investigation of LLMs…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs in navigation and decision-making. No clear, verifiable datasets are identified.",
      "processing_time": 61.09619474411011,
      "citing_paper_id": "271709656",
      "cited_paper_id": 258079021
    },
    {
      "context_text": "To address these issues and enhance the precision of action codes, we follow the idea of [19], which creates a skill library to store and retrieve behaviors for gaming agents and introduces a customized robot navigation tool library.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a skill library and a navigation tool library, which are not considered datasets.",
      "processing_time": 59.516608476638794,
      "citing_paper_id": "271709656",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Voyager [19] is an LLM-embodied gaming agent that plays Minecraft without human intervention through lifelong learning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an embodied agent using LLMs in Minecraft.",
      "processing_time": 58.793038845062256,
      "citing_paper_id": "271709656",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Entropy has been introduced as a method to model uncertainty in the large language model [24], [25], while conformal prediction [26] is another method applied to quantify uncertainty for next-token prediction in Multiple Choice Question Answering(MCQA) setups [27], [5].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for uncertainty quantification in large language models.",
      "processing_time": 58.70746159553528,
      "citing_paper_id": "271709656",
      "cited_paper_id": 258967487
    },
    {
      "context_text": "…that aligns with the user study annotation, and p total denotes the total number of audio clips that prompt the LLM. Inspired from prior works[24], [25] which use entropy as a method to quantify uncertainty in large language models, we define confidence score C ( ρ ) based on the model’s p θ…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for quantifying uncertainty in large language models. No verifiable resources are identified.",
      "processing_time": 59.84646248817444,
      "citing_paper_id": "271709656",
      "cited_paper_id": 258967487
    },
    {
      "context_text": "We employ the semantic knowledge embedded in language models in a tailored manner, utilizing it not just as a heuristic for search [6], but also as a way to visual grounding the trust of human guidance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of language models for navigation and planning.",
      "processing_time": 59.01154398918152,
      "citing_paper_id": "271709656",
      "cited_paper_id": 264146047
    },
    {
      "context_text": "While Large Language Models (LLMs) have demonstrated impressive proficiency in high-level task planning [43], [6], bridging the divide from strategic planning to practical execution remains a significant hurdle.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general capabilities of LLMs in task planning.",
      "processing_time": 59.24499011039734,
      "citing_paper_id": "271709656",
      "cited_paper_id": 264146047
    },
    {
      "context_text": "Recent advances in Large Language Models (LLMs), such as GPT-4 [1] or Gemini [2], and Robotics have shown significant improvement for human-robot interactions (HRI) areas such as task planning [3], [4], [5], or social navigation [6], [7], [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in LLMs and robotics. No verifiable resources are identified.",
      "processing_time": 59.992027282714844,
      "citing_paper_id": "271709656",
      "cited_paper_id": 264146047
    },
    {
      "context_text": "Agent Driver [20] and Inner Monologue [21] integrate LLM into autonomous driving systems and robot planning by incorporating environment feedback and making the LLM able to execute action through a versatile function library.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for integrating LLMs into autonomous driving systems and robot planning.",
      "processing_time": 59.997801542282104,
      "citing_paper_id": "271709656",
      "cited_paper_id": 265294541
    },
    {
      "context_text": "…( V ) , the prompt consists both textual and vocal cue information of human audio command, we use Chain-of-Thought reasoning [41], a prompting mechanism aims to emulate the human reasoning process, together with In-Context Learning [42] by providing only few-shot examples with no explicit training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited paper titles do not introduce any datasets either.",
      "processing_time": 59.51834464073181,
      "citing_paper_id": "271709656",
      "cited_paper_id": 279052856
    },
    {
      "context_text": "In this work, we consider a Markov game, or a stochastic game [30] defined as a tuple G : = ( N , S , A , the joint action space, with ( S i , A i ) as the state space and action space of agent i , respectively, γ ∈ [ 0 , 1 ) is the discounting factor [29], [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and definitions from game theory.",
      "processing_time": 58.55580019950867,
      "citing_paper_id": "273186556",
      "cited_paper_id": 547566
    },
    {
      "context_text": "The state transition p : S × A → ∆ ( S ) is controlled by the current state and joint action, where ∆ ( S ) represents the set of all probability distributions over the joint state space S .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of state transitions in game theory.",
      "processing_time": 58.68777322769165,
      "citing_paper_id": "273186556",
      "cited_paper_id": 547566
    },
    {
      "context_text": "We evaluate our framework in MPE [7] simple spread environment which is a fully cooperative game.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MPE"
      ],
      "dataset_descriptions": {
        "MPE": "Used to evaluate the framework in a fully cooperative game setting, focusing on the performance of agents in a simple spread environment."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'MPE' which stands for Multi-Agent Particle Environment, a known benchmark for multi-agent reinforcement learning. However, it does not specify a dataset but rather an environment for evaluation.",
      "processing_time": 68.462806224823,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "Methods like QMIX [9] and MADDPG [7] employ centralized critics during training to coordinate agents, while allowing independent execution at test time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (QMIX and MADDPG) but does not refer to any specific datasets. The context is about methodologies and their application in multi-agent systems.",
      "processing_time": 61.21636509895325,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "In our experiments, we compare the MARL algorithm MADDPG [7], MAPPO [8] and QMIX [9] and set default hyper-parameters according to the well-tuned performance of human-written reward, and fix that in all experiments on this task to do MARL training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only algorithms and methods. The citation intent is to reference research work and methods used in multi-agent reinforcement learning.",
      "processing_time": 60.9444854259491,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "In our experiments, we compare the MARL algorithm MADDPG [7], MAPPO [8] and QMIX [9] and set default hyper-parameters according to the well-tuned performance of human-written reward, and fix that in all experiments on this task to do MARL training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only algorithms and methods. The citation intent is to reference research work and methods used in multi-agent reinforcement learning.",
      "processing_time": 60.9444854259491,
      "citing_paper_id": "273186556",
      "cited_paper_id": 232092445
    },
    {
      "context_text": "We validate our framework on a sparse reward multi-agent environment: Level-Based Foraging (LBF) environment [1] as well as the Multi-Agent Particle (MPE) environment [7], and our results show that YOLO-MARL outperforms several MARL baselines, such as [7]–[9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Level-Based Foraging (LBF) environment",
        "Multi-Agent Particle (MPE) environment"
      ],
      "dataset_descriptions": {
        "Level-Based Foraging (LBF) environment": "Used to validate the YOLO-MARL framework in a sparse reward multi-agent setting, focusing on cooperative task performance.",
        "Multi-Agent Particle (MPE) environment": "Used to validate the YOLO-MARL framework in a multi-agent setting, comparing performance against several MARL baselines."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions two environments, LBF and MPE, which are used to validate the framework. These environments are specific and verifiable, though they are not traditional datasets.",
      "processing_time": 79.24114680290222,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "We validate our framework on a sparse reward multi-agent environment: Level-Based Foraging (LBF) environment [1] as well as the Multi-Agent Particle (MPE) environment [7], and our results show that YOLO-MARL outperforms several MARL baselines, such as [7]–[9].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Level-Based Foraging (LBF) environment",
        "Multi-Agent Particle (MPE) environment"
      ],
      "dataset_descriptions": {
        "Level-Based Foraging (LBF) environment": "Used to validate the YOLO-MARL framework in a sparse reward multi-agent setting, focusing on cooperative task performance.",
        "Multi-Agent Particle (MPE) environment": "Used to validate the YOLO-MARL framework in a multi-agent setting, comparing performance against several MARL baselines."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions two environments, LBF and MPE, which are used to validate the framework. These environments are specific and verifiable, though they are not traditional datasets.",
      "processing_time": 79.24114680290222,
      "citing_paper_id": "273186556",
      "cited_paper_id": 235417602
    },
    {
      "context_text": "For each agent i , it attempts to maximize its expected sum of discounted rewards, i.e. its objective function In the literature, MARL algorithms [7]–[9] have been designed to train neural network-based policies π i ( θ i ) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to MARL algorithms. No verifiable resources are identified.",
      "processing_time": 59.661465644836426,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "We follow the standard MARL algorithms and evaluation metrics within the literature, such as [8], [9], and [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to standard MARL algorithms and evaluation metrics. No verifiable resources are identified.",
      "processing_time": 60.60270428657532,
      "citing_paper_id": "273186556",
      "cited_paper_id": 26419660
    },
    {
      "context_text": "We follow the standard MARL algorithms and evaluation metrics within the literature, such as [8], [9], and [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to standard MARL algorithms and evaluation metrics. No verifiable resources are identified.",
      "processing_time": 60.60270428657532,
      "citing_paper_id": "273186556",
      "cited_paper_id": 232092445
    },
    {
      "context_text": "A BLATION S TUDY In this section, we conduct the ablation studies mainly in LBF 2 players 2 food fully cooperative environment since rewards in LBF are sparser compared to MPE [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to another paper for comparison. The citation is used to support the choice of environment for ablation studies.",
      "processing_time": 60.976457357406616,
      "citing_paper_id": "273186556",
      "cited_paper_id": 235417602
    },
    {
      "context_text": "With the rising applications of multi-agent systems, it is increasingly crucial for individual agents to cooperate or compete in dynamic environments without relying on centralized control [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of cooperation or competition in multi-agent systems. No verifiable resources are identified.",
      "processing_time": 60.19643974304199,
      "citing_paper_id": "273186556",
      "cited_paper_id": 235417602
    },
    {
      "context_text": "LBF [1] is a challenging sparse reward environment designed for MARL training.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a benchmark environment (LBF) which is not a dataset but a challenge environment.",
      "processing_time": 60.006797552108765,
      "citing_paper_id": "273186556",
      "cited_paper_id": 235417602
    },
    {
      "context_text": "[16] finds that code-writing LLMs can be re-purposed to write robot policy code.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about the capabilities of LLMs in writing robot policy code.",
      "processing_time": 59.66742730140686,
      "citing_paper_id": "273186556",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "For instance, [5] improve agent exploration by aligning LLM-suggested goals with observed behaviors, while [13] generate actions conditioned on language-based prompts during online RL. Similarly, [6] uses LLMs to provide scalar rewards that guide training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.99122405052185,
      "citing_paper_id": "273186556",
      "cited_paper_id": 256615643
    },
    {
      "context_text": "For instance, [5] improve agent exploration by aligning LLM-suggested goals with observed behaviors, while [13] generate actions conditioned on language-based prompts during online RL. Similarly, [6] uses LLMs to provide scalar rewards that guide training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.99122405052185,
      "citing_paper_id": "273186556",
      "cited_paper_id": 257255456
    },
    {
      "context_text": "Recent studies have showcased LLMs deployed as embodied agents [3], [4], as well as their use in guiding reinforcement learning (RL) as ELLM, which leverages LLMs to suggest goals [5], and work focusing on aligning LLM-provided actions with RL policies [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 60.16677498817444,
      "citing_paper_id": "273186556",
      "cited_paper_id": 257255456
    },
    {
      "context_text": "Recent studies have showcased LLMs deployed as embodied agents [3], [4], as well as their use in guiding reinforcement learning (RL) as ELLM, which leverages LLMs to suggest goals [5], and work focusing on aligning LLM-provided actions with RL policies [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 60.16677498817444,
      "citing_paper_id": "273186556",
      "cited_paper_id": 262055166
    },
    {
      "context_text": "The Strategy Generation is crucial for several reasons: Recent works like [17] and [18] have demonstrated the success of enhancing LLMs performance by providing relevant environment code.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of enhancing LLMs with environment code. No verifiable resources are identified.",
      "processing_time": 60.59831118583679,
      "citing_paper_id": "273186556",
      "cited_paper_id": 262053612
    },
    {
      "context_text": "Additionally, studies like [17] and [18] exploit LLMs’ prior knowledge and code-generation capabilities to produce reward functions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs and their application in generating reward functions.",
      "processing_time": 59.857115745544434,
      "citing_paper_id": "273186556",
      "cited_paper_id": 262053612
    },
    {
      "context_text": "And [14] highlights the limitations of LLMs in handling complex low-level tasks.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the limitations of LLMs in handling complex low-level tasks.",
      "processing_time": 60.31482529640198,
      "citing_paper_id": "273186556",
      "cited_paper_id": 267411736
    },
    {
      "context_text": "LLM-based multi-agent systems have been employed in diverse applications requiring varied agent roles and collaborative decision-making [10], [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based multi-agent systems. No verifiable resources are identified.",
      "processing_time": 60.747775077819824,
      "citing_paper_id": "273186556",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "LLM-based multi-agent systems have been employed in diverse applications requiring varied agent roles and collaborative decision-making [10], [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based multi-agent systems. No verifiable resources are identified.",
      "processing_time": 60.747775077819824,
      "citing_paper_id": "273186556",
      "cited_paper_id": 269921354
    },
    {
      "context_text": "Moreover, only limited work has explored the integration of LLMs with MARL [10], leaving open questions about leveraging LLMs for multi-agent decision-making.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general area of research. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 60.87411952018738,
      "citing_paper_id": "273186556",
      "cited_paper_id": 269921354
    },
    {
      "context_text": "To the best of our knowledge, this work represents one of the first trials to fuse the high-level reasoning and planning abilities of LLMs with MARL, pointing a new direction for scalable and efficient multi-agent coordination [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general direction for research combining LLMs and MARL.",
      "processing_time": 60.1564245223999,
      "citing_paper_id": "273186556",
      "cited_paper_id": 269921354
    },
    {
      "context_text": "In particular, prior work concentrated on reducing the cross-entropy between the distributions of teacher and student policies (Parisotto et al., 2016; Schmitt et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the technique of reducing cross-entropy between teacher and student policies.",
      "processing_time": 61.2646644115448,
      "citing_paper_id": "272330340",
      "cited_paper_id": 3883991
    },
    {
      "context_text": "Ψ a denotes the basic attention structure (Vaswani et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (attention mechanism) from the paper 'Attention is All you Need'.",
      "processing_time": 60.85611534118652,
      "citing_paper_id": "272330340",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "…rationale and transferring it sequentially, in line with the rationale dataset construction, the attention module Ψ includes a causal attention Ψ c (Vaswani et al., 2017) and gated attention Ψ g (Xue et al., 2020 where α is a scaling factor that regulates the influence of the attention…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models, not datasets.",
      "processing_time": 59.98999309539795,
      "citing_paper_id": "272330340",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "A relevant subset of policy distillation in RL is transferring the teacher policy in a supervised fashion (Yin & Pan, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for transferring teacher policy in reinforcement learning.",
      "processing_time": 59.83561873435974,
      "citing_paper_id": "272330340",
      "cited_paper_id": 29167732
    },
    {
      "context_text": "…contrastive learning loss is formulated as where ˆ z = Ψ ◦ Φ Enc ( g ; θ Pre , θ Pos ) , d represents the sum of a distance metric within the embedding space ˆ z ∈ Z corresponding to an element of the rationale embedding sequence (Chen et al., 2020; Oord et al., 2018), and ϵ is a margin parameter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the formulation of a contrastive learning loss, which is a methodological aspect.",
      "processing_time": 61.957722425460815,
      "citing_paper_id": "272330340",
      "cited_paper_id": 49670925
    },
    {
      "context_text": "The detail of instructions and excutable plans are listed in (Aeronautiques et al., 1998).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to a paper about planning domain definition language.",
      "processing_time": 59.99062132835388,
      "citing_paper_id": "272330340",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Here, s ∈ S is a state space, a ∈ A is an action space, P : The distinct aspect of embodied agents’ environment lies in its nature of partial observations, featured as an observation space o ∈ Ω and a conditional observation probability O : S × A → Ω (Sutton & Barto, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts from reinforcement learning.",
      "processing_time": 59.126601696014404,
      "citing_paper_id": "272330340",
      "cited_paper_id": 60035920
    },
    {
      "context_text": "Note that D is a distance function such as Kullback-Leibler divergence (Kullback & Leibler, 1951) and γ is a discount factor of the environment.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a distance function and a discount factor. No verifiable resources are identified.",
      "processing_time": 60.56143498420715,
      "citing_paper_id": "272330340",
      "cited_paper_id": 116908168
    },
    {
      "context_text": "For the planning-policy Φ P , we utilize a pre-trained language model with a decoder structure, specifically gpt2 (Radford et al., 2019), as our default setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a pre-trained language model, gpt2, but does not refer to it as a dataset. It is used as a model for the planning-policy, which is not within the scope of dataset extraction.",
      "processing_time": 63.76106405258179,
      "citing_paper_id": "272330340",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "For the reasoning-policy Φ R , we utilize a pre-trained language model with an encoder-decoder structure, specifically t5-small (Raffel et al., 2020), as our default setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a pre-trained language model (t5-small) but does not refer to any specific dataset. The citation is about using a model, not a dataset.",
      "processing_time": 62.07522511482239,
      "citing_paper_id": "272330340",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "…and transferring it sequentially, in line with the rationale dataset construction, the attention module Ψ includes a causal attention Ψ c (Vaswani et al., 2017) and gated attention Ψ g (Xue et al., 2020 where α is a scaling factor that regulates the influence of the attention mechanisms’ outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses attention mechanisms and their application in sequence data, but no dataset names are provided.",
      "processing_time": 61.30016779899597,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208527654
    },
    {
      "context_text": "For evaluation, we use the ALFRED (Shrid-har et al., 2020) environment.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the ALFRED environment, which is a benchmark for grounded instruction interpretation. However, it is not a traditional dataset but an environment used for evaluation.",
      "processing_time": 61.29732060432434,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "• Through extensive experiments on ALFRED, we show D E D ER ’s effectiveness and efficiency in achieving robust zero-shot performance for unseen embodied tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFRED' as a benchmark, but it does not refer to a specific, downloadable dataset. It is likely a benchmark or challenge, which is excluded unless it refers to a specific dataset.",
      "processing_time": 64.10135698318481,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "ALFRED features 58 different object types (e.g., bread) and 26 receptacle types (e.g., plate) across 120 various indoor scenes (e.g., kitchen).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFRED' but does not refer to it as a dataset. It describes a benchmark for grounded instruction interpretation, which is more aligned with a method or benchmark rather than a traditional dataset.",
      "processing_time": 64.09747052192688,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "We use two different metrics in AL-FRED (Shridhar et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics used in AL-FRED but does not specify any datasets. AL-FRED itself is a benchmark, not a dataset.",
      "processing_time": 60.92135977745056,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Using the ALFRED benchmark (Shridhar et al., 2020), our experiments exhibit the advantages of D E D ER .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the ALFRED benchmark, which is a specific dataset used for evaluating models on grounded instruction following tasks. However, the ALFRED benchmark is primarily a challenge/leaderboard and not a standalone dataset.",
      "processing_time": 64.50901937484741,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "For embodied reasoning tasks, ALFRED features a wide variety of interactive elements including 58 distinct object types (e.g., bread) and 26 recep-tacles object types (e.g., plate) across 120 different indoor scenes (e.g., kitchen).",
      "catation_intent": "research work",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFRED, which is a benchmark for embodied reasoning tasks. However, it does not specify a dataset but rather describes the components of the benchmark itself.",
      "processing_time": 62.361870765686035,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "ALFRED We utilize ALFRED (Shridhar et al., 2020), which provides comprehensive vision-and-language navigation and rearrangement tasks for embodied AI.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "ALFRED is a benchmark for grounded instruction interpretation, but it does not qualify as a dataset under the given criteria. It is more of a task or challenge set.",
      "processing_time": 62.55534839630127,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "This complexity and diversity makes ALFREDD an ideal benchmark for evaluating models that emphasize hierarchy, modularity, and advanced reasoning and planning capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ALFREDD as a benchmark, but it does not specify a dataset. The title 'ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks' suggests it is a benchmark, not a dataset.",
      "processing_time": 64.95712089538574,
      "citing_paper_id": "272330340",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "6) End2End (Micheli & Fleuret, 2021) is an embodied task planning method using a single-tier policy unlike D E D ER , which directly generates a plan from the inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'End2End'. No dataset names are present in the citation context.",
      "processing_time": 60.77466106414795,
      "citing_paper_id": "272330340",
      "cited_paper_id": 233289456
    },
    {
      "context_text": "Furthermore, the End2End baseline exhibits significantly low performance in directly conducting embodied task planning with the expert dataset, due to the limited reasoning capability of the sLM.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'expert dataset' but does not provide a specific name or identifier. It is used to highlight the limitations of the sLM in embodied task planning.",
      "processing_time": 62.05102801322937,
      "citing_paper_id": "272330340",
      "cited_paper_id": 233289456
    },
    {
      "context_text": "End2End (Micheli & Fleuret, 2021) is a method for embodied task planning that specifically utilizes the GPT-2 model, trained with direct supervision on expert data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'expert data' but does not specify a named dataset. The focus is on the method and the model used.",
      "processing_time": 60.390292167663574,
      "citing_paper_id": "272330340",
      "cited_paper_id": 233289456
    },
    {
      "context_text": "The hyperparameter settings for End2End are summarized in Table A.9.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only hyperparameter settings for a method called End2End.",
      "processing_time": 59.80880856513977,
      "citing_paper_id": "272330340",
      "cited_paper_id": 233289456
    },
    {
      "context_text": "This can be achieved by integrating Markov Decision Process (MDP) features such as goal, state, observation, action, return-to-go, and sub-goal, which RL formulations specify, into the reasoning process (Chane-Sane et al., 2021; Hausknecht & Stone, 2015; Chen et al., 2021; Janner et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and concepts related to reinforcement learning and Markov Decision Processes.",
      "processing_time": 59.953221797943115,
      "citing_paper_id": "272330340",
      "cited_paper_id": 235294299
    },
    {
      "context_text": "In the field of embodied control, there is a growing trend of utilizing LLMs for reasoning and execution of tasks in real-world settings (Brohan et al., 2023; Huang et al., 2022; Song et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a trend in the field. No verifiable resources are named.",
      "processing_time": 60.20232367515564,
      "citing_paper_id": "272330340",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "In evaluating in off-the-shelf devices, we adopt sLMs for these language planning baselines (Say-Can, ZSP, LLM-planner).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on using language models for planning baselines.",
      "processing_time": 60.5451340675354,
      "citing_paper_id": "272330340",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "2) ZSP (Huang et al., 2022) employs a step-wise planning to accomplish the embodied tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'embodied tasks' but does not specify a dataset. The context focuses on the method (ZSP) rather than a specific dataset.",
      "processing_time": 61.58885407447815,
      "citing_paper_id": "272330340",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "The language planning baselines (SayCan, LLM-Planner, ZSP), which are configured to adopt sLMs (LLaMA2, GPT2-large), exhibit low performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the performance of language planning baselines using specific language models.",
      "processing_time": 61.57969284057617,
      "citing_paper_id": "272330340",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Several works (Huang et al., 2022; Wu et al., 2023; Song et al., 2023; Singh et al., 2023) explore the grounding of LLMs to the environment through prompting based on sensory data, reference trajectories, and available skills.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear, verifiable datasets are identified.",
      "processing_time": 60.29650640487671,
      "citing_paper_id": "272330340",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "We also evaluate the dataset construction process without employing MDP-featured in-context learning and self-verification; This ablated method is denoted as Few-shot , as described in (Wei et al., 2022), where a fixed set of examples is used for prompting rationale extraction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'dataset construction process' but does not specify a named dataset. The reference to 'Few-shot' is a method, not a dataset.",
      "processing_time": 61.572314500808716,
      "citing_paper_id": "272330340",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…of LLMs. Numerous studies investigated the reasoning capabilities of LLMs, exploring meth-ods like retrieval-augmented in-context examples (Lewis et al., 2020; Ram et al., 2023), KG integration (Andrus et al., 2022; Baek et al., 2023), and CoT prompting (Wei et al., 2022; Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 60.705793619155884,
      "citing_paper_id": "272330340",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…of LLMs. Numerous studies investigated the reasoning capabilities of LLMs, exploring meth-ods like retrieval-augmented in-context examples (Lewis et al., 2020; Ram et al., 2023), KG integration (Andrus et al., 2022; Baek et al., 2023), and CoT prompting (Wei et al., 2022; Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 60.705793619155884,
      "citing_paper_id": "272330340",
      "cited_paper_id": 250287185
    },
    {
      "context_text": "In (Sumers et al., 2023), knowledge is distilled from pre-trained vision-language models to supervise the language grounded skills of instruction-following agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the process of distilling knowledge from pre-trained models. No clear, verifiable dataset names are present.",
      "processing_time": 61.645413398742676,
      "citing_paper_id": "272330340",
      "cited_paper_id": 256389594
    },
    {
      "context_text": "Subsequently, the rationale set undergoes LLM’s assessments, as discussed in (Sun et al., 2023), to be incorporated into the dataset D Rtn .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'dataset D Rtn' which appears to be a specific dataset used in the research. However, the name is not a multi-word proper noun, uppercase acronym, or hyphenated name with digits, making it less likely to be a verifiable resource.",
      "processing_time": 66.33486294746399,
      "citing_paper_id": "272330340",
      "cited_paper_id": 258297976
    },
    {
      "context_text": "The GPT series [2], [22], [23]is a decoder structure of the transformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (GPT series). No verifiable resources are identified.",
      "processing_time": 59.985549211502075,
      "citing_paper_id": "272532153",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "To address this issue, some efforts have been made to incorporate tasks from multiple modalities, such as GPT-4 [1] and Gato [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT-4 and Gato). No verifiable resources are identified.",
      "processing_time": 60.769235610961914,
      "citing_paper_id": "272532153",
      "cited_paper_id": 248722148
    },
    {
      "context_text": "For the first research trend, DeepMind constructed a generalist agent [8] that attempts to address more generalized multi-modal perception and cognitive decision-making problems through the sequence prediction capabilities of language models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a generalist agent constructed by DeepMind. No clear identifiers for datasets are present.",
      "processing_time": 60.76737952232361,
      "citing_paper_id": "272532153",
      "cited_paper_id": 248722148
    },
    {
      "context_text": "Another line of work involves the development of LLM-based Agents [9]–[13], including HuggingGPT [14] and AutoGPT [15], which extend LLM capabilities through external memory and the utilization of auxiliary tools.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 60.5875027179718,
      "citing_paper_id": "272532153",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "…open-source models like LLaMA have significantly reduced the entry barrier for massive AI models, the latest assessment results from the Vicuna team [26] indicate that the gap in capabilities between open-source large models and GPT in areas like Math, Coding, and Reasoning remains substantial.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance gap between open-source models and GPT in certain areas.",
      "processing_time": 60.33854627609253,
      "citing_paper_id": "272532153",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples of this paradigm include TaskMatrix.AI [26], HuggingGPT [14], and AutoGPT [15].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or systems. There are no verifiable resources that meet the criteria.",
      "processing_time": 60.32992935180664,
      "citing_paper_id": "272532153",
      "cited_paper_id": null
    },
    {
      "context_text": "It is initialized with the parameters from the text2vec-base [27] and frozen.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method (text2vec-base).",
      "processing_time": 59.44315266609192,
      "citing_paper_id": "272532153",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, Taori et al. [24] fine-tuned LLaMA and proposed Alpaca just two weeks after the release of LLaMA, improving the performance from 68% to 76% compared to ChatGPT.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and performance improvements.",
      "processing_time": 58.69949245452881,
      "citing_paper_id": "272532153",
      "cited_paper_id": null
    },
    {
      "context_text": "In this stage, a retrieval-augmented generation (RAG) [17] method is employed.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RAG). The context and title do not provide information about datasets used.",
      "processing_time": 60.751779079437256,
      "citing_paper_id": "275546429",
      "cited_paper_id": 218869575
    },
    {
      "context_text": "We conducted the experiments on the ALFWorld Benchmark [27], which integrates the TextWorld engine [28] to generate textual scenarios in household settings, thereby providing a text-based embodied interaction environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFWorld Benchmark"
      ],
      "dataset_descriptions": {
        "ALFWorld Benchmark": "Used to conduct experiments on planning capabilities in LLMs, integrating TextWorld to generate textual scenarios in household settings, focusing on text-based embodied interaction."
      },
      "confidence_score": 0.9,
      "reasoning": "ALFWorld Benchmark is mentioned as the environment where experiments were conducted, integrating TextWorld for generating textual scenarios.",
      "processing_time": 68.50678706169128,
      "citing_paper_id": "275546429",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Currently, methods based on fine-tuning and Reinforcement Learning from Human Feedback (RLHF) [12] have emerged as essential technological paradigms to achieve such alignment [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on fine-tuning and RLHF as paradigms for aligning language models.",
      "processing_time": 61.905789852142334,
      "citing_paper_id": "275546429",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "In particular, LLM-based embodied agents may generate misleading action “hallucination”, fabricating erroneous or unfounded content [11].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue of hallucination in LLMs. No verifiable resources are identified.",
      "processing_time": 60.801406145095825,
      "citing_paper_id": "275546429",
      "cited_paper_id": 246652372
    },
    {
      "context_text": "However, they are also known for their limitations, which include the propensity to generate content that diverges from human expectations, harmful responses, and inaccurate or false information (referred to as the “hal-lucination” phenomenon) [11].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general limitations of language models.",
      "processing_time": 58.72368669509888,
      "citing_paper_id": "275546429",
      "cited_paper_id": 246652372
    },
    {
      "context_text": "These studies employ LLMs as the central controller of agents to enhance their environmental grounding and interaction capabilities by integrating environment perception modules and tool-use strategies [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs in enhancing agent capabilities. No clear, verifiable datasets are referenced.",
      "processing_time": 61.22836232185364,
      "citing_paper_id": "275546429",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "By employing the inner and outer alignment, as shown in Table III, we significantly reduced hallucinations and enhanced performance on multiple open-source large language models, including LlaMA-2 [21], Bloomz [22], and OPT [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The cited papers do not provide additional information about datasets.",
      "processing_time": 60.115174531936646,
      "citing_paper_id": "275546429",
      "cited_paper_id": 248496292
    },
    {
      "context_text": "By employing the inner and outer alignment, as shown in Table III, we significantly reduced hallucinations and enhanced performance on multiple open-source large language models, including LlaMA-2 [21], Bloomz [22], and OPT [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The cited papers do not provide additional information about datasets.",
      "processing_time": 60.115174531936646,
      "citing_paper_id": "275546429",
      "cited_paper_id": 253264914
    },
    {
      "context_text": "7B [23] model, comparable in scale to LlaMA-2-7B, aiming to identify performance disparities among various pre-trained foundational models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model comparison. No dataset names are present in the citation span.",
      "processing_time": 60.336047887802124,
      "citing_paper_id": "275546429",
      "cited_paper_id": 248496292
    },
    {
      "context_text": "Additionally, our experimental studies on the Bloomz-3B [22] model aimed to deepen our understanding of how model scale influences efficacy, thereby broadening our insights into the impact of scale on algorithm performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Bloomz-3B). The context focuses on the impact of model scale on performance, which is not related to dataset usage.",
      "processing_time": 63.07648849487305,
      "citing_paper_id": "275546429",
      "cited_paper_id": 253264914
    },
    {
      "context_text": "In recent years, representative Large Language Models (LLMs), such as GPT-4 [1], Gemini [2], and LlaMA [3], have demonstrated outstanding, and in some cases, surpassing human performance in a wide range of natural language understanding and generation tasks [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance. There are no verifiable resources or datasets mentioned.",
      "processing_time": 60.575743198394775,
      "citing_paper_id": "275546429",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "Specifically, in the inner alignment, parameter-efficient fine-tuning (PEFT) methods including Q-Lora [24] and Deepspeed [26] are utilized to facilitate efficient model adaptation across diverse embodied tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PEFT methods such as Q-Lora and Deepspeed, but these are tools or methods, not datasets. No specific datasets are mentioned.",
      "processing_time": 61.57496500015259,
      "citing_paper_id": "275546429",
      "cited_paper_id": 258841328
    },
    {
      "context_text": "To effectively balance computational costs and prevent the catastrophic forgetting issue in foundation models, as shown on the left side of Fig 2, we employ the Q-LORA [24] efficient fine-tuning technique for the first phase of alignment.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Q-LORA) for fine-tuning quantized LLMs. The citation is focused on the technique rather than a dataset.",
      "processing_time": 62.55613851547241,
      "citing_paper_id": "275546429",
      "cited_paper_id": 258841328
    },
    {
      "context_text": "These approaches fine-tune models to adapt to specific tasks and utilize human feedback to optimize model behavior, aiming to increase the consistency between the outputs and human values and expectations [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 60.567282915115356,
      "citing_paper_id": "275546429",
      "cited_paper_id": 262824801
    },
    {
      "context_text": "Compared to traditional two-stage aligning methods using SFT and RL [9], the alignment strategy based on retrieval-augmented generation proposed in this paper demonstrates higher flexibility, and resource efficiency, and enhances the model’s interpretability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between different alignment strategies for large language models.",
      "processing_time": 59.49581742286682,
      "citing_paper_id": "275546429",
      "cited_paper_id": 262824801
    },
    {
      "context_text": "While LLM-based embodied agents show impressive performance across various tasks [7], [8], aligning their capabilities and behaviors with human preferences and executed action space remains a significant challenge [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges and capabilities of LLM-based embodied agents.",
      "processing_time": 59.492361068725586,
      "citing_paper_id": "275546429",
      "cited_paper_id": 262824801
    },
    {
      "context_text": "For the future trend of embodied intelligence towards collaboration and symbiosis with humans, it is crucial to ensure its safety and controllability [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is a general statement about the future of embodied intelligence.",
      "processing_time": 60.38825249671936,
      "citing_paper_id": "275546429",
      "cited_paper_id": 266191787
    },
    {
      "context_text": "However, transitioning such approaches to embodied agents in physical environments poses formidable challenges due to sparse rewards and the environments’ diversity and complexity [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in applying foundation models to robotics.",
      "processing_time": 58.88612389564514,
      "citing_paper_id": "275546429",
      "cited_paper_id": 266191787
    },
    {
      "context_text": "Beyond their natu-ral language processing capabilities, these foundation models have rapidly advanced in areas such as logical reasoning and task planning, bringing a new spark to the field of embodied intelligence [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advancements in foundation models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.98621439933777,
      "citing_paper_id": "275546429",
      "cited_paper_id": 266191787
    },
    {
      "context_text": "Current embodied agents’ action hallucinations mainly include (a) format hallucination, an incorrect control action; (b) state hallucination, misjudging states leading to non-executable actions [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of action hallucinations in embodied agents. No verifiable resources are identified.",
      "processing_time": 60.029162883758545,
      "citing_paper_id": "275546429",
      "cited_paper_id": 268532485
    },
    {
      "context_text": "These metrics together furnish a comprehensive framework for evaluating agent proficiency in complex embodied planning applications. c) Implementation Details: In this study, we use MuEP [30] demonstration dataset, extended from the ALF-World dataset, for model fine-tuning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MuEP",
        "ALF-World"
      ],
      "dataset_descriptions": {
        "MuEP": "Used for fine-tuning models in complex embodied planning applications, extending the ALF-World dataset to include multimodal data.",
        "ALF-World": "Serves as the base dataset for MuEP, providing foundational data for embodied planning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'MuEP' and 'ALF-World' datasets, which are used for model fine-tuning in the study. Both are specific, verifiable datasets.",
      "processing_time": 74.28857970237732,
      "citing_paper_id": "275546429",
      "cited_paper_id": 271497434
    },
    {
      "context_text": "H is initialized empty and records the human-robot interaction over time.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes an initialization step in a human-robot interaction system.",
      "processing_time": 60.27308964729309,
      "citing_paper_id": "272827816",
      "cited_paper_id": 11309330
    },
    {
      "context_text": "A key development in this field is the application of large language models (LLMs), such as GPT [6], [7], LLaMA [8], [9], and Mistral [10], which have proven to be powerful tools for understanding and generating human-like text.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 60.13868498802185,
      "citing_paper_id": "272827816",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "A key development in this field is the application of large language models (LLMs), such as GPT [6], [7], LLaMA [8], [9], and Mistral [10], which have proven to be powerful tools for understanding and generating human-like text.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 60.13868498802185,
      "citing_paper_id": "272827816",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "Alternatively, Liang et al. [5] utilize LLM for generating code policies that can be executed on the robot.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for generating code policies for robotic control.",
      "processing_time": 59.52304244041443,
      "citing_paper_id": "272827816",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "In recent years, the integration of artificial intelligence (AI) into robotics has led to significant advancements in automation and autonomous systems [1]–[5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in AI and robotics. No verifiable resources are identified.",
      "processing_time": 59.5316436290741,
      "citing_paper_id": "272827816",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Robotics transformer (RT)-2 [3] employs vision to enable a more generalizable approach to its former version RT-1 [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (RT-1 and RT-2). No verifiable resources are identified.",
      "processing_time": 59.94041991233826,
      "citing_paper_id": "272827816",
      "cited_paper_id": 254591260
    },
    {
      "context_text": "For instance, RT series [2], [3] introduces an end-to-end approach to applying transformers to robotics, requiring a large amount of data collected in seventeen months to map language to actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'RT series' which refers to research papers, not a specific dataset. No clear, verifiable dataset names are provided.",
      "processing_time": 60.03255486488342,
      "citing_paper_id": "272827816",
      "cited_paper_id": 254591260
    },
    {
      "context_text": "For instance, RT series [2], [3] introduces an end-to-end approach to applying transformers to robotics, requiring a large amount of data collected in seventeen months to map language to actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'RT series' which refers to research papers, not a specific dataset. No clear, verifiable dataset names are provided.",
      "processing_time": 60.03255486488342,
      "citing_paper_id": "272827816",
      "cited_paper_id": 273811174
    },
    {
      "context_text": "Vision-language models are also implemented to describe the error from the scene [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to vision-language models. No clear, verifiable resource is identified.",
      "processing_time": 59.43792247772217,
      "citing_paper_id": "272827816",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Conversely, SayCan [1] and Text2Motion [14] are notable for considering geometric feasibility when planning the action sequence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Text2Motion but does not specify a dataset. It refers to a method or system for generating feasible plans from natural language instructions.",
      "processing_time": 59.95905351638794,
      "citing_paper_id": "272827816",
      "cited_paper_id": 257663442
    },
    {
      "context_text": "Vemprala et al. [7] implement ChatGPT with HITL, emphasizing the dual benefits of enhanced model training through human feedback and increased safety during operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving ChatGPT and HITL. No verifiable resources are identified.",
      "processing_time": 59.383262157440186,
      "citing_paper_id": "272827816",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "On the contrary, we designed a multimodal LLM-based planner that incorporates action Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a multimodal LLM-based planner. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.3974826335907,
      "citing_paper_id": "272827816",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "Recent research indicates that LLMs may not fully replicate human reasoning capabilities [19].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about LLMs' reasoning capabilities.",
      "processing_time": 58.26978278160095,
      "citing_paper_id": "272827816",
      "cited_paper_id": 259982665
    },
    {
      "context_text": "A recent paper from [29] solves a self-recovery problem with an LLM-based planner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for solving a self-recovery problem using an LLM-based planner.",
      "processing_time": 59.67978239059448,
      "citing_paper_id": "272827816",
      "cited_paper_id": 262828493
    },
    {
      "context_text": "For feasibility verification, we utilized a reachability graph from R-LGP [31], a sampling-based approach that efficiently checks for path from a starting point to a goal while ensuring feasible configurations for the robot reaching towards the target object.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a reachability graph from R-LGP, which is a method or tool rather than a dataset. No specific dataset is identified or used in the described context.",
      "processing_time": 60.70736122131348,
      "citing_paper_id": "272827816",
      "cited_paper_id": 263620434
    },
    {
      "context_text": "Consequently, integrating HITL proves essential for maintaining the reliability and safety of robotic systems [20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of human-in-the-loop (HITL) for robotic systems.",
      "processing_time": 58.74359965324402,
      "citing_paper_id": "272827816",
      "cited_paper_id": 264564300
    },
    {
      "context_text": "The application of LLM models in robotics, driven by recent advancements in natural language processing, has become a focal point of intensive research [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application of LLMs in robotics. No verifiable resources are identified.",
      "processing_time": 59.343976974487305,
      "citing_paper_id": "272827816",
      "cited_paper_id": 265149884
    },
    {
      "context_text": "In contrast, we explore how onboard, LLM-based models can achieve similar cognitive capabilities while operating independently.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a general research direction.",
      "processing_time": 58.60980558395386,
      "citing_paper_id": "272827816",
      "cited_paper_id": 273811174
    },
    {
      "context_text": "This text-based-only fine-tuning approach enables the model to easily adapt to different robots without further modifications or re-parameterization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for adapting models to different robots.",
      "processing_time": 58.07570767402649,
      "citing_paper_id": "272827816",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "…prompting (Yao et al., 2022b; Shinn et al., 2023; Madaan et al., 2023) (e.g., the sequence of generate, self-evaluate, self-reflect), we present a modular perspective (Andreas et al., 2016) of designing and building agents from simple subtasks, with mostly intuitive natural language specification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on modular perspectives and prompting techniques.",
      "processing_time": 58.929171085357666,
      "citing_paper_id": "269188155",
      "cited_paper_id": 5276660
    },
    {
      "context_text": "…10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.662102460861206,
      "citing_paper_id": "269188155",
      "cited_paper_id": 19135734
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": 19135734
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": 216559511
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": 222133157
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": 255569874
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": 256846700
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 58.53387904167175,
      "citing_paper_id": "269188155",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al.,…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on comparing performance metrics of different reinforcement learning algorithms.",
      "processing_time": 59.36215662956238,
      "citing_paper_id": "269188155",
      "cited_paper_id": 19135734
    },
    {
      "context_text": "…1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3 independent trials.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 57.95995044708252,
      "citing_paper_id": "269188155",
      "cited_paper_id": 53115163
    },
    {
      "context_text": "…5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit are summarized over 3…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 58.03521227836609,
      "citing_paper_id": "269188155",
      "cited_paper_id": 216559511
    },
    {
      "context_text": "…(Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2 1M steps CoT (Wei et al., 2022) (GPT-4-turbo) N/A 3.9 ± 1.6 $20 Plan2Explore (Sekar et al., 2020) 2.1 ± 0.1% 2.1 ± 1.5 1M steps RND (Burda et al., 2018) 2.0 ± 0.1% 0.7 ± The results for AgentKit…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.217047691345215,
      "citing_paper_id": "269188155",
      "cited_paper_id": 216559511
    },
    {
      "context_text": "…et al., 2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et al., 2023; Madaan et al., 2023), and general problem solving (Brown et al., 2020; Yao et al., 2022b; Shinn et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No clear identifiers for datasets are present.",
      "processing_time": 58.047463178634644,
      "citing_paper_id": "269188155",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "…1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 57.89520025253296,
      "citing_paper_id": "269188155",
      "cited_paper_id": 222133157
    },
    {
      "context_text": "This design limits the agent in Minecraft to peace mode (Wang et al., 2023b) due to the inability to react to survival situations, an essential part of Crafter (Hafner, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing agent capabilities in Minecraft and Crafter.",
      "processing_time": 57.35265874862671,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "The consistency of our agent over SPRING is also reflected in the score metric, which favors low unlock rates for more achievements over consistently high unlock rates for less achievements (Hafner, 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric or scoring system used to evaluate agent performance.",
      "processing_time": 57.337730169296265,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Through environment interactions and error identification/- correction, the agent discovered two pieces of information regarding “wood per Do action” and “table wood consumption”, originally omitted by the instructions (Hafner, 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only interactions and discoveries made by an agent. No clear, verifiable resource names are provided.",
      "processing_time": 58.10689353942871,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "To demonstrate the full potential of AgentKit, we implement an agent for the Crafter (Hafner, 2021) game with several advanced capabilities: planning and dynamic goal prioritization; mistake identification and reflection; learning from experience.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Crafter' as a game used to implement an agent with advanced capabilities, but does not specify it as a dataset. No other datasets are mentioned.",
      "processing_time": 59.33411264419556,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Notably, the L A TEX source code (Hafner, 2021), from which Wu et al. (2023b) extrated the instructions, omitted many crucial details about the actual game, since the research paper was never intended to be a game manual.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a LaTeX source code which is not a dataset. The context is about the omission of game details in the source code, not about data usage.",
      "processing_time": 60.16109347343445,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Crafter (Hafner, 2021) is an open-world survival game featuring procedural generation, designed to benchmark RL algorithms.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Crafter, which is described as a game environment, not a dataset. No specific, verifiable dataset is mentioned.",
      "processing_time": 57.943764209747314,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "To demonstrate the power and versatility of AgentKit, we implement a WebShop (Yao et al., 2022a) agent and a Crafter (Hafner, 2021) agent, both achieving SOTA performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or agents. The context focuses on the implementation and performance of agents, not on the use of datasets.",
      "processing_time": 58.69144558906555,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "We demonstrate an agent for the Crafter (Hafner, 2021) game with several advanced capabilities: hierarchical planning, reflection, and learning from interactions.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a game environment called Crafter. The game itself is not a dataset but a simulation environment.",
      "processing_time": 58.26194763183594,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "First, we use Crafer (Hafner, 2021) for benchmarking and analysis of different modules, described in section 3.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Crafter' but does not specify it as a dataset. It is likely a benchmark or method, and the context does not provide enough information to confirm it as a reusable dataset.",
      "processing_time": 60.492499589920044,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "However, due to the omitted information (Hafner, 2021) on the amount of wood required for the table, the agent attempts to place the table with 1 wood and fails (step 8).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a scenario involving an agent's failure due to insufficient information.",
      "processing_time": 56.115802526474,
      "citing_paper_id": "269188155",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Recently, large language models (LLM) (Chowdhery et al., 2022; OpenAI, 2023; Manyika; Touvron et al., 2023; Jiang et al., 2023; Li et al., 2023; Parmar et al., 2024) have demonstrated remarkable performance in a variety of tasks, including embodied planning and acting (Wu et al., 2024a; Ahn et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 57.69196939468384,
      "citing_paper_id": "269188155",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "Recently, large language models (LLM) (Chowdhery et al., 2022; OpenAI, 2023; Manyika; Touvron et al., 2023; Jiang et al., 2023; Li et al., 2023; Parmar et al., 2024) have demonstrated remarkable performance in a variety of tasks, including embodied planning and acting (Wu et al., 2024a; Ahn et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 57.69196939468384,
      "citing_paper_id": "269188155",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "Yao et al. (2022b) solves simple natural language tasks through LLM interaction with chain-of-thought prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving tasks using LLMs with chain-of-thought prompts.",
      "processing_time": 57.46257972717285,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "As shown in Table 2, our zero-shot agent achieves a 5% performance improvement compared to SOTA few-shot agent, ReAct (Yao et al., 2022b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance between a zero-shot agent and a few-shot agent (ReAct).",
      "processing_time": 57.93214249610901,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et al., 2023; Madaan et al., 2023), and general problem solving (Brown et al., 2020; Yao et al., 2022b; Shinn et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and their applications. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.341745376586914,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et al., 2023; Madaan et al., 2023), and general problem solving (Brown et al., 2020; Yao et al., 2022b; Shinn et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and their applications. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.341745376586914,
      "citing_paper_id": "269188155",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "…2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et al., 2023; Madaan et al., 2023), and general problem solving (Brown et al., 2020; Yao et al., 2022b; Shinn et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and their applications. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.341745376586914,
      "citing_paper_id": "269188155",
      "cited_paper_id": 257900871
    },
    {
      "context_text": "…2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et al., 2023; Madaan et al., 2023), and general problem solving (Brown et al., 2020; Yao et al., 2022b; Shinn et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and their applications. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.341745376586914,
      "citing_paper_id": "269188155",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Method Act (Yaoetal.,2022b)PaLM-540B ReAct (Yaoetal., Yao et al., 2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is focused on describing the use of ReAct and PaLM-540B, which are not datasets.",
      "processing_time": 59.97183561325073,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Due to the specific nature of the shopping task, all prior LLM agents rely on few-shot demonstrations of complete human trajectories (Yao et al., 2022b; Shinn et al., 2023; Liu et al., 2023) to guide agent behavior.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'few-shot demonstrations of complete human trajectories' but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.85260844230652,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Due to the specific nature of the shopping task, all prior LLM agents rely on few-shot demonstrations of complete human trajectories (Yao et al., 2022b; Shinn et al., 2023; Liu et al., 2023) to guide agent behavior.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'few-shot demonstrations of complete human trajectories' but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.85260844230652,
      "citing_paper_id": "269188155",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "An error in one step can propogate and influence later steps (Yao et al., 2022b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept about errors propagating in steps.",
      "processing_time": 56.50966429710388,
      "citing_paper_id": "269188155",
      "cited_paper_id": 252762395
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 56.9535436630249,
      "citing_paper_id": "269188155",
      "cited_paper_id": 255569874
    },
    {
      "context_text": ") (Wu et al., 2023b) 27.3 ± 1.2 % 12.3 ± 0.7 $243 † DreamerV3 (Hafner et al., 2023) 14.5 ± 1.6% 11.7 ± 1.9 1M steps EDE (Jiang et al., 2022) 11.7 ± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model names and performance metrics. No verifiable resources are identified.",
      "processing_time": 56.9535436630249,
      "citing_paper_id": "269188155",
      "cited_paper_id": null
    },
    {
      "context_text": "Following Du et al. (2023); Wu et al. (2023b; 2024b), we provide the text description and instructions from Wu et al. (2023b) as input to the agent.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works. There is no indication of a dataset being used.",
      "processing_time": 57.12097692489624,
      "citing_paper_id": "269188155",
      "cited_paper_id": 256846700
    },
    {
      "context_text": "…2024) have demonstrated remarkable performance in a variety of tasks, including embodied planning and acting (Wu et al., 2024a; Ahn et al., 2022; Du et al., 2023; Wang et al., 2023c; Wu et al., 2023a;b; Wang et al., 2023a), QA or dialogue (Ouyang et al., 2022; Hendrycks et al., 2020; Bubeck et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and tasks. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 58.718605518341064,
      "citing_paper_id": "269188155",
      "cited_paper_id": 256846700
    },
    {
      "context_text": "…± 1.0% N/A 1M steps DreamerV2 (Hafner et al., 2020) 10.0 ± 1.2% 9.0 ± 1.7 1M steps SPRING (GPT-4-turbo on official repo) 7.74% ‡ 7.1 ± 3.4 $85 ELLM (Du et al., 2023) N/A 6.0 ± 0.4 5M steps Rainbow (Hessel et al., 2018) 4.3 ± 0.2% 5.0 ± 1.3 1M steps PPO (Schulman et al., 2017) 4.6 ± 0.3% 4.2 ± 1.2…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 57.228246450424194,
      "citing_paper_id": "269188155",
      "cited_paper_id": 256846700
    },
    {
      "context_text": "The existing agent frameworks (Significant-Gravitas; Chase, 2022; Khattab et al., 2023; Shinn et al., 2023) do not follow explicit reasoning procedures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only agent frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.44341039657593,
      "citing_paper_id": "269188155",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Shinn et al. (2023) further adds the ability to read task feedback signals.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability related to reading task feedback signals.",
      "processing_time": 56.38206195831299,
      "citing_paper_id": "269188155",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Unlike existing works that identify a particular structure of structured prompting (Yao et al., 2022b; Shinn et al., 2023; Madaan et al., 2023) (e.g., the sequence of generate, self-evaluate, self-reflect), we present a modular perspective (Andreas et al., 2016) of designing and building agents…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and a discussion on structured prompting and modular perspectives.",
      "processing_time": 57.060412883758545,
      "citing_paper_id": "269188155",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Wu et al. (2024b) observes that simple chain-of-thoughts agents (Wei et al., 2022) generate good actions in the first few steps of the Tower of Hanoi, but their performance quickly degrades in later steps.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of chain-of-thought agents in the Tower of Hanoi task.",
      "processing_time": 57.3137583732605,
      "citing_paper_id": "269188155",
      "cited_paper_id": 263608611
    },
    {
      "context_text": "However, two challenges remain to be overcome to apply LLMs to general real-world agent (Wooldridge & Jennings, 1995; Laird et al., 1987; Russell, 2010) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses challenges in applying LLMs to real-world tasks.",
      "processing_time": 57.916163206100464,
      "citing_paper_id": "269188155",
      "cited_paper_id": null
    },
    {
      "context_text": "This corroborates that LLMs struggle to process mathematical data [35], [36].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs struggle with mathematical data. No verifiable resource names are provided.",
      "processing_time": 57.585538387298584,
      "citing_paper_id": "274776721",
      "cited_paper_id": 18709606
    },
    {
      "context_text": "In this example, HIVE uses claude-3-5-sonnet-20241022 as the underlying LLM. Snapshots from the corresponding game are shown at the bottom. vironment [9] established key benchmarks for multi-agent control [10], [11].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to multi-agent control environments and benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 57.905256271362305,
      "citing_paper_id": "274776721",
      "cited_paper_id": 60440549
    },
    {
      "context_text": "In this example, HIVE uses claude-3-5-sonnet-20241022 as the underlying LLM. Snapshots from the corresponding game are shown at the bottom. vironment [9] established key benchmarks for multi-agent control [10], [11].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to multi-agent control environments and benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 57.905256271362305,
      "citing_paper_id": "274776721",
      "cited_paper_id": 213176860
    },
    {
      "context_text": "In this example, HIVE uses claude-3-5-sonnet-20241022 as the underlying LLM. Snapshots from the corresponding game are shown at the bottom. vironment [9] established key benchmarks for multi-agent control [10], [11].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to multi-agent control environments and benchmarks. No clear, verifiable datasets are identified.",
      "processing_time": 57.905256271362305,
      "citing_paper_id": "274776721",
      "cited_paper_id": 232092445
    },
    {
      "context_text": "The growing capabilities of Large Language Models (LLMs) have opened up new frontiers in artificial intelligence, including enabling human-AI collaboration across diverse and complex domains [1]–[3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.40974688529968,
      "citing_paper_id": "274776721",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "Cicero [3] showed that LLMs can achieve human-level performance in Diplomacy through natural language negotiation and tactical coordination.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs achieved human-level performance in Diplomacy. No verifiable resources are named.",
      "processing_time": 58.23600149154663,
      "citing_paper_id": "274776721",
      "cited_paper_id": 253759631
    },
    {
      "context_text": "SMACv2 [12] extends this work, introducing procedurally generated scenarios requiring generalizations.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'procedurally generated scenarios' which could imply a dataset, but there is no specific dataset name provided. The citation is more about the method or benchmark itself rather than a specific dataset.",
      "processing_time": 60.286536693573,
      "citing_paper_id": "274776721",
      "cited_paper_id": 254685791
    },
    {
      "context_text": "The player does not receive information about each unit’s health, as it would be difficult for them to assess the health of hundreds of units [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about player interaction in a game-like scenario.",
      "processing_time": 56.74366617202759,
      "citing_paper_id": "274776721",
      "cited_paper_id": 256783608
    },
    {
      "context_text": "Using behavior trees for low-level actions with LLMs has been explored in robotics, translating the behavior tree from one domain to another [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of behavior trees and LLMs in robotics. No verifiable resources are identified.",
      "processing_time": 57.86819100379944,
      "citing_paper_id": "274776721",
      "cited_paper_id": 257219432
    },
    {
      "context_text": "…the OpenAI, Claude, and Gemini families, and one small open model as a baseline: • 4o : gpt-4o-2024-11-20 [29]; • 4o-mini :gpt-4o-mini-2024-07-18 [29]; • oi-mini : o1-mini-2024-09-12 [29]: • Sonnet : claude-3-5-sonnet-20241022 [30]; • Haiku : claude-3-5-haiku-20241022 [30]; • Gemini 2 :…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several model names but does not refer to any specific datasets. The context is focused on describing different models used in the research.",
      "processing_time": 58.03584599494934,
      "citing_paper_id": "274776721",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "…closed-source models from the OpenAI, Claude, and Gemini families, and one small open model as a baseline: • 4o : gpt-4o-2024-11-20 [29]; • 4o-mini :gpt-4o-mini-2024-07-18 [29]; • oi-mini : o1-mini-2024-09-12 [29]: • Sonnet : claude-3-5-sonnet-20241022 [30]; • Haiku :…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.5376341342926,
      "citing_paper_id": "274776721",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "…Claude, and Gemini families, and one small open model as a baseline: • 4o : gpt-4o-2024-11-20 [29]; • 4o-mini :gpt-4o-mini-2024-07-18 [29]; • oi-mini : o1-mini-2024-09-12 [29]: • Sonnet : claude-3-5-sonnet-20241022 [30]; • Haiku : claude-3-5-haiku-20241022 [30]; • Gemini 2 : models/gemini-2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several model names but does not refer to any specific datasets. The context is focused on describing different models used in the research.",
      "processing_time": 58.274418115615845,
      "citing_paper_id": "274776721",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "A recent study of the effect of the temperature for code generation with ChatGPT [38] highlights that a temperature of 0 reduces the non-determinism but does not guarantee strict determinism.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the non-determinism of ChatGPT in code generation, but does not mention any specific datasets.",
      "processing_time": 57.35605978965759,
      "citing_paper_id": "274776721",
      "cited_paper_id": 260682436
    },
    {
      "context_text": "AndroidArene [22] is a benchmark and environment that evaluates an LLM’s ability to navigate an operating system (e.g., using a smartphone with apps), showing this to be a current limitation of LLMs. Models also struggle to generalize beyond their training data when moving toward geospatial…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AndroidArene' as a benchmark and environment, but it does not refer to a specific, downloadable dataset. It is more of a benchmark or challenge environment.",
      "processing_time": 59.505666732788086,
      "citing_paper_id": "274776721",
      "cited_paper_id": 267617061
    },
    {
      "context_text": "…to navigate an operating system (e.g., using a smartphone with apps), showing this to be a current limitation of LLMs. Models also struggle to generalize beyond their training data when moving toward geospatial modalities, prompting the need for dedicated Vision-Language Geo-Foundation models [23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of LLMs and a need for Vision-Language Geo-Foundation models.",
      "processing_time": 57.84996843338013,
      "citing_paper_id": "274776721",
      "cited_paper_id": 270440269
    },
    {
      "context_text": "Incorporating findings of recent research focusing on improving LLMs’ interaction in a GUI environment [40] and geospatial reasoning [41] could further improve HIVE’s ability to handle multi-modal inputs from the player and the game interface.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research findings and potential improvements for HIVE. No verifiable resources are named.",
      "processing_time": 58.499202728271484,
      "citing_paper_id": "274776721",
      "cited_paper_id": 270576866
    },
    {
      "context_text": "Incorporating findings of recent research focusing on improving LLMs’ interaction in a GUI environment [40] and geospatial reasoning [41] could further improve HIVE’s ability to handle multi-modal inputs from the player and the game interface.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research findings and potential improvements for HIVE. No verifiable resources are named.",
      "processing_time": 58.499202728271484,
      "citing_paper_id": "274776721",
      "cited_paper_id": 276482111
    },
    {
      "context_text": "This echoes recent research that shows VLMs often struggle with trivial tasks (e.g., determining if two circles intersect) and a necessity for spatial reasoning [24].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about the capabilities of vision language models.",
      "processing_time": 56.92129921913147,
      "citing_paper_id": "274776721",
      "cited_paper_id": 271064506
    },
    {
      "context_text": "Furthermore, long-range reasoning problems are exacerbated because these models are prone to hallucinations [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with long-range reasoning problems in models.",
      "processing_time": 56.91841244697571,
      "citing_paper_id": "274776721",
      "cited_paper_id": 273025556
    },
    {
      "context_text": "Although algorithms have been proposed to fortify against hallucination in multi-step reasoning by dividing up reasoning steps, the problem of long-range causal reasoning remains unsolved [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and challenges in causal reasoning.",
      "processing_time": 55.997891664505005,
      "citing_paper_id": "274776721",
      "cited_paper_id": 273025556
    },
    {
      "context_text": "LLMs also struggle with physical common-sense reasoning in 3D environments, performing worse than human children on basic spatial reasoning tasks [20].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general finding about LLMs' performance on spatial reasoning tasks.",
      "processing_time": 57.21878099441528,
      "citing_paper_id": "274776721",
      "cited_paper_id": 273695554
    },
    {
      "context_text": "A recent benchmark for using LLM based on vision for simple adversarial games (e.g., Tic-Tac-Toe, Reversi, Gomoku, and Chess) shows that current models struggle to perform well [42], highlighting the need for further research.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is excluded as per instructions.",
      "processing_time": 56.23094296455383,
      "citing_paper_id": "274776721",
      "cited_paper_id": 276766708
    },
    {
      "context_text": "5-pro [31]; • Gemini-flash : gemini-1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The terms '5-pro' and 'Gemini-flash' do not appear to be datasets and lack context to determine their nature.",
      "processing_time": 59.57843780517578,
      "citing_paper_id": "274776721",
      "cited_paper_id": null
    },
    {
      "context_text": "0-flash-exp [31]; • Gemini-pro : gemini-1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names '0-flash-exp' and 'Gemini-pro' do not appear to be datasets but could be models, tools, or other resources.",
      "processing_time": 60.66158699989319,
      "citing_paper_id": "274776721",
      "cited_paper_id": null
    },
    {
      "context_text": "5-flash [31]; • Llama3 (8B) : Llama3 (8B) [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models (Llama3). No verifiable resources are identified.",
      "processing_time": 56.8597195148468,
      "citing_paper_id": "274776721",
      "cited_paper_id": null
    },
    {
      "context_text": "Several studies have explored robust navigation in complex environments, but these systems struggle with real-time adaptability when unexpected situations or changes in human behavior arise [4], [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in navigation systems.",
      "processing_time": 55.42919564247131,
      "citing_paper_id": "272911445",
      "cited_paper_id": 6936600
    },
    {
      "context_text": "The recent LLM-integrated office assistant robot, OfficeMate [2], shows progress in human-robot interaction but still depends on voice commands or typing on a mounted computer, limiting its usability for effective long-distance communication in real-time.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a robot system called OfficeMate. The context focuses on the limitations of the robot's interaction methods.",
      "processing_time": 57.320576190948486,
      "citing_paper_id": "272911445",
      "cited_paper_id": 9885884
    },
    {
      "context_text": "• CoT [7]: Build on Direct by encouraging step-by-step reasoning, enhancing structured thinking.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing structured thinking in large language models.",
      "processing_time": 56.2227246761322,
      "citing_paper_id": "272911445",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Recent advancements in agentic systems, particularly with LLM integration, have significantly improved reasoning performance especially under Chain-of-Thought mechanism [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in agentic systems and LLM integration. No verifiable resources are identified.",
      "processing_time": 57.980003118515015,
      "citing_paper_id": "272911445",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Recent advancements in agentic systems, particularly with LLM integration, have significantly improved reasoning performance especially under Chain-of-Thought mechanism [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in agentic systems and LLM integration. No verifiable resources are identified.",
      "processing_time": 57.980003118515015,
      "citing_paper_id": "272911445",
      "cited_paper_id": 267311877
    },
    {
      "context_text": "Recent advancements in agentic systems, particularly with LLM integration, have significantly improved reasoning performance especially under Chain-of-Thought mechanism [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in agentic systems and LLM integration. No verifiable resources are identified.",
      "processing_time": 57.980003118515015,
      "citing_paper_id": "272911445",
      "cited_paper_id": 267412980
    },
    {
      "context_text": "…to guide low-level actions, our approach ensures better alignment with the evolving needs of the task, outperforming end-to-end methods like CoT [7] and ReAct [8] that directly generate actions, as their lack of a global plan limits their ability to take adaptive actions across the entire task…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (CoT and ReAct). The context focuses on comparing approaches for generating actions in tasks.",
      "processing_time": 58.04511022567749,
      "citing_paper_id": "272911445",
      "cited_paper_id": 249017743
    },
    {
      "context_text": "Additional studies further demonstrate improved human-agent interactions, decision-making, and intelligence sharing in multi-agent settings for user-centric tasks [12]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements in human-agent interactions, decision-making, and intelligence sharing. No verifiable resources are identified.",
      "processing_time": 58.23991584777832,
      "citing_paper_id": "272911445",
      "cited_paper_id": 259342058
    },
    {
      "context_text": "Additional studies further demonstrate improved human-agent interactions, decision-making, and intelligence sharing in multi-agent settings for user-centric tasks [12]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements in human-agent interactions, decision-making, and intelligence sharing. No verifiable resources are identified.",
      "processing_time": 58.23991584777832,
      "citing_paper_id": "272911445",
      "cited_paper_id": 270561990
    },
    {
      "context_text": "Additional studies further demonstrate improved human-agent interactions, decision-making, and intelligence sharing in multi-agent settings for user-centric tasks [12]–[15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general improvements in human-agent interactions, decision-making, and intelligence sharing. No verifiable resources are identified.",
      "processing_time": 58.23991584777832,
      "citing_paper_id": "272911445",
      "cited_paper_id": null
    },
    {
      "context_text": "Methods to track human positions from dialogue have been proposed to enhance situational awareness, but their reliance on speech makes real-time decision-making challenging [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for tracking human positions from dialogue. No verifiable resources are identified.",
      "processing_time": 56.95574760437012,
      "citing_paper_id": "272911445",
      "cited_paper_id": 266198147
    },
    {
      "context_text": "Methods like ReAct [8] and Reflexion [9] enhance reasoning by step-by-step thinking, while multi-agent frameworks like Mobile-Agent-v2 [10] and AppAgent [11] have been effectively applied to tasks like GUI operations on smart devices for long-horizon tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.44244432449341,
      "citing_paper_id": "272911445",
      "cited_paper_id": 266435868
    },
    {
      "context_text": "Methods like ReAct [8] and Reflexion [9] enhance reasoning by step-by-step thinking, while multi-agent frameworks like Mobile-Agent-v2 [10] and AppAgent [11] have been effectively applied to tasks like GUI operations on smart devices for long-horizon tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.44244432449341,
      "citing_paper_id": "272911445",
      "cited_paper_id": 271270974
    },
    {
      "context_text": "However, initial attempts lacked accuracy due to LLMs’ hallucinations ([10]–[13]), which resulted in incorrect plans.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with LLMs' hallucinations. No verifiable resources are identified.",
      "processing_time": 57.61592698097229,
      "citing_paper_id": "272880675",
      "cited_paper_id": 246652372
    },
    {
      "context_text": "For example, SayCan [6] grounds LLM-based plans in the environment by choosing the most affordable action based on camera input, but it lacks feedback systems to correct logical errors and practical feasibility issues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SayCan) and its limitations. The context focuses on the capabilities and shortcomings of the method rather than the use of a dataset.",
      "processing_time": 60.06741237640381,
      "citing_paper_id": "272880675",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Nonetheless, naively increasing the number of LLMs and assigning different LLMs as experts for various tasks ([16], [17]) or using additional LLMs as validators for plan refinement [18] have proven to be less effective and computationally heavy ([19], [20]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving LLMs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.98679852485657,
      "citing_paper_id": "272880675",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Nonetheless, naively increasing the number of LLMs and assigning different LLMs as experts for various tasks ([16], [17]) or using additional LLMs as validators for plan refinement [18] have proven to be less effective and computationally heavy ([19], [20]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving LLMs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.98679852485657,
      "citing_paper_id": "272880675",
      "cited_paper_id": 260440590
    },
    {
      "context_text": "Nonetheless, naively increasing the number of LLMs and assigning different LLMs as experts for various tasks ([16], [17]) or using additional LLMs as validators for plan refinement [18] have proven to be less effective and computationally heavy ([19], [20]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or approaches involving LLMs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.98679852485657,
      "citing_paper_id": "272880675",
      "cited_paper_id": 267413178
    },
    {
      "context_text": "Reflexion [18] attempts to address these issues by using an LLM-based Reinforcement Learning methodology: separate instances of LLMs are used to generate plans and observe the output and associated reward.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using LLMs for generating plans and observing outputs.",
      "processing_time": 57.19779348373413,
      "citing_paper_id": "272880675",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Additionally, recent approaches suggest leveraging visual scene information to generate plans, employing either VLMs ([31]–[34]) or object detectors ([35], [36]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on using visual scene information and VLMs or object detectors for generating plans.",
      "processing_time": 58.555118560791016,
      "citing_paper_id": "272880675",
      "cited_paper_id": 258947250
    },
    {
      "context_text": "To enhance planning capabilities, previous methods employed multiple LLMs or visual language models (VLMs) as critics to refine and update the initial plans ([14], [15]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of multiple LLMs or VLMs as critics to refine and update initial plans.",
      "processing_time": 59.59345889091492,
      "citing_paper_id": "272880675",
      "cited_paper_id": 267637077
    },
    {
      "context_text": "Recent work has explored data-driven heuristics to improve TAMP efficiency [8, 18], but these domain-specific heuristics lack generalizability across domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only domain-specific heuristics. No clear identifiers for datasets are present.",
      "processing_time": 56.97635793685913,
      "citing_paper_id": "268531200",
      "cited_paper_id": 67872126
    },
    {
      "context_text": "The simulation has an important role in Robotics [38, 39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to simulation in robotics.",
      "processing_time": 55.88160037994385,
      "citing_paper_id": "268531200",
      "cited_paper_id": 76665670
    },
    {
      "context_text": "Recent Large Language Models (LLMs) pre-trained on web-scale text data have demonstrated emergent capabilities in reasoning [11] and planning [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the citation span.",
      "processing_time": 57.668314695358276,
      "citing_paper_id": "268531200",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Robot Planning with LLMs: Recent Large Language Models (LLMs) [19, 20] encode vast world knowledge and exibit the emergent capability for planning [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.33666205406189,
      "citing_paper_id": "268531200",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Robot Planning with LLMs: Recent Large Language Models (LLMs) [19, 20] encode vast world knowledge and exibit the emergent capability for planning [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.33666205406189,
      "citing_paper_id": "268531200",
      "cited_paper_id": 246485514
    },
    {
      "context_text": "Robot Planning with LLMs: Recent Large Language Models (LLMs) [19, 20] encode vast world knowledge and exibit the emergent capability for planning [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.33666205406189,
      "citing_paper_id": "268531200",
      "cited_paper_id": 258762577
    },
    {
      "context_text": "Robot Planning with LLMs: Recent Large Language Models (LLMs) [19, 20] encode vast world knowledge and exibit the emergent capability for planning [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.33666205406189,
      "citing_paper_id": "268531200",
      "cited_paper_id": 270621087
    },
    {
      "context_text": "In particular, pre-trained LLMs are employed to generate discrete actions [12] and continuous parameters in an auto-regressive manner when provided in-context prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pre-trained LLMs for generating actions and parameters. No verifiable resources are identified.",
      "processing_time": 58.32935118675232,
      "citing_paper_id": "268531200",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "In particular, pre-trained LLMs are employed to generate discrete actions [12] and continuous parameters in an auto-regressive manner when provided in-context prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pre-trained LLMs for generating actions and parameters. No verifiable resources are identified.",
      "processing_time": 58.32935118675232,
      "citing_paper_id": "268531200",
      "cited_paper_id": 246485514
    },
    {
      "context_text": "In particular, pre-trained LLMs are employed to generate discrete actions [12] and continuous parameters in an auto-regressive manner when provided in-context prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of pre-trained LLMs for generating actions and parameters. No verifiable resources are identified.",
      "processing_time": 58.32935118675232,
      "citing_paper_id": "268531200",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Pre-trained LLMs can perform task planning [13], generate continuous parameters [14], and reason on environment feedback [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only capabilities of LLMs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.15310859680176,
      "citing_paper_id": "268531200",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Pre-trained LLMs can perform task planning [13], generate continuous parameters [14], and reason on environment feedback [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only capabilities of LLMs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.15310859680176,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259501163
    },
    {
      "context_text": "Pre-trained LLMs have been applied for task planning of robots or embodied agents [13, 21–27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to pre-trained LLMs and their application in task planning for robots or embodied agents.",
      "processing_time": 57.79246473312378,
      "citing_paper_id": "268531200",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Notably, Inner Monologue [13] takes in textualized environment feedback and generate actions to execute, while ReAct [28] further advanced this closed-loop approach by integrating reasoning and acting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 57.98509645462036,
      "citing_paper_id": "268531200",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Notably, Inner Monologue [13] takes in textualized environment feedback and generate actions to execute, while ReAct [28] further advanced this closed-loop approach by integrating reasoning and acting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 57.98509645462036,
      "citing_paper_id": "268531200",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Notably, Inner Monologue [13] takes in textualized environment feedback and generate actions to execute, while ReAct [28] further advanced this closed-loop approach by integrating reasoning and acting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 57.98509645462036,
      "citing_paper_id": "268531200",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Notably, Inner Monologue [13] takes in textualized environment feedback and generate actions to execute, while ReAct [28] further advanced this closed-loop approach by integrating reasoning and acting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 57.98509645462036,
      "citing_paper_id": "268531200",
      "cited_paper_id": 261898118
    },
    {
      "context_text": "Following previous attempts in utilizing LLMs for reasoning and planning [11–13], we prompt a pre-trained LLM to generate motion failure reasoning and action sequences in text format.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of pre-trained LLMs for generating motion failure reasoning and action sequences.",
      "processing_time": 57.45288348197937,
      "citing_paper_id": "268531200",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "AutoTAMP [30] leverages an LLM to translate natural-language task specifications into a formal language processable by off-the-shelf TAMP algorithms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AutoTAMP) that uses LLMs to translate task specifications. No verifiable datasets are referenced.",
      "processing_time": 58.75293517112732,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259138811
    },
    {
      "context_text": "Furthermore, LLMs have been applied to solve TAMP problems [29, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs have been applied to solve TAMP problems. No verifiable resources are identified.",
      "processing_time": 57.935981035232544,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259138811
    },
    {
      "context_text": "To mitigate this complexity, researchers have crafted symbolic domains [10, 33, 34] to prune infeasible symbolic action sequences, or learned heuristic samplers to sample action parameters [9, 35] that lead to feasible plans.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for task and motion planning.",
      "processing_time": 55.94610524177551,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259243610
    },
    {
      "context_text": "Second, it uses the LLM as a domain-independent informed parameter sampler to generate continuous action parameters, which benefits from the implicit heuristics of the LLM [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs as a parameter sampler. No verifiable resources are identified.",
      "processing_time": 57.57867455482483,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259501163
    },
    {
      "context_text": "In recent years, TAMP has enabled significant advances [1–7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references a body of work that has enabled significant advances in TAMP.",
      "processing_time": 58.331013202667236,
      "citing_paper_id": "268531200",
      "cited_paper_id": 259689601
    },
    {
      "context_text": "A primitive action a P A is parameterized by object variables ¯ o and continuous parameters θ , which can be instantiated with specific objects o “ p o 1 , ..., o m q and parameter values to produce a ground action a , e.g ., Place p block , r 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for parameterizing actions in robotics, which is not a dataset.",
      "processing_time": 57.924115896224976,
      "citing_paper_id": "268531200",
      "cited_paper_id": 260378734
    },
    {
      "context_text": "A primitive action a P A is parameterized by object variables ¯ o and continuous parameters θ , which can be instantiated with specific objects o “ p o 1 , ..., o m q and parameter values to produce a ground action a , e.g ., Place p block , r 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for parameterizing actions in robotics, which is not a dataset.",
      "processing_time": 57.924115896224976,
      "citing_paper_id": "268531200",
      "cited_paper_id": 268531565
    },
    {
      "context_text": "To identify and locate individual objects, we employed Grounded Segment Anything [40] for object segmentation, initially segmenting objects in the 2D RGB image and then projecting the results onto the corresponding 3D point cloud.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a method (Grounded Segment Anything) for object segmentation but does not refer to any specific dataset. The method is used for segmenting objects in 2D RGB images and projecting them onto 3D point clouds.",
      "processing_time": 60.95295286178589,
      "citing_paper_id": "268531200",
      "cited_paper_id": 267212047
    },
    {
      "context_text": "Human activity prediction in smart environments aims to enhance the user experience by reducing the need for direct interaction and enabling more proactive support (Minor and Cook 2017; Tax 2018; Almeida and Azkune 2018; Yang et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to human activity prediction in smart environments. No verifiable resources are identified.",
      "processing_time": 57.54356908798218,
      "citing_paper_id": "274789599",
      "cited_paper_id": 32893583
    },
    {
      "context_text": "Human activity prediction in smart environments aims to enhance the user experience by reducing the need for direct interaction and enabling more proactive support (Minor and Cook 2017; Tax 2018; Almeida and Azkune 2018; Yang et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to human activity prediction in smart environments. No verifiable resources are identified.",
      "processing_time": 57.54356908798218,
      "citing_paper_id": "274789599",
      "cited_paper_id": 57366648
    },
    {
      "context_text": "Human activity prediction in smart environments aims to enhance the user experience by reducing the need for direct interaction and enabling more proactive support (Minor and Cook 2017; Tax 2018; Almeida and Azkune 2018; Yang et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to human activity prediction in smart environments. No verifiable resources are identified.",
      "processing_time": 57.54356908798218,
      "citing_paper_id": "274789599",
      "cited_paper_id": 214770841
    },
    {
      "context_text": "Traditional methods tend to over-look the incomplete textual descriptions of events, opting instead to assign different IDs to recorded log signals to create event embedding tables for encoding (Tax 2018; Park et al. 2022; Wang et al. 2022b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 58.10662245750427,
      "citing_paper_id": "274789599",
      "cited_paper_id": 57366648
    },
    {
      "context_text": "Traditional methods tend to over-look the incomplete textual descriptions of events, opting instead to assign different IDs to recorded log signals to create event embedding tables for encoding (Tax 2018; Park et al. 2022; Wang et al. 2022b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 58.10662245750427,
      "citing_paper_id": "274789599",
      "cited_paper_id": 249304070
    },
    {
      "context_text": "Additionally, due to the limited number of users in previous smart space datasets (Tax 2018; Park et al. 2022; Wang et al. 2022b; Dunne, Morris, and Harper 2023), prior research in smart spaces typically employs a single model to predict a range of user actions, neglecting the individ-a r X i v :…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'smart space datasets' but does not provide specific names. It refers to a limitation in the number of users in previous datasets, which is relevant to the planning capabilities of LLMs in smart environments.",
      "processing_time": 60.88713502883911,
      "citing_paper_id": "274789599",
      "cited_paper_id": 57366648
    },
    {
      "context_text": "Additionally, due to the limited number of users in previous smart space datasets (Tax 2018; Park et al. 2022; Wang et al. 2022b; Dunne, Morris, and Harper 2023), prior research in smart spaces typically employs a single model to predict a range of user actions, neglecting the individ-a r X i v :…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'smart space datasets' but does not provide specific names. It refers to a limitation in the number of users in previous datasets, which is relevant to the planning capabilities of LLMs in smart environments.",
      "processing_time": 60.88713502883911,
      "citing_paper_id": "274789599",
      "cited_paper_id": 249304070
    },
    {
      "context_text": "Additionally, due to the limited number of users in previous smart space datasets (Tax 2018; Park et al. 2022; Wang et al. 2022b; Dunne, Morris, and Harper 2023), prior research in smart spaces typically employs a single model to predict a range of user actions, neglecting the individ-a r X i v :…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'smart space datasets' but does not provide specific names. It refers to a limitation in the number of users in previous datasets, which is relevant to the planning capabilities of LLMs in smart environments.",
      "processing_time": 60.88713502883911,
      "citing_paper_id": "274789599",
      "cited_paper_id": 253954372
    },
    {
      "context_text": "Advancements in the field have introduced sophisticated models like LSTM and GRU networks for discrete sequence modeling (Tax 2018; Krishna et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about advancements in modeling techniques, not the use of datasets.",
      "processing_time": 58.087830543518066,
      "citing_paper_id": "274789599",
      "cited_paper_id": 57366648
    },
    {
      "context_text": "…space attempt to build models from historical interaction data between users and their environments, with the goal of automating the recognition of users’ intentions and planning the next move (Tax 2018; Maharjan et al. 2019; Mohamed, El-Kilany, and El-Tazi 2022; Dunne, Morris, and Harper 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to historical interaction data. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 59.87493109703064,
      "citing_paper_id": "274789599",
      "cited_paper_id": 57366648
    },
    {
      "context_text": "…space attempt to build models from historical interaction data between users and their environments, with the goal of automating the recognition of users’ intentions and planning the next move (Tax 2018; Maharjan et al. 2019; Mohamed, El-Kilany, and El-Tazi 2022; Dunne, Morris, and Harper 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to historical interaction data. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 59.87493109703064,
      "citing_paper_id": "274789599",
      "cited_paper_id": 253954372
    },
    {
      "context_text": "…et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The cited papers also do not mention datasets in their titles.",
      "processing_time": 57.880407094955444,
      "citing_paper_id": "274789599",
      "cited_paper_id": 119181611
    },
    {
      "context_text": "…et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The cited papers also do not mention datasets in their titles.",
      "processing_time": 57.880407094955444,
      "citing_paper_id": "274789599",
      "cited_paper_id": 238634419
    },
    {
      "context_text": "…et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The cited papers also do not mention datasets in their titles.",
      "processing_time": 57.880407094955444,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "We utilized official implementations for BERT4Rec, DuoRec, CL4Rec, and RecFormer, while for the other models, we utilized PyTorch to replicate these methods as per the specifications provided in their original paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and frameworks but does not reference any specific datasets. The focus is on the implementation and replication of models.",
      "processing_time": 57.69206380844116,
      "citing_paper_id": "274789599",
      "cited_paper_id": 119181611
    },
    {
      "context_text": "We utilized official implementations for BERT4Rec, DuoRec, CL4Rec, and RecFormer, while for the other models, we utilized PyTorch to replicate these methods as per the specifications provided in their original paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and frameworks but does not reference any specific datasets. The focus is on the implementation and replication of models.",
      "processing_time": 57.69206380844116,
      "citing_paper_id": "274789599",
      "cited_paper_id": 238634419
    },
    {
      "context_text": "We utilized official implementations for BERT4Rec, DuoRec, CL4Rec, and RecFormer, while for the other models, we utilized PyTorch to replicate these methods as per the specifications provided in their original paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and frameworks but does not reference any specific datasets. The focus is on the implementation and replication of models.",
      "processing_time": 57.69206380844116,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "These methods are divided into two categories: 1) algorithms for human activity prediction in smart space: CasLSTM (Krishna et al. 2018), DnnLSTM (Park et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several algorithms but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 57.68994402885437,
      "citing_paper_id": "274789599",
      "cited_paper_id": 119181611
    },
    {
      "context_text": "These methods are divided into two categories: 1) algorithms for human activity prediction in smart space: CasLSTM (Krishna et al. 2018), DnnLSTM (Park et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several algorithms but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 57.68994402885437,
      "citing_paper_id": "274789599",
      "cited_paper_id": 238634419
    },
    {
      "context_text": "These methods are divided into two categories: 1) algorithms for human activity prediction in smart space: CasLSTM (Krishna et al. 2018), DnnLSTM (Park et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several algorithms but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 57.68994402885437,
      "citing_paper_id": "274789599",
      "cited_paper_id": 249304070
    },
    {
      "context_text": "These methods are divided into two categories: 1) algorithms for human activity prediction in smart space: CasLSTM (Krishna et al. 2018), DnnLSTM (Park et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online shopping: BERT4Rec (Sun et al. 2019), DuoRec (Qiu et al. 2022), CL4Rec (Xie et al. 2022), and RecFormer (Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several algorithms but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 57.68994402885437,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "In this process, we employ a transformer decoder endowed with self-attention mechanisms to aggregate sequence information (Radford et al. 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer decoder with self-attention).",
      "processing_time": 56.821579694747925,
      "citing_paper_id": "274789599",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Using a Chain of Thought approach (Brown et al. 2020), we construct prompts that enable the LLM to output possible user actions based on the event logs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain of Thought) and a type of model (LLM).",
      "processing_time": 57.73837685585022,
      "citing_paper_id": "274789599",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Prompt tuning is typically used to bridge the gap between a pre-trained model’s objectives and the specific requirements of a downstream task (Brown et al. 2020).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prompt tuning) used to adapt pre-trained models.",
      "processing_time": 56.96986150741577,
      "citing_paper_id": "274789599",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "This approach has recently been employed for recommendation (Wang et al. 2023; Li, Zhang, and Chen 2023; Wu et al. 2024; Yang et al. 2024; Dong et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent work in recommendation systems. No verifiable resources are identified.",
      "processing_time": 57.97392225265503,
      "citing_paper_id": "274789599",
      "cited_paper_id": 246863587
    },
    {
      "context_text": "This approach has recently been employed for recommendation (Wang et al. 2023; Li, Zhang, and Chen 2023; Wu et al. 2024; Yang et al. 2024; Dong et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent work in recommendation systems. No verifiable resources are identified.",
      "processing_time": 57.97392225265503,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "PEPLER (Li, Zhang, and Chen 2023) utilized item features and ID embeddings to generate recommendation explanations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of item features and ID embeddings. No clear, verifiable dataset names are present.",
      "processing_time": 58.556477785110474,
      "citing_paper_id": "274789599",
      "cited_paper_id": 246863587
    },
    {
      "context_text": "…two categories: 1) algorithms for human activity prediction in smart space: CasLSTM (Krishna et al. 2018), DnnLSTM (Park et al. 2022), ConvLMLC (Wang et al. 2022b), and ASGen (Song et al. 2024a); 2) sequential recommendation algorithms applicable to scenarios such as movie watching and online…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.3671600818634,
      "citing_paper_id": "274789599",
      "cited_paper_id": 249304070
    },
    {
      "context_text": "Alternative methods employing CNN (Wang et al. 2022b) and DNN (Park et al. 2022) have been proposed to extract local features from the status of all sensors, us-ing LSTM networks for the next activity prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of CNN and DNN for feature extraction and LSTM for next activity prediction.",
      "processing_time": 59.50101399421692,
      "citing_paper_id": "274789599",
      "cited_paper_id": 249304070
    },
    {
      "context_text": "Sequential recommendation systems are designed to anticipate a user’s next point of interest, which could range from movies to products, by analyzing their past interactions (Wang et al. 2022a; Hou et al. 2023; Rajput et al. 2023; Dong et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about sequential recommendation systems. No verifiable resources are identified.",
      "processing_time": 57.65948486328125,
      "citing_paper_id": "274789599",
      "cited_paper_id": 252904698
    },
    {
      "context_text": "Sequential recommendation systems are designed to anticipate a user’s next point of interest, which could range from movies to products, by analyzing their past interactions (Wang et al. 2022a; Hou et al. 2023; Rajput et al. 2023; Dong et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about sequential recommendation systems. No verifiable resources are identified.",
      "processing_time": 57.65948486328125,
      "citing_paper_id": "274789599",
      "cited_paper_id": 253098091
    },
    {
      "context_text": "This integration results in a more effective model than RecFormer.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.05387783050537,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "Furthermore, while both RecFormer and our method utilize LLM-enhanced textual Figure 3: Impact of event frequency on prediction performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a figure and a comparison between methods. No verifiable resources are identified.",
      "processing_time": 58.2288122177124,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "On the other hand, sequential recommendation focuses on the online activity context suggesting content that users may find engaging based on their behaviors (Hou et al. 2022; Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general focus on sequential recommendation based on user behaviors.",
      "processing_time": 57.192076444625854,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "On the other hand, sequential recommendation focuses on the online activity context suggesting content that users may find engaging based on their behaviors (Hou et al. 2022; Li et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general focus on sequential recommendation based on user behaviors.",
      "processing_time": 57.192076444625854,
      "citing_paper_id": "274789599",
      "cited_paper_id": null
    },
    {
      "context_text": "While recommendation systems can access extensive side information, which includes the names, brands, and attributes of purchased items (Li et al. 2023), and the content of movies that interest users, data logs from devices and sensors in smart spaces are often sparse or even incomplete.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to general types of data such as 'data logs from devices and sensors in smart spaces' and 'side information' which are too generic.",
      "processing_time": 60.86067867279053,
      "citing_paper_id": "274789599",
      "cited_paper_id": 258841284
    },
    {
      "context_text": "Simplifying the interaction between users and smart devices to allow the devices to understand and automatically adapt to users’ needs in various scenarios represents an urgent yet challenging goal (Desolda, Ardito, and Matera 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the challenge of user-device interaction.",
      "processing_time": 57.61584949493408,
      "citing_paper_id": "274789599",
      "cited_paper_id": null
    },
    {
      "context_text": "…OOD detection techniques on 1) an extensive suite of synthetic text-based domains, 2) simulated and real-world closed-loop quadrotor experiments resembling a drone delivery service, and 3) careful recreations of recent real-world failure modes of autonomous vehicles in the CARLA simulator [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CARLA"
      ],
      "dataset_descriptions": {
        "CARLA": "Used to recreate real-world failure modes of autonomous vehicles, focusing on simulating urban driving scenarios to test OOD detection techniques."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the CARLA simulator for recreating real-world failure modes of autonomous vehicles. CARLA is a specific, verifiable resource used in the research.",
      "processing_time": 66.3322343826294,
      "citing_paper_id": "271098044",
      "cited_paper_id": 5550767
    },
    {
      "context_text": "For example, through robust training (e.g., [41]) or by adapting the model to changing conditions (e.g., [16, 8]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods or approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.786250591278076,
      "citing_paper_id": "271098044",
      "cited_paper_id": 7646250
    },
    {
      "context_text": "For example, through robust training (e.g., [41]) or by adapting the model to changing conditions (e.g., [16, 8]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods or approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.786250591278076,
      "citing_paper_id": "271098044",
      "cited_paper_id": 213662188
    },
    {
      "context_text": "We implement the MPC (3) core to the AESOP algorithm in Python using the OSQP [48] solver using linearized dynamics and a quadratic objective.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of the OSQP solver, which is a method or tool, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 59.26639413833618,
      "citing_paper_id": "271098044",
      "cited_paper_id": 32182109
    },
    {
      "context_text": "…II shows the results of a quantitative ablation over a set of 500 scenarios and compares AESOP with 1) the fallback-planning method in [45] (which ignores the runtime monitor’s latency), and 2) a naive planner that only tries to compute a recovery plan post-hoc after detecting a dangerous event.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only a set of scenarios used for evaluation. No clear, verifiable dataset names are provided.",
      "processing_time": 58.78651452064514,
      "citing_paper_id": "271098044",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "…real-time integration within dynamic, agile robotic Out-of-Distribution Robustness: The fact that learning-based systems often behave unreliably on data that is dissimilar from their training data has been extensively documented in both the machine learning and robotics literature [14, 37, 33, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general issues with out-of-distribution robustness in learning-based systems.",
      "processing_time": 58.46662902832031,
      "citing_paper_id": "271098044",
      "cited_paper_id": 174803437
    },
    {
      "context_text": "…real-time integration within dynamic, agile robotic Out-of-Distribution Robustness: The fact that learning-based systems often behave unreliably on data that is dissimilar from their training data has been extensively documented in both the machine learning and robotics literature [14, 37, 33, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general issues with out-of-distribution robustness in learning-based systems.",
      "processing_time": 58.46662902832031,
      "citing_paper_id": "271098044",
      "cited_paper_id": 215786368
    },
    {
      "context_text": "Definition 1 (Control Invariant Set [4]) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a definition from a cited paper. No verifiable resources are identified.",
      "processing_time": 58.267515420913696,
      "citing_paper_id": "271098044",
      "cited_paper_id": 195984066
    },
    {
      "context_text": "We now restate Theorem 1 and provide its proof, which relies on a recursive feasibility argument [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a theorem and its proof, which is not a verifiable resource.",
      "processing_time": 59.36693572998047,
      "citing_paper_id": "271098044",
      "cited_paper_id": 195984066
    },
    {
      "context_text": "…function: BERT-base (110M) and BERT-large (336M) uncased [10], Sentence Transformer MPNet (110M; BERT-base architecture trained for embeddings) [47, 38], completion and instruction-tuned Llama 2 models (7B) [51], Mistral (7.11B) [21, 52], and three OpenAI embedding models (parameters not…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 57.35068225860596,
      "citing_paper_id": "271098044",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "As such, CLIP’s limitations are related to the SCOD [44] and Mahalanobis distance [26] base-line OOD detectors (taken from [12]) that wrap the AV’s base object detector (DETR [7]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on OOD detection methods and their relation to CLIP's limitations.",
      "processing_time": 59.13806438446045,
      "citing_paper_id": "271098044",
      "cited_paper_id": 218889832
    },
    {
      "context_text": "As such, CLIP’s limitations are related to the SCOD [44] and Mahalanobis distance [26] base-line OOD detectors (taken from [12]) that wrap the AV’s base object detector (DETR [7]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on OOD detection methods and their relation to CLIP's limitations.",
      "processing_time": 59.13806438446045,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "In contrast, as we further ablate in Appendix F, the two-stage detection approach is unaffected by differences in visual appearance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.529892444610596,
      "citing_paper_id": "271098044",
      "cited_paper_id": 232045968
    },
    {
      "context_text": "…work has shown the merits of generalist FMs like LLMs in both domains: Studies have shown that zero-shot application of a FM (e.g., in [54], the authors apply CLIP zero-shot on ImageNet), vastly improves OOD generalization over previous approaches, like distributionally robust training [31, 54, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to evaluate the zero-shot capabilities of CLIP, focusing on out-of-distribution generalization compared to distributionally robust training methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ImageNet' as a dataset used for zero-shot application of a foundation model (CLIP). No other specific datasets are mentioned.",
      "processing_time": 65.88927245140076,
      "citing_paper_id": "271098044",
      "cited_paper_id": 235795331
    },
    {
      "context_text": "…work has shown the merits of generalist FMs like LLMs in both domains: Studies have shown that zero-shot application of a FM (e.g., in [54], the authors apply CLIP zero-shot on ImageNet), vastly improves OOD generalization over previous approaches, like distributionally robust training [31, 54, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to evaluate the zero-shot capabilities of CLIP, focusing on out-of-distribution generalization compared to distributionally robust training methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ImageNet' as a dataset used for zero-shot application of a foundation model (CLIP). No other specific datasets are mentioned.",
      "processing_time": 65.88927245140076,
      "citing_paper_id": "271098044",
      "cited_paper_id": 237420687
    },
    {
      "context_text": "Recent work has shown the merits of generalist FMs like LLMs in both domains: Studies have shown that zero-shot application of a FM (e.g., in [54], the authors apply CLIP zero-shot on ImageNet), vastly improves OOD generalization over previous approaches, like distributionally robust training [31,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used to demonstrate zero-shot capabilities of CLIP, focusing on out-of-distribution generalization compared to distributionally robust training methods."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'zero-shot application of a FM' using CLIP on ImageNet, which is a specific dataset. However, the primary focus is on the method (CLIP) rather than the dataset itself.",
      "processing_time": 68.31971168518066,
      "citing_paper_id": "271098044",
      "cited_paper_id": 237420687
    },
    {
      "context_text": "However, the community has not converged on rigorous methods for grounding FMs without compromising on their generalist zero-shot reasoning abilities (e.g., fine-tuning [24] or linear probing [54] often underperform OOD); prompt design remains a standard practice.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the challenges of fine-tuning and prompt design for zero-shot models.",
      "processing_time": 59.52927207946777,
      "citing_paper_id": "271098044",
      "cited_paper_id": 237420687
    },
    {
      "context_text": "…fashion, since s ( e i ; e )= − 1 for e i ∈D e . in Determining the threshold τ using empirical quantiles as (2) is a standard approach [39], but could be extended in future work to make precise guarantees on false positive or negative rates using recent results in conformal prediction [3, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and potential extensions using conformal prediction.",
      "processing_time": 57.56187891960144,
      "citing_paper_id": "271098044",
      "cited_paper_id": 237589920
    },
    {
      "context_text": "Even if we train e.g., classifiers to detect obstructions on landing zones, there may always remain a class of semantic failure modes that we have not accounted for.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses a general issue in training classifiers for robotic tasks, which is not tied to a particular dataset.",
      "processing_time": 59.90535569190979,
      "citing_paper_id": "271098044",
      "cited_paper_id": 253734315
    },
    {
      "context_text": "Even if we train e.g., classifiers to detect obstructions on landing zones, there may always remain a class of semantic failure modes that we have not accounted for.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses a general issue in training classifiers for robotic tasks, which is not tied to a particular dataset.",
      "processing_time": 59.90535569190979,
      "citing_paper_id": "271098044",
      "cited_paper_id": 257079001
    },
    {
      "context_text": "As such, substantial effort is being dedicated to the acceleration of these models, of which several popular strategies have emerged such as model distillation [15, 17], quantization [20, 55], and parameter sparsification [49, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for accelerating models such as distillation, quantization, and parameter sparsification.",
      "processing_time": 58.99068474769592,
      "citing_paper_id": "271098044",
      "cited_paper_id": 256826987
    },
    {
      "context_text": "As such, substantial effort is being dedicated to the acceleration of these models, of which several popular strategies have emerged such as model distillation [15, 17], quantization [20, 55], and parameter sparsification [49, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for accelerating models such as distillation, quantization, and parameter sparsification.",
      "processing_time": 58.99068474769592,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258461606
    },
    {
      "context_text": "As such, substantial effort is being dedicated to the acceleration of these models, of which several popular strategies have emerged such as model distillation [15, 17], quantization [20, 55], and parameter sparsification [49, 34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for accelerating models such as distillation, quantization, and parameter sparsification.",
      "processing_time": 58.99068474769592,
      "citing_paper_id": "271098044",
      "cited_paper_id": 259203115
    },
    {
      "context_text": "However, we nuance this finding by noting that the observations in the CARLA dataset were constructed by driving a car along simulated routes and placing object assets that trick the vehicle into making unsafe decision along those routes [12].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CARLA"
      ],
      "dataset_descriptions": {
        "CARLA": "Used to construct observations for testing autonomous vehicle safety, focusing on simulated routes and object placements that induce unsafe decisions."
      },
      "confidence_score": 1.0,
      "reasoning": "The CARLA dataset is mentioned as a specific resource used to construct observations for testing the safety of autonomous vehicle decisions.",
      "processing_time": 63.89056634902954,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "Instead, recent work showed that LLMs may provide a more general mechanism to detect context dependent safety hazards, especially those that are hard to measure with predefined performance metrics [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of LLMs. No verifiable resources are identified.",
      "processing_time": 58.485697507858276,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "We do so because, as noted in [12], the vision model suffers from a real-to-sim domain shift, and sometimes tends to, e.g., characterize nominal observations of stop signs as “images of stop signs”.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with a vision model. No dataset names are present in the citation context.",
      "processing_time": 58.981351137161255,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "In our ablations on self-driving scenarios, we adopt the semantic anomaly dataset presented in [12].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "semantic anomaly dataset"
      ],
      "dataset_descriptions": {
        "semantic anomaly dataset": "Used to evaluate self-driving scenario ablations, focusing on detecting semantic anomalies using large language models."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a 'semantic anomaly dataset' which is specific and plausible. The cited paper title confirms it is a dataset used for semantic anomaly detection.",
      "processing_time": 65.27390170097351,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "…that the internet-scale pretraining data provides FMs with strong zero-shot reasoning capabilities, which has enabled robots to perform complex tasks [5], identify and correct failures [18], and reason about potential safety hazards in their surroundings [12] without explicit training to do so.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of large language models. No clear, verifiable resource names are provided.",
      "processing_time": 58.75918459892273,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "As noted in [12], because CARLA’s synthetic visual features represent a distribution shift from the realistic images on which OWL-ViT was trained, OWL-ViT periodically hallucinates object detections, such as the presence of an anomaly when none is present, or misses an anomaly detection entirely.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (OWL-ViT) and a simulation environment (CARLA). No verifiable datasets are referenced.",
      "processing_time": 59.71086382865906,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "We run ablations on self-driving scenarios curated in [12].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable dataset. It only refers to 'self-driving scenarios' which is too generic and lacks a clear identifier.",
      "processing_time": 59.46647357940674,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "In the spirit of [12], we refer to such events as semantic failure modes , as they do not necessarily constitute violation of precise state constraints X , but instead depend on the qualitative context of the robot’s task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to semantic failure modes in the context of robot tasks.",
      "processing_time": 58.46550893783569,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "To do so, we follow [12] in using a VLM to convert the robot’s current visual observation into a text description of the environment.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method (VLM) for converting visual observations into text descriptions.",
      "processing_time": 59.21246385574341,
      "citing_paper_id": "271098044",
      "cited_paper_id": 258823112
    },
    {
      "context_text": "This is most likely because the CLIP embeddings contain both visual features (e.g., those suitable for object detection as used in [49]) and semantic features.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to visual features suitable for object detection. No clear, verifiable dataset names are present.",
      "processing_time": 59.62009358406067,
      "citing_paper_id": "271098044",
      "cited_paper_id": 259203115
    },
    {
      "context_text": "…BERT-base (110M) and BERT-large (336M) uncased [10], Sentence Transformer MPNet (110M; BERT-base architecture trained for embeddings) [47, 38], completion and instruction-tuned Llama 2 models (7B) [51], Mistral (7.11B) [21, 52], and three OpenAI embedding models (parameters not disclosed).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and architectures but does not refer to any specific datasets. The context is focused on describing the models used in the research.",
      "processing_time": 59.61290454864502,
      "citing_paper_id": "271098044",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "5s to return an output, a number consistent with the latency of cloud querying GPT-4 reported in [50] using a conversational chat-based prompting approach.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (GPT-4) and its latency. There are no verifiable resources that meet the criteria.",
      "processing_time": 60.11324501037598,
      "citing_paper_id": "271098044",
      "cited_paper_id": 264935466
    },
    {
      "context_text": "Various approaches utilizing these models have been developed for online use in applications in areas such as manipulation [19], navigation [43], drone flight [9], and long-horizon planning [5, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of models. No dataset names are present in the text.",
      "processing_time": 58.71183466911316,
      "citing_paper_id": "271098044",
      "cited_paper_id": 266550843
    },
    {
      "context_text": "FMs have also been used to define reinforcement learning reward functions [58], generate robot policy code [27], or create additional training data [56, 57, 2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is about using foundation models in various applications, which does not align with the criteria for extracting datasets.",
      "processing_time": 60.42289900779724,
      "citing_paper_id": "271098044",
      "cited_paper_id": 266906759
    },
    {
      "context_text": "TOPGUN significantly surpassed ReAct and DFSDT in all categories, achieving win rates of 80.27% versus GPT4-ReACT, 78.59% against GPT4-DFSDT, and 86.54% against ChatGPT-ReACT, showing improvements of 22.54% and 16.14% respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons between different models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.53491544723511,
      "citing_paper_id": "267681745",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Significant advancements in Large Language Models (LLMs) like GPT (Radford et al. (2018); Rad-ford et al. (2019); Brown et al. (2020); Achiam et al. (2023)) and PaLM (Chowdhery et al. (2023);Anil et al. (2023)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their advancements. No verifiable resources are identified.",
      "processing_time": 58.49296307563782,
      "citing_paper_id": "267681745",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Significant advancements in Large Language Models (LLMs) like GPT (Radford et al. (2018); Rad-ford et al. (2019); Brown et al. (2020); Achiam et al. (2023)) and PaLM (Chowdhery et al. (2023);Anil et al. (2023)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their advancements. No verifiable resources are identified.",
      "processing_time": 58.49296307563782,
      "citing_paper_id": "267681745",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Significant advancements in Large Language Models (LLMs) like GPT (Radford et al. (2018); Rad-ford et al. (2019); Brown et al. (2020); Achiam et al. (2023)) and PaLM (Chowdhery et al. (2023);Anil et al. (2023)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their advancements. No verifiable resources are identified.",
      "processing_time": 58.49296307563782,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258740735
    },
    {
      "context_text": "TOPGUN surpasses Reverse Chain and undergoes comparison with GPT4-DFSDT and GPT4-ReACT within gray box Preprint evaluations, emphasizing output trajectories.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.74500846862793,
      "citing_paper_id": "267681745",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Results : Win Rate comparisons for ToolLLaMa-ReACT, ToolLLaMA-DFSDT, ChatGPT-DFSDT, GPT4-DFSDT, and GPT4-TOPGUN against ChatGPT-ReACT and GPT4-TOPGUN are summarized, with averages taken from 7 runs per model pair, detailed in Tables 1 and 2.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.72924256324768,
      "citing_paper_id": "267681745",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "…math word problems using gpt-4 code interpreter with code-based self-verification\" Zhou et al. (2023b) and \"PAL: Program-aided Language Models\" Gao et al. (2023) have exploited code interpreters for zero-shot verified solving, substantially surpassing few-shot learning benchmarks by enabling…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of code interpreters and program-aided language models for solving math problems.",
      "processing_time": 60.779897928237915,
      "citing_paper_id": "267681745",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "To this end, few works also upheld the reasoning capability of LLMs using code like \"TORA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving\" Gou et al. (2023), \"Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification\" Zhou et al. (2023b) and \"PAL: Program-aided Language Models\" Gao et al. (2023) have exploited code interpreters for zero-shot verified solving, substantially surpassing few-shot learning benchmarks by enabling semi-verification of proposed solutions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the reasoning capabilities of LLMs using code interpreters.",
      "processing_time": 59.83709144592285,
      "citing_paper_id": "267681745",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "Works like The Chain of Code Li et al. (2023) and Program-of-thoughts Chen et al. (2022) are great examples of using code generation to improve decision-making for answering general open-domain questions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the use of code generation to improve decision-making, which is not directly related to dataset usage.",
      "processing_time": 61.060892820358276,
      "citing_paper_id": "267681745",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Works like The Chain of Code Li et al. (2023) and Program-of-thoughts Chen et al. (2022) are great examples of using code generation to improve decision-making for answering general open-domain questions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the use of code generation to improve decision-making, which is not directly related to dataset usage.",
      "processing_time": 61.060892820358276,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Our framework incorporates these retrievers, with Instructor-XL set as the default option, owing to its proven efficacy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Instructor-XL). No verifiable resources are identified.",
      "processing_time": 59.020592212677,
      "citing_paper_id": "267681745",
      "cited_paper_id": 254853816
    },
    {
      "context_text": "These include ToolBench IR Qin et al. (2023), APIRetriever Zan et al. (2022), Instructor-XL Su et al. (2022), and GEAR Lu et al. (2023b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods, but does not refer to any specific datasets. The context does not provide information about the usage of datasets in the research.",
      "processing_time": 60.68320298194885,
      "citing_paper_id": "267681745",
      "cited_paper_id": 254853816
    },
    {
      "context_text": ";) have demonstrated profound abilities in reasoning and following instructions over an extensive array of tasks Huang & Chang (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of large language models. No verifiable resources are identified.",
      "processing_time": 59.005367040634155,
      "citing_paper_id": "267681745",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "…LLMs to interact with external tools for addressing complex real-world challenges marks a significant area of interest (Hao et al. (2023) Preprint necessitate a sequence of intermediate reasoning steps (Schick et al. (2023);Lu et al. (2023a);Lu et al. (2023a);Patil et al. (2023);Qin et al. (2023)).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other research works. The cited papers' titles also do not provide any specific dataset names.",
      "processing_time": 59.715484380722046,
      "citing_paper_id": "267681745",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "…LLMs to interact with external tools for addressing complex real-world challenges marks a significant area of interest (Hao et al. (2023) Preprint necessitate a sequence of intermediate reasoning steps (Schick et al. (2023);Lu et al. (2023a);Lu et al. (2023a);Patil et al. (2023);Qin et al. (2023)).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other research works. The cited papers' titles also do not provide any specific dataset names.",
      "processing_time": 59.715484380722046,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Moreover, the code generation facilitated by this block benefits from validation through Reflexion, as outlined in Eq.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Reflexion. No verifiable resources are identified.",
      "processing_time": 58.16245150566101,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "In our framework, we leverage Reflexion Shinn et al. (2023), detailed in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called Reflexion. No verifiable datasets are referenced.",
      "processing_time": 58.85784912109375,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "4b. Leveraging Reflexion detailed in Eq.1, the framework iteratively refines responses to the query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called Reflexion.",
      "processing_time": 57.54145574569702,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "The integration of Reflexion Shinn et al. (2023) with Large Language Models (LLM) ρ and Python Interpreter I has significantly advanced coding tasks by enabling iterative code refinement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the integration of Reflexion with LLMs and a Python Interpreter for coding tasks.",
      "processing_time": 60.80516815185547,
      "citing_paper_id": "267681745",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Similarly, works such as Xu et al. (2023), which deploys language models for real-time code generation and command execution within controlled environments, are limited by their narrow tool range and a deficit in generalizability. strategic tool planning designed for complex tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of a method.",
      "processing_time": 57.15846824645996,
      "citing_paper_id": "267681745",
      "cited_paper_id": 263831032
    },
    {
      "context_text": "CodeSynth significantly improved F1-scores on both HumanEval-X and MBPP datasets, achieving a perfect score of 1.0 by the fifth iteration from initial scores of 0.844 and 0.912 , respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanEval-X",
        "MBPP"
      ],
      "dataset_descriptions": {
        "HumanEval-X": "Used to evaluate the performance of CodeSynth, focusing on F1-scores across multiple iterations, demonstrating improvements in code synthesis accuracy.",
        "MBPP": "Used to assess the effectiveness of CodeSynth, specifically measuring F1-scores and showing significant improvement over iterations, highlighting the model's capability in solving programming problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets, HumanEval-X and MBPP, which are used to evaluate the performance of CodeSynth. These datasets are specific and verifiable.",
      "processing_time": 75.2601728439331,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "A critical aspect of CodeSynth is the inclusion of an example return value, which is designed to mimic all potential operations the returned object might undergo during the verification process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general aspect of the CodeSynth system. No verifiable resources are identified.",
      "processing_time": 59.131094217300415,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "1 This paper details our methodology and its evaluation by first elucidating the background on Tool planning 2.1 and Code generation using LLM 2.2 followed by detailing individual components of the pipeline 3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general components of a methodology. There are no clear identifiers for datasets.",
      "processing_time": 58.81077599525452,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "These findings highlight CodeSynth’s ability to produce function signatures closely resembling the semantics of the target function.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of CodeSynth in producing function signatures.",
      "processing_time": 58.39308166503906,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Your Code-->\\n ``` Always remember to use .get() to fetch values from a dictionary or a JSON.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a general instruction about using .get() method in code. No verifiable resources are identified.",
      "processing_time": 59.89959454536438,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "In the context of our SwissNYF implementation, we have adopted a straightforward yet effective method for generating these function signatures, termed CodeSynth .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called CodeSynth. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.81658935546875,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Your Code-->\\n ``` Always remember if a function is to input or output an object assume the object to be a string.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a general rule about functions and objects. There are no verifiable resources or datasets mentioned.",
      "processing_time": 59.5977578163147,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Ultimately, the methodologies applied within CodeSynth can be encapsulated in Algo.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodology or tool called 'CodeSynth'. No verifiable datasets are referenced.",
      "processing_time": 58.9711639881134,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Prompts for CodeSynth can be documented in A.1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a section reference for prompts. No verifiable resources are identified.",
      "processing_time": 58.165470600128174,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Your Code-->\\n ``` Pseudo Function: TOPGUN prompt for code-based plan generation on ToolBench You are a Python code assistant.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a pseudo function and a cited paper title which is not a dataset.",
      "processing_time": 58.59626913070679,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Results: We evaluate CodeSynth across multiple reflection cycles, tracking the F1 score for each cycle to illustrate consistent enhancements in function signature quality, as depicted in Table 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of a method (CodeSynth) across multiple reflection cycles.",
      "processing_time": 58.75966143608093,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "The output generated by CodeSynth is illustrated in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a figure illustrating output. No verifiable resources are identified.",
      "processing_time": 58.44075036048889,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "Your Code-->\\n ``` Previous python code implementation: {} Self-reflection: {} Refactored Python code: CodeSynth prompt for function signature generation on PrivateEval You are a Python code assistant that can generate a pseudo Python function given its name, description, and arguments. function name: {} function description: {} Provided Libraries: {} Always remember to import the required classes from one of the provided library, according to the function arguments and the provided documentation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a refactored Python code and a prompt for function signature generation. No verifiable resources are identified.",
      "processing_time": 59.89341902732849,
      "citing_paper_id": "267681745",
      "cited_paper_id": 266051661
    },
    {
      "context_text": "While the default LPG local search doesn’t aim to minimize the changes to the suggested plan (there do exist versions of LPG that do this; see [23]) , the edit distances also give an idea of how partially or approximately correct the original LLM plan is.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (LPG) and a concept (edit distances). There are no clear identifiers for datasets in the text.",
      "processing_time": 60.75864315032959,
      "citing_paper_id": "260440590",
      "cited_paper_id": 1762453
    },
    {
      "context_text": "(2) both participant sets were asked to offer subjective feedback via the NASA-TLX assessment tool [10], gauging their cognitive load.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the NASA-TLX assessment tool, which is a method for assessing cognitive load, not a dataset. No specific dataset is mentioned.",
      "processing_time": 59.863117933273315,
      "citing_paper_id": "260440590",
      "cited_paper_id": 15252590
    },
    {
      "context_text": "It is interesting to note the similarities between this LLM+LPG approach, and the approaches used in case-based planning in the past [9, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to case-based planning approaches.",
      "processing_time": 57.90664100646973,
      "citing_paper_id": "260440590",
      "cited_paper_id": 15597128
    },
    {
      "context_text": "To see if the LLM generated plans can provide heuristic guidance to sound external planners, we use a local-search planner LPG [6] which generates plans by starting with a seed plan and iteratively repairing flaws until a correct plan is found.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a planner method. The context focuses on the use of the LPG planner for generating and repairing plans, which is a methodological approach rather than a dataset.",
      "processing_time": 61.780160427093506,
      "citing_paper_id": "260440590",
      "cited_paper_id": 18976919
    },
    {
      "context_text": "Specifically we show that a well known automated planner called LPG [6], that uses local search to locate and remove flaws in a candidate plan to make it correct, is able to repair the LLM plans with relative ease.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LPG, which is a planner, not a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 59.29541063308716,
      "citing_paper_id": "260440590",
      "cited_paper_id": 18976919
    },
    {
      "context_text": "In this work, we look at LLMs’ planning capabilities when the domain is given as part of the prompt (as is the standard practice in automated planning [8]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to automated planning practices.",
      "processing_time": 57.67397880554199,
      "citing_paper_id": "260440590",
      "cited_paper_id": 21174343
    },
    {
      "context_text": "In automated planning community, the notion of relaxations of the domain model are used to simplify the problem–chiefly to derive heuristics for planning problems [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept in automated planning. No verifiable resources are identified.",
      "processing_time": 58.90152382850647,
      "citing_paper_id": "260440590",
      "cited_paper_id": 21174343
    },
    {
      "context_text": "Open-source models: In addition to the GPT family of LLMs, we have also conducted preliminary experiments with an open-source LLM, BLOOM [31], and found that BLOOM too is ineffective in plan generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLOOM, which is an open-source LLM, but does not refer to it as a dataset. The context focuses on the effectiveness of BLOOM in plan generation, which is a methodological aspect rather than a dataset.",
      "processing_time": 63.57840037345886,
      "citing_paper_id": "260440590",
      "cited_paper_id": 36102848
    },
    {
      "context_text": "A standard representation to specify such kind of planning problems is the Planning Definition and Domain Language (PDDL) [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PDDL but does not refer to it as a dataset. It is described as a language for specifying planning problems, which is more aligned with a method or tool.",
      "processing_time": 61.15480446815491,
      "citing_paper_id": "260440590",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Of particular interest to us in this paper is the thread of efforts that aim to investigate (and showcase) reasoning abilities of LLMs–including commonsense reasoning [35, 29, 7], logical reasoning [33], and even ethical reasoning [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of reasoning abilities being investigated in LLMs. No verifiable resources are named.",
      "processing_time": 59.87580871582031,
      "citing_paper_id": "260440590",
      "cited_paper_id": 199370376
    },
    {
      "context_text": "Of particular interest to us in this paper is the thread of efforts that aim to investigate (and showcase) reasoning abilities of LLMs–including commonsense reasoning [35, 29, 7], logical reasoning [33], and even ethical reasoning [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of reasoning abilities being investigated in LLMs. No verifiable resources are named.",
      "processing_time": 59.87580871582031,
      "citing_paper_id": "260440590",
      "cited_paper_id": 238857096
    },
    {
      "context_text": "5, InstructGPT-3 [26] and GPT-3 [3]), as well as fine tuning does not seem to have a major effect on this dismal performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (InstructGPT-3 and GPT-3). The context focuses on the performance of these models rather than the use of a specific dataset.",
      "processing_time": 62.19494700431824,
      "citing_paper_id": "260440590",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "The macro-tenor of the drumbeat of these works has been suggesting that LLM’s are indeed capable of doing such kinds of reasoning [19, 37, 4].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works suggesting LLM capabilities.",
      "processing_time": 58.592416524887085,
      "citing_paper_id": "260440590",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "[33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only authors and a paper title. There is no information about the use of datasets in the given context.",
      "processing_time": 60.32025861740112,
      "citing_paper_id": "260440590",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "One other mode of evaluation of planning capabilities in the literature involves the user incrementally interacting with the LLM, and re-prompting it to point out flaws in its plans, with the hope that the LLM eventually reaches an executable plan [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method of evaluating planning capabilities through interaction and re-prompting.",
      "processing_time": 59.18731236457825,
      "citing_paper_id": "260440590",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "There have also been efforts which mostly depended on LLMs as “translators\" of natural language problem/goal specification into formal specifications, which are then thrown over to sound external planners [38, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs in conjunction with external planners. No verifiable resources are identified.",
      "processing_time": 59.88048958778381,
      "citing_paper_id": "260440590",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Finally, after our initial study and benchmark were made public, other groups did parallel studies that largely corroborate our results on the ineffectiveness of LLMs in finding executable plans [32, 21].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that other studies corroborate the results. No clear, verifiable resources are identified.",
      "processing_time": 59.53239870071411,
      "citing_paper_id": "260440590",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "While it is tempting to have a self-critiquing architecture with LLM also serving as the verifier, our recent work shows that approach to be of questionable utility as LLMs are no better at verifying plans than they are at generating them [36, 34].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research findings about LLMs' capabilities in verification and generation.",
      "processing_time": 59.158010959625244,
      "citing_paper_id": "260440590",
      "cited_paper_id": 264305982
    },
    {
      "context_text": "Such evaluations are notorious for their Clever Hans effect [1] with the actual planning being done by the humans in the loop rather than the LLMs themselves.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a general issue in evaluations of LLMs, which is not a reusable resource.",
      "processing_time": 60.63697338104248,
      "citing_paper_id": "260440590",
      "cited_paper_id": null
    },
    {
      "context_text": "To demonstrate the effectiveness of PoG on complex reasoning over knowledge graphs, we adopt three representative multi-hop KGQA datasets: CWQ [37], WebQSP [56], and GrailQA [17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CWQ",
        "WebQSP",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "CWQ": "Used to evaluate PoG's performance on complex reasoning over knowledge graphs, focusing on multi-hop question answering tasks.",
        "WebQSP": "Used to assess PoG's capability in handling multi-hop questions over knowledge graphs, emphasizing complex reasoning and semantic parsing.",
        "GrailQA": "Used to test PoG's effectiveness in multi-hop question answering, specifically designed to challenge models with complex reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the effectiveness of PoG on complex reasoning over knowledge graphs. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 79.26613211631775,
      "citing_paper_id": "273707190",
      "cited_paper_id": 3986974
    },
    {
      "context_text": "To demonstrate the effectiveness of PoG on complex reasoning over knowledge graphs, we adopt three representative multi-hop KGQA datasets: CWQ [37], WebQSP [56], and GrailQA [17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CWQ",
        "WebQSP",
        "GrailQA"
      ],
      "dataset_descriptions": {
        "CWQ": "Used to evaluate PoG's performance on complex reasoning over knowledge graphs, focusing on multi-hop question answering tasks.",
        "WebQSP": "Used to assess PoG's capability in handling multi-hop questions over knowledge graphs, emphasizing complex reasoning and semantic parsing.",
        "GrailQA": "Used to test PoG's effectiveness in multi-hop question answering, specifically designed to challenge models with complex reasoning tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating the effectiveness of PoG on complex reasoning over knowledge graphs. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 79.26613211631775,
      "citing_paper_id": "273707190",
      "cited_paper_id": 13905064
    },
    {
      "context_text": "Intuitively, as large-scale structural knowledge bases, Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Knowledge Graphs' as a reusable resource but does not specify any particular dataset. The cited papers are about specific Knowledge Graphs, but they are not explicitly named in the citation context.",
      "processing_time": 61.79156231880188,
      "citing_paper_id": "273707190",
      "cited_paper_id": 7278297
    },
    {
      "context_text": "Intuitively, as large-scale structural knowledge bases, Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Knowledge Graphs' as a reusable resource but does not specify any particular dataset. The cited papers are about specific Knowledge Graphs, but they are not explicitly named in the citation context.",
      "processing_time": 61.79156231880188,
      "citing_paper_id": "273707190",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "Intuitively, as large-scale structural knowledge bases, Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Knowledge Graphs' as a reusable resource but does not specify any particular dataset. The cited papers are about specific Knowledge Graphs, but they are not explicitly named in the citation context.",
      "processing_time": 61.79156231880188,
      "citing_paper_id": "273707190",
      "cited_paper_id": 269704509
    },
    {
      "context_text": "…these limitations is to leverage KGs for explicit and editable knowledge provision to LLMs. Previous studies integrated KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage, but they merely inject structured knowledge into LLMs’ parameters and still leave these limitations unexplored.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to knowledge graphs (KGs) and their integration into LLMs. No verifiable, named datasets are identified.",
      "processing_time": 60.526182413101196,
      "citing_paper_id": "273707190",
      "cited_paper_id": 158046772
    },
    {
      "context_text": "…these limitations is to leverage KGs for explicit and editable knowledge provision to LLMs. Previous studies integrated KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage, but they merely inject structured knowledge into LLMs’ parameters and still leave these limitations unexplored.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to knowledge graphs (KGs) and their integration into LLMs. No verifiable, named datasets are identified.",
      "processing_time": 60.526182413101196,
      "citing_paper_id": "273707190",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "…these limitations is to leverage KGs for explicit and editable knowledge provision to LLMs. Previous studies integrated KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage, but they merely inject structured knowledge into LLMs’ parameters and still leave these limitations unexplored.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to knowledge graphs (KGs) and their integration into LLMs. No verifiable, named datasets are identified.",
      "processing_time": 60.526182413101196,
      "citing_paper_id": "273707190",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "…these limitations is to leverage KGs for explicit and editable knowledge provision to LLMs. Previous studies integrated KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage, but they merely inject structured knowledge into LLMs’ parameters and still leave these limitations unexplored.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to knowledge graphs (KGs) and their integration into LLMs. No verifiable, named datasets are identified.",
      "processing_time": 60.526182413101196,
      "citing_paper_id": "273707190",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "…Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to Knowledge Graphs and their integration into LLMs. No specific, verifiable datasets are named.",
      "processing_time": 60.238842487335205,
      "citing_paper_id": "273707190",
      "cited_paper_id": 158046772
    },
    {
      "context_text": "…Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to Knowledge Graphs and their integration into LLMs. No specific, verifiable datasets are named.",
      "processing_time": 60.238842487335205,
      "citing_paper_id": "273707190",
      "cited_paper_id": 202539519
    },
    {
      "context_text": "…Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to Knowledge Graphs and their integration into LLMs. No specific, verifiable datasets are named.",
      "processing_time": 60.238842487335205,
      "citing_paper_id": "273707190",
      "cited_paper_id": 208006241
    },
    {
      "context_text": "…Knowledge Graphs (KGs) [5, 1, 12, 43] provide explicit and editable depictions of massive real-world knowledge, which have the potential to be a promising complement to the drawbacks of LLMs. Previous studies manage to integrate KGs into LLM pre-training [61, 47] or fine-tuning [53, 27] stage.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to Knowledge Graphs and their integration into LLMs. No specific, verifiable datasets are named.",
      "processing_time": 60.238842487335205,
      "citing_paper_id": "273707190",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "All three datasets rely on the external knowledge graph from Freebase [5].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Freebase"
      ],
      "dataset_descriptions": {
        "Freebase": "Used as an external knowledge graph to provide structured information for the datasets, enhancing the semantic richness of the data."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'external knowledge graph from Freebase' which is a specific, verifiable resource. However, it does not specify how it is used in the research context.",
      "processing_time": 67.20603036880493,
      "citing_paper_id": "273707190",
      "cited_paper_id": 207167677
    },
    {
      "context_text": "They can be categorized into two groups: (1) LLM-only methods , including standard prompting (IO prompt) [6], Chain-of-Thought prompting (CoT) [49], and Self-Consistency (SC) [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for using large language models.",
      "processing_time": 57.85752558708191,
      "citing_paper_id": "273707190",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "They can be categorized into two groups: (1) LLM-only methods , including standard prompting (IO prompt) [6], Chain-of-Thought prompting (CoT) [49], and Self-Consistency (SC) [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for using large language models.",
      "processing_time": 57.85752558708191,
      "citing_paper_id": "273707190",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "• Standard prompting (IO prompt) [6] verifies the ability of LLMs to achieve better performance in task-agnostic, few-shot problems than traditional LMs. • Chain-of-Thought prompting (CoT) [49] generates a series of intermediate reasoning steps in prompts to help LLMs perform better in several NLP…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on prompting techniques for LLMs.",
      "processing_time": 58.75264096260071,
      "citing_paper_id": "273707190",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "For GrailQA, we utilize RnG-KBQA [55], TIARA [34], FC-KBQA [58], Pangu [16], FlexKBQA [26], and GAIN [33] as fine-tuned baselines and KB-BINDER [23] and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for GrailQA, but does not specify any datasets. The cited paper titles do not indicate the presence of datasets either.",
      "processing_time": 60.475419759750366,
      "citing_paper_id": "273707190",
      "cited_paper_id": 237562927
    },
    {
      "context_text": "For GrailQA, we utilize RnG-KBQA [55], TIARA [34], FC-KBQA [58], Pangu [16], FlexKBQA [26], and GAIN [33] as fine-tuned baselines and KB-BINDER [23] and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for GrailQA, but does not specify any datasets. The cited paper titles do not indicate the presence of datasets either.",
      "processing_time": 60.475419759750366,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "For GrailQA, we utilize RnG-KBQA [55], TIARA [34], FC-KBQA [58], Pangu [16], FlexKBQA [26], and GAIN [33] as fine-tuned baselines and KB-BINDER [23] and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for GrailQA, but does not specify any datasets. The cited paper titles do not indicate the presence of datasets either.",
      "processing_time": 60.475419759750366,
      "citing_paper_id": "273707190",
      "cited_paper_id": 261076103
    },
    {
      "context_text": "For GrailQA, we utilize RnG-KBQA [55], TIARA [34], FC-KBQA [58], Pangu [16], FlexKBQA [26], and GAIN [33] as fine-tuned baselines and KB-BINDER [23] and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for GrailQA, but does not specify any datasets. The cited paper titles do not indicate the presence of datasets either.",
      "processing_time": 60.475419759750366,
      "citing_paper_id": "273707190",
      "cited_paper_id": 268417087
    },
    {
      "context_text": "• RnG-KBQA [55] first uses BERT to rank a set of enumerated candidate programs (up to a limited complexity), and then uses T5 to edit the top programs into more complex programs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT and T5, which are models, not datasets. No specific datasets are mentioned or used according to the citation.",
      "processing_time": 59.302348375320435,
      "citing_paper_id": "273707190",
      "cited_paper_id": 237562927
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 239011786
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 247082469
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 264995471
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 266176297
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 267411800
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 268691587
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": 268697551
    },
    {
      "context_text": "Large Language Models (LLMs) have manifested outstanding performance in various natural language processing and data science tasks, such as question answering [42, 25, 62], text generation [18, 13, 8, 15], recommender systems [60, 59, 51, 44], and domain-specific applications [46, 45, 14, 50, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No dataset names are present in the text.",
      "processing_time": 59.1406683921814,
      "citing_paper_id": "273707190",
      "cited_paper_id": null
    },
    {
      "context_text": "…prompting (IO prompt) [6] verifies the ability of LLMs to achieve better performance in task-agnostic, few-shot problems than traditional LMs. • Chain-of-Thought prompting (CoT) [49] generates a series of intermediate reasoning steps in prompts to help LLMs perform better in several NLP tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only prompting methods. No dataset names are present in the citation span.",
      "processing_time": 58.11347842216492,
      "citing_paper_id": "273707190",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "In the early stages, Chain of Thought (CoT) [49] was designed to provide a few examples of intermediate natural language reasoning steps as the prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Chain of Thought) used in prompting large language models.",
      "processing_time": 58.288984298706055,
      "citing_paper_id": "273707190",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "To encourage LLMs to engage in reasoning rather than simply providing answers directly, many researchers instruct LLMs to generate the process of thinking in their outputs [49, 22, 63].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for encouraging reasoning in LLMs.",
      "processing_time": 57.57494783401489,
      "citing_paper_id": "273707190",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Additionally, large efforts were dedicated to guiding LLMs in understanding complex graph structures [39, 38] and improving their graph reasoning across different graph tasks [11, 10, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to efforts in guiding LLMs in understanding complex graph structures and improving graph reasoning. No verifiable resources are named.",
      "processing_time": 60.46361041069031,
      "citing_paper_id": "273707190",
      "cited_paper_id": 251518434
    },
    {
      "context_text": "Additionally, large efforts were dedicated to guiding LLMs in understanding complex graph structures [39, 38] and improving their graph reasoning across different graph tasks [11, 10, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to efforts in guiding LLMs in understanding complex graph structures and improving graph reasoning. No verifiable resources are named.",
      "processing_time": 60.46361041069031,
      "citing_paper_id": "273707190",
      "cited_paper_id": 268296710
    },
    {
      "context_text": "Additionally, large efforts were dedicated to guiding LLMs in understanding complex graph structures [39, 38] and improving their graph reasoning across different graph tasks [11, 10, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to efforts in guiding LLMs in understanding complex graph structures and improving graph reasoning. No verifiable resources are named.",
      "processing_time": 60.46361041069031,
      "citing_paper_id": "273707190",
      "cited_paper_id": 269687228
    },
    {
      "context_text": "After that, several variants of CoT reasoning with different forms like Tree-of-Thought [54], Graph-of-Thought [4], Memory of Thought [24], and Skeleton-of-Thought [30] were proposed to enhance the thinking process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various forms of CoT reasoning but does not specify any datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.87920522689819,
      "citing_paper_id": "273707190",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "After that, several variants of CoT reasoning with different forms like Tree-of-Thought [54], Graph-of-Thought [4], Memory of Thought [24], and Skeleton-of-Thought [30] were proposed to enhance the thinking process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various forms of CoT reasoning but does not specify any datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.87920522689819,
      "citing_paper_id": "273707190",
      "cited_paper_id": 260315904
    },
    {
      "context_text": "Therefore, several works [3, 2, 40] first retrieved information from KGs and then directly fed explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general approach of using knowledge graphs (KGs) with language models. No verifiable resource names are provided.",
      "processing_time": 59.846587896347046,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259095910
    },
    {
      "context_text": "Therefore, several works [3, 2, 40] first retrieved information from KGs and then directly fed explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general approach of using knowledge graphs (KGs) with language models. No verifiable resource names are provided.",
      "processing_time": 59.846587896347046,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259138909
    },
    {
      "context_text": "Therefore, several works [3, 2, 40] first retrieved information from KGs and then directly fed explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the general approach of using knowledge graphs (KGs) with language models. No verifiable resource names are provided.",
      "processing_time": 59.846587896347046,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259924559
    },
    {
      "context_text": "Therefore, several attempts [3, 2, 40] first retrieve information from KGs and then deliver explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach of retrieving information from knowledge graphs (KGs). No specific dataset names are provided.",
      "processing_time": 59.22072911262512,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259095910
    },
    {
      "context_text": "Therefore, several attempts [3, 2, 40] first retrieve information from KGs and then deliver explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach of retrieving information from knowledge graphs (KGs). No specific dataset names are provided.",
      "processing_time": 59.22072911262512,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259138909
    },
    {
      "context_text": "Therefore, several attempts [3, 2, 40] first retrieve information from KGs and then deliver explicit knowledge into LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach of retrieving information from knowledge graphs (KGs). No specific dataset names are provided.",
      "processing_time": 59.22072911262512,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259924559
    },
    {
      "context_text": "For the large dataset GrailQA, we utilize the same testing samples as those in ToG [35] to improve computational efficiency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GrailQA"
      ],
      "dataset_descriptions": {
        "GrailQA": "Used to test and improve computational efficiency in the research, focusing on the same testing samples as those in ToG."
      },
      "confidence_score": 1.0,
      "reasoning": "GrailQA is mentioned as a specific dataset used for testing samples. The context indicates it is used to improve computational efficiency in the research.",
      "processing_time": 66.14172291755676,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "The interactions are under three designed universal APIs for KGs. • ToG [35] iteratively retrieves relevant triplets from KGs and employs the LLM to assess whether the reasoning paths in beam search are sufficient for answering the question and if further retrieval of the next hop is necessary.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the interaction with knowledge graphs (KGs) through APIs. The cited paper title confirms the focus on reasoning with KGs, not on specific datasets.",
      "processing_time": 61.026665925979614,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "Then, a novel KG-augmented LLM paradigm [19, 35] was proposed to treat the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving knowledge graphs and LLMs.",
      "processing_time": 57.51498079299927,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "Following prior research [23, 19, 35], we use exact match accuracy (Hits@1) as the evaluation metric.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an evaluation metric. No datasets are referenced or used in the context provided.",
      "processing_time": 57.80788016319275,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "Following previous studies [35], we assume any entity e q ∈ T q mentioned in q and answers a q ∈ A q are labeled and linked to the corresponding entities in G , i.e., T q , A q ⊆ E .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only abstract sets and entities. There are no clear identifiers for datasets.",
      "processing_time": 57.40508961677551,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "For CWQ and WebQSP, we utilize UniKGQA [20], TIARA [34], RE-KBQA [7], DeCAF [57], and RoG [27] as fine-tuned baselines and KD-CoT [41], KB-BINDER [23], StructGPT [19], Interactive KBQA [52], and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for fine-tuning and prompting, but does not specify any datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.29417586326599,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "For CWQ and WebQSP, we utilize UniKGQA [20], TIARA [34], RE-KBQA [7], DeCAF [57], and RoG [27] as fine-tuned baselines and KD-CoT [41], KB-BINDER [23], StructGPT [19], Interactive KBQA [52], and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for fine-tuning and prompting, but does not specify any datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.29417586326599,
      "citing_paper_id": "273707190",
      "cited_paper_id": 261214582
    },
    {
      "context_text": "For CWQ and WebQSP, we utilize UniKGQA [20], TIARA [34], RE-KBQA [7], DeCAF [57], and RoG [27] as fine-tuned baselines and KD-CoT [41], KB-BINDER [23], StructGPT [19], Interactive KBQA [52], and ToG [35] as prompting baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods used as baselines for fine-tuning and prompting, but does not specify any datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.29417586326599,
      "citing_paper_id": "273707190",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "For instance, StructGPT [19] and ToG [35] predefine the breadth of reasoning paths explored on the KG, and leverage the LLM to iterate the process of unidirectionally extending along the reasoning paths relevant to the question and reasoning the answer using these reasoning paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of knowledge graphs and large language models for reasoning.",
      "processing_time": 58.51879572868347,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "Similar to prior research [19, 35], topic entities have been pre-identified and are part of the annotated datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'annotated datasets' but does not specify any particular dataset name. The term 'annotated datasets' is too generic and lacks a specific identifier.",
      "processing_time": 59.3133819103241,
      "citing_paper_id": "273707190",
      "cited_paper_id": 259936842
    },
    {
      "context_text": "Hence, many works [32, 21, 28, 29] designed self-correction mechanisms based on feedback to rectify flawed reasoning and ensure accuracy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to self-correction mechanisms in LLMs. No verifiable resources are identified.",
      "processing_time": 58.27831768989563,
      "citing_paper_id": "273707190",
      "cited_paper_id": 260350986
    },
    {
      "context_text": "Hence, many works [32, 21, 28, 29] designed self-correction mechanisms based on feedback to rectify flawed reasoning and ensure accuracy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to self-correction mechanisms in LLMs. No verifiable resources are identified.",
      "processing_time": 58.27831768989563,
      "citing_paper_id": "273707190",
      "cited_paper_id": 274060374
    },
    {
      "context_text": "It consists of a symbolic agent with a cooperative neural LLM. • FlexKBQA [26] is a flexible KGQA framework with LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called FlexKBQA. No dataset names are present in the citation context.",
      "processing_time": 58.275341510772705,
      "citing_paper_id": "273707190",
      "cited_paper_id": 261076103
    },
    {
      "context_text": "• KD-CoT [41] retrieves relevant knowledge from KGs to generate faithful reasoning plans for LLMs. • StructGPT [19] defines the interface of KG data to implement knowledge access and filtering with finite quantity, and leverage the LLM to infer the answer or subsequent planning repeatedly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of knowledge graphs (KGs) but does not specify any particular dataset. The focus is on the method and its application.",
      "processing_time": 58.633251667022705,
      "citing_paper_id": "273707190",
      "cited_paper_id": 261214582
    },
    {
      "context_text": "• RoG [27] collaborates LLMs with KGs to achieve trustworthy reasoning to leverage structural information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a collaboration between LLMs and KGs for reasoning.",
      "processing_time": 57.1627893447876,
      "citing_paper_id": "273707190",
      "cited_paper_id": 263605944
    },
    {
      "context_text": "• GAIN [33] pays attention to the robustness of KGQA models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on the robustness of KGQA models. No verifiable resources are identified.",
      "processing_time": 58.11089038848877,
      "citing_paper_id": "273707190",
      "cited_paper_id": 268417087
    },
    {
      "context_text": "Using some form of supervision, such as providing details about the sub-tasks or intermediate rewards or high-level human guidance, is one approach [18, 14, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches to supervision in reinforcement learning.",
      "processing_time": 56.34345722198486,
      "citing_paper_id": "264558778",
      "cited_paper_id": 3694591
    },
    {
      "context_text": "Using some form of supervision, such as providing details about the sub-tasks or intermediate rewards or high-level human guidance, is one approach [18, 14, 16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches to supervision in reinforcement learning.",
      "processing_time": 56.34345722198486,
      "citing_paper_id": "264558778",
      "cited_paper_id": 189998275
    },
    {
      "context_text": "There is significant prior work on learning hierarchical policies to break down tasks into smaller sub-tasks [22, 9, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior work on hierarchical policies.",
      "processing_time": 56.327186822891235,
      "citing_paper_id": "264558778",
      "cited_paper_id": 6627476
    },
    {
      "context_text": "The most common approach is to incorporate temporally extended actions, also known as options or skills [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or concept (options or skills).",
      "processing_time": 56.63406276702881,
      "citing_paper_id": "264558778",
      "cited_paper_id": 6627476
    },
    {
      "context_text": "RL and Foundation Models Recently, large language models such as GPT-3 have been used to build agents capable of acting in the real world based on language instructions [4, 3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only large language models and their applications in reinforcement learning.",
      "processing_time": 56.78650665283203,
      "citing_paper_id": "264558778",
      "cited_paper_id": 70350059
    },
    {
      "context_text": "RL and Foundation Models Recently, large language models such as GPT-3 have been used to build agents capable of acting in the real world based on language instructions [4, 3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only large language models and their applications in reinforcement learning.",
      "processing_time": 56.78650665283203,
      "citing_paper_id": "264558778",
      "cited_paper_id": 232170216
    },
    {
      "context_text": "Natural language is a popular way to specify sub-tasks and achieve generalization due to its inherent compositionality and hierarchical structure [14, 18, 25, 12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of natural language for specifying sub-tasks and achieving generalization. No verifiable resources are identified.",
      "processing_time": 59.04982042312622,
      "citing_paper_id": "264558778",
      "cited_paper_id": 173991054
    },
    {
      "context_text": "Natural language is a popular way to specify sub-tasks and achieve generalization due to its inherent compositionality and hierarchical structure [14, 18, 25, 12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of natural language for specifying sub-tasks and achieving generalization. No verifiable resources are identified.",
      "processing_time": 59.04982042312622,
      "citing_paper_id": "264558778",
      "cited_paper_id": 189998275
    },
    {
      "context_text": "Natural language is a popular way to specify sub-tasks and achieve generalization due to its inherent compositionality and hierarchical structure [14, 18, 25, 12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the use of natural language for specifying sub-tasks and achieving generalization. No verifiable resources are identified.",
      "processing_time": 59.04982042312622,
      "citing_paper_id": "264558778",
      "cited_paper_id": 207870268
    },
    {
      "context_text": "The NetHack Learning Environment [15] is an RL environment based on the classic game of NetHack.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'NetHack Learning Environment' as an RL environment, which is not a dataset but an environment for reinforcement learning. It does not fit the criteria for a dataset.",
      "processing_time": 60.39148139953613,
      "citing_paper_id": "264558778",
      "cited_paper_id": 220042384
    },
    {
      "context_text": "This could be automated by using modern vision to language models such as [19], but that is left for future work.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context suggests future work using vision-to-language models, but no dataset is explicitly referenced.",
      "processing_time": 59.415953159332275,
      "citing_paper_id": "264558778",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We evaluate this approach on several simulated environments (MiniGrid [5], SkillHack [17], and Crafter [11]), showing that it can learn to solve complex, long-horizon tasks much faster than baseline methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MiniGrid",
        "SkillHack",
        "Crafter"
      ],
      "dataset_descriptions": {
        "MiniGrid": "Used to evaluate the approach on solving complex, long-horizon tasks, demonstrating faster learning compared to baseline methods.",
        "SkillHack": "Used to evaluate the approach on solving complex, long-horizon tasks, demonstrating faster learning compared to baseline methods.",
        "Crafter": "Used to evaluate the approach on solving complex, long-horizon tasks, demonstrating faster learning compared to baseline methods."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions specific environments used for evaluation, but they are not datasets in the traditional sense. They are simulation environments used to test agent capabilities.",
      "processing_time": 77.26921844482422,
      "citing_paper_id": "264558778",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "Crafter [11] is a 2D version Minecraft which has the same complex dynamics but with a simpler observation space and faster simulation speeds.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. Crafter is described as a 2D version of Minecraft used for benchmarking agent capabilities, but it is not a dataset.",
      "processing_time": 60.47530460357666,
      "citing_paper_id": "264558778",
      "cited_paper_id": 237504552
    },
    {
      "context_text": "[10] SayCan w/o Aff A SayCan [1] like architecure but without an affordance function and blindly trusting the LLM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model architecture variation. No verifiable resources are identified.",
      "processing_time": 56.995418071746826,
      "citing_paper_id": "264558778",
      "cited_paper_id": 244921108
    },
    {
      "context_text": "[13] use LLMs as zero-shot planners to enable embodied agents to act in real world scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs as zero-shot planners. No verifiable resources are identified.",
      "processing_time": 57.96457505226135,
      "citing_paper_id": "264558778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Suppose a user requests a tough task (e.g. Video Question Answering [19]), however, there is no labeled data available for training the model, and the user demands a high accuracy for the task.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a hypothetical scenario where labeled data is not available.",
      "processing_time": 57.36534404754639,
      "citing_paper_id": "265149831",
      "cited_paper_id": 207716
    },
    {
      "context_text": "Visual Grounding (VG) [2,3,7,8,12,14,20,21,23] aims to localize the objects on an image according to a text query.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the task of Visual Grounding. No dataset names are provided in the context or titles.",
      "processing_time": 58.00574588775635,
      "citing_paper_id": "265149831",
      "cited_paper_id": 9926549
    },
    {
      "context_text": "Visual Grounding (VG) [2,3,7,8,12,14,20,21,23] aims to localize the objects on an image according to a text query.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the task of Visual Grounding. No dataset names are provided in the context or titles.",
      "processing_time": 58.00574588775635,
      "citing_paper_id": "265149831",
      "cited_paper_id": 10248021
    },
    {
      "context_text": "Visual Grounding (VG) [2,3,7,8,12,14,20,21,23] aims to localize the objects on an image according to a text query.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the task of Visual Grounding. No dataset names are provided in the context or titles.",
      "processing_time": 58.00574588775635,
      "citing_paper_id": "265149831",
      "cited_paper_id": 232335456
    },
    {
      "context_text": "These features collectively contribute to the overall functionality and effectiveness of the TrainerAgent system.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes the contribution of features to the TrainerAgent system.",
      "processing_time": 57.79731011390686,
      "citing_paper_id": "265149831",
      "cited_paper_id": 237364324
    },
    {
      "context_text": "These features collectively contribute to the overall functionality and effectiveness of the TrainerAgent system.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes the contribution of features to the TrainerAgent system.",
      "processing_time": 57.79731011390686,
      "citing_paper_id": "265149831",
      "cited_paper_id": 251979702
    },
    {
      "context_text": "These features collectively contribute to the overall functionality and effectiveness of the TrainerAgent system.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes the contribution of features to the TrainerAgent system.",
      "processing_time": 57.79731011390686,
      "citing_paper_id": "265149831",
      "cited_paper_id": 261034676
    },
    {
      "context_text": "Nowadays, autonomous agents [1, 4, 5, 9, 11, 13, 17, 24] utilizing Large Language Models (LLMs) offer promising opportunities to enhance and replicate human workflows, which seems to able to solve ease the concern above.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general capabilities of LLMs in enhancing human workflows.",
      "processing_time": 58.44549512863159,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Nowadays, autonomous agents [1, 4, 5, 9, 11, 13, 17, 24] utilizing Large Language Models (LLMs) offer promising opportunities to enhance and replicate human workflows, which seems to able to solve ease the concern above.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general capabilities of LLMs in enhancing human workflows.",
      "processing_time": 58.44549512863159,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258947222
    },
    {
      "context_text": "Nowadays, autonomous agents [1, 4, 5, 9, 11, 13, 17, 24] utilizing Large Language Models (LLMs) offer promising opportunities to enhance and replicate human workflows, which seems to able to solve ease the concern above.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general capabilities of LLMs in enhancing human workflows.",
      "processing_time": 58.44549512863159,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258947227
    },
    {
      "context_text": "Nowadays, autonomous agents [1, 4, 5, 9, 11, 13, 17, 24] utilizing Large Language Models (LLMs) offer promising opportunities to enhance and replicate human workflows, which seems to able to solve ease the concern above.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general capabilities of LLMs in enhancing human workflows.",
      "processing_time": 58.44549512863159,
      "citing_paper_id": "265149831",
      "cited_paper_id": 259765919
    },
    {
      "context_text": "Firstly, like most LLM-powered agents [1, 4, 5, 9, 11, 13, 17, 24], each agent in our system comprises the following components: profile, memory, perception, planning, action, and response, as illustrated in Figure 1(a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only components of LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 57.36309313774109,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Firstly, like most LLM-powered agents [1, 4, 5, 9, 11, 13, 17, 24], each agent in our system comprises the following components: profile, memory, perception, planning, action, and response, as illustrated in Figure 1(a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only components of LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 57.36309313774109,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258947222
    },
    {
      "context_text": "Firstly, like most LLM-powered agents [1, 4, 5, 9, 11, 13, 17, 24], each agent in our system comprises the following components: profile, memory, perception, planning, action, and response, as illustrated in Figure 1(a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only components of LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 57.36309313774109,
      "citing_paper_id": "265149831",
      "cited_paper_id": 258947227
    },
    {
      "context_text": "Firstly, like most LLM-powered agents [1, 4, 5, 9, 11, 13, 17, 24], each agent in our system comprises the following components: profile, memory, perception, planning, action, and response, as illustrated in Figure 1(a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only components of LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 57.36309313774109,
      "citing_paper_id": "265149831",
      "cited_paper_id": 259765919
    },
    {
      "context_text": "AutoGen [18] provides an open-source platform for building complex LLM applications, allowing for inter-agent communication and a blend of LLM capabilities, human inputs, and additional tools.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an open-source platform for building LLM applications. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.91493797302246,
      "citing_paper_id": "265149831",
      "cited_paper_id": 260925901
    },
    {
      "context_text": "Prompt2Model [16] advances the field by proposing a method that uses natural language task descriptions to train specialized models, offering competence with fewer computational resources than LLM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training models using natural language task descriptions.",
      "processing_time": 56.24190902709961,
      "citing_paper_id": "265149831",
      "cited_paper_id": 261075905
    },
    {
      "context_text": "(iii) Instruction-tuning LLMs with the synthesized trajectory data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "synthesized trajectory data"
      ],
      "dataset_descriptions": {
        "synthesized trajectory data": "Used to instruction-tune LLMs, focusing on generating and evaluating planning capabilities through synthesized trajectories. The dataset provides structured data for training and testing planning algorithms."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'synthesized trajectory data' which is a plausible dataset name given the research topic on planning capabilities of LLMs. However, it is not a clearly identifiable, publicly accessible dataset.",
      "processing_time": 68.45686912536621,
      "citing_paper_id": "271600631",
      "cited_paper_id": 1739732
    },
    {
      "context_text": "(iii) Instruction-tuning LLMs with the synthesized trajectory data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "synthesized trajectory data"
      ],
      "dataset_descriptions": {
        "synthesized trajectory data": "Used to instruction-tune LLMs, focusing on generating and evaluating planning capabilities through synthesized trajectories. The dataset provides structured data for training and testing planning algorithms."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'synthesized trajectory data' which is a plausible dataset name given the research topic on planning capabilities of LLMs. However, it is not a clearly identifiable, publicly accessible dataset.",
      "processing_time": 68.45686912536621,
      "citing_paper_id": "271600631",
      "cited_paper_id": null
    },
    {
      "context_text": "For Out-of-Domain Tasks , we select two challenging partial-observable planning tasks: Alfworld [52] and BabyAI [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Alfworld' and 'BabyAI' as platforms for studying planning tasks. However, these are not datasets but environments or platforms for research.",
      "processing_time": 59.01415038108826,
      "citing_paper_id": "271600631",
      "cited_paper_id": 59536625
    },
    {
      "context_text": "For Out-of-Domain Tasks , we select two challenging partial-observable planning tasks: Alfworld [52] and BabyAI [10].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Alfworld' and 'BabyAI' as platforms for studying planning tasks. However, these are not datasets but environments or platforms for research.",
      "processing_time": 59.01415038108826,
      "citing_paper_id": "271600631",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "As shown in Figure 1, similar to imitation learning [21], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general process of agent training. The cited papers' titles suggest benchmarks and datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 59.58408188819885,
      "citing_paper_id": "271600631",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "As shown in Figure 1, similar to imitation learning [21], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general process of agent training. The cited papers' titles suggest benchmarks and datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 59.58408188819885,
      "citing_paper_id": "271600631",
      "cited_paper_id": 249889477
    },
    {
      "context_text": "As shown in Figure 1, similar to imitation learning [21], a typical agent training process can be divided into three stages: (i) Preparing environments and planning tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general process of agent training. The cited papers' titles suggest benchmarks and datasets, but they are not explicitly mentioned in the context.",
      "processing_time": 59.58408188819885,
      "citing_paper_id": "271600631",
      "cited_paper_id": 270357954
    },
    {
      "context_text": "Checkpoints from epochs 5 through 10 were pre-served and subsequently evaluated on In-Domain Tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'In-Domain Tasks' which is too generic and lacks a specific identifier.",
      "processing_time": 58.37164807319641,
      "citing_paper_id": "271600631",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Alternatively, a domain-specific corpus, such as a code generation dataset [25, 7], can be used to generate environments for a specific domain.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'code generation dataset' but does not specify a name. The cited papers do not provide a specific dataset name either.",
      "processing_time": 57.657249450683594,
      "citing_paper_id": "271600631",
      "cited_paper_id": 235755472
    },
    {
      "context_text": "Alternatively, a domain-specific corpus, such as a code generation dataset [25, 7], can be used to generate environments for a specific domain.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'code generation dataset' but does not specify a name. The cited papers do not provide a specific dataset name either.",
      "processing_time": 57.657249450683594,
      "citing_paper_id": "271600631",
      "cited_paper_id": 253734939
    },
    {
      "context_text": "These LLM-based agents, now fortified with capabilities like Memorizing [89, 30, 27, 82, 51, 86, 93, 59, 20], Tool-use [9, 43, 50, 26, 49, 45], and Planning [12, 5, 39, 38, 47, 2], exhibit a marked enhancement in their overall efficacy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only capabilities of LLM-based agents. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.200905323028564,
      "citing_paper_id": "271600631",
      "cited_paper_id": 258741194
    },
    {
      "context_text": "Bidirectional Evolution Many studies have proposed evolving instructions, primarily focusing on making instructions more difficult [78, 34, 33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies proposing evolving instructions. No verifiable resources are identified.",
      "processing_time": 56.75172734260559,
      "citing_paper_id": "271600631",
      "cited_paper_id": 259164815
    },
    {
      "context_text": "5 in overall performance and achieves results that either exceed or are comparable to GPT-4 on certain specific tasks; ii) A GENT G EN fine-tuned Llama3 has achieved a significant improvement in success rate; iii) A GENT G EN consistently surpasses other models with similar parameter scales in performance.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.21274137496948,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Advanced works such as AgentTuning [84] utilize GPT-4 to generate trajectory data across six distinct environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'trajectory data' but does not specify a named dataset. The term 'trajectory data' is too generic and lacks a specific identifier.",
      "processing_time": 57.83661699295044,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "5-turbo ) and GPT-4 [42] ( gpt-4-turbo ), CodeLlama-7B [46], Mistral-7B [22], Llama2-7B [60], and Llama3-8B [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on listing models used in the research.",
      "processing_time": 57.46449422836304,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "For example, utilizing state-of-the-art LLMs (e.g., GPT-4 [41]) as the agent and filtering trajectory based on reward score [84, 6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.46217083930969,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Furthermore, in the barman task, A GENT G EN even surpassed GPT-4’s performance (15 vs. 10).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance between two models. No verifiable resources are identified.",
      "processing_time": 57.015942335128784,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "A GENT G EN also achieved a comparable level to GPT-4 in tyreworld.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between GENT G EN and GPT-4 in a specific task (tyreworld).",
      "processing_time": 58.555917501449585,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "5 across multiple tasks and, in certain specific instances, exceeded the performance of GPT-4.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison of performance between models.",
      "processing_time": 55.69236707687378,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Additionally, it outperformed GPT-4 in specific tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models.",
      "processing_time": 55.03075909614563,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Since the trajectory data is structured, such as \"pickup(o1)\" , we employ GPT-4 to generate a natural language mapping, for example, \"pick up object {arg1}\" , to transform structured actions into natural language actions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for transforming structured actions into natural language using GPT-4.",
      "processing_time": 57.64759373664856,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "For environment generation and task generation, we employ GPT-4 2 , configuring the inference parameters with a temperature of 0 and a top_p value of 0.95.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of GPT-4 for environment and task generation. No verifiable resources are identified.",
      "processing_time": 58.133912324905396,
      "citing_paper_id": "271600631",
      "cited_paper_id": 261100919
    },
    {
      "context_text": "Planning is one of the key applications of LLM-based agents, applicable in various scenarios such as robotic planning [52, 44, 17, 61, 11, 74, 29, 13], travel planning [76, 1], calendar scheduling [88], code generation [4] and others [70].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of LLM-based planning. No verifiable resources are identified.",
      "processing_time": 57.27092790603638,
      "citing_paper_id": "271600631",
      "cited_paper_id": 262217135
    },
    {
      "context_text": "AgentLM is instruction-tuned using a combination of a lightweight trajectory dataset across 6 environments and a larger general instruction-tuning dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions two types of datasets but does not provide specific names. The datasets are described generically as 'a lightweight trajectory dataset' and 'a larger general instruction-tuning dataset'. These do not meet the criteria for specific, verifiable datasets.",
      "processing_time": 62.436891317367554,
      "citing_paper_id": "271600631",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of task formulation and difficulty progression.",
      "processing_time": 57.61680030822754,
      "citing_paper_id": "271600631",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of task formulation and difficulty progression.",
      "processing_time": 57.61680030822754,
      "citing_paper_id": "271600631",
      "cited_paper_id": 267897975
    },
    {
      "context_text": "Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of task formulation and difficulty progression.",
      "processing_time": 57.61680030822754,
      "citing_paper_id": "271600631",
      "cited_paper_id": 268249221
    },
    {
      "context_text": "Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of task formulation and difficulty progression.",
      "processing_time": 57.61680030822754,
      "citing_paper_id": "271600631",
      "cited_paper_id": 270823634
    },
    {
      "context_text": "Additionally, formulating tasks often demands creating a task set with a gradual difficulty progression.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general concept of task formulation and difficulty progression.",
      "processing_time": 57.61680030822754,
      "citing_paper_id": "271600631",
      "cited_paper_id": null
    },
    {
      "context_text": "More explicitly, designing diverse environments requires defining a range of rich and practical scenarios, and implementing these environments typically involves the participation of human experts with programming skills.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general requirements for designing environments for LLMs.",
      "processing_time": 56.130680084228516,
      "citing_paper_id": "271600631",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "Additionally, some models have undergone specialized training on agent trajectory data, such as AgentLM-7B [84].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'agent trajectory data' but does not specify a named dataset. The term is too generic and lacks a clear identifier.",
      "processing_time": 57.48756432533264,
      "citing_paper_id": "271600631",
      "cited_paper_id": 264306101
    },
    {
      "context_text": "We consider goal-directed deterministic planning problems [48], which are formally defined as a tuple P = ( T , E ) , where E denotes the environment in which the agent interacts and T denotes the task that the agent needs to complete.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formal definition of planning problems. No verifiable resources are identified.",
      "processing_time": 57.02055597305298,
      "citing_paper_id": "271600631",
      "cited_paper_id": null
    },
    {
      "context_text": "Central to this evolution is the aspiration to create robots that can seamlessly integrate into human environments, interact intuitively with humans, and autonomously navigate the complexities of the real world [3][4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the goals of robotics research.",
      "processing_time": 57.82852053642273,
      "citing_paper_id": "269091505",
      "cited_paper_id": 3752864
    },
    {
      "context_text": "Central to this evolution is the aspiration to create robots that can seamlessly integrate into human environments, interact intuitively with humans, and autonomously navigate the complexities of the real world [3][4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the goals of robotics research.",
      "processing_time": 57.82852053642273,
      "citing_paper_id": "269091505",
      "cited_paper_id": 254070076
    },
    {
      "context_text": "Traditional robotic systems, while efficient, have predominantly been rule-based, leading to rigidity in their operations [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the nature of traditional robotic systems.",
      "processing_time": 57.09654712677002,
      "citing_paper_id": "269091505",
      "cited_paper_id": 235574660
    },
    {
      "context_text": "From rudimentary machines designed for specific tasks to sophisticated entities capable of learning and adapting, the evolution of robots mirrors the broader advancements in the fields of artificial intelligence (AI) and machine learning [1][2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about the evolution of robots and AI.",
      "processing_time": 57.70603632926941,
      "citing_paper_id": "269091505",
      "cited_paper_id": 252814432
    },
    {
      "context_text": "5 model while reducing the cost by a tenth compared to other prevailing GPT-3 [1] and 3.5 models [14] ( e.g. , text-davinci-002 and text-davinci-003 .)",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their costs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.17243981361389,
      "citing_paper_id": "258947337",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "5 model while reducing the cost by a tenth compared to other prevailing GPT-3 [1] and 3.5 models [14] ( e.g. , text-davinci-002 and text-davinci-003 .)",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their costs. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.17243981361389,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "We also found a similar hypothesis drawn based on various experiments in [26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a similar hypothesis based on various experiments.",
      "processing_time": 57.43193197250366,
      "citing_paper_id": "258947337",
      "cited_paper_id": 240088413
    },
    {
      "context_text": "Table 5: Per-task success rate (%) of AdaPlanner, CC-Net [7], WGE [11], WebN-T5-3B [4], and RCI [9].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their performance metrics. No dataset names are present in the text.",
      "processing_time": 57.44598340988159,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "Consistent with previous works [18, 22, 8, 7, 4, 11, 9], we use success rate (%) to evaluate the performance of tested methods.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (success rate) for evaluating methods. No verifiable resources are identified.",
      "processing_time": 57.42694401741028,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "%) of AdaPlanner, CC-Net [7], WGE [11], WebN-T5-3B [4], and RCI [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.227580308914185,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "5 (text-davinci-003), along with three training or finetuning-based baselines: Computer Control Agent Architecture(CCNet) [7], Workflow-Guided Exploration(WGE) [11], and WebN-T5-3B [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 56.99175786972046,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "Task Name AdaPlanner CC-Net [7] WGE [11] WebN-T5-3B [4] RCI [9]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model or method names. There is no clear indication of a reusable dataset being used.",
      "processing_time": 57.69625949859619,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "Evaluations results of RCI, CC-Net, WGE, and WebN-T5-3B are sourced from the works of [9, 7, 11, 4], respectively.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works where evaluation results are sourced. No dataset names are provided.",
      "processing_time": 57.70420527458191,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "I need to revise solution function according to the new plan: Now the given list of possible_numbers is [6, 7, 8, 9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a list of numbers. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 57.7436728477478,
      "citing_paper_id": "258947337",
      "cited_paper_id": 246867455
    },
    {
      "context_text": "I need to revise solution function according to the new plan: Now the given list of possible_numbers is [6, 7, 8, 9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a list of numbers. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 57.7436728477478,
      "citing_paper_id": "258947337",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "I need to revise solution function according to the new plan: Now the given list of possible_numbers is [6, 7, 8, 9].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a list of numbers. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 57.7436728477478,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Open-loop methods [24, 27, 5, 18, 12, 17, 16] rely on pre-determined plans to accomplish the desired task without any feedback adaptation mechanism.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to open-loop methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.494802951812744,
      "citing_paper_id": "258947337",
      "cited_paper_id": 249017698
    },
    {
      "context_text": "ReAct [25] and Inner Monologue [6] allow LLM agents to take single-step actions according to the environmental feedback.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the capabilities of LLM agents using ReAct and Inner Monologue.",
      "processing_time": 58.338446855545044,
      "citing_paper_id": "258947337",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Inner Monologue [6] Taking Action Prompting Language - - RCI [8] Taking Action Prompting Language - - ProgPrompt [22] Taking Action Prompting Code - - Code as Policies [9] Taking Action Prompting Code - - Reflexion [19] Taking The method can employ either natural language or code for its planning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.344926595687866,
      "citing_paper_id": "258947337",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Inner Monologue [6] Taking Action Prompting Language - - RCI [8] Taking Action Prompting Language - - ProgPrompt [22] Taking Action Prompting Code - - Code as Policies [9] Taking Action Prompting Code - - Reflexion [19] Taking The method can employ either natural language or code for its planning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.344926595687866,
      "citing_paper_id": "258947337",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Inner Monologue [6] Taking Action Prompting Language - - RCI [8] Taking Action Prompting Language - - ProgPrompt [22] Taking Action Prompting Code - - Code as Policies [9] Taking Action Prompting Code - - Reflexion [19] Taking The method can employ either natural language or code for its planning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.344926595687866,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "On the other hand, closed-loop systems [25, 6, 8, 22, 9, 19, 23] incorporate environment feedback to continuously monitor system behaviors and make refinements and adjustments of the plans accordingly, which therefore is more flexible.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general discussion about closed-loop systems. No verifiable resources are identified.",
      "processing_time": 57.56139945983887,
      "citing_paper_id": "258947337",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "On the other hand, closed-loop systems [25, 6, 8, 22, 9, 19, 23] incorporate environment feedback to continuously monitor system behaviors and make refinements and adjustments of the plans accordingly, which therefore is more flexible.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general discussion about closed-loop systems. No verifiable resources are identified.",
      "processing_time": 57.56139945983887,
      "citing_paper_id": "258947337",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "On the other hand, closed-loop systems [25, 6, 8, 22, 9, 19, 23] incorporate environment feedback to continuously monitor system behaviors and make refinements and adjustments of the plans accordingly, which therefore is more flexible.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general discussion about closed-loop systems. No verifiable resources are identified.",
      "processing_time": 57.56139945983887,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "On the other hand, closed-loop systems [25, 6, 8, 22, 9, 19, 23] incorporate environment feedback to continuously monitor system behaviors and make refinements and adjustments of the plans accordingly, which therefore is more flexible.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general discussion about closed-loop systems. No verifiable resources are identified.",
      "processing_time": 57.56139945983887,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Existing code-generation-based methods [22, 9, 3] face a challenge in this task, especially when there is no prior knowledge of the structure and organization of these feedback sentences.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general challenge in code-generation-based methods. No verifiable resources are identified.",
      "processing_time": 57.82951378822327,
      "citing_paper_id": "258947337",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "Existing code-generation-based methods [22, 9, 3] face a challenge in this task, especially when there is no prior knowledge of the structure and organization of these feedback sentences.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general challenge in code-generation-based methods. No verifiable resources are identified.",
      "processing_time": 57.82951378822327,
      "citing_paper_id": "258947337",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "AdaPlanner plans and refines by using Pythonic code prompts for LLMs. Consistent with previous observations [3, 2], we have found that using code prompts instead of natural language prompts for LLMs reduces ambiguity and misinterpretation, which significantly reduces LLM hallucination during plan…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.55589270591736,
      "citing_paper_id": "258947337",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "AdaPlanner plans and refines by using Pythonic code prompts for LLMs. Consistent with previous observations [3, 2], we have found that using code prompts instead of natural language prompts for LLMs reduces ambiguity and misinterpretation, which significantly reduces LLM hallucination during plan…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.55589270591736,
      "citing_paper_id": "258947337",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "For baselines, we compare AdaPlanner with BUTLER [20], ReAct [25], and Reflexion [19].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.876487493515015,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "Following a set of previous works [20, 25, 19], we evaluate AdaPlanner on 134 different environments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to '134 different environments' which is too generic and lacks a clear identifier.",
      "processing_time": 59.01664757728577,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "Consistent with previous works [20, 25, 19, 7, 4, 11, 8], we use success rate (",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a metric (success rate). No verifiable resources are identified.",
      "processing_time": 56.61759042739868,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "Consistent with previous works [20, 25, 19, 7, 4, 11, 8], we use success rate (",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a metric (success rate). No verifiable resources are identified.",
      "processing_time": 56.61759042739868,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Reflexion [19], as an extension of ReAct, tries to resolve this issue by enabling the ReAct agent to revise itself from past trials and errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called Reflexion. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.29321074485779,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "…[6] Taking Action Prompting Language - - RCI [8] Taking Action Prompting Language - - ProgPrompt [22] Taking Action Prompting Code - - Code as Policies [9] Taking Action Prompting Code - - Reflexion [19] Taking The method can employ either natural language or code for its planning backend.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or tools. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.589407444000244,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "Following the setting in [25, 19], each episode will be terminated after reaching 50 steps.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a setting for episodes in an experimental setup.",
      "processing_time": 55.88294863700867,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257636839
    },
    {
      "context_text": "Following RCI [8], we define the actions space as two sets of operations, i.e., clicking and typing actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only actions spaces defined in another paper. No verifiable resources are identified.",
      "processing_time": 57.107810974121094,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "We select 9 MiniWoB++ tasks with environmental feedback, and we also adopt and test the 53 tasks evaluated in RCI [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'MiniWoB++ tasks' and 'RCI tasks', which are specific sets of tasks used for evaluating models. However, these are not traditional datasets but rather task suites or benchmarks.",
      "processing_time": 60.468363523483276,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "AdaPlanner CC-Net [7] WGE [11] WebN-T5-3B [4] RCI [8] choose-list Table 8: Per-task success rate (",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets.",
      "processing_time": 56.61378860473633,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "We also adopt and test the 53 tasks evaluated in RCI [8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions '53 tasks' but does not specify a dataset name. The term 'tasks' is not a clear identifier for a dataset.",
      "processing_time": 58.04696011543274,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Moreover, RCI [8] starts by formulating a comprehensive plan, modifying the immediate action when the agent encounters a failure at the current step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called RCI. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 58.26992893218994,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Overall, we report the evaluation results of RCI [8] and the proposed AdaPlanner in GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of models. No clear identifiers for datasets are present.",
      "processing_time": 56.85628843307495,
      "citing_paper_id": "258947337",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "5-turbo is primarily optimized for human conversation tasks, which could potentially compromise its performance on tasks such as code generation and reasoning[13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the optimization of 5-turbo for human conversation tasks.",
      "processing_time": 57.87005591392517,
      "citing_paper_id": "258947337",
      "cited_paper_id": null
    },
    {
      "context_text": "However, such AI systems are not good at dealing with out-of-distribution data [44, 72], and their intrinsic probabilistic nature brings much uncertainty in practice [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general issues with AI systems. No dataset names are present in the text.",
      "processing_time": 57.41920876502991,
      "citing_paper_id": "276107382",
      "cited_paper_id": 216356
    },
    {
      "context_text": "However, such AI systems are not good at dealing with out-of-distribution data [44, 72], and their intrinsic probabilistic nature brings much uncertainty in practice [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general issues with AI systems. No dataset names are present in the text.",
      "processing_time": 57.41920876502991,
      "citing_paper_id": "276107382",
      "cited_paper_id": 7228830
    },
    {
      "context_text": "With the wish that autonomous agents can make our life better, many autonomous agents have been designed as virtual personal assistants [49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of virtual personal assistants. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.86353540420532,
      "citing_paper_id": "276107382",
      "cited_paper_id": 3674213
    },
    {
      "context_text": "Only when users can obtain a sense of con-trol by being able to modify the outcomes of imperfect AI can they overcome such algorithm aversion and be willing to collaborate with imperfect AI systems [14].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on user behavior and interaction with AI systems.",
      "processing_time": 57.23292112350464,
      "citing_paper_id": "276107382",
      "cited_paper_id": 4662765
    },
    {
      "context_text": "Trust and reliance have been important research topics since human adoption of automation systems [17, 56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research topics related to trust and automation.",
      "processing_time": 55.94508242607117,
      "citing_paper_id": "276107382",
      "cited_paper_id": 5210390
    },
    {
      "context_text": "The two constructs have been shown to be highly related [55, 56]: for example, user trust can substantially affect user reliance [56].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a relationship between constructs. No verifiable resources are identified.",
      "processing_time": 56.67900586128235,
      "citing_paper_id": "276107382",
      "cited_paper_id": 5210390
    },
    {
      "context_text": "According to existing literature [56], such a bonus setup can help incentivize participants to reach a correct decision.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a general concept about incentivizing participants.",
      "processing_time": 57.388893127441406,
      "citing_paper_id": "276107382",
      "cited_paper_id": 5210390
    },
    {
      "context_text": "User trust in the context of human-AI collaboration is typically operationalized as a subjective attitude toward AI systems/AI advice [56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of user trust in human-AI collaboration.",
      "processing_time": 56.67338562011719,
      "citing_paper_id": "276107382",
      "cited_paper_id": 5210390
    },
    {
      "context_text": "According to prior work [74], most people can only handle 5 to 9 concepts at the same time.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general finding about cognitive limits.",
      "processing_time": 54.676801443099976,
      "citing_paper_id": "276107382",
      "cited_paper_id": 15654531
    },
    {
      "context_text": "Such a simulation setup is effective in developing and validating theory [12] and has been widely adopted in existing research on agent-based modeling and HCI studies [76].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general method for developing and validating theory through simulations.",
      "processing_time": 56.25427865982056,
      "citing_paper_id": "276107382",
      "cited_paper_id": 18394514
    },
    {
      "context_text": "To ensure sufficient statistical power, we estimated the required sample size for a 2 × 2 factorial design based on G*Power [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions G*Power, which is a tool for statistical power analysis, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 57.64804029464722,
      "citing_paper_id": "276107382",
      "cited_paper_id": 18433083
    },
    {
      "context_text": "However, on tasks entailing potential risks ( e.g., monetary payments or hiring an employee), humans hesitate to trust such AI systems due to loss aversion [96] and algorithmic aversion [13, 19, 40, 71].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts related to human behavior towards AI systems. No verifiable resources are identified.",
      "processing_time": 57.73414707183838,
      "citing_paper_id": "276107382",
      "cited_paper_id": 44451104
    },
    {
      "context_text": "However, on tasks entailing potential risks ( e.g., monetary payments or hiring an employee), humans hesitate to trust such AI systems due to loss aversion [96] and algorithmic aversion [13, 19, 40, 71].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts related to human behavior towards AI systems. No verifiable resources are identified.",
      "processing_time": 57.73414707183838,
      "citing_paper_id": "276107382",
      "cited_paper_id": 239020696
    },
    {
      "context_text": "However, on tasks entailing potential risks ( e.g., monetary payments or hiring an employee), humans hesitate to trust such AI systems due to loss aversion [96] and algorithmic aversion [13, 19, 40, 71].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts related to human behavior towards AI systems. No verifiable resources are identified.",
      "processing_time": 57.73414707183838,
      "citing_paper_id": "276107382",
      "cited_paper_id": 245212848
    },
    {
      "context_text": "Based on existing literature on task complexity [85, 107], we considered component complexity to inform our selection.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to task complexity literature.",
      "processing_time": 55.34204983711243,
      "citing_paper_id": "276107382",
      "cited_paper_id": 144296456
    },
    {
      "context_text": "For example, increasing the transparency of AI systems by providing confidence scores [112], explanations [104], trustworthiness cues [58], and uncertainty communication [50].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches to improve AI transparency. No verifiable resources are identified.",
      "processing_time": 57.46975588798523,
      "citing_paper_id": "276107382",
      "cited_paper_id": 210023849
    },
    {
      "context_text": "For example, increasing the transparency of AI systems by providing confidence scores [112], explanations [104], trustworthiness cues [58], and uncertainty communication [50].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches to improve AI transparency. No verifiable resources are identified.",
      "processing_time": 57.46975588798523,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233224125
    },
    {
      "context_text": "Similarly, task characteristics like task complexity and uncertainty [85, 86] and factors of the AI system ( e.g., performance feedback [4, 65], AI transparency [100], stated accuracy [33], and confidence of AI advice [95, 112]) also affect user trust and reliance on the AI system.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only factors affecting user trust and reliance on AI systems. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.71948790550232,
      "citing_paper_id": "276107382",
      "cited_paper_id": 210023849
    },
    {
      "context_text": "Similarly, task characteristics like task complexity and uncertainty [85, 86] and factors of the AI system ( e.g., performance feedback [4, 65], AI transparency [100], stated accuracy [33], and confidence of AI advice [95, 112]) also affect user trust and reliance on the AI system.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only factors affecting user trust and reliance on AI systems. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.71948790550232,
      "citing_paper_id": "276107382",
      "cited_paper_id": 225588887
    },
    {
      "context_text": "Similarly, task characteristics like task complexity and uncertainty [85, 86] and factors of the AI system ( e.g., performance feedback [4, 65], AI transparency [100], stated accuracy [33], and confidence of AI advice [95, 112]) also affect user trust and reliance on the AI system.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only factors affecting user trust and reliance on AI systems. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.71948790550232,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233987077
    },
    {
      "context_text": "Earlier work exploring human-AI trust primarily focused on the impact of different contextual factors surrounding user ( e.g., risk perception [29]), task ( e.g., task complexity [85]), and system ( e.g., stated accuracy [111, 112]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors affecting human-AI trust. No clear, verifiable resources are identified.",
      "processing_time": 57.92594003677368,
      "citing_paper_id": "276107382",
      "cited_paper_id": 210023849
    },
    {
      "context_text": "Through empirical user studies with different confidence levels of AI predictions, Zhang et al. [112] found that “trust calibration alone is not sufficient to improve AI-assisted decision making”.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only empirical user studies. No clear, verifiable dataset names are provided.",
      "processing_time": 57.00850319862366,
      "citing_paper_id": "276107382",
      "cited_paper_id": 210023849
    },
    {
      "context_text": "Researchers have proposed various interventions to promote appropriate reliance [8, 9, 31, 34, 64, 65] and calibrate user trust in AI systems [7, 112].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to interventions and calibrating user trust in AI systems.",
      "processing_time": 57.98131275177002,
      "citing_paper_id": "276107382",
      "cited_paper_id": 210023849
    },
    {
      "context_text": "Researchers have proposed various interventions to promote appropriate reliance [8, 9, 31, 34, 64, 65] and calibrate user trust in AI systems [7, 112].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to interventions and calibrating user trust in AI systems.",
      "processing_time": 57.98131275177002,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233987077
    },
    {
      "context_text": "Researchers have proposed various interventions to promote appropriate reliance [8, 9, 31, 34, 64, 65] and calibrate user trust in AI systems [7, 112].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to interventions and calibrating user trust in AI systems.",
      "processing_time": 57.98131275177002,
      "citing_paper_id": "276107382",
      "cited_paper_id": 235599489
    },
    {
      "context_text": "Researchers have proposed various interventions to promote appropriate reliance [8, 9, 31, 34, 64, 65] and calibrate user trust in AI systems [7, 112].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to interventions and calibrating user trust in AI systems.",
      "processing_time": 57.98131275177002,
      "citing_paper_id": "276107382",
      "cited_paper_id": 266999465
    },
    {
      "context_text": "Users with an uncalibrated trust in the AI system can be easily misled to disuse or misuse AI systems [42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the concept of trust in AI systems.",
      "processing_time": 57.23888349533081,
      "citing_paper_id": "276107382",
      "cited_paper_id": 222379602
    },
    {
      "context_text": "In recent decades, deep learning-based AI systems have shown promising performance across various domains [23, 110] and applications [15, 81].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general performance of deep learning systems. No specific resource names are provided.",
      "processing_time": 57.22143197059631,
      "citing_paper_id": "276107382",
      "cited_paper_id": 227305659
    },
    {
      "context_text": "In recent decades, deep learning-based AI systems have shown promising performance across various domains [23, 110] and applications [15, 81].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general performance of deep learning systems. No specific resource names are provided.",
      "processing_time": 57.22143197059631,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233547699
    },
    {
      "context_text": "For example, explainable AI methods have been shown to help reduce over-reliance [98] and under-reliance [104] in different scenarios albeit with little consistency across contexts.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies about explainable AI methods. No verifiable resources are identified.",
      "processing_time": 57.48813271522522,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233224125
    },
    {
      "context_text": "Existing human-AI collaboration literature has provided potential solutions for such problems, ranging from performance feedback interventions [34] to agreement-in-confidence heuristic [65, 79].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or findings in human-AI collaboration literature.",
      "processing_time": 56.78890776634216,
      "citing_paper_id": "276107382",
      "cited_paper_id": 233987077
    },
    {
      "context_text": "Even if the AI systems are accurate on specific datasets, they still suffer from out-of-distribution data [9, 62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with out-of-distribution data. No clear, verifiable resource names are provided.",
      "processing_time": 58.11842346191406,
      "citing_paper_id": "276107382",
      "cited_paper_id": 235599489
    },
    {
      "context_text": "User factors like AI literacy [8], cognitive bias [82], peer input [84], and risk perception [24, 29] have been observed to substantially impact user trust and reliance on the AI system.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general factors affecting user trust and reliance on AI systems. No verifiable resources are identified.",
      "processing_time": 58.115551471710205,
      "citing_paper_id": "276107382",
      "cited_paper_id": 237416499
    },
    {
      "context_text": "User factors like AI literacy [8], cognitive bias [82], peer input [84], and risk perception [24, 29] have been observed to substantially impact user trust and reliance on the AI system.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general factors affecting user trust and reliance on AI systems. No verifiable resources are identified.",
      "processing_time": 58.115551471710205,
      "citing_paper_id": "276107382",
      "cited_paper_id": 270638927
    },
    {
      "context_text": "Compared to human delegation, AI delegation has been observed to achieve more consistent benefits in team performance [25, 39].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only observations about AI delegation compared to human delegation.",
      "processing_time": 56.37650394439697,
      "citing_paper_id": "276107382",
      "cited_paper_id": 245062830
    },
    {
      "context_text": "Another relevant stream of recent research has explored AI delegation to humans [25, 70, 80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers exploring AI delegation to humans.",
      "processing_time": 56.51609468460083,
      "citing_paper_id": "276107382",
      "cited_paper_id": 245062830
    },
    {
      "context_text": "The main issues that lead to sub-optimal human-AI collaboration are: under-reliance ( i.e., disuse AI assistance when AI systems outperform humans) and over-reliance ( i.e., misuse AI assistance when AI systems are wrong or perform worse than humans) [87].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues in human-AI collaboration. No verifiable resources are identified.",
      "processing_time": 57.54118204116821,
      "citing_paper_id": "276107382",
      "cited_paper_id": 248177934
    },
    {
      "context_text": "Such user reliance patterns are denoted as appropriate reliance [87, 88], which is the key to achieving complementary team performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concepts related to human-AI decision-making.",
      "processing_time": 56.286454916000366,
      "citing_paper_id": "276107382",
      "cited_paper_id": 248177934
    },
    {
      "context_text": "In that context, researchers have theorized and empirically analyzed when and where users could and should delegate to AI systems [52, 66].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical and empirical analyses of human-AI collaboration.",
      "processing_time": 56.50639247894287,
      "citing_paper_id": "276107382",
      "cited_paper_id": 248377511
    },
    {
      "context_text": "While humans can provide more reliable and accountable task outcomes, too much user involvement to check and control AI outcomes is undesirable [52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses human-AI collaboration and the balance of user involvement in AI outcomes.",
      "processing_time": 58.36513352394104,
      "citing_paper_id": "276107382",
      "cited_paper_id": 248377511
    },
    {
      "context_text": "Apart from manual delegation decisions, it is common to apply automatic rules for human delegation ( e.g., heuristics obtained from domain expertise or manually crafted rules [52]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods or approaches for human delegation in AI systems.",
      "processing_time": 56.721702575683594,
      "citing_paper_id": "276107382",
      "cited_paper_id": 248377511
    },
    {
      "context_text": "Different techniques to help users realize the trustworthiness of the AI system have been proposed [48, 68, 83].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques for promoting trust in AI-assisted decision-making.",
      "processing_time": 56.98384165763855,
      "citing_paper_id": "276107382",
      "cited_paper_id": 255941863
    },
    {
      "context_text": "With this in the backdrop, existing work on automated task completion has revealed that LLM agents can exhibit promising performance in handling complex tasks like playing games [105], answering complex questions [116], and in simulating social behavior [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.68765735626221,
      "citing_paper_id": "276107382",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "With this in the backdrop, existing work on automated task completion has revealed that LLM agents can exhibit promising performance in handling complex tasks like playing games [105], answering complex questions [116], and in simulating social behavior [78].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 58.68765735626221,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259243960
    },
    {
      "context_text": "The variables and measures used in our study refer to existing empirical studies of human-AI collaboration [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that the variables and measures refer to existing empirical studies. There is no clear identifier for a dataset.",
      "processing_time": 58.478312253952026,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "Most existing work has conducted empirical studies [53] and structured interviews [45] to understand how factors surrounding the user, task, and AI systems affect human-AI collaboration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods of empirical studies and structured interviews.",
      "processing_time": 56.484437465667725,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "In comparison with existing literature exploring human-AI decision making [53], our reward setup is above the average payment and can be considered as being sufficient to elicit ecologically valid behavior among participants ( i.e., aiming to arrive at accurate execution results).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the reward setup in the context of human-AI decision making.",
      "processing_time": 58.84743332862854,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "Under such circumstances, human-AI collaboration has been recognized as a well-suited approach to taking advantage of their promising predictive power and ensuring trust-worthy outcomes [45, 53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to human-AI collaboration. No verifiable resources are identified.",
      "processing_time": 57.94990801811218,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "For a more comprehensive survey of existing work on AI-assisted decision making, readers can refer to [53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a survey of existing work. No verifiable resources are identified.",
      "processing_time": 57.94753837585449,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "For a more comprehensive overview of interventions to facilitate trust calibration and appropriate reliance, readers can refer to [18, 46, 53, 73].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to other works for a more comprehensive overview.",
      "processing_time": 58.1112596988678,
      "citing_paper_id": "276107382",
      "cited_paper_id": 259139714
    },
    {
      "context_text": "For a more comprehensive overview of interventions to facilitate trust calibration and appropriate reliance, readers can refer to [18, 46, 53, 73].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to other works for a more comprehensive overview.",
      "processing_time": 58.1112596988678,
      "citing_paper_id": "276107382",
      "cited_paper_id": null
    },
    {
      "context_text": "The term ‘ LLM agent ’ refers to an artificial entity based on LLMs that perceives its context, makes decisions, and then takes actions in response [109].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of LLM agents. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.46806478500366,
      "citing_paper_id": "276107382",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "LLM agents have been shown to have good planning, memory, and toolkit usage capabilities [101, 109].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLM agents. No verifiable resources are identified.",
      "processing_time": 57.591721057891846,
      "citing_paper_id": "276107382",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "With the recent rise of large language models (LLMs) in natural language understanding and generation [114], researchers have started to analyze LLM-based agents and their applicability in a plethora of tasks [101, 109].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the rise and potential of LLM-based agents. No verifiable resources are identified.",
      "processing_time": 58.178563833236694,
      "citing_paper_id": "276107382",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "First, with a planning module, LLM agents can generate a dynamic plan based on the tools provided [101, 109].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of a planning module for LLM agents.",
      "processing_time": 57.14902925491333,
      "citing_paper_id": "276107382",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "As a result, users over-rely on AI assistance, which is misuse akin to behavior that resonates with algorithm appreciation [63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses user behavior and AI assistance, but no verifiable resources are referenced.",
      "processing_time": 58.75676393508911,
      "citing_paper_id": "276107382",
      "cited_paper_id": 262041623
    },
    {
      "context_text": "As a result, users over-rely on AI assistance, which is misuse akin to behavior that resonates with algorithm appreciation [63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses user behavior and AI assistance, but no verifiable resources are referenced.",
      "processing_time": 58.75676393508911,
      "citing_paper_id": "276107382",
      "cited_paper_id": null
    },
    {
      "context_text": "Attracted by the promise of LLM agents, there have been some early explorations [27, 113, 115] of adopting them in human-AI collaboration contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general explorations of LLM agents in human-AI collaboration.",
      "processing_time": 57.305588483810425,
      "citing_paper_id": "276107382",
      "cited_paper_id": 269329951
    },
    {
      "context_text": "Drawn by the promise of LLM agents, there have been some early explorations [27] of adopting them in human-AI collaboration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general exploration of LLM agents in human-AI collaboration.",
      "processing_time": 57.29540181159973,
      "citing_paper_id": "276107382",
      "cited_paper_id": 269329951
    },
    {
      "context_text": "However, existing works have primarily analyzed how LLM agents can serve specific use cases ( e.g., design creation [27]), while others have conducted structured interviews to obtain expert insights [113, 115].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general use cases and expert interviews.",
      "processing_time": 56.22317433357239,
      "citing_paper_id": "276107382",
      "cited_paper_id": 269329951
    },
    {
      "context_text": "These works were mostly analyzed in specific use cases ( e.g., design creation [27]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'specific use cases'. No clear, verifiable resource is identified.",
      "processing_time": 58.36045455932617,
      "citing_paper_id": "276107382",
      "cited_paper_id": 269329951
    },
    {
      "context_text": "With user involvement in the planning stage, users have the opportunity to fix potential mistakes or issues in the plan generated by LLMs. Working on such plan editing tasks is similar to debugging, which has been argued to bring about a critical mindset [32] to the generated plan.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to debugging and user involvement in AI systems.",
      "processing_time": 57.286545276641846,
      "citing_paper_id": "276107382",
      "cited_paper_id": 272201868
    },
    {
      "context_text": "Moreover, similar bonus structures akin to our setup have been effective in incentivizing reliable participant behavior and improving data quality across different studies with crowdsourced participants [21, 57, 69, 86].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general studies involving crowdsourced participants. No verifiable resources are identified.",
      "processing_time": 57.618332386016846,
      "citing_paper_id": "276107382",
      "cited_paper_id": 274131547
    },
    {
      "context_text": "Such trust is vulnerable, as the AI system may provide an illusion of competence with persuasive technology ( e.g., explanations [10, 30]) or overclaimed performance [111].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to explanations and overclaimed performance. No verifiable resources are identified.",
      "processing_time": 57.640631437301636,
      "citing_paper_id": "276107382",
      "cited_paper_id": 275954224
    },
    {
      "context_text": "Due to the widespread integration of AI systems and LLMs in all walks of society, there has been a growing interest in understanding user trust [2, 73, 99] and reliance [18] on AI systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to general interest in understanding user trust and reliance on AI systems.",
      "processing_time": 58.754355669021606,
      "citing_paper_id": "276107382",
      "cited_paper_id": null
    },
    {
      "context_text": "Early studies develop ranking-based methods (Rao and Daumé III, 2018; Xu et al., 2019) to select clarification questions from a candidate pool, or apply end-to-end solutions (Deng et al., 2022) for generating questions on the fly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for generating clarification questions.",
      "processing_time": 56.586480140686035,
      "citing_paper_id": "270561990",
      "cited_paper_id": 202773799
    },
    {
      "context_text": "Early studies develop ranking-based methods (Rao and Daumé III, 2018; Xu et al., 2019) to select clarification questions from a candidate pool, or apply end-to-end solutions (Deng et al., 2022) for generating questions on the fly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for generating clarification questions.",
      "processing_time": 56.586480140686035,
      "citing_paper_id": "270561990",
      "cited_paper_id": 252918278
    },
    {
      "context_text": "Another line of research investigates environmental reward feedback to formulate questions (Zamani et al., 2020; Pyatkin et al., 2023; Andukuri et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only lines of research and papers. No verifiable resources are identified.",
      "processing_time": 57.39943242073059,
      "citing_paper_id": "270561990",
      "cited_paper_id": 214148289
    },
    {
      "context_text": "Another line of research investigates environmental reward feedback to formulate questions (Zamani et al., 2020; Pyatkin et al., 2023; Andukuri et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only lines of research and papers. No verifiable resources are identified.",
      "processing_time": 57.39943242073059,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "Asking Clarification Questions The problem of asking clarification questions (Aliannejadi et al., 2021) typically involves two phases: Clarification Need Prediction and Clarification Question Generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the problem of asking clarification questions and its phases. No verifiable resources are identified.",
      "processing_time": 57.93295454978943,
      "citing_paper_id": "270561990",
      "cited_paper_id": 237492197
    },
    {
      "context_text": "LLM-based language agents have been explored in various real-world planning problems, such as travel planning (Xie et al., 2024), web navigation (Deng et al., 2023a), online shopping (Yao et al., 2022), etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based language agents. No verifiable resources are identified.",
      "processing_time": 57.92964506149292,
      "citing_paper_id": "270561990",
      "cited_paper_id": 250264533
    },
    {
      "context_text": "LLM-based language agents have been explored in various real-world planning problems, such as travel planning (Xie et al., 2024), web navigation (Deng et al., 2023a), online shopping (Yao et al., 2022), etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based language agents. No verifiable resources are identified.",
      "processing_time": 57.92964506149292,
      "citing_paper_id": "270561990",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "LLM-based language agents have been explored in various real-world planning problems, such as travel planning (Xie et al., 2024), web navigation (Deng et al., 2023a), online shopping (Yao et al., 2022), etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based language agents. No verifiable resources are identified.",
      "processing_time": 57.92964506149292,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "The rapid development and application of language agents span various domains, including web agents (Deng et al., 2023a; Yao et al., 2022; Deng et al., 2024), game agents (Wang et al., 2023a; Zhu et al., 2023), and medical agents (Li et al., 2024a; Schmidgall et al., 2024), etc. Existing studies…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application for language agents. No verifiable resources are identified.",
      "processing_time": 57.91454100608826,
      "citing_paper_id": "270561990",
      "cited_paper_id": 250264533
    },
    {
      "context_text": "The rapid development and application of language agents span various domains, including web agents (Deng et al., 2023a; Yao et al., 2022; Deng et al., 2024), game agents (Wang et al., 2023a; Zhu et al., 2023), and medical agents (Li et al., 2024a; Schmidgall et al., 2024), etc. Existing studies…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application for language agents. No verifiable resources are identified.",
      "processing_time": 57.91454100608826,
      "citing_paper_id": "270561990",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "The rapid development and application of language agents span various domains, including web agents (Deng et al., 2023a; Yao et al., 2022; Deng et al., 2024), game agents (Wang et al., 2023a; Zhu et al., 2023), and medical agents (Li et al., 2024a; Schmidgall et al., 2024), etc. Existing studies…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains of application for language agents. No verifiable resources are identified.",
      "processing_time": 57.91454100608826,
      "citing_paper_id": "270561990",
      "cited_paper_id": 267897510
    },
    {
      "context_text": "Recently, LLMs have also been employed to generate clarifying questions (Kuhn et al., 2022; Deng et al., 2023c; Zhang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that LLMs have been used to generate clarifying questions. No verifiable resources are identified.",
      "processing_time": 58.833882331848145,
      "citing_paper_id": "270561990",
      "cited_paper_id": 257038525
    },
    {
      "context_text": "Recently, LLMs have also been employed to generate clarifying questions (Kuhn et al., 2022; Deng et al., 2023c; Zhang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that LLMs have been used to generate clarifying questions. No verifiable resources are identified.",
      "processing_time": 58.833882331848145,
      "citing_paper_id": "270561990",
      "cited_paper_id": 270123654
    },
    {
      "context_text": "To produce unclear instructions with indefinite details, we focus on two typical scenarios observed in proactive information-seeking dialogues: (1) clarifying under-specified queries, and (2) managing over-specified queries (Wu et al., 2023; Deng et al., 2023b): 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general scenarios in proactive information-seeking dialogues. No verifiable resources are identified.",
      "processing_time": 58.119072914123535,
      "citing_paper_id": "270561990",
      "cited_paper_id": 257920051
    },
    {
      "context_text": "To produce unclear instructions with indefinite details, we focus on two typical scenarios observed in proactive information-seeking dialogues: (1) clarifying under-specified queries, and (2) managing over-specified queries (Wu et al., 2023; Deng et al., 2023b): 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general scenarios in proactive information-seeking dialogues. No verifiable resources are identified.",
      "processing_time": 58.119072914123535,
      "citing_paper_id": "270561990",
      "cited_paper_id": 258479667
    },
    {
      "context_text": "Concerning the status quo, LLMs struggle to spontaneously clarify users’ intentions during the conversation (Deng et al., 2023b) and accurately obtain necessary information via tool utilization without hallucination (Li et al., 2024b), even after the instruction tuning or providing the well-crafted…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with LLMs. No clear identifiers for datasets are present.",
      "processing_time": 57.69538068771362,
      "citing_paper_id": "270561990",
      "cited_paper_id": 258479667
    },
    {
      "context_text": "Inspired by proactive information-seeking systems (Deng et al., 2023b; Liao et al., 2023) that can proactively ask clarification questions for clarifying the query uncertainty, latest studies on language agents (Qian et al., 2024; Zhang et al., 2024) emphasize intention clarifications by…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to proactive dialogue systems and language agents. No verifiable resources are identified.",
      "processing_time": 58.14835715293884,
      "citing_paper_id": "270561990",
      "cited_paper_id": 258479667
    },
    {
      "context_text": "Inspired by proactive information-seeking systems (Deng et al., 2023b; Liao et al., 2023) that can proactively ask clarification questions for clarifying the query uncertainty, latest studies on language agents (Qian et al., 2024; Zhang et al., 2024) emphasize intention clarifications by…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to proactive dialogue systems and language agents. No verifiable resources are identified.",
      "processing_time": 58.14835715293884,
      "citing_paper_id": "270561990",
      "cited_paper_id": 259949693
    },
    {
      "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Tell Me More",
        "Clamber",
        "TravelPlanner",
        "Mind2Web"
      ],
      "dataset_descriptions": {
        "Tell Me More": "Used to evaluate language agents' ability to ask clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions.",
        "Clamber": "Used to assess language agents' capability to seek additional information when faced with unclear user instructions, enhancing the agent's conversational skills.",
        "TravelPlanner": "Used to test language agents' planning capabilities under the assumption that user instructions are clear and explicit, focusing on task execution rather than clarification.",
        "Mind2Web": "Used to evaluate language agents' generalist capabilities on web tasks, assuming clear and explicit user instructions, focusing on the agent's ability to navigate and interact with web interfaces."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
      "processing_time": 85.45229768753052,
      "citing_paper_id": "270561990",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "3 Ask-before-Plan Dataset Existing benchmarks on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Tell Me More",
        "Clamber",
        "TravelPlanner",
        "Mind2Web"
      ],
      "dataset_descriptions": {
        "Tell Me More": "Used to evaluate language agents' ability to ask clarification questions, focusing on improving user interaction through better understanding of ambiguous instructions.",
        "Clamber": "Used to assess language agents' capability to seek additional information when faced with unclear user instructions, enhancing the agent's conversational skills.",
        "TravelPlanner": "Used to test language agents' planning capabilities under the assumption that user instructions are clear and explicit, focusing on task execution rather than clarification.",
        "Mind2Web": "Used to evaluate language agents' generalist capabilities on web tasks, assuming clear and explicit user instructions, focusing on the agent's ability to navigate and interact with web interfaces."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several datasets/benchmarks but does not specify their usage in the current research. It only describes their focus areas.",
      "processing_time": 85.45229768753052,
      "citing_paper_id": "270561990",
      "cited_paper_id": 270123654
    },
    {
      "context_text": "…either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The context focuses on the capabilities of different models in handling user instructions and asking clarification questions.",
      "processing_time": 58.93342590332031,
      "citing_paper_id": "270561990",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "…either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web (Deng et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The context focuses on the capabilities of different models in handling user instructions and asking clarification questions.",
      "processing_time": 58.93342590332031,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "The emergence of large language models (LLMs) and their integration into autonomous agents exhibits the potential for logical reasoning, decision-making, and problem-solving capabilities (Wang et al., 2023b; Xi et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.960150718688965,
      "citing_paper_id": "270561990",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "The emergence of large language models (LLMs) and their integration into autonomous agents exhibits the potential for logical reasoning, decision-making, and problem-solving capabilities (Wang et al., 2023b; Xi et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 57.960150718688965,
      "citing_paper_id": "270561990",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "Language Agents Language-based agents (Xi et al., 2023; Wang et al., 2023b) aim to perform real-world tasks that require professional expertise or extensive training by utilizing LLMs to conduct reasoning (Yao et al., 2023; Shinn et al., 2023), memory storage and retrieval (Wen et al., 2023; Zhong…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to language-based agents and their capabilities. No verifiable resources are identified.",
      "processing_time": 58.132304668426514,
      "citing_paper_id": "270561990",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Language Agents Language-based agents (Xi et al., 2023; Wang et al., 2023b) aim to perform real-world tasks that require professional expertise or extensive training by utilizing LLMs to conduct reasoning (Yao et al., 2023; Shinn et al., 2023), memory storage and retrieval (Wen et al., 2023; Zhong…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to language-based agents and their capabilities. No verifiable resources are identified.",
      "processing_time": 58.132304668426514,
      "citing_paper_id": "270561990",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "…the status quo, LLMs struggle to spontaneously clarify users’ intentions during the conversation (Deng et al., 2023b) and accurately obtain necessary information via tool utilization without hallucination (Li et al., 2024b), even after the instruction tuning or providing the well-crafted prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations and challenges of LLMs.",
      "processing_time": 56.78260397911072,
      "citing_paper_id": "270561990",
      "cited_paper_id": 267365239
    },
    {
      "context_text": "Qian et al. (2024) explore how language agents understand implicit user intentions but do not clarify users’ needs based on their predefined preferences and consider the whole language agent problem, such as planning and task-solving.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities and limitations of language agents in understanding user intentions and planning.",
      "processing_time": 57.942086935043335,
      "citing_paper_id": "270561990",
      "cited_paper_id": 268732915
    },
    {
      "context_text": "Qian et al. (2024) explore how language agents understand implicit user intentions but do not clarify users’ needs based on their predefined preferences and consider the whole language agent problem, such as planning and task-solving.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities and limitations of language agents in understanding user intentions and planning.",
      "processing_time": 57.942086935043335,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Deng et al., 2023a; Yao et al., 2022; Deng et al., 2024), game agents (Wang et al., 2023a; Zhu et al., 2023), and medical agents (Li et al., 2024a; Schmidgall et al., 2024), etc. Existing studies typically assume that the user instructions are clear enough for language agents to execute the task,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of agents and studies. No clear, verifiable datasets are identified.",
      "processing_time": 58.115381717681885,
      "citing_paper_id": "270561990",
      "cited_paper_id": 269757778
    },
    {
      "context_text": "In addition, while cooperation and division of labor among different LLM-based agents improve planning capabilities in uncertain situations (Song et al., 2024; Xie and Zou, 2024), our work does not thoroughly explore this.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about cooperation and division of labor among LLM-based agents.",
      "processing_time": 57.93615508079529,
      "citing_paper_id": "270561990",
      "cited_paper_id": 270123654
    },
    {
      "context_text": "In addition, while cooperation and division of labor among different LLM-based agents improve planning capabilities in uncertain situations (Song et al., 2024; Xie and Zou, 2024), our work does not thoroughly explore this.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts about cooperation and division of labor among LLM-based agents.",
      "processing_time": 57.93615508079529,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "…on language agents either only focus on their capabilities of asking clarification questions, e.g. , Tell Me More (Qian et al., 2024) and Clamber (Zhang et al., 2024), or simply assume that all the user instructions are clear and explicit, e.g. , TravelPlanner (Xie et al., 2024) and Mind2Web…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.08509278297424,
      "citing_paper_id": "270561990",
      "cited_paper_id": 270123654
    },
    {
      "context_text": "…2023) that can proactively ask clarification questions for clarifying the query uncertainty, latest studies on language agents (Qian et al., 2024; Zhang et al., 2024) emphasize intention clarifications by incorporating implicit intention understanding into down-stream applications (Figure 1(b)).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No verifiable resources are identified.",
      "processing_time": 58.20659327507019,
      "citing_paper_id": "270561990",
      "cited_paper_id": 270123654
    },
    {
      "context_text": "…2023) that can proactively ask clarification questions for clarifying the query uncertainty, latest studies on language agents (Qian et al., 2024; Zhang et al., 2024) emphasize intention clarifications by incorporating implicit intention understanding into down-stream applications (Figure 1(b)).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and methods. No verifiable resources are identified.",
      "processing_time": 58.20659327507019,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "In the planning subtask, the definition of micro and macro strategies is consistent with TravelPlanner (Xie et al., 2024), which is the ratio of passed constraints to the total number of constraints and the ratio of passed constraints among all tested plans, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'TravelPlanner'. No verifiable datasets are referenced.",
      "processing_time": 58.396427392959595,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "5, we observe the same trend in TravelPlanner (Xie et al., 2024), the performance degrades from direct prompting to Reflexion, demonstrating the difficulty for most advanced agents to finish the planning task.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a trend observed in a system called TravelPlanner. No verifiable resources are identified.",
      "processing_time": 58.548916816711426,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "The Ask-before-Plan dataset was built from the TravelPlanner dataset (Xie et al., 2024), which is publicly available.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Ask-before-Plan",
        "TravelPlanner"
      ],
      "dataset_descriptions": {
        "Ask-before-Plan": "Used to build a dataset for evaluating planning capabilities in language models, focusing on interactive travel planning scenarios.",
        "TravelPlanner": "Served as the foundation for the Ask-before-Plan dataset, providing travel planning data for model training and evaluation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two datasets: 'Ask-before-Plan' and 'TravelPlanner'. Both are specific and have clear identifiers.",
      "processing_time": 71.13547539710999,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "…2023b) aim to perform real-world tasks that require professional expertise or extensive training by utilizing LLMs to conduct reasoning (Yao et al., 2023; Shinn et al., 2023), memory storage and retrieval (Wen et al., 2023; Zhong et al., 2024), and tool use (Qin et al., 2024; Schick et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and applications of LLMs. No verifiable resources are identified.",
      "processing_time": 58.53518342971802,
      "citing_paper_id": "270561990",
      "cited_paper_id": null
    },
    {
      "context_text": "ALFWorld ALFWorld (Shridhar et al., 2020) are household tasks that require agents to explore rooms and use commonsense reasoning to perform tasks, such as \"put a pencil on the desk\".",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld' but does not refer to it as a dataset. It describes a simulation environment for agents to perform household tasks, which is more aligned with a method or tool rather than a dataset.",
      "processing_time": 61.45917510986328,
      "citing_paper_id": "276765428",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "We evaluate our approach on two representative benchmarks: ALFWorld (Shridhar et al., 2020) for embodied household task and ScienceWorld (Wang et al., 2022) for textual science experiment task.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld' and 'ScienceWorld' as benchmarks, but they are excluded because they are primarily used for score comparison and do not refer to specific, downloadable datasets. No other datasets are mentioned.",
      "processing_time": 61.63532733917236,
      "citing_paper_id": "276765428",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Recent advancements in large language models (LLMs) (Achiam et al., 2023; Liu et al., 2024; Yang et al., 2024a) have enabled LLM-based agents to tackle complex multi-step tasks, including embodied housework (Shridhar et al., 2020) and science experiments (Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only advancements in LLMs and their applications. No verifiable resources are identified.",
      "processing_time": 58.49136137962341,
      "citing_paper_id": "276765428",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Datasets We conducted experiments on two representative agent datasets: ScienceWorld (Wang et al., 2022) for textual science experiment tasks and ALFWorld (Shridhar et al., 2020) for embodied household tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ScienceWorld",
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "ScienceWorld": "Used for textual science experiment tasks, focusing on the planning capabilities of LLMs in solving complex scientific problems through text-based interactions.",
        "ALFWorld": "Used for embodied household tasks, evaluating the planning capabilities of LLMs in navigating and interacting with simulated environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, ScienceWorld and ALFWorld, which are used for conducting experiments on textual science experiment tasks and embodied household tasks, respectively.",
      "processing_time": 72.8083016872406,
      "citing_paper_id": "276765428",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "While prior studies typically rely on reward models trained on human preference annotations (Bai et al., 2022a; Ouyang et al., 2022; Dubey et al., 2024) or advanced AI (Bai et al., 2022b; Lee et al., 2023) models to assess model outputs, these approaches have limitations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.989070892333984,
      "citing_paper_id": "276765428",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "While prior studies typically rely on reward models trained on human preference annotations (Bai et al., 2022a; Ouyang et al., 2022; Dubey et al., 2024) or advanced AI (Bai et al., 2022b; Lee et al., 2023) models to assess model outputs, these approaches have limitations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.989070892333984,
      "citing_paper_id": "276765428",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "While prior studies typically rely on reward models trained on human preference annotations (Bai et al., 2022a; Ouyang et al., 2022; Dubey et al., 2024) or advanced AI (Bai et al., 2022b; Lee et al., 2023) models to assess model outputs, these approaches have limitations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.989070892333984,
      "citing_paper_id": "276765428",
      "cited_paper_id": 254823489
    },
    {
      "context_text": "While prior studies typically rely on reward models trained on human preference annotations (Bai et al., 2022a; Ouyang et al., 2022; Dubey et al., 2024) or advanced AI (Bai et al., 2022b; Lee et al., 2023) models to assess model outputs, these approaches have limitations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.989070892333984,
      "citing_paper_id": "276765428",
      "cited_paper_id": null
    },
    {
      "context_text": "These tasks require sophisticated planning abilities, as agents need to understand long-term dependencies (Zhang et al., 2024), reason about sequential actions, and adapt to dynamic environments (Yao et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and capabilities required for planning in language models.",
      "processing_time": 57.955368995666504,
      "citing_paper_id": "276765428",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "For example, ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2024) perform planning on-the-fly during task execution and are prone to getting lost due to planning hallucination (Zhu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their behaviors. The context focuses on the performance and limitations of ReAct and Reflexion, which are methods, not datasets.",
      "processing_time": 60.41119933128357,
      "citing_paper_id": "276765428",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Base Agents We evaluate our method on two types of agents, guided by MPO-optimized meta plans: (1) Agents without training, which deploy the ReAct framework using foundation models without additional training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and frameworks. There are no clear identifiers for datasets.",
      "processing_time": 57.854281425476074,
      "citing_paper_id": "276765428",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Previous works (Yao et al., 2022; Shinn et al., 2024) primarily focus on implicit planning, where planning occurs through interleaved reasoning and action generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only previous works focusing on implicit planning in language models.",
      "processing_time": 57.632365703582764,
      "citing_paper_id": "276765428",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "We choose DPO (Rafailov et al., 2024) as our optimization algorithm due to its training stability and low resource consumption.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions an optimization algorithm (DPO) but does not refer to any specific dataset. The context is about the choice of optimization method, not a dataset.",
      "processing_time": 59.326130390167236,
      "citing_paper_id": "276765428",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "For DPO (Rafailov et al., 2024) meta plans per task with a generation temperature of 0.7.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DPO) and its parameters. No verifiable resources are referenced.",
      "processing_time": 58.21502423286438,
      "citing_paper_id": "276765428",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "After identifying contrastive meta plan pairs—those leading to the highest and lowest task completion rates—we apply DPO (Rafailov et al., 2024) to refine the meta planner on these plan pairs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DPO) and a general process of refining a meta planner using plan pairs.",
      "processing_time": 58.707961320877075,
      "citing_paper_id": "276765428",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Some works (Zhou et al., 2023; Ye et al., 2023; Fu et al., 2024) use language models to automate task knowledge synthesis, but the generated knowledge cannot be further optimized through exploration and environmental feed-back, leading to suboptimal performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the limitations of using language models for task knowledge synthesis.",
      "processing_time": 57.68732929229736,
      "citing_paper_id": "276765428",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…prompting methods (Wei et al., 2022b; Song et al., 2023) or more complex strategies (Koh et al., 2024) to build agents capable of leveraging tools (Qin et al., 2023), solving problems, writing code (Qian et al., 2023), and completing real-world tasks (Patil et al., 2023; Gur et al., 2023; Yang et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works. The context focuses on prompting methods and strategies for building agents using LLMs.",
      "processing_time": 59.371415853500366,
      "citing_paper_id": "276765428",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "We utilize vLLM (Kwon et al., 2023) to accelerate the generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions vLLM, which is a method or tool, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 58.32743835449219,
      "citing_paper_id": "276765428",
      "cited_paper_id": 261697361
    },
    {
      "context_text": "All training procedures are implemented using Llama-Factory (Zheng et al., 2024) with full parameter fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Llama-Factory, which is a method or toolkit, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 58.85267186164856,
      "citing_paper_id": "276765428",
      "cited_paper_id": 268536974
    },
    {
      "context_text": "…some works (Zeng et al., 2023; Song et al., 2024a) have begun using expert trajectories for supervised fine-tuning LLMs, while others (Song et al., 2024b; Xiong et al., 2024) enable agents to explore the environment autonomously and leverage reinforcement learning to learn from failed experiences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for training LLMs. The context focuses on the use of expert trajectories and reinforcement learning, which are methods rather than datasets.",
      "processing_time": 60.6457953453064,
      "citing_paper_id": "276765428",
      "cited_paper_id": 270558898
    },
    {
      "context_text": "Following Xiong et al. (2024), we evaluate action efficiency using the average reward per step, calculated for each task as the ratio of the final reward to the number of steps required to complete the task, and then averaging these values across the entire test set.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating action efficiency. No dataset names are provided in the context.",
      "processing_time": 58.18508577346802,
      "citing_paper_id": "276765428",
      "cited_paper_id": 270558898
    },
    {
      "context_text": "…models (Wei et al., 2022a), researchers have begun using prompting methods (Wei et al., 2022b; Song et al., 2023) or more complex strategies (Koh et al., 2024) to build agents capable of leveraging tools (Qin et al., 2023), solving problems, writing code (Qian et al., 2023), and completing…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 58.224592447280884,
      "citing_paper_id": "276765428",
      "cited_paper_id": 270870063
    },
    {
      "context_text": "…in advantages such as explicit guidance, interpretability, and low integration costs for agents, they either require substantial manual design efforts or lack quality assurance in the process of complex knowledge acquisition, resulting in inconsistent improvements for agents (Wang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses general challenges and limitations in the development of planning capabilities for agents.",
      "processing_time": 58.88052749633789,
      "citing_paper_id": "276765428",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Wei et al., 2022b; Song et al., 2023) or more complex strategies (Koh et al., 2024) to build agents capable of leveraging tools (Qin et al., 2023), solving problems, writing code (Qian et al., 2023), and completing real-world tasks (Patil et al., 2023; Gur et al., 2023; Yang et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and capabilities of agents built using different strategies.",
      "processing_time": 58.08623743057251,
      "citing_paper_id": "276765428",
      "cited_paper_id": null
    },
    {
      "context_text": "…in reasoning and instruction-following capabilities of large language models (Wei et al., 2022a), researchers have begun using prompting methods (Wei et al., 2022b; Song et al., 2023) or more complex strategies (Koh et al., 2024) to build agents capable of leveraging tools (Qin et al., 2023),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works. There are no clear identifiers for datasets.",
      "processing_time": 57.70177245140076,
      "citing_paper_id": "276765428",
      "cited_paper_id": null
    },
    {
      "context_text": "Prior work has shown how this can be addressed via problem decomposition (Yang et al., 2022; Zhou et al., 2022; Drozdov et al., 2022; Khot et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on problem decomposition in the context of large language models.",
      "processing_time": 58.080626487731934,
      "citing_paper_id": "262084273",
      "cited_paper_id": 248986239
    },
    {
      "context_text": "Prior work has shown how this can be addressed via problem decomposition (Yang et al., 2022; Zhou et al., 2022; Drozdov et al., 2022; Khot et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to prior work on problem decomposition in the context of large language models.",
      "processing_time": 58.080626487731934,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252715485
    },
    {
      "context_text": "On most categories of Compositional Celebrities and on HotPotQA, REBEL without the use of an external search tool improved performance over baseline GPT3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotPotQA"
      ],
      "dataset_descriptions": {
        "HotPotQA": "Used to evaluate the performance of REBEL compared to GPT3, focusing on question-answering capabilities without external search tools."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'Compositional Celebrities' and 'HotPotQA', which are likely datasets or benchmarks. However, since 'HotPotQA' is a benchmark and 'Compositional Celebrities' is not clearly identified as a dataset, only 'HotPotQA' is included.",
      "processing_time": 71.54405331611633,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "Adding external search augmented the REBEL algorithm’s ability to reason with current facts, and therefore furthered the REBEL algorithm performance on most categories of Compositional Celebrities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Compositional Celebrities"
      ],
      "dataset_descriptions": {
        "Compositional Celebrities": "Used to evaluate the REBEL algorithm's performance on reasoning tasks involving current facts, focusing on compositionality in language models."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Compositional Celebrities' which appears to be a dataset or benchmark, but it is not clear if it is a reusable dataset or just a category of evaluation. The title does not provide additional clarity.",
      "processing_time": 69.2890887260437,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "We found that the REBEL system largely outperformed the ReAct system at all of the 5 categories that were experimented on for Compositional Celebrities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Compositional Celebrities' as a category in experiments but does not specify it as a dataset. No other datasets are mentioned.",
      "processing_time": 58.939574241638184,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "Compositional Celebrities is a dataset consisting of 8.6k questions about Celebrities in different categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Compositional Celebrities"
      ],
      "dataset_descriptions": {
        "Compositional Celebrities": "Used to evaluate compositionality in language models by posing 8.6k questions about celebrities in different categories, focusing on the model's ability to understand and generate compositional language."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Compositional Celebrities' as a dataset, which fits the criteria for a specific, verifiable resource. It is used to evaluate compositionality in language models.",
      "processing_time": 69.98501062393188,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "We tested REBEL on 3 datasets: Compositional Celebrities (Press et al., 2022), FEVER (Thorne et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Compositional Celebrities",
        "FEVER"
      ],
      "dataset_descriptions": {
        "Compositional Celebrities": "Used to test REBEL's ability to handle compositional reasoning in language models, focusing on the compositionality gap in celebrity-related tasks.",
        "FEVER": "Used to evaluate REBEL's fact verification capabilities, specifically assessing its performance in identifying factual claims and supporting evidence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Compositional Celebrities' and 'FEVER', which are used to test the REBEL model. These datasets are clearly named and relevant to the research topic.",
      "processing_time": 74.6605315208435,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "An example of this is on the Birthyear NobelLiterature category of Compositional Celebrities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Compositional Celebrities"
      ],
      "dataset_descriptions": {
        "Compositional Celebrities": "Used to evaluate the compositionality gap in language models, focusing on the 'Birthyear NobelLiterature' category to assess model performance on compositional tasks."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Birthyear NobelLiterature category of Compositional Celebrities' which appears to be a specific dataset or category within a larger dataset. However, without more context, it is unclear if this is a reusable resource or just a reference to a category.",
      "processing_time": 72.42346358299255,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "On Compositional Celebrities, due to computational limitations, we tested using 5 of the 17 categories available, using 100 questions per category, randomly chosen.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Compositional Celebrities' but does not provide enough information to determine if it is a dataset, method, or something else. The context also lacks specific details about how it is used.",
      "processing_time": 60.76649355888367,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "ReAct (Yao et al., 2023) builds off of CoT by generating task-specific actions in response to reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ReAct) that builds upon another method (CoT).",
      "processing_time": 57.97413682937622,
      "citing_paper_id": "262084273",
      "cited_paper_id": 252762395
    },
    {
      "context_text": ", 2023) and finetuning (Micheli & Fleuret, 2021; Schick et al., 2023) techniques, or combinations of the above.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques such as finetuning. No verifiable resources are identified.",
      "processing_time": 57.85025978088379,
      "citing_paper_id": "262084273",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "WebGPT [60] and WebCPM [58] use network search to assist in implementing Question Answering tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions WebGPT and WebCPM, which are models or methods, not datasets. No specific datasets are mentioned or used in the context provided.",
      "processing_time": 59.26549983024597,
      "citing_paper_id": "265381326",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "Some models, like Claude, exhibit a robust capability in SQL generation, no matter whether the approach is direct or CoT-based.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.92804193496704,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Conversely, models like Ziya and InternLM show a drop in performance when tasks are guided in the CoT-based format.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance in a particular format.",
      "processing_time": 57.04189133644104,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "To endow AI Agents with the aforementioned abilities, some techniques that can be used include chain-of-thought (CoT) and vector databases, as shown in Table 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques such as chain-of-thought and vector databases.",
      "processing_time": 57.476338148117065,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "While the direct-guidance approach focuses on testing the model’s raw ability to generate SQL queries when explicitly instructed, the CoT-based approach evaluates a more nuanced capability: the model’s reasoning and problem-solving skills in a step-by-step manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only different approaches to evaluating language models.",
      "processing_time": 56.87839937210083,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "The other is based on the Chain-of-Thought (CoT) [20] approach, which leverages the model’s ability to reason step by step to comprehend and craft SQL tools, and the prompt is shown in Figure 14 in Appendix B.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought prompting).",
      "processing_time": 57.2569637298584,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Specifically, some models such as ChatGLM show a distinct preference for the CoT-based approach, their performance improves when problems are broken down into smaller, manageable sub-tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CoT-based approach) and its impact on model performance.",
      "processing_time": 58.2822060585022,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "ART [82] leverages CoT [20] and In-context Learning [76, 35] techniques to automatically generate multi-step reasoning processes for new tasks, while also selecting and utilizing the most appropriate available tool at each step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and methods. No verifiable resources are identified.",
      "processing_time": 57.513285875320435,
      "citing_paper_id": "265381326",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Code as Policies (CaP) [52] facilitates the transformation of natural language instructions into code fragments that can be directly compiled and executed on robots.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'Code as Policies' (CaP).",
      "processing_time": 58.02652168273926,
      "citing_paper_id": "265381326",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "In addition, RCI [83] recursively criticizes and improves itself to execute computer tasks guided by natural language according to the prompting scheme.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (RCI) that improves itself for executing computer tasks guided by natural language.",
      "processing_time": 58.971787452697754,
      "citing_paper_id": "265381326",
      "cited_paper_id": 257834038
    },
    {
      "context_text": "Organization Model Name Model Parameters OpenAI ChatGPT[15] 200B Anthropic Claude[16] >52B Shanghai AI Lab InternLM 120B IDEA Ziya-13B 13B Tsinghua University ChatGLM-130B[17] 130B Chinese-Alpaca-Plus-33B[18, 19] 33B",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their parameters. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.96193170547485,
      "citing_paper_id": "265381326",
      "cited_paper_id": 258180548
    },
    {
      "context_text": "The LLMs evaluated in this paper are listed in Table 2, elaborated as follows:\nOrganization Model Name Model Parameters\nOpenAI ChatGPT[15] 200B Anthropic Claude[16] >52B\nShanghai AI Lab InternLM 120B IDEA Ziya-13B 13B\nTsinghua University ChatGLM-130B[17] 130B - Chinese-Alpaca-Plus-33B[18, 19] 33B",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their parameters. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.771997690200806,
      "citing_paper_id": "265381326",
      "cited_paper_id": 258180548
    },
    {
      "context_text": "LATM [84], for example, leverages the prowess of GPT-4 to create tools, and the usage of more cost-effective models has shown potential in exhibiting performance on par with larger models for these tool applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of GPT-4 and other models for creating tools. There are no clear identifiers for datasets.",
      "processing_time": 59.301653146743774,
      "citing_paper_id": "265381326",
      "cited_paper_id": 258947222
    },
    {
      "context_text": "PromptCraft [51], on the other hand, devises a function library tailored to ChatGPT on the robot platform, streamlining the conversion of user intentions into executable tasks via the underlying backend API.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (PromptCraft) for integrating ChatGPT with robotics.",
      "processing_time": 58.312318325042725,
      "citing_paper_id": "265381326",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "To improve the training efficiency, we adopt mixed-precision training (Micikevicius et al., 2017), gradient checkpointing (Chen et al., 2016), FlashAttention2 (Dao, 2024), and ZeRO implemented in Deepspeed (Rasley et al., 2020).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and tools used to improve training efficiency.",
      "processing_time": 57.18828749656677,
      "citing_paper_id": "273163663",
      "cited_paper_id": 3297437
    },
    {
      "context_text": "This motivates us to use programming languages—which are Turing complete (Boyer & Moore, 1983)—as a general representation of Z ∗ , thereby framing planning generation as code generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to the Turing completeness of programming languages.",
      "processing_time": 57.25780701637268,
      "citing_paper_id": "273163663",
      "cited_paper_id": 17362777
    },
    {
      "context_text": "Effective planning can frame the reasoning procedure, thereby guiding LLMs through the intricate low-level steps to ultimately solve the tasks (Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to planning in LLMs.",
      "processing_time": 57.244834423065186,
      "citing_paper_id": "273163663",
      "cited_paper_id": 30196678
    },
    {
      "context_text": "We use nucleus sampling (Holtzman et al., 2020) ( p = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions nucleus sampling, which is a method, not a dataset. No specific datasets are mentioned in the citation context.",
      "processing_time": 58.00374102592468,
      "citing_paper_id": "273163663",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Under few-shot evaluations, we observe that all models struggle to achieve non-trivial accuracy due to the degeneration phenomena (Holtzman et al., 2020), an inherent limitation of LLMs in long text generation.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a phenomenon observed in LLMs. The cited paper is used to support a finding about model limitations.",
      "processing_time": 58.68301463127136,
      "citing_paper_id": "273163663",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "With the rapid progress in pre-training and post-training (Brown, 2020; Ouyang et al., 2022; Chung et al., 2024), large language models (LLMs) have exhibited remarkable performance across a wide range of natural language processing (NLP) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in pre-training and post-training of LLMs.",
      "processing_time": 58.06613349914551,
      "citing_paper_id": "273163663",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We use one benchmark to evaluate the performance in sequential decision-making scenarios: ALFWorld (Shridhar et al., 2020), a text-based virtual household environment comprising six distinct task types.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFWorld"
      ],
      "dataset_descriptions": {
        "ALFWorld": "Used to evaluate sequential decision-making in a text-based virtual household environment, focusing on six distinct task types to assess planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "ALFWorld is mentioned as a benchmark for evaluating sequential decision-making scenarios, which aligns with the topic of planning capabilities of LLMs.",
      "processing_time": 65.66820287704468,
      "citing_paper_id": "273163663",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "The vanilla pre-trained model fails to correctly incorporate all found mug = take (”mug” , receptable ) goto (” fridge ”) cool ( found mug , ” fridge ”) goto (”coffeemachine”) put ( found mug , ”coffeemachine”) Table 14: One example for the decision-making task (from the ALFWorld benchmark).",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ALFWorld benchmark' which is a benchmark environment, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 58.7419216632843,
      "citing_paper_id": "273163663",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…such as mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021), multi-hop question-answering (Yang et al., 2018; Trivedi et al., 2022), and decision-making (Shridhar et al., 2020; Xie et al., 2024), their limited planning capability has become a critical bottleneck (Yang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It refers to various capabilities of language models but does not cite any particular dataset used for these studies.",
      "processing_time": 59.3719437122345,
      "citing_paper_id": "273163663",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…such as mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021), multi-hop question-answering (Yang et al., 2018; Trivedi et al., 2022), and decision-making (Shridhar et al., 2020; Xie et al., 2024), their limited planning capability has become a critical bottleneck (Yang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name. It refers to various capabilities of language models but does not cite any particular dataset used for these studies.",
      "processing_time": 59.3719437122345,
      "citing_paper_id": "273163663",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "Similarly, when expanding for loops, the model is guided to systematically process each element, following the encoded iteration logic, which is critical for tasks involving iterative reasoning or sequential decision-making (Shridhar et al., 2020; Xie et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses model behavior and capabilities. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.1316077709198,
      "citing_paper_id": "273163663",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Similarly, when expanding for loops, the model is guided to systematically process each element, following the encoded iteration logic, which is critical for tasks involving iterative reasoning or sequential decision-making (Shridhar et al., 2020; Xie et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses model behavior and capabilities. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.1316077709198,
      "citing_paper_id": "273163663",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "As presented in Figure 5, while C ODE R EASON achieves the highest performance on SVAMP, a benchmark comprised of relatively simple math word problems, it consistently underperforms C ODE P LAN across a diverse array of reasoning benchmarks that demand extensive natural language understanding capabilities, such as MATH, MuSiQue, and HotpotQA.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SVAMP"
      ],
      "dataset_descriptions": {
        "SVAMP": "Used to evaluate the performance of C ODE R EASON on simple math word problems, comparing it against C ODE P LAN and other models."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions several benchmarks but does not specify them as reusable datasets. However, 'SVAMP' is a specific dataset used for evaluating math word problem solving.",
      "processing_time": 66.6309802532196,
      "citing_paper_id": "273163663",
      "cited_paper_id": 232223322
    },
    {
      "context_text": "For mathematical reasoning tasks, we use the full test set of GSM8K and SVAMP, and a non-calculation-intensive subset of MATH, including “Prealgebra,” “Geometry,” “Counting & Probability” since we focus on planning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "SVAMP",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate mathematical reasoning tasks, focusing on problem-solving and planning capabilities in LLMs.",
        "SVAMP": "Used to assess mathematical reasoning, particularly in solving word problems that require planning and step-by-step solutions.",
        "MATH": "Used to evaluate non-calculation-intensive mathematical reasoning tasks, including Prealgebra, Geometry, and Counting & Probability, to focus on planning aspects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating mathematical reasoning tasks, which are directly relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 76.96961545944214,
      "citing_paper_id": "273163663",
      "cited_paper_id": 232223322
    },
    {
      "context_text": "For mathematical reasoning tasks, we use the full test set of GSM8K and SVAMP, and a non-calculation-intensive subset of MATH, including “Prealgebra,” “Geometry,” “Counting & Probability” since we focus on planning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "SVAMP",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate mathematical reasoning tasks, focusing on problem-solving and planning capabilities in LLMs.",
        "SVAMP": "Used to assess mathematical reasoning, particularly in solving word problems that require planning and step-by-step solutions.",
        "MATH": "Used to evaluate non-calculation-intensive mathematical reasoning tasks, including Prealgebra, Geometry, and Counting & Probability, to focus on planning aspects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating mathematical reasoning tasks, which are directly relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 76.96961545944214,
      "citing_paper_id": "273163663",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "We employ three benchmark datasets of mathematical problems: (1) GSM8K (Cobbe et al., 2021), a collection of grade-school problems; (2) MATH (Hendrycks et al., 2021), a more challenging suite of high-school-level problems; and (3) SVAMP (Patel et al., 2021), a robustness evaluation benchmark.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "SVAMP"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate models on grade-school math problems, focusing on basic arithmetic and problem-solving skills.",
        "MATH": "Used to assess models on high-school-level math problems, emphasizing more complex and challenging questions.",
        "SVAMP": "Used to test the robustness of models in solving math word problems, focusing on various types of errors and edge cases."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for evaluating the performance of models on mathematical problems. These datasets are clearly named and their purposes are described.",
      "processing_time": 75.64270758628845,
      "citing_paper_id": "273163663",
      "cited_paper_id": 232223322
    },
    {
      "context_text": "We employ three benchmark datasets of mathematical problems: (1) GSM8K (Cobbe et al., 2021), a collection of grade-school problems; (2) MATH (Hendrycks et al., 2021), a more challenging suite of high-school-level problems; and (3) SVAMP (Patel et al., 2021), a robustness evaluation benchmark.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "SVAMP"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate models on grade-school math problems, focusing on basic arithmetic and problem-solving skills.",
        "MATH": "Used to assess models on high-school-level math problems, emphasizing more complex and challenging questions.",
        "SVAMP": "Used to test the robustness of models in solving math word problems, focusing on various types of errors and edge cases."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for evaluating the performance of models on mathematical problems. These datasets are clearly named and their purposes are described.",
      "processing_time": 75.64270758628845,
      "citing_paper_id": "273163663",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "However, as LLMs are tasked with solving increasingly complex problems that require multi-step reasoning, such as mathematical problems (Cobbe et al., 2021; Hendrycks et al., 2021), multi-hop question-answering (Yang et al., 2018; Trivedi et al., 2022), and decision-making (Shridhar et al., 2020;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss various capabilities of LLMs. No clear, verifiable datasets are identified.",
      "processing_time": 59.69074201583862,
      "citing_paper_id": "273163663",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "…techniques leverage expert-designed prompts and demonstrations to elicit LLMs’ reasoning skills without additional training (Wei et al., 2022b; Press et al., 2023; Imani et al., 2023; Hong et al., 2024) (2) Task-specific fine-tuning aims to enhance LLMs’ reasoning capabilities on specific…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and approaches. The context focuses on methods for enhancing LLMs' reasoning capabilities.",
      "processing_time": 59.373664140701294,
      "citing_paper_id": "273163663",
      "cited_paper_id": 252762102
    },
    {
      "context_text": "…by seamlessly incorporating external tools such as calculators (Schick et al., 2024), retrievers (Asai et al., 2023), and code interpreters (Gao et al., 2023) into the reasoning process, despite the inherent shortcomings of LLMs in utilizing diverse tools in concert (Wang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.79461359977722,
      "citing_paper_id": "273163663",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "The most direct approach is to employ executable code as a replacement for natural language in representing reasoning steps (Gao et al., 2023; Ye et al., 2023), albeit facing challenges when extending to broader, more complex reasoning domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.065088510513306,
      "citing_paper_id": "273163663",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "The most direct approach is to employ executable code as a replacement for natural language in representing reasoning steps (Gao et al., 2023; Ye et al., 2023), albeit facing challenges when extending to broader, more complex reasoning domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 59.065088510513306,
      "citing_paper_id": "273163663",
      "cited_paper_id": 256416408
    },
    {
      "context_text": "Previous work has primarily focused on leveraging executable code to directly solve mathematical reasoning tasks (Gao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on using executable code for mathematical reasoning tasks.",
      "processing_time": 57.86199688911438,
      "citing_paper_id": "273163663",
      "cited_paper_id": 253708270
    },
    {
      "context_text": "…expert-designed prompts and demonstrations to elicit LLMs’ reasoning skills without additional training (Wei et al., 2022b; Press et al., 2023; Imani et al., 2023; Hong et al., 2024) (2) Task-specific fine-tuning aims to enhance LLMs’ reasoning capabilities on specific tasks by curating…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for enhancing LLMs' reasoning capabilities.",
      "processing_time": 58.21147394180298,
      "citing_paper_id": "273163663",
      "cited_paper_id": 257427208
    },
    {
      "context_text": "The code should balance conciseness and informativeness.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general requirement for balancing conciseness and informativeness in the code. No verifiable resources are identified.",
      "processing_time": 59.83814549446106,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "In contrast, C ODE P LAN ’s novel paradigm of generating code plans as an intermediate representation seamlessly integrates robust planning capabilities with the rich language understanding abilities of LLMs, yielding superior performance on intricate multi-step reasoning challenges.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'C ODE P LAN'. The context focuses on the integration of planning capabilities with language understanding in LLMs.",
      "processing_time": 61.06873846054077,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "Expressivness Learning Structuring Versatility Interpretability Data Efficiency Abundance CoT (Wei et al., 2022a) Steps Intertwined with Surface Realization ✗ ✓ ✓ ✗ N/A Plan-and-Solve Wang et al. (2023a) Free-Form Natural Language Text ✗ ✓ ✓ ✗ ✗ A MOR (Guan et al., 2024) Expert-Designed Finite State Machine ✓ ✗ ✓ ✗ ✗ Predicted-PA (Cornille et al., 2024) current approaches mainly frame LLMs’ reasoning procedures through either advanced prompting techniques (Wei et al., 2022a; Yao et al., 2024) or task-specific fine-tuning (Zelikman et al., 2022; Guan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches for improving LLM reasoning capabilities.",
      "processing_time": 58.08156991004944,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "Delving deeper, LLMs’ planning deficiencies largely stem from the fact that massive pre-training text corpora often highly compress the underlying reasoning structures, thereby obscuring the latent, high-level planning signals that LLMs should learn (Zelikman et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'pre-training text corpora'. No specific, verifiable datasets are identified.",
      "processing_time": 59.39691686630249,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "…Versatility Interpretability Data Efficiency Abundance CoT (Wei et al., 2022a) Steps Intertwined with Surface Realization ✗ ✓ ✓ ✗ N/A Plan-and-Solve Wang et al. (2023a) Free-Form Natural Language Text ✗ ✓ ✓ ✗ ✗ A MOR (Guan et al., 2024) Expert-Designed Finite State Machine ✓ ✗ ✓ ✗ ✗ Predicted-PA…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing different prompting strategies for large language models.",
      "processing_time": 58.7367422580719,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258558102
    },
    {
      "context_text": "…Expert-Designed Finite State Machine ✓ ✗ ✓ ✗ ✗ Predicted-PA (Cornille et al., 2024) current approaches mainly frame LLMs’ reasoning procedures through either advanced prompting techniques (Wei et al., 2022a; Yao et al., 2024) or task-specific fine-tuning (Zelikman et al., 2022; Guan et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for enhancing LLM reasoning capabilities.",
      "processing_time": 57.258305072784424,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "As illustrated in Figure 1, this enables the controllable generation of Y that adheres to the human-readable plan Z .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to controllable generation. No verifiable resources are identified.",
      "processing_time": 58.19447731971741,
      "citing_paper_id": "273163663",
      "cited_paper_id": 258947222
    },
    {
      "context_text": "This automated data curation approach enables us to construct a large-scale dataset, and is readily extensible to other existing data sources such as Li et al. (2024) and (Cheng et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide specific names for datasets, only mentions a general approach to constructing a large-scale dataset. No specific datasets are named or referenced.",
      "processing_time": 59.145275831222534,
      "citing_paper_id": "273163663",
      "cited_paper_id": 260866107
    },
    {
      "context_text": "This automated data curation approach enables us to construct a large-scale dataset, and is readily extensible to other existing data sources such as Li et al. (2024) and (Cheng et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide specific names for datasets, only mentions a general approach to constructing a large-scale dataset. No specific datasets are named or referenced.",
      "processing_time": 59.145275831222534,
      "citing_paper_id": "273163663",
      "cited_paper_id": 270620509
    },
    {
      "context_text": "(3) Tool integration seeks to augment LLMs’ reasoning capabilities by seamlessly incorporating external tools such as calculators (Schick et al., 2024), retrievers (Asai et al., 2023), and code interpreters (Gao et al., 2023) into the reasoning process, despite the inherent shortcomings of LLMs in…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. The context is about integrating external tools to enhance LLMs' reasoning capabilities.",
      "processing_time": 58.92042279243469,
      "citing_paper_id": "273163663",
      "cited_paper_id": 260866107
    },
    {
      "context_text": "Figure 1: Two examples for the mathematical reasoning task (Top) and instruction-following task (Bottom) with Mistral-7B as the base model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and a model. No verifiable resources are identified.",
      "processing_time": 57.32638955116272,
      "citing_paper_id": "273163663",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "For Mistral-7B, we use a learning rate of 5e-6 and a global batch size of 512.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model training parameters.",
      "processing_time": 55.885401010513306,
      "citing_paper_id": "273163663",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "We validate the effectiveness of C ODE P LAN using multiple base models, including Mistral (Jiang et al., 2023) and Llama (Touvron et al., 2023; Dubey et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.93100094795227,
      "citing_paper_id": "273163663",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "We select Mistral-7B (Jiang et al., 2023) and Llama-2-7B/13B (Touvron et al., 2023) as our backbone models 2 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (Mistral-7B and Llama-2-7B/13B) but does not reference any specific datasets. The context is focused on selecting models for a study, not on using datasets.",
      "processing_time": 62.02951741218567,
      "citing_paper_id": "273163663",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "For instance, the performance of Mistral-7B drops by 2 to 9 points on several symbolic, multi-hop QA and decision-making tasks after vanilla training.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics on unspecified tasks. No clear, verifiable resource is identified.",
      "processing_time": 57.92833089828491,
      "citing_paper_id": "273163663",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "…et al., 2022a; Wang et al., 2023a; Khot et al., 2022); (2) Expert-guided planning , which trains LLMs based on human-curated, task-specific plans (Yin et al., 2024; Guan et al., 2024); and (3) Implicit planning , which models planning as latent variables to capture the high-level aspects when…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for training LLMs. There are no clear identifiers for datasets.",
      "processing_time": 58.12386870384216,
      "citing_paper_id": "273163663",
      "cited_paper_id": 265128672
    },
    {
      "context_text": "…as mathematical reasoning (Yu et al., 2023; Mitra et al., 2024; Shao et al., 2024; OpenAI, 2024), code reasoning (Le et al., 2022; Shen et al., 2023), instruction-following (Cui et al., 2023), visual reasoning (Cheng et al., 2024), and decision-making (Zeng et al., 2023; Guan et al., 2024) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of reasoning tasks. No clear identifiers for datasets are present.",
      "processing_time": 57.68163824081421,
      "citing_paper_id": "273163663",
      "cited_paper_id": 270620509
    },
    {
      "context_text": "However, prompting approaches typically impose strict requirements on model inherent capabilities as well as carefully designed prompts (Anagnostidis & Bulian, 2024), while task-specific fine-tuning approaches limit the models’ ability to generalize to new domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodologies and limitations of prompting and fine-tuning approaches.",
      "processing_time": 57.21491527557373,
      "citing_paper_id": "273163663",
      "cited_paper_id": 271923851
    },
    {
      "context_text": "We assess the proficiency in following real-world instructions through two benchmarks: (1) AlpacaEval 1.0 (Li et al., 2023) and 2.0 (Dubois et al., 2024), which capture representative user interactions, for which we use win rates against OpenAI’s text-davinci-003 and GPT-4, respectively, as…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AlpacaEval 1.0' and 'AlpacaEval 2.0', which are specific benchmarks used to evaluate the performance of language models. However, these are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 63.80382823944092,
      "citing_paper_id": "273163663",
      "cited_paper_id": null
    },
    {
      "context_text": "Other approaches include gradient-based optimization [37], [38] and prior-knowledge based scenario construction [39]– [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.18170762062073,
      "citing_paper_id": "269043242",
      "cited_paper_id": 2188447
    },
    {
      "context_text": "Other approaches include gradient-based optimization [37], [38] and prior-knowledge based scenario construction [39]– [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.18170762062073,
      "citing_paper_id": "269043242",
      "cited_paper_id": 13905150
    },
    {
      "context_text": "Other approaches include gradient-based optimization [37], [38] and prior-knowledge based scenario construction [39]– [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.18170762062073,
      "citing_paper_id": "269043242",
      "cited_paper_id": 248426863
    },
    {
      "context_text": "Rule-based methods, on the other hand, employ a hand-designed decision-making framework with interpretable rules [14]–[20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only rule-based methods and their characteristics.",
      "processing_time": 56.29476308822632,
      "citing_paper_id": "269043242",
      "cited_paper_id": 49902052
    },
    {
      "context_text": "To assess the interactive behavior in complex and interactive scenarios, closed-loop evaluation is indispensable, particularly as open-loop ego-forecasting evaluation was shown to be misaligned with closed-loop driving [2], [22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses evaluation methods in driving models.",
      "processing_time": 56.388866901397705,
      "citing_paper_id": "269043242",
      "cited_paper_id": 52035171
    },
    {
      "context_text": "Mastering these bi-directional interactions is crucial for smooth progress, ultimately making it a fundamental enabler for autonomy [3], [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references the importance of bi-directional interactions for autonomy.",
      "processing_time": 58.02299880981445,
      "citing_paper_id": "269043242",
      "cited_paper_id": 233324485
    },
    {
      "context_text": "Being able to make plans in anticipation of surrounding agents’ intentions and reactions is a fundamental pillar of real-world driving and essential to solving scenarios like unprotected turns, overtakes, or lane changes [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability required for real-world driving scenarios.",
      "processing_time": 56.79293203353882,
      "citing_paper_id": "269043242",
      "cited_paper_id": 233324485
    },
    {
      "context_text": "The most widely adopted driving simulators are CARLA [5] and nuPlan [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'nuPlan' which is a benchmark for autonomous vehicle planning. However, it is not a traditional dataset but a benchmark challenge. Therefore, it is excluded.",
      "processing_time": 58.99461054801941,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "We chose the large-scale nuPlan dataset [1] and the respective closed-loop simulator with reactive agents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "nuPlan dataset"
      ],
      "dataset_descriptions": {
        "nuPlan dataset": "Used to benchmark planning algorithms in autonomous vehicles, focusing on large-scale, real-world driving scenarios with reactive agents in a closed-loop simulation environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'nuPlan dataset' which is a specific, verifiable dataset used in the research. The dataset is used in conjunction with a closed-loop simulator for planning capabilities in autonomous vehicles.",
      "processing_time": 67.66566896438599,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "We take nuPlan [1] scenarios as a starting point for our benchmark and augment them with additional agents, obstacles, or alternative navigation goals, creating realistic long-tail scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "nuPlan"
      ],
      "dataset_descriptions": {
        "nuPlan": "Used as a foundation for creating benchmark scenarios for autonomous vehicle planning, specifically augmented with additional elements to simulate realistic long-tail scenarios."
      },
      "confidence_score": 0.9,
      "reasoning": "nuPlan is mentioned as a starting point for creating benchmark scenarios, indicating it is a dataset or collection of scenarios used for autonomous vehicle planning.",
      "processing_time": 65.32181906700134,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "Yet, simple rule-based planners outperform learning-based methods in the recently proposed closed-loop nuPlan benchmark [1].",
      "catation_intent": "findings",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the 'nuPlan' benchmark, which is a closed-loop ML-based planning benchmark for autonomous vehicles. However, it is used as a benchmark/competition, not as a reusable dataset.",
      "processing_time": 60.637775182724,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "…the following situations: Nudging around parked vehicles, overtaking obstacles, passing a construction zone, passing an accident site, nuPlan [1] Carla [5] interPlan (ours) We conduct experiments with an exhaustive set of state-of-the-art planning methods and reveal critical shortcomings in…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'nuPlan' and 'Carla', which are specific resources. However, 'nuPlan' is a benchmark and 'Carla' is a simulation platform, both excluded according to the rules. No valid datasets are mentioned.",
      "processing_time": 62.633116483688354,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "On the other hand, nu-Plan [1] is based on real-world data, which comes at the cost of undersampling rare and critical scenarios, since collecting them is prohibitive due to the cost and danger involved.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "nu-Plan"
      ],
      "dataset_descriptions": {
        "nu-Plan": "Used to train and evaluate planning models for autonomous vehicles, focusing on real-world scenarios. Highlights the challenge of undersampling rare and critical events due to cost and safety concerns."
      },
      "confidence_score": 0.9,
      "reasoning": "nu-Plan is mentioned as a dataset derived from real-world data, but it is also part of a benchmark. However, it is used here as a dataset for training and evaluation.",
      "processing_time": 68.7340247631073,
      "citing_paper_id": "269043242",
      "cited_paper_id": 235593036
    },
    {
      "context_text": "While the Carla Leaderboard fueled research for end-to-end methods that directly process sensor information [8], [9], [23]–[27], the nuPlan competition focused on modular planners that use outputs of upstream perception modules (e.g., bounding boxes, maps) as inputs [28]–[33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions the nuPlan competition but does not specify a dataset. It focuses on the competition's focus on modular planners rather than a specific dataset.",
      "processing_time": 58.62926268577576,
      "citing_paper_id": "269043242",
      "cited_paper_id": 237263814
    },
    {
      "context_text": "While the Carla Leaderboard fueled research for end-to-end methods that directly process sensor information [8], [9], [23]–[27], the nuPlan competition focused on modular planners that use outputs of upstream perception modules (e.g., bounding boxes, maps) as inputs [28]–[33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions the nuPlan competition but does not specify a dataset. It focuses on the competition's focus on modular planners rather than a specific dataset.",
      "processing_time": 58.62926268577576,
      "citing_paper_id": "269043242",
      "cited_paper_id": 249209900
    },
    {
      "context_text": "We test an exhaustive list of state-of-the-art planners, including learning-based (PDM-Open [2], UrbanDriver [28], GC-PGP [30], Gameformer [31], DTPP [32]) and rule-based planners (IDM [21], IDM+MOBIL [55], PDM-Closed [2]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several planners but does not refer to any specific datasets. The context is focused on describing the planners used in the experiments.",
      "processing_time": 58.26358199119568,
      "citing_paper_id": "269043242",
      "cited_paper_id": 237263814
    },
    {
      "context_text": "We test an exhaustive list of state-of-the-art planners, including learning-based (PDM-Open [2], UrbanDriver [28], GC-PGP [30], Gameformer [31], DTPP [32]) and rule-based planners (IDM [21], IDM+MOBIL [55], PDM-Closed [2]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several planners but does not refer to any specific datasets. The context is focused on describing the planners used in the experiments.",
      "processing_time": 58.26358199119568,
      "citing_paper_id": "269043242",
      "cited_paper_id": 256868369
    },
    {
      "context_text": "We test an exhaustive list of state-of-the-art planners, including learning-based (PDM-Open [2], UrbanDriver [28], GC-PGP [30], Gameformer [31], DTPP [32]) and rule-based planners (IDM [21], IDM+MOBIL [55], PDM-Closed [2]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several planners but does not refer to any specific datasets. The context is focused on describing the planners used in the experiments.",
      "processing_time": 58.26358199119568,
      "citing_paper_id": "269043242",
      "cited_paper_id": 257482793
    },
    {
      "context_text": "We test an exhaustive list of state-of-the-art planners, including learning-based (PDM-Open [2], UrbanDriver [28], GC-PGP [30], Gameformer [31], DTPP [32]) and rule-based planners (IDM [21], IDM+MOBIL [55], PDM-Closed [2]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several planners but does not refer to any specific datasets. The context is focused on describing the planners used in the experiments.",
      "processing_time": 58.26358199119568,
      "citing_paper_id": "269043242",
      "cited_paper_id": 263830880
    },
    {
      "context_text": "We test various LLMs, including Llama-7B, Llama-13B [56], and GPT-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.672980546951294,
      "citing_paper_id": "269043242",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "While prior work demonstrated strong reasoning capabilities of LLMs in traffic scenarios [45]–[48], methods are often only evaluated in open-loop evaluation [6], [42], [44], [49], [50], in the synthetic Carla simulator [5], [51], or the simplistic HighwayEnv environment [52]–[54].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions specific environments used for evaluating LLMs in traffic scenarios, but does not mention any specific datasets by name.",
      "processing_time": 57.98874020576477,
      "citing_paper_id": "269043242",
      "cited_paper_id": 265294541
    },
    {
      "context_text": "While prior work demonstrated strong reasoning capabilities of LLMs in traffic scenarios [45]–[48], methods are often only evaluated in open-loop evaluation [6], [42], [44], [49], [50], in the synthetic Carla simulator [5], [51], or the simplistic HighwayEnv environment [52]–[54].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions specific environments used for evaluating LLMs in traffic scenarios, but does not mention any specific datasets by name.",
      "processing_time": 57.98874020576477,
      "citing_paper_id": "269043242",
      "cited_paper_id": 266435584
    },
    {
      "context_text": "This was extended by AgentDriver [44], which uses another LLM to generate the task prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'AgentDriver'.",
      "processing_time": 56.44902229309082,
      "citing_paper_id": "269043242",
      "cited_paper_id": 265294541
    },
    {
      "context_text": "• TextCraft [25], which is a text-based environment to create Minecraft items.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "TextCraft is mentioned as a text-based environment for creating Minecraft items, but it does not fit the criteria for a dataset. It appears to be a tool or method rather than a reusable dataset.",
      "processing_time": 60.43372917175293,
      "citing_paper_id": "277150799",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "We conduct extensive experiments in three interactive environments [10; 24; 25], demonstrating that CGI substantially enhances performance over baseline approaches.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only interactive environments which are not clearly identified as datasets.",
      "processing_time": 56.97863554954529,
      "citing_paper_id": "277150799",
      "cited_paper_id": 265128575
    },
    {
      "context_text": "…next action a t ∼ π θ ( a | τ t , e ) at each time step t , based on the interaction history τ t , which is defined as The trajectory is then represented as Iterative Supervised Fine-Tuning in Agentic Task Iterative Supervised Fine-Tuning (SFT) is a process of exploration and learning [26; 27; 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for iterative supervised fine-tuning. No verifiable resources are identified.",
      "processing_time": 57.59510087966919,
      "citing_paper_id": "277150799",
      "cited_paper_id": 266335848
    },
    {
      "context_text": "Numerical feedback [39; 40; 41; 42] is typically provided by training a reward model (RM) or verifiers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of training a reward model or verifiers. No verifiable resources are named.",
      "processing_time": 58.2160427570343,
      "citing_paper_id": "277150799",
      "cited_paper_id": 270379625
    },
    {
      "context_text": "Numerical feedback [39; 40; 41; 42] is typically provided by training a reward model (RM) or verifiers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of training a reward model or verifiers. No verifiable resources are named.",
      "processing_time": 58.2160427570343,
      "citing_paper_id": "277150799",
      "cited_paper_id": 274024100
    },
    {
      "context_text": "Training-based methods [31; 11; 52] rely on techniques such as Supervised Fine-Tuning (SFT) [38; 53] or Direct Preference Optimization (DPO) [54; 55] to train large language models (LLMs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only training methods and techniques. No verifiable resources are identified.",
      "processing_time": 57.24458146095276,
      "citing_paper_id": "277150799",
      "cited_paper_id": 270688227
    },
    {
      "context_text": "Training-based methods [31; 11; 52] rely on techniques such as Supervised Fine-Tuning (SFT) [38; 53] or Direct Preference Optimization (DPO) [54; 55] to train large language models (LLMs).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only training methods and techniques. No verifiable resources are identified.",
      "processing_time": 57.24458146095276,
      "citing_paper_id": "277150799",
      "cited_paper_id": 273234147
    },
    {
      "context_text": "3) Inference-time sampling methods [13; 14; 15] employ techniques such as Best-of-N (BoN) [16; 17] and Tree-of-Thought (ToT) [56] to identify optimal actions or trajectories during inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. No dataset names are present in the text.",
      "processing_time": 57.250898599624634,
      "citing_paper_id": "277150799",
      "cited_paper_id": 278532827
    },
    {
      "context_text": "A common approach for providing feedback is to rely on numerical signals, such as verifiers [12; 13] or reward models [14; 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches. No dataset names are present in the text.",
      "processing_time": 57.698254346847534,
      "citing_paper_id": "277150799",
      "cited_paper_id": 278532827
    },
    {
      "context_text": "We then compare our critic model against two types of approaches (see the Appendix for implementation details): 1) Numerical based : We use DGAP [13], a discriminator trained to assess the alignment between actor actions and expert actions at the step level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DGAP' as a discriminator but does not specify it as a dataset. It is likely a method or model used for assessing action alignment.",
      "processing_time": 58.9051947593689,
      "citing_paper_id": "277150799",
      "cited_paper_id": 278532827
    },
    {
      "context_text": "Previous approaches to agent learning in interactive environments can be classified into three main categories: 1) Prompt-based methods [20; 47; 48; 49] utilize human-written prompts to guide LLMs in summarizing experiences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.20330190658569,
      "citing_paper_id": "277150799",
      "cited_paper_id": null
    },
    {
      "context_text": "Beyond these foundational tasks, LLMs exhibit advanced capabilities in planning and reasoning (OpenAI, 2024; Guo et al., 2025), enabling the development of more complex AI systems, including LLM agents (Nakano et al., 2021; Deng et al., 2024; Gur et al., 2023; Zhou et al., 2023; Le et al., 2022;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing LLM capabilities. No verifiable resources are identified.",
      "processing_time": 57.670451641082764,
      "citing_paper_id": "278481145",
      "cited_paper_id": null
    },
    {
      "context_text": "Training-free defenses use prompt engineering and behavioral constraints, such as input delimiters (Hines et al., 2024; Mendes, 2023; Willison, 2023), prompt repetition (lea, 2023), or response consistency checks (Liu et al., 2024), though these primarily detect attacks post-execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for training-free defenses in LLMs.",
      "processing_time": 57.00671315193176,
      "citing_paper_id": "278481145",
      "cited_paper_id": null
    },
    {
      "context_text": "Despite their impressive capabilities, LLM agents suffer from serious security challenges of indirect prompt injection (Chen et al., 2024d; wunderwuzzi, 2025; Debenedetti et al., 2024; Greshake et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only security challenges related to LLMs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.28309345245361,
      "citing_paper_id": "278481145",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, including Preprint natural language processing (NLP) (Wang, 2018), code generation (Chen et al., 2021), and mathematical problem-solving (Hendrycks et al., 2021; Cobbe et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs across various tasks. No verifiable resources are named.",
      "processing_time": 58.022223472595215,
      "citing_paper_id": "278481145",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "…Guo et al., 2025), enabling the development of more complex AI systems, including LLM agents (Nakano et al., 2021; Deng et al., 2024; Gur et al., 2023; Zhou et al., 2023; Le et al., 2022; Gao et al., 2023; Li et al., 2022; Schick et al., 2024; Qin et al., 2023; Patil et al., 2023; OpenAI, 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and a tool called ToolLLM. There are no clear identifiers for datasets within the provided context.",
      "processing_time": 59.48903155326843,
      "citing_paper_id": "278481145",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…et al., 2022; Gao et al., 2023; Li et al., 2022) aid humans in writing code, providing code completion, debugging, etc; ③ Personal assistants (Schick et al., 2024; Qin et al., 2023; Patil et al., 2023; OpenAI, 2023b) that assist users with daily tasks (e.g., setting calendars and sending emails).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of applications for LLMs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.28905653953552,
      "citing_paper_id": "278481145",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "It is worth noting that A GENT V IGIL achieve approximately 50% and 60% on GPT-4o-mini and GPT-4o, respectively, suggesting that the instruction hierarchy (Wallace et al., 2024) defense mechanism is not sufficiently effective.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context focuses on the effectiveness of an instruction hierarchy defense mechanism.",
      "processing_time": 58.298776626586914,
      "citing_paper_id": "278481145",
      "cited_paper_id": 269294048
    },
    {
      "context_text": "Training-dependent methods rely on adversarial training or additional models to detect injected prompts (Wallace et al., 2024; Chen et al., 2024a;b; ProtectAI, 2024; Inan et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 57.0742552280426,
      "citing_paper_id": "278481145",
      "cited_paper_id": 269294048
    },
    {
      "context_text": "…et al., 2024b) utilize gradient-based methods and require white-box access to target components, while GPTFuzzer (Yu et al., 2023), and RLBreaker (Chen et al., 2024c) focusing on direct prompt injections, which require detailed feedback and have limited applicability in complex real-world agents…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The context focuses on comparing different approaches to jailbreaking LLMs using various techniques.",
      "processing_time": 58.5424485206604,
      "citing_paper_id": "278481145",
      "cited_paper_id": 270440318
    },
    {
      "context_text": "For example, AgentPoison (Chen et al., 2024d) and VWA-adv (Wu et al., 2024b) utilize gradient-based methods and require white-box access to target components, while GPTFuzzer (Yu et al., 2023), and RLBreaker (Chen et al., 2024c) focusing on direct prompt injections, which require detailed feedback and have limited applicability in complex real-world agents where direct prompt manipulation is often restricted.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses various methods and tools but does not mention any specific datasets. The focus is on the methodologies and their applications rather than on data sources.",
      "processing_time": 58.288307905197144,
      "citing_paper_id": "278481145",
      "cited_paper_id": 270440318
    },
    {
      "context_text": "Other proposed defenses, including those requiring human over-sight (Wu et al., 2025), human labeling (Wu et al., 2024c), or action reversal capabilities (Patil et al., 2024), often make impractical assumptions or demand significant human intervention, limiting their real-world applicability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or approaches. No verifiable resources are identified.",
      "processing_time": 57.07638597488403,
      "citing_paper_id": "278481145",
      "cited_paper_id": 272986898
    },
    {
      "context_text": "Generally speaking, industrial knowledge operations SOPs, spanning domains like customer service, content moderation, and data annotation, present challenges distinct from traditional process automation tasks [20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in industrial knowledge operations SOPs. No verifiable resources are identified.",
      "processing_time": 57.50513434410095,
      "citing_paper_id": "279260580",
      "cited_paper_id": 218551201
    },
    {
      "context_text": "These operations frequently encounter edge cases that challenge policy interpretations [6], making them particularly difficult for existing LLM-based automation approaches that assume well-defined success criteria and stable procedures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in LLM-based automation. No verifiable resources are identified.",
      "processing_time": 57.176238775253296,
      "citing_paper_id": "279260580",
      "cited_paper_id": 246634179
    },
    {
      "context_text": "Current evaluation frameworks primarily focus on isolated capabilities—tool use [19], planning [30], or instruction following [26], using clean, synthetic prompts that sidestep the messiness of real-world procedures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and evaluation frameworks. No clear, verifiable datasets are identified.",
      "processing_time": 57.62519359588623,
      "citing_paper_id": "279260580",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Current evaluation frameworks primarily focus on isolated capabilities—tool use [19], planning [30], or instruction following [26], using clean, synthetic prompts that sidestep the messiness of real-world procedures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and evaluation frameworks. No clear, verifiable datasets are identified.",
      "processing_time": 57.62519359588623,
      "citing_paper_id": "279260580",
      "cited_paper_id": null
    },
    {
      "context_text": "While recent advances in LLMs have enabled sophisticated instruction following and tool use [26, 30], automating SOP execution poses unique challenges that existing benchmarks fail to capture.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges with existing benchmarks. No clear, verifiable resource names are provided.",
      "processing_time": 57.3697624206543,
      "citing_paper_id": "279260580",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Architectures such as ReAct [30], AutoGPT [29], and LangChain [2] have demonstrated autonomous decision-making via multi-step reasoning and API calls.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectures and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.63126564025879,
      "citing_paper_id": "279260580",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…supporting agent evaluation; and (3) Empirical evidence showing that some of the most popular agent paradigms, Function-Calling [13] and ReAct [30], achieve average task success rates of only 27% and 48% respectively, with wide variation across task types, while achieving higher than 90% on…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only task success rates for certain agent paradigms. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.04161882400513,
      "citing_paper_id": "279260580",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "With recent advances in large language models (LLMs), there is growing interest in automating SOP execution using LLM-based agents [25, 30, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing LLMs and their capabilities.",
      "processing_time": 56.62604212760925,
      "citing_paper_id": "279260580",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Benchmarks like Gorilla [16], API-Bank [9], BENCHA-GENTS [1] and ToolBench [19] assess tool invocation and API usage capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not specify their use as datasets. They are likely used for evaluating models, not as traditional datasets.",
      "processing_time": 58.065210819244385,
      "citing_paper_id": "279260580",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "Benchmarks like Gorilla [16], API-Bank [9], BENCHA-GENTS [1] and ToolBench [19] assess tool invocation and API usage capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not specify their use as datasets. They are likely used for evaluating models, not as traditional datasets.",
      "processing_time": 58.065210819244385,
      "citing_paper_id": "279260580",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "Benchmarks like Gorilla [16], API-Bank [9], BENCHA-GENTS [1] and ToolBench [19] assess tool invocation and API usage capabilities.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not specify their use as datasets. They are likely used for evaluating models, not as traditional datasets.",
      "processing_time": 58.065210819244385,
      "citing_paper_id": "279260580",
      "cited_paper_id": null
    },
    {
      "context_text": "However, deploying such agents in production poses challenges that are not captured by existing benchmarks like Gorilla [16], API-Bank [9], ComplexBench [27].",
      "catation_intent": "none",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing benchmarks' and lists three specific benchmarks, but these are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 58.355960845947266,
      "citing_paper_id": "279260580",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "However, deploying such agents in production poses challenges that are not captured by existing benchmarks like Gorilla [16], API-Bank [9], ComplexBench [27].",
      "catation_intent": "none",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing benchmarks' and lists three specific benchmarks, but these are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 58.355960845947266,
      "citing_paper_id": "279260580",
      "cited_paper_id": 258865184
    },
    {
      "context_text": "However, deploying such agents in production poses challenges that are not captured by existing benchmarks like Gorilla [16], API-Bank [9], ComplexBench [27].",
      "catation_intent": "none",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing benchmarks' and lists three specific benchmarks, but these are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 58.355960845947266,
      "citing_paper_id": "279260580",
      "cited_paper_id": 271039484
    },
    {
      "context_text": "Instruction-following ability has been benchmarked through datasets like SUPER-NATURAL-INSTRUCTIONS [26], AlpacaEval [10], and FollowEval [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SUPER-NATURAL-INSTRUCTIONS",
        "AlpacaEval",
        "FollowEval"
      ],
      "dataset_descriptions": {
        "SUPER-NATURAL-INSTRUCTIONS": "Used to benchmark instruction-following capabilities of large language models, focusing on natural language instructions across various domains and complexities.",
        "AlpacaEval": "Used to evaluate the performance of large language models in following instructions, emphasizing multi-dimensional assessment of instruction-following capabilities.",
        "FollowEval": "Used to assess the instruction-following capability of large language models, providing a multi-dimensional benchmark that evaluates various aspects of instruction-following performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions datasets used for benchmarking instruction-following abilities of large language models. These are specific, named datasets that are relevant to the research topic.",
      "processing_time": 78.72496104240417,
      "citing_paper_id": "279260580",
      "cited_paper_id": 265220706
    },
    {
      "context_text": "Instruction-following ability has been benchmarked through datasets like SUPER-NATURAL-INSTRUCTIONS [26], AlpacaEval [10], and FollowEval [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SUPER-NATURAL-INSTRUCTIONS",
        "AlpacaEval",
        "FollowEval"
      ],
      "dataset_descriptions": {
        "SUPER-NATURAL-INSTRUCTIONS": "Used to benchmark instruction-following capabilities of large language models, focusing on natural language instructions across various domains and complexities.",
        "AlpacaEval": "Used to evaluate the performance of large language models in following instructions, emphasizing multi-dimensional assessment of instruction-following capabilities.",
        "FollowEval": "Used to assess the instruction-following capability of large language models, providing a multi-dimensional benchmark that evaluates various aspects of instruction-following performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions datasets used for benchmarking instruction-following abilities of large language models. These are specific, named datasets that are relevant to the research topic.",
      "processing_time": 78.72496104240417,
      "citing_paper_id": "279260580",
      "cited_paper_id": null
    },
    {
      "context_text": "More recent multi-step benchmarks like ComplexBench [27] and InFoBench [17] explore instruction complexity, but typically use machine-formatted inputs that omit the ambiguity and variability of human-authored SOPs.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ComplexBench and InFoBench as benchmarks exploring instruction complexity, but does not indicate they are specific datasets. They are excluded as they are likely benchmark suites rather than reusable datasets.",
      "processing_time": 60.201263666152954,
      "citing_paper_id": "279260580",
      "cited_paper_id": 271039484
    },
    {
      "context_text": "Ind.Ctx. AlpacaEval [7] ✓ Instruction Following and Benchmarking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper titled 'AlpacaEval' which is likely a method or tool for benchmarking.",
      "processing_time": 58.6718008518219,
      "citing_paper_id": "279260580",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional planning systems based on PDDL [3] struggle with tasks requiring common sense reasoning, often over-looking essential preconditions (e.g., checking whether a container is empty before filling it [4]) and fail to consider action consequences(e.g., checking that the fridge is closed after…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only planning systems and their limitations. The context focuses on the challenges of traditional planning systems in handling common sense reasoning.",
      "processing_time": 58.46118927001953,
      "citing_paper_id": "280150778",
      "cited_paper_id": 1397894
    },
    {
      "context_text": "These classical techniques [19], [20] verify plans against domain specifications by systematically exploring state spaces to ensure all preconditions and effects are consistent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only classical techniques and methods for plan validation and formal verification.",
      "processing_time": 56.56939911842346,
      "citing_paper_id": "280150778",
      "cited_paper_id": 3098522
    },
    {
      "context_text": "These classical techniques [19], [20] verify plans against domain specifications by systematically exploring state spaces to ensure all preconditions and effects are consistent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only classical techniques and methods for plan validation and formal verification.",
      "processing_time": 56.56939911842346,
      "citing_paper_id": "280150778",
      "cited_paper_id": 37954891
    },
    {
      "context_text": "Traditionally, researchers apply LTL in model checking and theorem proving to verify system properties [14], [15]; however, these methods often face challenges when addressing the nuances of natural language understanding and real-world complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and challenges. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.49344444274902,
      "citing_paper_id": "280150778",
      "cited_paper_id": 7646504
    },
    {
      "context_text": "Traditionally, researchers apply LTL in model checking and theorem proving to verify system properties [14], [15]; however, these methods often face challenges when addressing the nuances of natural language understanding and real-world complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and challenges. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.49344444274902,
      "citing_paper_id": "280150778",
      "cited_paper_id": null
    },
    {
      "context_text": "The example demonstrates verification of a tea preparation plan, where the system identifies and corrects issues with action ordering and missing prerequisites. in robotics [14], [15], though these methods often face challenges with natural language understanding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and challenges in robotics and natural language understanding, but no dataset names are provided.",
      "processing_time": 58.21269392967224,
      "citing_paper_id": "280150778",
      "cited_paper_id": 7646504
    },
    {
      "context_text": "The example demonstrates verification of a tea preparation plan, where the system identifies and corrects issues with action ordering and missing prerequisites. in robotics [14], [15], though these methods often face challenges with natural language understanding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and challenges in robotics and natural language understanding, but no dataset names are provided.",
      "processing_time": 58.21269392967224,
      "citing_paper_id": "280150778",
      "cited_paper_id": null
    },
    {
      "context_text": "Linear Temporal Logic (LTL) [27] has emerged as a powerful formalism for specifying robotic task requirements [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formalism (LTL) and its application in robotic task requirements. No verifiable resources are identified.",
      "processing_time": 58.64507865905762,
      "citing_paper_id": "280150778",
      "cited_paper_id": 14105338
    },
    {
      "context_text": "Linear Temporal Logic (LTL) [27] has emerged as a powerful formalism for specifying robotic task requirements [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formalism (LTL) and its application in robotic task requirements. No verifiable resources are identified.",
      "processing_time": 58.64507865905762,
      "citing_paper_id": "280150778",
      "cited_paper_id": 20483647
    },
    {
      "context_text": "Researchers use Linear Temporal Logic (LTL) as a powerful formalism to specify robotic task requirements [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Linear Temporal Logic) used for specifying robotic task requirements.",
      "processing_time": 56.760937213897705,
      "citing_paper_id": "280150778",
      "cited_paper_id": 14105338
    },
    {
      "context_text": "The seminal work in [27] laid the foundation for using LTL in system verification, an approach later adapted to robotics in [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only foundational work and adaptations in robotics. No verifiable resources are identified.",
      "processing_time": 56.90968418121338,
      "citing_paper_id": "280150778",
      "cited_paper_id": 14105338
    },
    {
      "context_text": "The seminal work in [27] laid the foundation for using LTL in system verification, an approach later adapted to robotics in [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only foundational work and adaptations in robotics. No verifiable resources are identified.",
      "processing_time": 56.90968418121338,
      "citing_paper_id": "280150778",
      "cited_paper_id": 20483647
    },
    {
      "context_text": "We use Linear Temporal Logic [27] as our formal language for specifying robotic task requirements.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formal language (Linear Temporal Logic) used for specifying task requirements.",
      "processing_time": 56.89308977127075,
      "citing_paper_id": "280150778",
      "cited_paper_id": 20483647
    },
    {
      "context_text": "With its ability to express temporal relationships and logical constraints, LTL is well-suited for representing the structure and dependencies within complex task plans [28].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LTL) for representing task plans.",
      "processing_time": 56.04865908622742,
      "citing_paper_id": "280150778",
      "cited_paper_id": 34600346
    },
    {
      "context_text": "For our experiments, we used the VirtualHome [18] dataset from the ZeroShot planner [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VirtualHome"
      ],
      "dataset_descriptions": {
        "VirtualHome": "Used to simulate household activities via programs, focusing on zero-shot planning capabilities of language models. The dataset provides actionable knowledge for embodied agents."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'VirtualHome' dataset, which is a specific, verifiable resource used in the experiments. The dataset is associated with household activity simulation and is used in the context of zero-shot planning.",
      "processing_time": 66.89116024971008,
      "citing_paper_id": "280150778",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "For our experiments, we used the VirtualHome [18] dataset from the ZeroShot planner [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VirtualHome"
      ],
      "dataset_descriptions": {
        "VirtualHome": "Used to simulate household activities via programs, focusing on zero-shot planning capabilities of language models. The dataset provides actionable knowledge for embodied agents."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'VirtualHome' dataset, which is a specific, verifiable resource used in the experiments. The dataset is associated with household activity simulation and is used in the context of zero-shot planning.",
      "processing_time": 66.89116024971008,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "VirtualHome-LTL is adapted from VirtualHome [18], a dataset containing human-like activities modeled in a virtual environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VirtualHome"
      ],
      "dataset_descriptions": {
        "VirtualHome": "Adapted to simulate human-like activities in a virtual environment, focusing on modeling household tasks for AI planning capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VirtualHome', which is a specific dataset used for simulating household activities in a virtual environment. The citation indicates that this dataset is adapted for use in the current research.",
      "processing_time": 65.60226607322693,
      "citing_paper_id": "280150778",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "For instance, [30] proposed frameworks for verifiable reinforcement learning that incorporate safety constraints, while [12] introduced methods for common-sense verification through LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 57.13011908531189,
      "citing_paper_id": "280150778",
      "cited_paper_id": 96452977
    },
    {
      "context_text": "For instance, [30] proposed frameworks for verifiable reinforcement learning that incorporate safety constraints, while [12] introduced methods for common-sense verification through LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 57.13011908531189,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273374421
    },
    {
      "context_text": "Subsequent advancements, such as those in [29], introduced robust temporal logics that enable more realistic modeling of continuous systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only advancements in temporal logics for modeling continuous systems.",
      "processing_time": 54.66916489601135,
      "citing_paper_id": "280150778",
      "cited_paper_id": 205763978
    },
    {
      "context_text": "ALFRED-LTL is derived from the ALFRED dataset [17], which consists of household task instructions executed in a simulated environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFRED dataset"
      ],
      "dataset_descriptions": {
        "ALFRED dataset": "Used to derive ALFRED-LTL, focusing on household task instructions executed in a simulated environment to study planning capabilities of LLMs."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ALFRED dataset, which is a specific, verifiable dataset used for interpreting grounded instructions for everyday tasks in a simulated environment.",
      "processing_time": 65.41091442108154,
      "citing_paper_id": "280150778",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Similarly, in [32], grounding language to non-Markovian tasks using LTL was explored with a focus on learning rather than strict formal verification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a research direction. The context focuses on the method and findings rather than a reusable dataset.",
      "processing_time": 57.08311414718628,
      "citing_paper_id": "280150778",
      "cited_paper_id": 219179089
    },
    {
      "context_text": "Recent approaches have evolved from classical planning methods to learning-based techniques [5], which better handle uncertainty and complex environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a shift in methodologies from classical planning to learning-based techniques.",
      "processing_time": 55.84399938583374,
      "citing_paper_id": "280150778",
      "cited_paper_id": 238583814
    },
    {
      "context_text": "In [31], mapping natural language to lifted LTL representations was investigated to improve generalization across domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for mapping natural language to lifted LTL representations.",
      "processing_time": 56.55725049972534,
      "citing_paper_id": "280150778",
      "cited_paper_id": 238634478
    },
    {
      "context_text": "While most works focus on plan generation [6], [11], relatively few address the critical challenge of plan verification.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the focus of existing works on plan generation versus plan verification.",
      "processing_time": 57.5242645740509,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "While most works focus on plan generation [6], [11], relatively few address the critical challenge of plan verification.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the focus of existing works on plan generation versus plan verification.",
      "processing_time": 57.5242645740509,
      "citing_paper_id": "280150778",
      "cited_paper_id": 247957917
    },
    {
      "context_text": "Current approaches primarily use LLMs to generate task plans from natural language instructions [6] or translate instructions into formal specifications [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches using LLMs for task planning and instruction translation. No verifiable resources are identified.",
      "processing_time": 57.773502349853516,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Current approaches primarily use LLMs to generate task plans from natural language instructions [6] or translate instructions into formal specifications [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches using LLMs for task planning and instruction translation. No verifiable resources are identified.",
      "processing_time": 57.773502349853516,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273098318
    },
    {
      "context_text": "However, most studies focus primarily on generating plans rather than verifying them [6], [8], [11], or provide verification mechanisms with limited scope due to their reliance on simple templates [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about the focus of previous studies. No verifiable resources are identified.",
      "processing_time": 56.74326181411743,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "However, most studies focus primarily on generating plans rather than verifying them [6], [8], [11], or provide verification mechanisms with limited scope due to their reliance on simple templates [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about the focus of previous studies. No verifiable resources are identified.",
      "processing_time": 56.74326181411743,
      "citing_paper_id": "280150778",
      "cited_paper_id": 247957917
    },
    {
      "context_text": "However, most studies focus primarily on generating plans rather than verifying them [6], [8], [11], or provide verification mechanisms with limited scope due to their reliance on simple templates [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about the focus of previous studies. No verifiable resources are identified.",
      "processing_time": 56.74326181411743,
      "citing_paper_id": "280150778",
      "cited_paper_id": 258991231
    },
    {
      "context_text": "However, most studies focus primarily on generating plans rather than verifying them [6], [8], [11], or provide verification mechanisms with limited scope due to their reliance on simple templates [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about the focus of previous studies. No verifiable resources are identified.",
      "processing_time": 56.74326181411743,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273374421
    },
    {
      "context_text": "Large language model (LLM)-based frameworks have shown promising results for robotic task planning [6]–[10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to LLM-based frameworks for robotic task planning. No verifiable resources are identified.",
      "processing_time": 57.170668840408325,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Large language model (LLM)-based frameworks have shown promising results for robotic task planning [6]–[10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to LLM-based frameworks for robotic task planning. No verifiable resources are identified.",
      "processing_time": 57.170668840408325,
      "citing_paper_id": "280150778",
      "cited_paper_id": null
    },
    {
      "context_text": "To understand the typical failure modes in language model-generated plans, we analyzed 71 instructions from the work [6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not mention any specific dataset names, only a count of instructions analyzed. The context suggests the instructions are part of the cited work but does not provide a specific dataset name.",
      "processing_time": 58.9527485370636,
      "citing_paper_id": "280150778",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "…based on PDDL [3] struggle with tasks requiring common sense reasoning, often over-looking essential preconditions (e.g., checking whether a container is empty before filling it [4]) and fail to consider action consequences(e.g., checking that the fridge is closed after interaction with it).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of PDDL-based systems in handling common sense reasoning tasks.",
      "processing_time": 56.29129600524902,
      "citing_paper_id": "280150778",
      "cited_paper_id": 248085271
    },
    {
      "context_text": "This combination leverages the contextual understanding and common sense reasoning capabilities of LLMs [16] to detect potential errors before execution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs. There are no clear identifiers for datasets in the context.",
      "processing_time": 57.02294921875,
      "citing_paper_id": "280150778",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Verifying robot action plans before execution remains a challenging task in robotics [1], [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only states a general challenge in robotics.",
      "processing_time": 56.0483283996582,
      "citing_paper_id": "280150778",
      "cited_paper_id": 256615643
    },
    {
      "context_text": "Verifying robot action plans before execution remains a challenging task in robotics [1], [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only states a general challenge in robotics.",
      "processing_time": 56.0483283996582,
      "citing_paper_id": "280150778",
      "cited_paper_id": 268512756
    },
    {
      "context_text": "The Verification Module (lines 5-23 ) Our VerifyLLM system would identify several issues with this plan: • Redundancy : Action 7 “pour tea” duplicates action 4, violating the non-redundancy principle • Missing prerequisite : There’s no action for adding a tea bag or tea leaves before pouring, which…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the functionality of the VerifyLLM system.",
      "processing_time": 55.330275774002075,
      "citing_paper_id": "280150778",
      "cited_paper_id": 257663442
    },
    {
      "context_text": "Approaches such as [23] and [21] focus on feasibility checking for generated plans but are limited to specific domains like motion planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only approaches and their limitations. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.054147243499756,
      "citing_paper_id": "280150778",
      "cited_paper_id": 257663442
    },
    {
      "context_text": "Approaches such as [23] and [21] focus on feasibility checking for generated plans but are limited to specific domains like motion planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only approaches and their limitations. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.054147243499756,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273098318
    },
    {
      "context_text": "Works like [22] attempt to verify plans by detecting and recovering from execution failures, showing improved success rates on small-scale tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to verifying plans using vision-language models.",
      "processing_time": 55.631638288497925,
      "citing_paper_id": "280150778",
      "cited_paper_id": 258187415
    },
    {
      "context_text": "Unlike approaches in [24] and [25], we focus specifically on robotic task execution safety with actionable plan corrections that maintain commonsense consistency.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a focus on robotic task execution safety with actionable plan corrections. No verifiable resources are identified.",
      "processing_time": 57.42700982093811,
      "citing_paper_id": "280150778",
      "cited_paper_id": 265157583
    },
    {
      "context_text": "Unlike approaches in [24] and [25], we focus specifically on robotic task execution safety with actionable plan corrections that maintain commonsense consistency.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a focus on robotic task execution safety with actionable plan corrections. No verifiable resources are identified.",
      "processing_time": 57.42700982093811,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273229503
    },
    {
      "context_text": "In [24], researchers introduced LLatrieval, a system where LLMs verify and iteratively improve retrieval results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LLatrieval, which is a system, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 56.22646164894104,
      "citing_paper_id": "280150778",
      "cited_paper_id": 265157583
    },
    {
      "context_text": "More specialized approaches have emerged, such as [26], which focuses on error recovery through continuous plan monitoring, and [12], which addresses common-sense knowledge integration for robotics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only approaches and methods. The titles of the cited papers do not introduce any datasets either.",
      "processing_time": 56.74846839904785,
      "citing_paper_id": "280150778",
      "cited_paper_id": 266899650
    },
    {
      "context_text": "More specialized approaches have emerged, such as [26], which focuses on error recovery through continuous plan monitoring, and [12], which addresses common-sense knowledge integration for robotics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only approaches and methods. The titles of the cited papers do not introduce any datasets either.",
      "processing_time": 56.74846839904785,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273374421
    },
    {
      "context_text": "In [25], researchers proposed collaborative verification combining Chain-of-Thought and Program-of-Thought methods, demonstrating how different solution representations improve verification reliability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings.",
      "processing_time": 53.72226548194885,
      "citing_paper_id": "280150778",
      "cited_paper_id": 273229503
    },
    {
      "context_text": "We employ the Adam optimizer [33] with weight decay and set the initial learning rate to 2e-5.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 55.82900047302246,
      "citing_paper_id": "279403157",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "The agent then goes through a reasoning process ( e . g ., chain-of-thought [31]) and selects a suitable tool to call.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (chain-of-thought prompting).",
      "processing_time": 54.964170694351196,
      "citing_paper_id": "279403157",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "For the online models, we compare our method with agents using ReAct [34], a technique that allows the LLM to reason and call tools in an agentic manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ReAct) used for comparison. The citation is focused on the method rather than a dataset.",
      "processing_time": 57.1995050907135,
      "citing_paper_id": "279403157",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "By processing natural language data streams, LLM agents interact with external environments, such as applications [1, 4], computing systems [3], via a set of pre-defined tools to carry out complex user tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general interactions of LLM agents with external environments. No verifiable resources are identified.",
      "processing_time": 56.64194178581238,
      "citing_paper_id": "279403157",
      "cited_paper_id": 260126067
    },
    {
      "context_text": "Large Language Models (LLMs), Large Language Models (LLMs), empowered by their exceptional planning and reasoning abilities, are increasingly integrated into agentic systems [1–3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 55.777440786361694,
      "citing_paper_id": "279403157",
      "cited_paper_id": 260126067
    },
    {
      "context_text": "Although datasets like ToolBench [30] have been collected to support tool-use reasoning in LLMs, their formats do not align well with the structure of our security policy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ToolBench"
      ],
      "dataset_descriptions": {
        "ToolBench": "Used to support tool-use reasoning in LLMs, but its format does not align with the structure required for the current security policy research."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'ToolBench' as a dataset, but indicates it is not suitable for the current research due to format issues.",
      "processing_time": 63.47683334350586,
      "citing_paper_id": "279403157",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "Moreover, we finetune our policy on DRIFT, with the dataset collected from ToolBench [30], achieving significant improvements in both security and utility.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ToolBench"
      ],
      "dataset_descriptions": {
        "ToolBench": "Used to finetune a policy on DRIFT, focusing on improving security and utility through real-world API interactions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'ToolBench' as a dataset used for finetuning a policy, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 63.94169497489929,
      "citing_paper_id": "279403157",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "This involves designing a new data collection pipeline that extracts policy-aligned samples from existing agent datasets, followed by efficient instruction tuning using Low-Rank Adaptation (LoRA) [32] on Qwen2.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'existing agent datasets' but does not specify any particular dataset name. The term 'Qwen2' is mentioned but appears to be a model or method, not a dataset.",
      "processing_time": 59.05758762359619,
      "citing_paper_id": "279403157",
      "cited_paper_id": 262825568
    },
    {
      "context_text": "This form of attack [5–9] may bring risks such as economic losses [6], privacy leakage [10], and system damage [11] to users, severely undermining the reliability of the agentic system.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only potential risks and attacks on agentic systems. No verifiable resources are identified.",
      "processing_time": 56.803595304489136,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268248325
    },
    {
      "context_text": "This form of attack [5–9] may bring risks such as economic losses [6], privacy leakage [10], and system damage [11] to users, severely undermining the reliability of the agentic system.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only potential risks and attacks on agentic systems. No verifiable resources are identified.",
      "processing_time": 56.803595304489136,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268296913
    },
    {
      "context_text": "This form of attack [5–9] may bring risks such as economic losses [6], privacy leakage [10], and system damage [11] to users, severely undermining the reliability of the agentic system.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only potential risks and attacks on agentic systems. No verifiable resources are identified.",
      "processing_time": 56.803595304489136,
      "citing_paper_id": "279403157",
      "cited_paper_id": 273821674
    },
    {
      "context_text": "Since the need for interaction with untrusted external environments, a new security threat of prompt injection attacks is introduced [5–9], where attackers inject malicious instructions into third-party platforms, misleading the agent workflow after external interaction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a security threat related to prompt injection attacks. No verifiable resources are identified.",
      "processing_time": 56.381423473358154,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268296913
    },
    {
      "context_text": "Recently, system-level [18–22] defenses have gained increasing attention, as they are able to overcome the inherent vulnerabilities of models when facing unseen attacks, thereby achieving high practical reliability in real-world applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only system-level defenses. No verifiable resources are identified.",
      "processing_time": 55.746177673339844,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268296997
    },
    {
      "context_text": "Recently, system-level [18–22] defenses have gained increasing attention, as they are able to overcome the inherent vulnerabilities of models when facing unseen attacks, thereby achieving high practical reliability in real-world applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only system-level defenses. No verifiable resources are identified.",
      "processing_time": 55.746177673339844,
      "citing_paper_id": "279403157",
      "cited_paper_id": 277824060
    },
    {
      "context_text": "For instance, IsolateGPT [18] mitigates information leakage risks by enforcing isolation mechanisms and maintaining a separate memory bank for each application.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called IsolateGPT. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.91709303855896,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268296997
    },
    {
      "context_text": "Existing defense mechanisms can be broadly categorized into model-level [12–17] and system-level [18–22] approaches.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of defense mechanisms. No verifiable resources are identified.",
      "processing_time": 55.5770468711853,
      "citing_paper_id": "279403157",
      "cited_paper_id": 268296997
    },
    {
      "context_text": "Existing defense mechanisms can be broadly categorized into model-level [12–17] and system-level [18–22] approaches.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only categories of defense mechanisms. No verifiable resources are identified.",
      "processing_time": 55.5770468711853,
      "citing_paper_id": "279403157",
      "cited_paper_id": 277824060
    },
    {
      "context_text": "5-7B [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It only mentions a range of parameters (5-7B) which likely refers to model size.",
      "processing_time": 57.87387275695801,
      "citing_paper_id": "279403157",
      "cited_paper_id": 274859421
    },
    {
      "context_text": "We evaluate our method with AgentDojo [24], a benchmark that simulates realistic interactions in agent-based systems.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "AgentDojo is mentioned as a benchmark, but it is excluded because it is primarily used for score comparison and does not refer to a specific, downloadable dataset.",
      "processing_time": 57.13192176818848,
      "citing_paper_id": "279403157",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, we evaluate DRIFT on the AgentDojo [24] benchmark, a simulated agent environment featuring various task scenarios and types of injection attacks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AgentDojo' as a benchmark, which is primarily used for evaluating models. However, since it is described as a 'simulated agent environment', it does not fit the criteria for a traditional dataset.",
      "processing_time": 60.238483905792236,
      "citing_paper_id": "279403157",
      "cited_paper_id": null
    },
    {
      "context_text": "In addition, a checklist for each function node in the trajectory, with detailed parameter requirement and value dependencies, is encoded in JSON schema format [23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a JSON schema format for encoding checklists. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.92394685745239,
      "citing_paper_id": "279403157",
      "cited_paper_id": null
    },
    {
      "context_text": "This design mimics human efficiency [23, 20]: experts solve problems faster not because they compute more, but because they remember better.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to human efficiency and problem-solving, which is a general concept.",
      "processing_time": 56.77522349357605,
      "citing_paper_id": "278886937",
      "cited_paper_id": 12508462
    },
    {
      "context_text": "Prompts can also guide the agent’s reasoning process [68, 76, 106, 110], as seen in paradigms like ReAct [109] and Reflexion [81].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or paradigms like ReAct and Reflexion. These are not datasets but rather approaches or frameworks.",
      "processing_time": 57.5094051361084,
      "citing_paper_id": "278886937",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Prompts can also guide the agent’s reasoning process [68, 76, 106, 110], as seen in paradigms like ReAct [109] and Reflexion [81].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or paradigms like ReAct and Reflexion. These are not datasets but rather approaches or frameworks.",
      "processing_time": 57.5094051361084,
      "citing_paper_id": "278886937",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Early systems like ChatDev [72] used fixed roles and steps, while later frameworks such as LangGraph [42] enable dynamic task decomposition and delegation [110, 83, 79, 113].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only systems and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.56461262702942,
      "citing_paper_id": "278886937",
      "cited_paper_id": 257833781
    },
    {
      "context_text": "Early systems like ChatDev [72] used fixed roles and steps, while later frameworks such as LangGraph [42] enable dynamic task decomposition and delegation [110, 83, 79, 113].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only systems and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.56461262702942,
      "citing_paper_id": "278886937",
      "cited_paper_id": 270257715
    },
    {
      "context_text": "Early systems like ChatDev [72] used fixed roles and steps, while later frameworks such as LangGraph [42] enable dynamic task decomposition and delegation [110, 83, 79, 113].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only systems and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.56461262702942,
      "citing_paper_id": "278886937",
      "cited_paper_id": null
    },
    {
      "context_text": "Despite this promise, the actual deployment of LLM agents in mass-market, production-level applications remains sparse compared to more established AI systems, such as recommender systems [48, 100, 49] or search engines [115, 21, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI systems and applications. No verifiable resources are identified.",
      "processing_time": 56.196797609329224,
      "citing_paper_id": "278886937",
      "cited_paper_id": 259129651
    },
    {
      "context_text": "Despite this promise, the actual deployment of LLM agents in mass-market, production-level applications remains sparse compared to more established AI systems, such as recommender systems [48, 100, 49] or search engines [115, 21, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI systems and applications. No verifiable resources are identified.",
      "processing_time": 56.196797609329224,
      "citing_paper_id": "278886937",
      "cited_paper_id": 259202547
    },
    {
      "context_text": "Despite this promise, the actual deployment of LLM agents in mass-market, production-level applications remains sparse compared to more established AI systems, such as recommender systems [48, 100, 49] or search engines [115, 21, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI systems and applications. No verifiable resources are identified.",
      "processing_time": 56.196797609329224,
      "citing_paper_id": "278886937",
      "cited_paper_id": 261065228
    },
    {
      "context_text": "Despite this promise, the actual deployment of LLM agents in mass-market, production-level applications remains sparse compared to more established AI systems, such as recommender systems [48, 100, 49] or search engines [115, 21, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to AI systems and applications. No verifiable resources are identified.",
      "processing_time": 56.196797609329224,
      "citing_paper_id": "278886937",
      "cited_paper_id": null
    },
    {
      "context_text": "Multi-agent systems like MetaGPT [30], AutoGen [98], and CrewAI [12] introduce agent collaboration [27, 117, 4], though each agent still typically follows a predefined workflow, often reflecting hierarchical structures such as supervisor-worker dynamics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only multi-agent systems and their workflows. No verifiable resources are identified.",
      "processing_time": 56.0625581741333,
      "citing_paper_id": "278886937",
      "cited_paper_id": 263611068
    },
    {
      "context_text": "ReST [5] refines ReAct agents for multi-step reasoning using an iterative RL-based self-improvement algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for refining agents. The context is focused on the methodology and improvements in multi-step reasoning.",
      "processing_time": 56.85282230377197,
      "citing_paper_id": "278886937",
      "cited_paper_id": 266335848
    },
    {
      "context_text": "This has sparked immense interest in deploying LLM agents across domains ranging from customer support to scientific research [92, 101, 77, 11, 19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the broad interest in deploying LLM agents.",
      "processing_time": 56.4191677570343,
      "citing_paper_id": "278886937",
      "cited_paper_id": 266844635
    },
    {
      "context_text": "…✓ ✗ Text ✗ - VisualWebArena [38] Web Browsing ✓ ✗ Text, Image ✗ - VideoWebArena [34] Web Browsing ✓ ✗ Text, Image, Video ✗ - OSWorld [103] Control ✓ ✗ Text, Image ✓ - WorkArena [18] Web Browsing ✓ ✗ Text, Image ✗ 10.0 InfoDeepSeek [99] Web Search ✓ ✗ Text ✓ 5.0 • Multi-step dependent tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several environments but does not specify them as datasets. They are likely environments or platforms used for testing or training agents.",
      "processing_time": 56.41593098640442,
      "citing_paper_id": "278886937",
      "cited_paper_id": 268363855
    },
    {
      "context_text": "This line of work uses predefined paths or pipelines to coordinate between LLMs and a set of tools, often involving deterministic planning, tool invocation, and human-in-the-loop decision making [43, 84].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.198143005371094,
      "citing_paper_id": "278886937",
      "cited_paper_id": 271092420
    },
    {
      "context_text": "CORY [59] and MARFT [45] extend these ideas into multi-agent settings, enabling RL-fine-tuned coordination between agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CORY' and 'MARFT' as extensions of ideas into multi-agent settings, but these are not datasets. They are methods or frameworks for reinforcement learning in multi-agent systems.",
      "processing_time": 58.64935922622681,
      "citing_paper_id": "278886937",
      "cited_paper_id": 273228480
    },
    {
      "context_text": "CORY [59] and MARFT [45] extend these ideas into multi-agent settings, enabling RL-fine-tuned coordination between agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CORY' and 'MARFT' as extensions of ideas into multi-agent settings, but these are not datasets. They are methods or frameworks for reinforcement learning in multi-agent systems.",
      "processing_time": 58.64935922622681,
      "citing_paper_id": "278886937",
      "cited_paper_id": 277999502
    },
    {
      "context_text": "WebRL [71] trains LLM web agents via self-evolving online curriculum reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WebRL) for training LLM web agents.",
      "processing_time": 55.4361674785614,
      "citing_paper_id": "278886937",
      "cited_paper_id": 273821797
    },
    {
      "context_text": "Test-time training allows model parameters to be updated at test-time using the incoming unlabeled test data [125, 6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for updating model parameters at test-time. No verifiable resources are identified.",
      "processing_time": 55.98095774650574,
      "citing_paper_id": "278886937",
      "cited_paper_id": 273970146
    },
    {
      "context_text": "Test-time training allows model parameters to be updated at test-time using the incoming unlabeled test data [125, 6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for updating model parameters at test-time. No verifiable resources are identified.",
      "processing_time": 55.98095774650574,
      "citing_paper_id": "278886937",
      "cited_paper_id": 277993666
    },
    {
      "context_text": "Recent study has revealed early signs of strategic deception [25], where models appear to selectively comply with its training objective in training, potentially to avoid post-deployment modifications to their behavior.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a phenomenon observed in models. No verifiable resources are identified.",
      "processing_time": 55.288047552108765,
      "citing_paper_id": "278886937",
      "cited_paper_id": 274823033
    },
    {
      "context_text": "Researchers also explore agents that can optimize their own workflows [73, 102, 63, 39], such as FlowAgent [80] and AFlow [114], allowing more autonomy in structuring their processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.24579882621765,
      "citing_paper_id": "278886937",
      "cited_paper_id": 275515996
    },
    {
      "context_text": "Researchers also explore agents that can optimize their own workflows [73, 102, 63, 39], such as FlowAgent [80] and AFlow [114], allowing more autonomy in structuring their processes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and tools. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.24579882621765,
      "citing_paper_id": "278886937",
      "cited_paper_id": 276482713
    },
    {
      "context_text": "This approach has been employed in recent models such as DeepSeek-R1 and the Qwen3 series [26, 105], and we believe it is also used in OpenAI’s models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the given citation span.",
      "processing_time": 56.09918165206909,
      "citing_paper_id": "278886937",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "This approach has been employed in recent models such as DeepSeek-R1 and the Qwen3 series [26, 105], and we believe it is also used in OpenAI’s models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the given citation span.",
      "processing_time": 56.09918165206909,
      "citing_paper_id": "278886937",
      "cited_paper_id": 278602855
    },
    {
      "context_text": "Post-training scaling refers to the enhancement of a base language model’s capabilities through techniques such as supervised fine-tuning (SFT) or reinforcement learning (RL) [26, 58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques such as supervised fine-tuning and reinforcement learning.",
      "processing_time": 54.92672920227051,
      "citing_paper_id": "278886937",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "Adaptively increasing the reasoning steps for complex tasks allows agents to produce more correct, complete, and trustworthy outputs [26], directly contributing to the information quality of Agentic ROI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to improving reasoning capabilities in LLMs.",
      "processing_time": 55.10682678222656,
      "citing_paper_id": "278886937",
      "cited_paper_id": 275789950
    },
    {
      "context_text": "Its success has inspired a wave of research focused on improving search agents via RL [85, 36, 10, 35, 121, 118].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to research on improving search agents via reinforcement learning.",
      "processing_time": 55.36705565452576,
      "citing_paper_id": "278886937",
      "cited_paper_id": 276742133
    },
    {
      "context_text": "Its success has inspired a wave of research focused on improving search agents via RL [85, 36, 10, 35, 121, 118].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to research on improving search agents via reinforcement learning.",
      "processing_time": 55.36705565452576,
      "citing_paper_id": "278886937",
      "cited_paper_id": 276884818
    },
    {
      "context_text": "For instance, in code generation tasks, agents have been shown to manipulate test cases or selectively bypass verifications to create the illusion of correctness [8].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general behavior of agents in code generation tasks.",
      "processing_time": 54.980767488479614,
      "citing_paper_id": "278886937",
      "cited_paper_id": 276937204
    },
    {
      "context_text": "RAGEN [96] and SWEET-RL [124] both design specialized multi-turn reinforcement learning algorithms specifically for agents that require multi-round interactions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SWEET-RL but does not refer to it as a dataset. It is described as a method for training multi-turn LLM agents.",
      "processing_time": 56.95724630355835,
      "citing_paper_id": "278886937",
      "cited_paper_id": 277113131
    },
    {
      "context_text": "Moreover, by integrating environmental interaction trajectories—such as tool-use logs or state-action transitions—agents can reduce hallucinations [74] and improve consistency between perception, reasoning, and action.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about reducing hallucinations in agents through environmental interaction trajectories.",
      "processing_time": 55.51064896583557,
      "citing_paper_id": "278886937",
      "cited_paper_id": 277634095
    },
    {
      "context_text": "In particular, the current generation of LLM agents focuses on specialized, professional tasks such as software development [97] and scientific research [24, 65], where the typical users are already domain experts and occasional errors are acceptable.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and user types. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.798190116882324,
      "citing_paper_id": "278886937",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, AGILE [22] constructs a complex conversational agent by RL tuning to internalize various agent abilities including memory management, tool use, planning, and self-reflection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AGILE) and its capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.7935357093811,
      "citing_paper_id": "278886937",
      "cited_paper_id": null
    },
    {
      "context_text": "Trust can be evaluated using Likert-scale user questionnaires [5, 19, 43] and broader instruments like the Propensity to Trust scale [21], which assesses general attitudes toward machines.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions the Propensity to Trust scale, which is a specific instrument for assessing trust attitudes. However, it does not qualify as a dataset according to the criteria.",
      "processing_time": 58.188159704208374,
      "citing_paper_id": "276647935",
      "cited_paper_id": 32417
    },
    {
      "context_text": "Trust can be evaluated using Likert-scale user questionnaires [5, 19, 43] and broader instruments like the Propensity to Trust scale [21], which assesses general attitudes toward machines.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions the Propensity to Trust scale, which is a specific instrument for assessing trust attitudes. However, it does not qualify as a dataset according to the criteria.",
      "processing_time": 58.188159704208374,
      "citing_paper_id": "276647935",
      "cited_paper_id": 27250165
    },
    {
      "context_text": "Specifically, LLMs possess unique capabilities and limitations compared to classical PDDL planners [8, 20] that may affect trust levels.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between LLMs and classical PDDL planners. No verifiable resources are identified.",
      "processing_time": 56.771939516067505,
      "citing_paper_id": "276647935",
      "cited_paper_id": 2730999
    },
    {
      "context_text": "Trust, defined as the willingness to rely on automated systems [16], is vital for the adoption of planning systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of trust in automation. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.42996025085449,
      "citing_paper_id": "276647935",
      "cited_paper_id": 5210390
    },
    {
      "context_text": "The propensity to trust scale consists of six questions used to assess an individual’s tendency to trust machines based on their current behaviors [21].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a scale, which is a method or tool, not a dataset.",
      "processing_time": 57.095349073410034,
      "citing_paper_id": "276647935",
      "cited_paper_id": 27250165
    },
    {
      "context_text": "We also measured participants’ propensity to trust at the end of each session using a six-item scale [21] to assess their general tendency to trust AI planners.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a six-item scale, which is a method or tool rather than a dataset.",
      "processing_time": 57.081385135650635,
      "citing_paper_id": "276647935",
      "cited_paper_id": 27250165
    },
    {
      "context_text": "These capabilities have been shown in other contexts to enhance user trust by making the planning process more transparent and interactive [14, 28].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and their effects on user trust. No verifiable resources are identified.",
      "processing_time": 56.437642335891724,
      "citing_paper_id": "276647935",
      "cited_paper_id": 85499781
    },
    {
      "context_text": "These capabilities have been shown in other contexts to enhance user trust by making the planning process more transparent and interactive [14, 28].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities and their effects on user trust. No verifiable resources are identified.",
      "processing_time": 56.437642335891724,
      "citing_paper_id": "276647935",
      "cited_paper_id": 140254691
    },
    {
      "context_text": "These factors amplify the importance of trust, as both over-trust and under-trust can introduce errors or inefficiencies in planning and can have cascading effects on task success [15, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the theoretical and practical considerations of trust in planning.",
      "processing_time": 56.460444688797,
      "citing_paper_id": "276647935",
      "cited_paper_id": 144787755
    },
    {
      "context_text": "We recruited 30 participants through Prolific [23] (fluent English speakers over the age of 18).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions recruiting participants through Prolific, which is a platform for finding participants, not a dataset. No specific dataset is mentioned.",
      "processing_time": 56.91989850997925,
      "citing_paper_id": "276647935",
      "cited_paper_id": 196127175
    },
    {
      "context_text": "In contrast, Large Language Models (LLMs) have recently demonstrated strong performance in various domains, including text generation [17], question answering [24, 25], and code completion [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of LLMs. No dataset names are present in the text.",
      "processing_time": 56.4738404750824,
      "citing_paper_id": "276647935",
      "cited_paper_id": 211258652
    },
    {
      "context_text": "For instance, LLMs can generate natural language explanations to clarify why specific decisions were made [13, 41] and iteratively refine their outputs based on user feedback [6, 22, 34, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 57.8476779460907,
      "citing_paper_id": "276647935",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "For instance, LLMs can generate natural language explanations to clarify why specific decisions were made [13, 41] and iteratively refine their outputs based on user feedback [6, 22, 34, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of LLMs. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 57.8476779460907,
      "citing_paper_id": "276647935",
      "cited_paper_id": 264555641
    },
    {
      "context_text": "These strengths have sparked growing interest in using LLMs as planners across diverse domains, including robotics [11, 26, 33, 45], healthcare [3, 27], and law [4, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains where LLMs are used as planners. No verifiable resources are identified.",
      "processing_time": 56.91861653327942,
      "citing_paper_id": "276647935",
      "cited_paper_id": 259342058
    },
    {
      "context_text": "These strengths have sparked growing interest in using LLMs as planners across diverse domains, including robotics [11, 26, 33, 45], healthcare [3, 27], and law [4, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains where LLMs are used as planners. No verifiable resources are identified.",
      "processing_time": 56.91861653327942,
      "citing_paper_id": "276647935",
      "cited_paper_id": 260775783
    },
    {
      "context_text": "These strengths have sparked growing interest in using LLMs as planners across diverse domains, including robotics [11, 26, 33, 45], healthcare [3, 27], and law [4, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains where LLMs are used as planners. No verifiable resources are identified.",
      "processing_time": 56.91861653327942,
      "citing_paper_id": "276647935",
      "cited_paper_id": null
    },
    {
      "context_text": "These strengths have sparked growing interest in using LLMs as planners across diverse domains, including robotics [11, 26, 33, 45], healthcare [3, 27], and law [4, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains where LLMs are used as planners. No verifiable resources are identified.",
      "processing_time": 56.91861653327942,
      "citing_paper_id": "276647935",
      "cited_paper_id": null
    },
    {
      "context_text": "However, LLMs also exhibit significant limitations, such as their inability to reliably generate or validate plans independently, even for relatively simple tasks [12, 32, 38, 39].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general limitations of LLMs in planning. No verifiable resources are identified.",
      "processing_time": 56.24043536186218,
      "citing_paper_id": "276647935",
      "cited_paper_id": 260440590
    },
    {
      "context_text": "However, LLMs also exhibit significant limitations, such as their inability to reliably generate or validate plans independently, even for relatively simple tasks [12, 32, 38, 39].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general limitations of LLMs in planning. No verifiable resources are identified.",
      "processing_time": 56.24043536186218,
      "citing_paper_id": "276647935",
      "cited_paper_id": null
    },
    {
      "context_text": "While prior research has explored factors influencing trust in LLM-based systems, such as anthropomorphic cues [7], the framing and presence of explanations [30], and user interface design [35], factors influencing human trust in LLMs in the context of planning tasks remain underexplored.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors influencing trust in LLM-based systems. No verifiable resources are identified.",
      "processing_time": 56.710915088653564,
      "citing_paper_id": "276647935",
      "cited_paper_id": 270226047
    },
    {
      "context_text": "While prior research has explored factors influencing trust in LLM-based systems, such as anthropomorphic cues [7], the framing and presence of explanations [30], and user interface design [35], factors influencing human trust in LLMs in the context of planning tasks remain underexplored.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors influencing trust in LLM-based systems. No verifiable resources are identified.",
      "processing_time": 56.710915088653564,
      "citing_paper_id": "276647935",
      "cited_paper_id": null
    },
    {
      "context_text": "We set this accuracy to ensure non-perfect but meaningful performance across two tasks per session, approximating the observed accuracy in practice [9, 46].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is excluded according to the instructions.",
      "processing_time": 55.224074602127075,
      "citing_paper_id": "276647935",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "Next, we convert the generated plan into natural language for user studies following the procedure in [29] and display it to users.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a procedural reference for converting plans into natural language.",
      "processing_time": 55.42541027069092,
      "citing_paper_id": "276647935",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing writing interfaces, typically designed around a single text editor layout, have been extensively studied [19, 77].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general studies on text editor layouts and usage. No verifiable resources are identified.",
      "processing_time": 56.730470418930054,
      "citing_paper_id": "276408841",
      "cited_paper_id": 7575616
    },
    {
      "context_text": "Existing writing interfaces, typically designed around a single text editor layout, have been extensively studied [19, 77].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general studies on text editor layouts and usage. No verifiable resources are identified.",
      "processing_time": 56.730470418930054,
      "citing_paper_id": "276408841",
      "cited_paper_id": 18688952
    },
    {
      "context_text": "As shown in Figure 1 Script&Shift reduces the gulf of envisioning [72] and execution [37] by ofering intuitive interactions such as inline friends, contextual toolbars, intelligent structuring, and other layer-based user interactions such as stacking and folding.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses interface interactions and cognitive challenges in using LLMs.",
      "processing_time": 55.68963360786438,
      "citing_paper_id": "276408841",
      "cited_paper_id": 16355120
    },
    {
      "context_text": "As shown in Figure 1 Script&Shift reduces the gulf of envisioning [72] and execution [37] by ofering intuitive interactions such as inline friends, contextual toolbars, intelligent structuring, and other layer-based user interactions such as stacking and folding.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses interface interactions and cognitive challenges in using LLMs.",
      "processing_time": 55.68963360786438,
      "citing_paper_id": "276408841",
      "cited_paper_id": 268533038
    },
    {
      "context_text": "However, the quality of writing tools can make or break this dynamic process of composition, largely depending on the afordances they provide – or fail to provide [41, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the impact of writing tools on the composition process.",
      "processing_time": 56.71372699737549,
      "citing_paper_id": "276408841",
      "cited_paper_id": 54767605
    },
    {
      "context_text": "However, the quality of writing tools can make or break this dynamic process of composition, largely depending on the afordances they provide – or fail to provide [41, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the impact of writing tools on the composition process.",
      "processing_time": 56.71372699737549,
      "citing_paper_id": "276408841",
      "cited_paper_id": 142669959
    },
    {
      "context_text": "Once they have written their thoughts, they determine that an argument style structure can be efective to organize the text, consisting of the following components: Claim, Grounds, Warrant, Backing, Qualifer, and Rebuttal [52, 73].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of an argument structure. No verifiable resources are identified.",
      "processing_time": 55.873273849487305,
      "citing_paper_id": "276408841",
      "cited_paper_id": 63614656
    },
    {
      "context_text": "Instead, it involves a complex interplay of cognitive activities spanning various levels of abstractions and perspectives [41, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to cognitive activities and perspectives, which are too generic to be considered as verifiable resources.",
      "processing_time": 57.96287727355957,
      "citing_paper_id": "276408841",
      "cited_paper_id": 142669959
    },
    {
      "context_text": "This dynamic interplay between knowledge creation and rhetorical problem-solving is what distinguishes good writing as a process of knowledge transformation rather than mere knowledge telling [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a concept in writing and composition, which is not a verifiable resource.",
      "processing_time": 57.72696876525879,
      "citing_paper_id": "276408841",
      "cited_paper_id": 143781031
    },
    {
      "context_text": "Advanced writing often transitions from the basic knowledge-telling model, which relies on cues from topics and discourse schemas, to a more complex, knowledge-transforming model [7], where coherence and rhetorical development become central.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a theoretical transition in writing models, which does not involve any verifiable resources.",
      "processing_time": 57.96687340736389,
      "citing_paper_id": "276408841",
      "cited_paper_id": 143781031
    },
    {
      "context_text": "As a result, writers often avoid engaging in iterative, exploratory, and divergent writing, even though exploratory writing [5, 20] can produce good writing [7].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to general findings about exploratory writing, which is not a verifiable resource.",
      "processing_time": 57.53491830825806,
      "citing_paper_id": "276408841",
      "cited_paper_id": 143781031
    },
    {
      "context_text": "Revision is a critical, recursive process in writing that bene-fts from supportive interfaces [23, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the importance of revision in writing and supportive interfaces.",
      "processing_time": 56.85458731651306,
      "citing_paper_id": "276408841",
      "cited_paper_id": 146578268
    },
    {
      "context_text": "Revision is a critical, recursive process in writing that bene-fts from supportive interfaces [23, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the importance of revision in writing and supportive interfaces.",
      "processing_time": 56.85458731651306,
      "citing_paper_id": "276408841",
      "cited_paper_id": 260697887
    },
    {
      "context_text": "For the rich-text editor, we integrated Lexical [22], which provides a hierarchical, node-based WYSIWYG editor with customizable listeners, allowing us to capture and process user inputs dynamically.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a tool (Lexical) used for integrating a rich-text editor. No datasets are referenced.",
      "processing_time": 56.8510582447052,
      "citing_paper_id": "276408841",
      "cited_paper_id": 153933608
    },
    {
      "context_text": "In exploring LLMs for writing [10, 17, 55, 64], substantial work has demonstrated various ways in which LLM co-writing interfaces can be designed [46].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers about LLMs and co-writing interfaces.",
      "processing_time": 55.744009494781494,
      "citing_paper_id": "276408841",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "However, general-purpose word processors often fail to address the specifc needs of professionals and creatives, who require more robust tools for maintaining consistency, managing dependencies, and organizing related information in structured documents [31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the limitations of general-purpose word processors and the need for more robust tools for professionals and creatives.",
      "processing_time": 58.36699175834656,
      "citing_paper_id": "276408841",
      "cited_paper_id": 221904092
    },
    {
      "context_text": "These tools force users to rely heavily on memory to manage tasks crucial to advanced writing, such as tracking coherence across sections and adapting content to evolving goals [31, 32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and their impact on managing tasks in advanced writing.",
      "processing_time": 55.49283242225647,
      "citing_paper_id": "276408841",
      "cited_paper_id": 221904092
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 231693275
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 239009871
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 239459629
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 247085270
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 250311134
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": 258833277
    },
    {
      "context_text": "Several LLM co-writing systems have emerged to bridge this gap [8, 12, 15, 16, 26–29, 44, 47, 50, 56, 69, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various LLM co-writing systems. No verifiable resources are identified.",
      "processing_time": 56.67160964012146,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "While some tools focus on specifc aspects of the writing process, such as brainstorming [13, 62], content transformation [3, 18], and feedback [36, 75], others ofer broader support across multiple writing tasks [3, 26, 42, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various tools and their functions in the writing process.",
      "processing_time": 55.94033885002136,
      "citing_paper_id": "276408841",
      "cited_paper_id": 239009871
    },
    {
      "context_text": "There are also opportunities to explore tools that provide constrained support for late-stage brainstorming, planning, and reviewing, particularly for specifc writing genres such as storytelling [25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tools and writing support. No clear identifiers for datasets are present.",
      "processing_time": 56.14584183692932,
      "citing_paper_id": "276408841",
      "cited_paper_id": 248780453
    },
    {
      "context_text": "Furthermore, users often desire greater control over the narrative and the AI’s output [38, 63], as LLM tools have been shown to infuence the writer’s opinions, raising concerns about the extent to which language models shape user thinking [39].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concerns about AI influence on user thinking.",
      "processing_time": 55.09927582740784,
      "citing_paper_id": "276408841",
      "cited_paper_id": 253420678
    },
    {
      "context_text": "Our work with Script&Shift builds on Zhang et al. [79], supporting visual representation for hierarchical planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called Script&Shift. The context is about visual representation for hierarchical planning, which is not directly related to a dataset.",
      "processing_time": 57.98226022720337,
      "citing_paper_id": "276408841",
      "cited_paper_id": 258179241
    },
    {
      "context_text": "In their analysis, VISAR [79] supports the Collage form factor best (Collage Factor=10).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool called VISAR. The context does not provide information about the usage of a dataset.",
      "processing_time": 56.282530784606934,
      "citing_paper_id": "276408841",
      "cited_paper_id": 258179241
    },
    {
      "context_text": "Additionally, designing AI systems that help writers understand the model’s capabilities, adapt to individual writing styles, and ofer authentic feedback will be vital [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the social dynamics and support aspects of AI in creative writing.",
      "processing_time": 56.8456768989563,
      "citing_paper_id": "276408841",
      "cited_paper_id": 258217017
    },
    {
      "context_text": "While chat-based LLM interfaces have become ubiquitous and accessible [2, 30, 57], there remains a signifcant gap in efectively prompting these systems for usable responses and guiding the iterative process of generating meaningful textual outputs [44, 78].",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the gap in effectively prompting LLMs and guiding the iterative process of generating textual outputs.",
      "processing_time": 58.45150709152222,
      "citing_paper_id": "276408841",
      "cited_paper_id": 258217984
    },
    {
      "context_text": "While chat-based LLM interfaces have become ubiquitous and accessible [2, 30, 57], there remains a signifcant gap in efectively prompting these systems for usable responses and guiding the iterative process of generating meaningful textual outputs [44, 78].",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the gap in effectively prompting LLMs and guiding the iterative process of generating textual outputs.",
      "processing_time": 58.45150709152222,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "While chat-based LLM interfaces have become ubiquitous and accessible [2, 30, 57], there remains a signifcant gap in efectively prompting these systems for usable responses and guiding the iterative process of generating meaningful textual outputs [44, 78].",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the gap in effectively prompting LLMs and guiding the iterative process of generating textual outputs.",
      "processing_time": 58.45150709152222,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "While chat-based LLM interfaces have become ubiquitous and accessible [2, 30, 57], there remains a signifcant gap in efectively prompting these systems for usable responses and guiding the iterative process of generating meaningful textual outputs [44, 78].",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the gap in effectively prompting LLMs and guiding the iterative process of generating textual outputs.",
      "processing_time": 58.45150709152222,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, while LLMs excel at generating content, they often fall short in creative tasks, which is why systems should focus on preserving the user’s creative control, as seen in our work with Script&Shift [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a system called Script&Shift. The context focuses on the limitations of LLMs in creative tasks and the importance of user creative control.",
      "processing_time": 59.322585344314575,
      "citing_paper_id": "276408841",
      "cited_paper_id": 262826094
    },
    {
      "context_text": "We wanted to understand the manners in which personas and layered afordances could bridge the envisioning gap [72] for writers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a cognitive challenge in interactions with LLMs.",
      "processing_time": 55.36501908302307,
      "citing_paper_id": "276408841",
      "cited_paper_id": 268533038
    },
    {
      "context_text": "These distinct personas, each representing diferent writing assistance features, efectively bridged the gulf of envisioning [72].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to cognitive challenges in interactions with LLMs.",
      "processing_time": 56.11335635185242,
      "citing_paper_id": "276408841",
      "cited_paper_id": 268533038
    },
    {
      "context_text": "The LLM component in our system leverages the Claude 3.5 Son-net [2] model from Anthropic, selected for its strong language comprehension, ability to handle complex writing tasks, and ef-ciency in generating high-quality creative outputs.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Claude 3.5 Son-net) which is excluded according to the rules.",
      "processing_time": 57.26468276977539,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "On the backend, we utilize Anthropic’s Claude 3.5 Sonnet model [2] for LLM functionality.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (Claude 3.5 Sonnet) rather than a dataset. No specific, verifiable dataset is referenced.",
      "processing_time": 56.884970903396606,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "While the current implementation relies on commercial LLM safety measures, integrating chain-of-thought reasoning could further reduce hallucination [40] and bias by ensuring more transparent and grounded outputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to reducing hallucination and bias in LLMs.",
      "processing_time": 56.32866835594177,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "To implement this framework in our interface, we built the application using React [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (React) used for building the application.",
      "processing_time": 55.63288450241089,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "Or they might use LLMs to receive feedback from diferent perspectives [6] as well as structure suggestions for their text.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a potential use case for LLMs.",
      "processing_time": 57.12126445770264,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "We also borrow the concept of LLM personas from Benharrak et al. [6] to improve human-centered writing.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept from another paper. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.9667432308197,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "The application is deployed on Heroku [66], and Firebase’s NoSQL cloud real-time database [49] manages the storage of user data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to deployment platforms and a database system.",
      "processing_time": 56.16456699371338,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "The application is deployed on Heroku [66], and Firebase’s NoSQL cloud real-time database [49] manages the storage of user data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to deployment platforms and a database system.",
      "processing_time": 56.16456699371338,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent discussions have also explored the implications of students using LLMs for academic writing, which may hinder the development of critical writing skills [9, 24, 53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the implications of students using LLMs for academic writing.",
      "processing_time": 57.56519436836243,
      "citing_paper_id": "276408841",
      "cited_paper_id": null
    },
    {
      "context_text": "6,7 Their mobility and navigational adaptability across diverse terrains present an opportunity to overcome location-based constraints.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the capabilities of flying machines in overcoming location-based constraints.",
      "processing_time": 57.16350531578064,
      "citing_paper_id": "277272723",
      "cited_paper_id": 9426935
    },
    {
      "context_text": "29,30 Current State Detector: To determine the position of a block on the pad in the pad coordinate system and also to know whether a block is stacked or placed behind another block, we perform the following steps:",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It describes a method for determining the position of blocks in a coordinate system.",
      "processing_time": 57.19313621520996,
      "citing_paper_id": "277272723",
      "cited_paper_id": 57825693
    },
    {
      "context_text": "27 Within each bounding box, the features of the top corner of the block are i represent the position of the i -th feature point at time t .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It describes a method for feature representation within bounding boxes.",
      "processing_time": 56.40400719642639,
      "citing_paper_id": "277272723",
      "cited_paper_id": 236586513
    },
    {
      "context_text": "8 Recognizing these limitations, we introduce a novel approach that reimagines drone-assisted manufacturing.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only introduces a novel approach without referencing any external resources.",
      "processing_time": 57.50425577163696,
      "citing_paper_id": "277272723",
      "cited_paper_id": 252409485
    },
    {
      "context_text": "23,24 The location of this coordinate in the world frame is computed as: Interpolate All Pad Points: The location of each pad coordinate (represented by the LLM as a pair of indices from (0,0) (bottom left) to (environment size, environment size) (top right) can be interpolated based on the known…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a method for interpolating pad coordinates in an environment using an LLM. No verifiable resources are identified.",
      "processing_time": 58.58461880683899,
      "citing_paper_id": "277272723",
      "cited_paper_id": 253499063
    },
    {
      "context_text": "25,26 The blocks are 3D printed with various colors to illustrate that our vision system is robust to alterations in lighting conditions and block appearance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for 3D printing blocks to test a vision system's robustness.",
      "processing_time": 58.379263162612915,
      "citing_paper_id": "277272723",
      "cited_paper_id": 260939243
    },
    {
      "context_text": "We use Lora finetuning [45] for InternVL2-4B [46] as the VLM intention planner and Qwen2.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Lora finetuning' and 'InternVL2-4B', but these are methods/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 58.826223373413086,
      "citing_paper_id": "276928119",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "We use Lora finetuning [45] for InternVL2-4B [46] as the VLM intention planner and Qwen2.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Lora finetuning' and 'InternVL2-4B', but these are methods/models, not datasets. No specific datasets are mentioned.",
      "processing_time": 58.826223373413086,
      "citing_paper_id": "276928119",
      "cited_paper_id": 266521410
    },
    {
      "context_text": "This collaborative information-sharing mechanism helps autonomous vehicles surmount the inherent limitations in single-vehicle driving, such as incomplete environmental perception [8–11] and uncertainty in forecasting the future states of surrounding traffic participants [12, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general challenges and limitations in autonomous vehicle technology.",
      "processing_time": 56.513941526412964,
      "citing_paper_id": "276928119",
      "cited_paper_id": 235899403
    },
    {
      "context_text": "This collaborative information-sharing mechanism helps autonomous vehicles surmount the inherent limitations in single-vehicle driving, such as incomplete environmental perception [8–11] and uncertainty in forecasting the future states of surrounding traffic participants [12, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general challenges and limitations in autonomous vehicle technology.",
      "processing_time": 56.513941526412964,
      "citing_paper_id": "276928119",
      "cited_paper_id": 267028230
    },
    {
      "context_text": "Unlike single-vehicle autonomous driving [1–5], where each vehicle makes driving decisions based solely on the observations from its own sensors, cooperative driving enables vehicles to exchange driving-related data [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts of autonomous driving and cooperative driving. No verifiable resources are identified.",
      "processing_time": 57.86801862716675,
      "citing_paper_id": "276928119",
      "cited_paper_id": 251135295
    },
    {
      "context_text": "Some methods, such as NEAT [31], TransFuser [2], UniAD [3], InterFuser [1], and ReasonNet [5], leverage transformer architectures to capture more nuanced representations of driving scenarios, enhancing the model’s ability to process complex environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.80980968475342,
      "citing_paper_id": "276928119",
      "cited_paper_id": 251135295
    },
    {
      "context_text": "To collect reasonable driving intention in different environments, we use V2Xverse [8] platform and employ an expert agent [1] to record driving data, capturing a wide range of urban scenarios.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'driving data' but does not specify a named dataset. The term 'V2Xverse' is mentioned, but it is described as a platform, not a dataset.",
      "processing_time": 59.71617126464844,
      "citing_paper_id": "276928119",
      "cited_paper_id": 251135295
    },
    {
      "context_text": "Specifically, the lateral controller is configured with [1, 0.2, 0.1, 5], while the longitudinal controller uses [5, 1, 0.1, 20].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters for controllers. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.17713379859924,
      "citing_paper_id": "276928119",
      "cited_paper_id": 251135295
    },
    {
      "context_text": "Optimization-based cooperative driving meth-ods [12, 14, 15] formulate multi-vehicle planning as constrained optimization problems to determine optimal actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for cooperative driving and multi-vehicle planning.",
      "processing_time": 56.938863039016724,
      "citing_paper_id": "276928119",
      "cited_paper_id": 255595818
    },
    {
      "context_text": "Optimization-based cooperative driving meth-ods [12, 14, 15] formulate multi-vehicle planning as constrained optimization problems to determine optimal actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for cooperative driving and multi-vehicle planning.",
      "processing_time": 56.938863039016724,
      "citing_paper_id": "276928119",
      "cited_paper_id": 267028230
    },
    {
      "context_text": "Optimization-based cooperative driving meth-ods [12, 14, 15] formulate multi-vehicle planning as constrained optimization problems to determine optimal actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for cooperative driving and multi-vehicle planning.",
      "processing_time": 56.938863039016724,
      "citing_paper_id": "276928119",
      "cited_paper_id": null
    },
    {
      "context_text": "While these approaches have been applied to several driving tasks [19–21], they struggle with declined performance when encountering unseen multi-vehicle interaction patterns [22, 23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general performance issues in deep reinforcement learning.",
      "processing_time": 55.92854309082031,
      "citing_paper_id": "276928119",
      "cited_paper_id": 255912658
    },
    {
      "context_text": "Second, with extensive pre-trained commonsense knowledge, LLMs have demonstrated strong capabilities in understanding traffic scenarios and making driving decisions [26–28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the capabilities of LLMs in understanding traffic scenarios and making driving decisions. No verifiable resources are named.",
      "processing_time": 58.48247790336609,
      "citing_paper_id": "276928119",
      "cited_paper_id": 263136146
    },
    {
      "context_text": "Second, with extensive pre-trained commonsense knowledge, LLMs have demonstrated strong capabilities in understanding traffic scenarios and making driving decisions [26–28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the capabilities of LLMs in understanding traffic scenarios and making driving decisions. No verifiable resources are named.",
      "processing_time": 58.48247790336609,
      "citing_paper_id": "276928119",
      "cited_paper_id": 266435584
    },
    {
      "context_text": "In the field of autonomous driving (AD), recent research [26, 35–37] has integrated LLMs into AD systems to improve interpretability and facilitate human-like interactions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of LLMs into AD systems. No verifiable resources are identified.",
      "processing_time": 57.594512939453125,
      "citing_paper_id": "276928119",
      "cited_paper_id": 263136146
    },
    {
      "context_text": "First, LLMs’ limited ability to understand and plan in continuous road spaces makes direct application infeasible [29], requiring additional spatial information for effective cooperation.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of LLMs in understanding and planning in continuous road spaces.",
      "processing_time": 57.65379524230957,
      "citing_paper_id": "276928119",
      "cited_paper_id": 263605637
    },
    {
      "context_text": "Some studies [28, 38–40] leverage VLMs to process multi-modal input data, providing both descriptive text and control signals suited to driving scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'multi-modal input data' but does not specify any named datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 58.100082874298096,
      "citing_paper_id": "276928119",
      "cited_paper_id": 266435584
    },
    {
      "context_text": "Some studies [28, 38–40] leverage VLMs to process multi-modal input data, providing both descriptive text and control signals suited to driving scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'multi-modal input data' but does not specify any named datasets. The cited papers do not provide additional clarity on specific datasets used.",
      "processing_time": 58.100082874298096,
      "citing_paper_id": "276928119",
      "cited_paper_id": 267750682
    },
    {
      "context_text": "First, language-based cooperation offers greater flexibility compared to fixed-protocol communication [12], as it can incorporate both local motion details and global scene semantics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between language-based cooperation and fixed-protocol communication.",
      "processing_time": 56.866535663604736,
      "citing_paper_id": "276928119",
      "cited_paper_id": 267028230
    },
    {
      "context_text": "AgentsCoDriver [36] promotes lifelong learning through interaction with the environment, enabling simple negotiations between agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called AgentsCoDriver. The context focuses on the capabilities of the system rather than the use of a dataset.",
      "processing_time": 58.921226501464844,
      "citing_paper_id": "276928119",
      "cited_paper_id": 269009845
    },
    {
      "context_text": "5-3B [47] as the LLM negotiator, for both accuracy and efficiency consideration.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model size and considerations for accuracy and efficiency.",
      "processing_time": 56.39672780036926,
      "citing_paper_id": "276928119",
      "cited_paper_id": 274859421
    },
    {
      "context_text": "InterDrive incorporates five metrics: Route Completion , In-fraction Score , and Driving Score , which are adopted from CARLA Leaderboard [43], along with a additional metrics: Success Rate Route Completion (RC) is the percentage of the total planned route distance completed by the under-test…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics adopted from CARLA Leaderboard, but does not refer to a specific, downloadable dataset. It is more about a benchmark or leaderboard, which is excluded.",
      "processing_time": 58.42308759689331,
      "citing_paper_id": "276928119",
      "cited_paper_id": null
    },
    {
      "context_text": "ZINC [92, 91] is another chemical database of compounds for virtual screening, including over one billion of molecules.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC"
      ],
      "dataset_descriptions": {
        "ZINC": "Used as a chemical database for virtual screening, containing over one billion molecules for ligand discovery and drug development."
      },
      "confidence_score": 1.0,
      "reasoning": "ZINC is identified as a chemical database, which fits the criteria for a dataset. The context mentions it includes over one billion molecules, making it a specific, verifiable resource.",
      "processing_time": 64.02035856246948,
      "citing_paper_id": "280010642",
      "cited_paper_id": 327319
    },
    {
      "context_text": "ZINC [92, 91] is another chemical database of compounds for virtual screening, including over one billion of molecules.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC"
      ],
      "dataset_descriptions": {
        "ZINC": "Used as a chemical database for virtual screening, containing over one billion molecules for ligand discovery and drug development."
      },
      "confidence_score": 1.0,
      "reasoning": "ZINC is identified as a chemical database, which fits the criteria for a dataset. The context mentions it includes over one billion molecules, making it a specific, verifiable resource.",
      "processing_time": 64.02035856246948,
      "citing_paper_id": "280010642",
      "cited_paper_id": 226059428
    },
    {
      "context_text": "Widely used datasets such as ZINC [91, 92] and ChEMBL [93, 94] include billions of molecules but typically provide only 2D representations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ZINC": "Used to provide 2D representations of billions of molecules for ligand discovery, focusing on the limitations of 2D data in chemical databases.",
        "ChEMBL": "Used to provide 2D representations of billions of molecules for drug discovery, focusing on the limitations of 2D data in chemical databases."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, ZINC and ChEMBL, which are widely used in ligand discovery and drug discovery. Both are clearly identified and have specific names.",
      "processing_time": 72.09556341171265,
      "citing_paper_id": "280010642",
      "cited_paper_id": 327319
    },
    {
      "context_text": "Widely used datasets such as ZINC [91, 92] and ChEMBL [93, 94] include billions of molecules but typically provide only 2D representations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ZINC": "Used to provide 2D representations of billions of molecules for ligand discovery, focusing on the limitations of 2D data in chemical databases.",
        "ChEMBL": "Used to provide 2D representations of billions of molecules for drug discovery, focusing on the limitations of 2D data in chemical databases."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, ZINC and ChEMBL, which are widely used in ligand discovery and drug discovery. Both are clearly identified and have specific names.",
      "processing_time": 72.09556341171265,
      "citing_paper_id": "280010642",
      "cited_paper_id": 6675645
    },
    {
      "context_text": "Widely used datasets such as ZINC [91, 92] and ChEMBL [93, 94] include billions of molecules but typically provide only 2D representations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ZINC": "Used to provide 2D representations of billions of molecules for ligand discovery, focusing on the limitations of 2D data in chemical databases.",
        "ChEMBL": "Used to provide 2D representations of billions of molecules for drug discovery, focusing on the limitations of 2D data in chemical databases."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, ZINC and ChEMBL, which are widely used in ligand discovery and drug discovery. Both are clearly identified and have specific names.",
      "processing_time": 72.09556341171265,
      "citing_paper_id": "280010642",
      "cited_paper_id": 226059428
    },
    {
      "context_text": "Widely used datasets such as ZINC [91, 92] and ChEMBL [93, 94] include billions of molecules but typically provide only 2D representations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ZINC": "Used to provide 2D representations of billions of molecules for ligand discovery, focusing on the limitations of 2D data in chemical databases.",
        "ChEMBL": "Used to provide 2D representations of billions of molecules for drug discovery, focusing on the limitations of 2D data in chemical databases."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, ZINC and ChEMBL, which are widely used in ligand discovery and drug discovery. Both are clearly identified and have specific names.",
      "processing_time": 72.09556341171265,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265041119
    },
    {
      "context_text": "…DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular library SMILES, conformers > 1B T3, T4 ChEMBL [93, 94] Bioactive molecules Molecules ∼ 2.2M T3, T4",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Guacamol",
        "Moses",
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "Guacamol": "Used as a benchmark for molecular design, evaluating the performance of generative models in creating novel molecules with desired properties.",
        "Moses": "Used as a benchmark for molecular design, assessing the ability of models to generate diverse and realistic molecular structures.",
        "ZINC": "Used as a molecular library for ligand discovery, providing a vast collection of small molecules for virtual screening and drug discovery.",
        "ChEMBL": "Used as a database of bioactive molecules, offering a rich source of chemical structures and associated biological activities for drug discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several molecular datasets, which are clearly identified and used for benchmarking and molecular design. These datasets are specific and have clear identifiers.",
      "processing_time": 80.64675211906433,
      "citing_paper_id": "280010642",
      "cited_paper_id": 327319
    },
    {
      "context_text": "…DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular library SMILES, conformers > 1B T3, T4 ChEMBL [93, 94] Bioactive molecules Molecules ∼ 2.2M T3, T4",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Guacamol",
        "Moses",
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "Guacamol": "Used as a benchmark for molecular design, evaluating the performance of generative models in creating novel molecules with desired properties.",
        "Moses": "Used as a benchmark for molecular design, assessing the ability of models to generate diverse and realistic molecular structures.",
        "ZINC": "Used as a molecular library for ligand discovery, providing a vast collection of small molecules for virtual screening and drug discovery.",
        "ChEMBL": "Used as a database of bioactive molecules, offering a rich source of chemical structures and associated biological activities for drug discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several molecular datasets, which are clearly identified and used for benchmarking and molecular design. These datasets are specific and have clear identifiers.",
      "processing_time": 80.64675211906433,
      "citing_paper_id": "280010642",
      "cited_paper_id": 6675645
    },
    {
      "context_text": "…DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular library SMILES, conformers > 1B T3, T4 ChEMBL [93, 94] Bioactive molecules Molecules ∼ 2.2M T3, T4",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Guacamol",
        "Moses",
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "Guacamol": "Used as a benchmark for molecular design, evaluating the performance of generative models in creating novel molecules with desired properties.",
        "Moses": "Used as a benchmark for molecular design, assessing the ability of models to generate diverse and realistic molecular structures.",
        "ZINC": "Used as a molecular library for ligand discovery, providing a vast collection of small molecules for virtual screening and drug discovery.",
        "ChEMBL": "Used as a database of bioactive molecules, offering a rich source of chemical structures and associated biological activities for drug discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several molecular datasets, which are clearly identified and used for benchmarking and molecular design. These datasets are specific and have clear identifiers.",
      "processing_time": 80.64675211906433,
      "citing_paper_id": "280010642",
      "cited_paper_id": 226059428
    },
    {
      "context_text": "…DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular library SMILES, conformers > 1B T3, T4 ChEMBL [93, 94] Bioactive molecules Molecules ∼ 2.2M T3, T4",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Guacamol",
        "Moses",
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "Guacamol": "Used as a benchmark for molecular design, evaluating the performance of generative models in creating novel molecules with desired properties.",
        "Moses": "Used as a benchmark for molecular design, assessing the ability of models to generate diverse and realistic molecular structures.",
        "ZINC": "Used as a molecular library for ligand discovery, providing a vast collection of small molecules for virtual screening and drug discovery.",
        "ChEMBL": "Used as a database of bioactive molecules, offering a rich source of chemical structures and associated biological activities for drug discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several molecular datasets, which are clearly identified and used for benchmarking and molecular design. These datasets are specific and have clear identifiers.",
      "processing_time": 80.64675211906433,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265041119
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 1553193
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 3922816
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 173188048
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 208290939
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 252383606
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266448819
    },
    {
      "context_text": "…are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and ScienceQA [172].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ",
        "ScienceQA"
      ],
      "dataset_descriptions": {
        "MathQA": "Used to solve math word problems with operation-based formalisms, focusing on interpretable solutions and reasoning processes.",
        "MatbookQA": "Used to answer questions related to mathematical concepts and problems, enhancing the model's ability to handle diverse math topics.",
        "MaScQA": "Used to address science-related questions, focusing on reasoning and comprehension in various scientific domains.",
        "ARC": "Used to evaluate and improve the model's reasoning capabilities on challenging science questions, emphasizing complex problem-solving.",
        "PIQA": "Used to reason about physical commonsense in natural language, enhancing the model's ability to understand and explain physical phenomena.",
        "SciQ": "Used to answer science questions from a variety of subjects, improving the model's general science knowledge and reasoning skills.",
        "ScienceQA": "Used to address a wide range of science questions, focusing on comprehensive understanding and reasoning across multiple scientific disciplines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used for question answering, particularly in science and mathematics. These datasets are clearly identified and used to support IFT (Instruction Fine-Tuning) in the research.",
      "processing_time": 96.16094946861267,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "SchNet [95] is one of the earliest invariant GNN models designed for simulating quantum interactions in molecules using continuous-filter convolutional layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SchNet as a model, not a dataset. There are no specific, verifiable datasets mentioned in the context.",
      "processing_time": 57.02545690536499,
      "citing_paper_id": "280010642",
      "cited_paper_id": 4897444
    },
    {
      "context_text": "1-8B [78], and Mistral-7B [79]) for a range of different chemical questions, such as predicting properties of monomers from SMILES (Simplified Molecular Input Line Entry System) [80], and benchmark their performance against traditional binary classification models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and their performance on chemical questions, but no dataset names are provided.",
      "processing_time": 57.51817178726196,
      "citing_paper_id": "280010642",
      "cited_paper_id": 5445756
    },
    {
      "context_text": "1-8B [78], and Mistral-7B [79]) for a range of different chemical questions, such as predicting properties of monomers from SMILES (Simplified Molecular Input Line Entry System) [80], and benchmark their performance against traditional binary classification models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and their performance on chemical questions, but no dataset names are provided.",
      "processing_time": 57.51817178726196,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271571434
    },
    {
      "context_text": "ChEMBL [93, 94] is a database for drug discovery, including bioactive molecules and drug-like properties.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ChEMBL": "ChEMBL is used as a database for drug discovery, providing bioactive molecules and drug-like properties, though not directly related to LLM planning capabilities."
      },
      "confidence_score": 0.6,
      "reasoning": "ChEMBL is mentioned as a database for drug discovery, which is not directly related to the planning capabilities of LLMs. However, it is a specific, verifiable resource.",
      "processing_time": 66.4256591796875,
      "citing_paper_id": "280010642",
      "cited_paper_id": 6675645
    },
    {
      "context_text": "ChEMBL [93, 94] is a database for drug discovery, including bioactive molecules and drug-like properties.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "ChEMBL": "ChEMBL is used as a database for drug discovery, providing bioactive molecules and drug-like properties, though not directly related to LLM planning capabilities."
      },
      "confidence_score": 0.6,
      "reasoning": "ChEMBL is mentioned as a database for drug discovery, which is not directly related to the planning capabilities of LLMs. However, it is a specific, verifiable resource.",
      "processing_time": 66.4256591796875,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265041119
    },
    {
      "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RedPajama",
        "BookCorus",
        "Wikitext",
        "Common Crawl"
      ],
      "dataset_descriptions": {
        "RedPajama": "Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset.",
        "BookCorus": "Used for pretraining LLMs with textual content from books, enhancing language understanding and generation.",
        "Wikitext": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text.",
        "Common Crawl": "Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 79.53242254257202,
      "citing_paper_id": "280010642",
      "cited_paper_id": 6866988
    },
    {
      "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RedPajama",
        "BookCorus",
        "Wikitext",
        "Common Crawl"
      ],
      "dataset_descriptions": {
        "RedPajama": "Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset.",
        "BookCorus": "Used for pretraining LLMs with textual content from books, enhancing language understanding and generation.",
        "Wikitext": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text.",
        "Common Crawl": "Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 79.53242254257202,
      "citing_paper_id": "280010642",
      "cited_paper_id": 16299141
    },
    {
      "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RedPajama",
        "BookCorus",
        "Wikitext",
        "Common Crawl"
      ],
      "dataset_descriptions": {
        "RedPajama": "Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset.",
        "BookCorus": "Used for pretraining LLMs with textual content from books, enhancing language understanding and generation.",
        "Wikitext": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text.",
        "Common Crawl": "Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 79.53242254257202,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274140947
    },
    {
      "context_text": "For training LLMs with general knowledge, RedPajama [160], BookCorpus [161], Wikitext [162] and Common Crawl [163] are common open-source datasets including billions of entries for LLM pretraining.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "RedPajama",
        "BookCorus",
        "Wikitext",
        "Common Crawl"
      ],
      "dataset_descriptions": {
        "RedPajama": "Used for pretraining LLMs with general knowledge, providing a large-scale, open-source dataset.",
        "BookCorus": "Used for pretraining LLMs with textual content from books, enhancing language understanding and generation.",
        "Wikitext": "Used for pretraining LLMs with Wikipedia articles, improving factual knowledge and coherence in generated text.",
        "Common Crawl": "Used for pretraining LLMs with web-scale data, enriching the model's exposure to diverse content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used for training LLMs, which are relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 79.53242254257202,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "…forces Graphs, 3D coords ∼ 1.3M T2, T3, T5 Alexandria [151] 1D, 2D, 3D materials Molecular structures, DFT calculations ∼ 5M T2, T3, T4 QM9 [97, 98] Molecules, DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9",
        "Guacamol"
      ],
      "dataset_descriptions": {
        "QM9": "Used to study molecular structures and DFT calculations, focusing on 134k molecules with SMILES and 3D structures for tasks T2, T3.",
        "Guacamol": "Applied as a benchmark for molecular design, evaluating models on a dataset of approximately 1.5M molecules for task T4."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets, including QM9 and Guacamol, which are specific and verifiable. However, 'Alexandria' and 'Moses' are not clearly identified as datasets in the context.",
      "processing_time": 74.08988428115845,
      "citing_paper_id": "280010642",
      "cited_paper_id": 12358271
    },
    {
      "context_text": "…forces Graphs, 3D coords ∼ 1.3M T2, T3, T5 Alexandria [151] 1D, 2D, 3D materials Molecular structures, DFT calculations ∼ 5M T2, T3, T4 QM9 [97, 98] Molecules, DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9",
        "Guacamol"
      ],
      "dataset_descriptions": {
        "QM9": "Used to study molecular structures and DFT calculations, focusing on 134k molecules with SMILES and 3D structures for tasks T2, T3.",
        "Guacamol": "Applied as a benchmark for molecular design, evaluating models on a dataset of approximately 1.5M molecules for task T4."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets, including QM9 and Guacamol, which are specific and verifiable. However, 'Alexandria' and 'Moses' are not clearly identified as datasets in the context.",
      "processing_time": 74.08988428115845,
      "citing_paper_id": "280010642",
      "cited_paper_id": 15367821
    },
    {
      "context_text": "QM9 [97, 98] includes over 100,000 small organic molecules with DFT-calculated quantum chemical properties.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9"
      ],
      "dataset_descriptions": {
        "QM9": "Used to study DFT-calculated quantum chemical properties of over 100,000 small organic molecules, focusing on molecular structure and property relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions QM9, which is a specific dataset containing small organic molecules with DFT-calculated properties. The dataset is clearly identified and used for its chemical properties.",
      "processing_time": 66.02794361114502,
      "citing_paper_id": "280010642",
      "cited_paper_id": 12358271
    },
    {
      "context_text": "QM9 [97, 98] includes over 100,000 small organic molecules with DFT-calculated quantum chemical properties.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9"
      ],
      "dataset_descriptions": {
        "QM9": "Used to study DFT-calculated quantum chemical properties of over 100,000 small organic molecules, focusing on molecular structure and property relationships."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions QM9, which is a specific dataset containing small organic molecules with DFT-calculated properties. The dataset is clearly identified and used for its chemical properties.",
      "processing_time": 66.02794361114502,
      "citing_paper_id": "280010642",
      "cited_paper_id": 15367821
    },
    {
      "context_text": "MEGNet [58] is another GNN-based model and is trained on crystal structures from the Materials Project [96] or QM9 [97, 98] molecules, aiming to predict the formation energies, band gaps, or elastic moduli of crystals.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project",
        "QM9"
      ],
      "dataset_descriptions": {
        "Materials Project": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using crystal structures.",
        "QM9": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using molecular structures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Materials Project' and 'QM9', which are used to train the MEGNet model for predicting properties of crystals.",
      "processing_time": 70.06735610961914,
      "citing_paper_id": "280010642",
      "cited_paper_id": 12358271
    },
    {
      "context_text": "MEGNet [58] is another GNN-based model and is trained on crystal structures from the Materials Project [96] or QM9 [97, 98] molecules, aiming to predict the formation energies, band gaps, or elastic moduli of crystals.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project",
        "QM9"
      ],
      "dataset_descriptions": {
        "Materials Project": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using crystal structures.",
        "QM9": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using molecular structures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Materials Project' and 'QM9', which are used to train the MEGNet model for predicting properties of crystals.",
      "processing_time": 70.06735610961914,
      "citing_paper_id": "280010642",
      "cited_paper_id": 15367821
    },
    {
      "context_text": "MEGNet [58] is another GNN-based model and is trained on crystal structures from the Materials Project [96] or QM9 [97, 98] molecules, aiming to predict the formation energies, band gaps, or elastic moduli of crystals.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project",
        "QM9"
      ],
      "dataset_descriptions": {
        "Materials Project": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using crystal structures.",
        "QM9": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using molecular structures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Materials Project' and 'QM9', which are used to train the MEGNet model for predicting properties of crystals.",
      "processing_time": 70.06735610961914,
      "citing_paper_id": "280010642",
      "cited_paper_id": 94929253
    },
    {
      "context_text": "MEGNet [58] is another GNN-based model and is trained on crystal structures from the Materials Project [96] or QM9 [97, 98] molecules, aiming to predict the formation energies, band gaps, or elastic moduli of crystals.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project",
        "QM9"
      ],
      "dataset_descriptions": {
        "Materials Project": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using crystal structures.",
        "QM9": "Used to train MEGNet for predicting formation energies, band gaps, and elastic moduli of crystals using molecular structures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'Materials Project' and 'QM9', which are used to train the MEGNet model for predicting properties of crystals.",
      "processing_time": 70.06735610961914,
      "citing_paper_id": "280010642",
      "cited_paper_id": 119413748
    },
    {
      "context_text": "In 2017, Vaswani et al. [44] first introduced the Transformer architecture based entirely on attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the introduction of the Transformer architecture but does not refer to any specific dataset. The citation is about the method, not a dataset.",
      "processing_time": 58.13780426979065,
      "citing_paper_id": "280010642",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "CGCNN [57] is designed for handling crystal structures and it can capture long-range interactions and global geometric structures.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (CGCNN) for handling crystal structures. The context focuses on the capabilities of the method rather than the use of a dataset.",
      "processing_time": 59.236966133117676,
      "citing_paper_id": "280010642",
      "cited_paper_id": 13838309
    },
    {
      "context_text": "These include FireWorks [176], Custodian [174], Atomate [183], and Jobflow [184], which facilitate tasks such as job writing, execution, automation, management, and result analysis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and libraries (FireWorks, Custodian, Atomate, Jobflow) but does not refer to any specific datasets. These are software tools or methods, not datasets.",
      "processing_time": 59.29785513877869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 21722946
    },
    {
      "context_text": "These include FireWorks [176], Custodian [174], Atomate [183], and Jobflow [184], which facilitate tasks such as job writing, execution, automation, management, and result analysis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and libraries (FireWorks, Custodian, Atomate, Jobflow) but does not refer to any specific datasets. These are software tools or methods, not datasets.",
      "processing_time": 59.29785513877869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 40344783
    },
    {
      "context_text": "These include FireWorks [176], Custodian [174], Atomate [183], and Jobflow [184], which facilitate tasks such as job writing, execution, automation, management, and result analysis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and libraries (FireWorks, Custodian, Atomate, Jobflow) but does not refer to any specific datasets. These are software tools or methods, not datasets.",
      "processing_time": 59.29785513877869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 51903808
    },
    {
      "context_text": "These include FireWorks [176], Custodian [174], Atomate [183], and Jobflow [184], which facilitate tasks such as job writing, execution, automation, management, and result analysis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions tools and libraries (FireWorks, Custodian, Atomate, Jobflow) but does not refer to any specific datasets. These are software tools or methods, not datasets.",
      "processing_time": 59.29785513877869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266862195
    },
    {
      "context_text": "ANI [85] utilizes Behler and Parrinello’s symmetry functions [86] and neural network potentials to extract molecular representations, which are then utilized to predict molecular energy.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 57.65998125076294,
      "citing_paper_id": "280010642",
      "cited_paper_id": 24604537
    },
    {
      "context_text": "ANI [85] utilizes Behler and Parrinello’s symmetry functions [86] and neural network potentials to extract molecular representations, which are then utilized to predict molecular energy.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 57.65998125076294,
      "citing_paper_id": "280010642",
      "cited_paper_id": 37065565
    },
    {
      "context_text": "The evaluations on datasets such as ChEMBL and ESOL [124] demonstrate state-of-the-art results in both sequence-to-sequence synthesis tasks and discriminative tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ChEMBL",
        "ESOL"
      ],
      "dataset_descriptions": {
        "ChEMBL": "Used to evaluate sequence-to-sequence synthesis tasks and discriminative tasks, demonstrating state-of-the-art results in chemical property prediction.",
        "ESOL": "Used to evaluate the estimation of aqueous solubility directly from molecular structure, demonstrating state-of-the-art results in chemical property prediction."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'ChEMBL' and 'ESOL' as datasets used for evaluation. ESOL is confirmed as a dataset through the cited paper title.",
      "processing_time": 71.16351318359375,
      "citing_paper_id": "280010642",
      "cited_paper_id": 29799633
    },
    {
      "context_text": "Pymatgen (Python Materials Genomics) [174] is an open-source Python library for processing, analyzing, and visualizing crystal structures, phase diagrams, and material properties in different formats (e.g., VASP, ABINIT, CIF, XYZ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context describes Pymatgen as a library for materials analysis, which is a method or tool, not a dataset. No specific datasets are mentioned.",
      "processing_time": 58.29998540878296,
      "citing_paper_id": "280010642",
      "cited_paper_id": 40344783
    },
    {
      "context_text": "GPT (Generative Pretrained Transformer) [45] was introduced in 2018 as a decoder-only, left-to-right unidirectional language model to predict the next word in a sequence based on previous words, without an encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the introduction of GPT as a language model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.11950135231018,
      "citing_paper_id": "280010642",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Inspired by the transformative impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their impact. No dataset names are present in the citation span.",
      "processing_time": 57.3499219417572,
      "citing_paper_id": "280010642",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Inspired by the transformative impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their impact. No dataset names are present in the citation span.",
      "processing_time": 57.3499219417572,
      "citing_paper_id": "280010642",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Inspired by the transformative impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their impact. No dataset names are present in the citation span.",
      "processing_time": 57.3499219417572,
      "citing_paper_id": "280010642",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Inspired by the transformative impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their impact. No dataset names are present in the citation span.",
      "processing_time": 57.3499219417572,
      "citing_paper_id": "280010642",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "Inspired by the transformative impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their impact. No dataset names are present in the citation span.",
      "processing_time": 57.3499219417572,
      "citing_paper_id": "280010642",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "BERT (Bidirectional Encoder Representations from Transformers) [7] was introduced in 2018 and utilizes only the encoder component.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. It is described as a model or method, which is excluded according to the instructions.",
      "processing_time": 58.45297026634216,
      "citing_paper_id": "280010642",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Molecular Transformer [122] first applies a Transformer for synthesis prediction, in particular, translating reactants and reagents into the final product.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Molecular Transformer) and its application. No verifiable datasets are referenced.",
      "processing_time": 57.63066244125366,
      "citing_paper_id": "280010642",
      "cited_paper_id": 53232749
    },
    {
      "context_text": "…1D, 2D, 3D materials Molecular structures, DFT calculations ∼ 5M T2, T3, T4 QM9 [97, 98] Molecules, DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9",
        "Guacamol",
        "ZINC"
      ],
      "dataset_descriptions": {
        "QM9": "Used for benchmarking models with molecular structures and DFT calculations, focusing on 1D, 2D, and 3D materials with 134k molecules.",
        "Guacamol": "Benchmark for molecular design, used to evaluate models on molecular generation tasks with approximately 1.5 million molecules.",
        "ZINC": "Benchmark for molecular design, used to train and evaluate models on a large dataset of 4.5 million molecules."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets, including QM9, Guacamol, and ZINC, which are all relevant to molecular design and computational chemistry. These datasets are used for benchmarking and training models.",
      "processing_time": 79.46938037872314,
      "citing_paper_id": "280010642",
      "cited_paper_id": 53787096
    },
    {
      "context_text": "To support molecular design, Guacamol [152] and Moses [153] are benchmarks for evaluating models trained for molecular generation task.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Guacamol and Moses as benchmarks for evaluating models trained for molecular generation tasks. However, these are excluded as they are benchmark suites primarily used for score comparison rather than specific, downloadable datasets.",
      "processing_time": 59.78847813606262,
      "citing_paper_id": "280010642",
      "cited_paper_id": 53787096
    },
    {
      "context_text": "To support molecular design, Guacamol [152] and Moses [153] are benchmarks for evaluating models trained for molecular generation task.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Guacamol and Moses as benchmarks for evaluating models trained for molecular generation tasks. However, these are excluded as they are benchmark suites primarily used for score comparison rather than specific, downloadable datasets.",
      "processing_time": 59.78847813606262,
      "citing_paper_id": "280010642",
      "cited_paper_id": 54434517
    },
    {
      "context_text": "…5M T2, T3, T4 QM9 [97, 98] Molecules, DFT SMILES, 3D structures 134k T2, T3 Guacamol [152] Benchmark for molecular design Molecules ∼ 1.5M T4 Moses [153] Benchmark for molecular design Molecules ∼ 4.5M T4 ZINC [91, 92] Molecular library SMILES, conformers > 1B T3, T4 ChEMBL [93, 94] Bioactive…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QM9",
        "Guacamol",
        "Moses",
        "ZINC",
        "ChEMBL"
      ],
      "dataset_descriptions": {
        "QM9": "Used to train and evaluate molecular generation models, focusing on small organic molecules with up to 9 heavy atoms, including 3D structures and DFT properties.",
        "Guacamol": "Benchmark for evaluating molecular generation models, focusing on generating molecules with desired properties and similarity to existing drugs.",
        "Moses": "Benchmark for molecular design, used to evaluate the performance of molecular generation models on tasks such as de novo molecule generation and property prediction.",
        "ZINC": "Large molecular library used for virtual screening and molecular design, containing over 1 billion molecules with SMILES strings and conformers.",
        "ChEMBL": "Database of bioactive molecules with drug-like properties, used for training and evaluating molecular generation models in the context of drug discovery."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several molecular datasets and benchmarks, which are specific and verifiable resources. These datasets are used for molecular design and generation, which is relevant to the planning capabilities of LLMs in the context of molecular chemistry.",
      "processing_time": 91.85670328140259,
      "citing_paper_id": "280010642",
      "cited_paper_id": 54434517
    },
    {
      "context_text": ") [96] Structure, property Structured text, graphs ∼ 200k T2, T3, T5, T6 OQMD [145, 146] Inorganic crystals Structure, DFT props ∼ 1M T2, T3 NOMAD [147] Hypothetical crystals Crystal structures, props ∼ 214k T3, T4 OMat24 [148] Inorganic materials Crystal structures, props ∼ 118M T3, T4 SNUMAT…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OQMD",
        "NOMAD",
        "OMat24",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "OQMD": "Used to store and analyze inorganic crystal structures and properties, specifically for high-throughput density functional theory calculations.",
        "NOMAD": "Utilized for sharing and processing data on hypothetical crystals, including crystal structures and properties, supporting materials design and discovery.",
        "OMat24": "Employed to manage and analyze inorganic materials data, including crystal structures and properties, for materials science research.",
        "SNUMAT": "Used to store and analyze inorganic materials data, including crystal structures and properties, for materials science research."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions several databases with specific names and properties, which are likely to be reusable datasets. However, they are not directly related to the planning capabilities of LLMs, so the confidence is lowered.",
      "processing_time": 83.93738460540771,
      "citing_paper_id": "280010642",
      "cited_paper_id": 94929253
    },
    {
      "context_text": ") [96] Structure, property Structured text, graphs ∼ 200k T2, T3, T5, T6 OQMD [145, 146] Inorganic crystals Structure, DFT props ∼ 1M T2, T3 NOMAD [147] Hypothetical crystals Crystal structures, props ∼ 214k T3, T4 OMat24 [148] Inorganic materials Crystal structures, props ∼ 118M T3, T4 SNUMAT…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OQMD",
        "NOMAD",
        "OMat24",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "OQMD": "Used to store and analyze inorganic crystal structures and properties, specifically for high-throughput density functional theory calculations.",
        "NOMAD": "Utilized for sharing and processing data on hypothetical crystals, including crystal structures and properties, supporting materials design and discovery.",
        "OMat24": "Employed to manage and analyze inorganic materials data, including crystal structures and properties, for materials science research.",
        "SNUMAT": "Used to store and analyze inorganic materials data, including crystal structures and properties, for materials science research."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions several databases with specific names and properties, which are likely to be reusable datasets. However, they are not directly related to the planning capabilities of LLMs, so the confidence is lowered.",
      "processing_time": 83.93738460540771,
      "citing_paper_id": "280010642",
      "cited_paper_id": 108374190
    },
    {
      "context_text": ") [96] Structure, property Structured text, graphs ∼ 200k T2, T3, T5, T6 OQMD [145, 146] Inorganic crystals Structure, DFT props ∼ 1M T2, T3 NOMAD [147] Hypothetical crystals Crystal structures, props ∼ 214k T3, T4 OMat24 [148] Inorganic materials Crystal structures, props ∼ 118M T3, T4 SNUMAT…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OQMD",
        "NOMAD",
        "OMat24",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "OQMD": "Used to store and analyze inorganic crystal structures and properties, specifically for high-throughput density functional theory calculations.",
        "NOMAD": "Utilized for sharing and processing data on hypothetical crystals, including crystal structures and properties, supporting materials design and discovery.",
        "OMat24": "Employed to manage and analyze inorganic materials data, including crystal structures and properties, for materials science research.",
        "SNUMAT": "Used to store and analyze inorganic materials data, including crystal structures and properties, for materials science research."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions several databases with specific names and properties, which are likely to be reusable datasets. However, they are not directly related to the planning capabilities of LLMs, so the confidence is lowered.",
      "processing_time": 83.93738460540771,
      "citing_paper_id": "280010642",
      "cited_paper_id": 125046626
    },
    {
      "context_text": ") [96] Structure, property Structured text, graphs ∼ 200k T2, T3, T5, T6 OQMD [145, 146] Inorganic crystals Structure, DFT props ∼ 1M T2, T3 NOMAD [147] Hypothetical crystals Crystal structures, props ∼ 214k T3, T4 OMat24 [148] Inorganic materials Crystal structures, props ∼ 118M T3, T4 SNUMAT…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OQMD",
        "NOMAD",
        "OMat24",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "OQMD": "Used to store and analyze inorganic crystal structures and properties, specifically for high-throughput density functional theory calculations.",
        "NOMAD": "Utilized for sharing and processing data on hypothetical crystals, including crystal structures and properties, supporting materials design and discovery.",
        "OMat24": "Employed to manage and analyze inorganic materials data, including crystal structures and properties, for materials science research.",
        "SNUMAT": "Used to store and analyze inorganic materials data, including crystal structures and properties, for materials science research."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions several databases with specific names and properties, which are likely to be reusable datasets. However, they are not directly related to the planning capabilities of LLMs, so the confidence is lowered.",
      "processing_time": 83.93738460540771,
      "citing_paper_id": "280010642",
      "cited_paper_id": 135497159
    },
    {
      "context_text": "After that, other common datasets for crystalline solids, material properties, and DFT calculations—such as Materials Project (MP) [96], OQMD [146, 145], NOMAD [147], Open Material 2024 (OMat24) [148], and SNUMAT [149]—have been released to mainly serve as training data for deep learning and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project (MP)",
        "OQMD",
        "NOMAD",
        "Open Material 2024 (OMat24)",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "Materials Project (MP)": "Used as training data for deep learning models to predict material properties, focusing on accelerating materials innovation through high-throughput computational methods.",
        "OQMD": "Serves as a training dataset for deep learning models, specifically for assessing the accuracy of DFT formation energies and facilitating materials design and discovery.",
        "NOMAD": "Provides training data for deep learning models, focusing on material properties and DFT calculations to support high-throughput computational materials science.",
        "Open Material 2024 (OMat24)": "Used as a training dataset for deep learning models, focusing on material properties and DFT calculations to advance materials science research.",
        "SNUMAT": "Serves as training data for deep learning models, focusing on material properties and DFT calculations to support materials design and discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training deep learning models in materials science. These datasets are specific and have clear identifiers.",
      "processing_time": 93.2988030910492,
      "citing_paper_id": "280010642",
      "cited_paper_id": 94929253
    },
    {
      "context_text": "After that, other common datasets for crystalline solids, material properties, and DFT calculations—such as Materials Project (MP) [96], OQMD [146, 145], NOMAD [147], Open Material 2024 (OMat24) [148], and SNUMAT [149]—have been released to mainly serve as training data for deep learning and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project (MP)",
        "OQMD",
        "NOMAD",
        "Open Material 2024 (OMat24)",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "Materials Project (MP)": "Used as training data for deep learning models to predict material properties, focusing on accelerating materials innovation through high-throughput computational methods.",
        "OQMD": "Serves as a training dataset for deep learning models, specifically for assessing the accuracy of DFT formation energies and facilitating materials design and discovery.",
        "NOMAD": "Provides training data for deep learning models, focusing on material properties and DFT calculations to support high-throughput computational materials science.",
        "Open Material 2024 (OMat24)": "Used as a training dataset for deep learning models, focusing on material properties and DFT calculations to advance materials science research.",
        "SNUMAT": "Serves as training data for deep learning models, focusing on material properties and DFT calculations to support materials design and discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training deep learning models in materials science. These datasets are specific and have clear identifiers.",
      "processing_time": 93.2988030910492,
      "citing_paper_id": "280010642",
      "cited_paper_id": 125046626
    },
    {
      "context_text": "After that, other common datasets for crystalline solids, material properties, and DFT calculations—such as Materials Project (MP) [96], OQMD [146, 145], NOMAD [147], Open Material 2024 (OMat24) [148], and SNUMAT [149]—have been released to mainly serve as training data for deep learning and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project (MP)",
        "OQMD",
        "NOMAD",
        "Open Material 2024 (OMat24)",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "Materials Project (MP)": "Used as training data for deep learning models to predict material properties, focusing on accelerating materials innovation through high-throughput computational methods.",
        "OQMD": "Serves as a training dataset for deep learning models, specifically for assessing the accuracy of DFT formation energies and facilitating materials design and discovery.",
        "NOMAD": "Provides training data for deep learning models, focusing on material properties and DFT calculations to support high-throughput computational materials science.",
        "Open Material 2024 (OMat24)": "Used as a training dataset for deep learning models, focusing on material properties and DFT calculations to advance materials science research.",
        "SNUMAT": "Serves as training data for deep learning models, focusing on material properties and DFT calculations to support materials design and discovery."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training deep learning models in materials science. These datasets are specific and have clear identifiers.",
      "processing_time": 93.2988030910492,
      "citing_paper_id": "280010642",
      "cited_paper_id": 135497159
    },
    {
      "context_text": "…other common datasets for crystalline solids, material properties, and DFT calculations—such as Materials Project (MP) [96], OQMD [146, 145], NOMAD [147], Open Material 2024 (OMat24) [148], and SNUMAT [149]—have been released to mainly serve as training data for deep learning and foundation models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Materials Project (MP)",
        "OQMD",
        "NOMAD",
        "Open Material 2024 (OMat24)",
        "SNUMAT"
      ],
      "dataset_descriptions": {
        "Materials Project (MP)": "Used as training data for deep learning models, focusing on material properties and DFT calculations.",
        "OQMD": "Serves as a dataset for training deep learning models, specifically for material properties and DFT calculations.",
        "NOMAD": "Provides data for training deep learning models, emphasizing material properties and DFT calculations.",
        "Open Material 2024 (OMat24)": "Used as training data for deep learning models, focusing on material properties and DFT calculations.",
        "SNUMAT": "Serves as a dataset for training deep learning models, specifically for material properties and DFT calculations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training deep learning and foundation models in the domain of material properties and DFT calculations. These datasets are clearly identified and have specific names.",
      "processing_time": 88.79575967788696,
      "citing_paper_id": "280010642",
      "cited_paper_id": 108374190
    },
    {
      "context_text": "While MatterSim adopts MEGNET [58] and Graphormer [83] to build the prediction pipeline, MACE-MP-0 is built on a state-of-the-art architecture, MACE [84], which utilizes equivariant and many-body message passing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.554290533065796,
      "citing_paper_id": "280010642",
      "cited_paper_id": 119413748
    },
    {
      "context_text": "While MatterSim adopts MEGNET [58] and Graphormer [83] to build the prediction pipeline, MACE-MP-0 is built on a state-of-the-art architecture, MACE [84], which utilizes equivariant and many-body message passing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.554290533065796,
      "citing_paper_id": "280010642",
      "cited_paper_id": 249674746
    },
    {
      "context_text": "While MatterSim adopts MEGNET [58] and Graphormer [83] to build the prediction pipeline, MACE-MP-0 is built on a state-of-the-art architecture, MACE [84], which utilizes equivariant and many-body message passing.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.554290533065796,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265104899
    },
    {
      "context_text": "G-SchNet [112] is a pioneer in molecule generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (G-SchNet) for molecule generation.",
      "processing_time": 56.76419115066528,
      "citing_paper_id": "280010642",
      "cited_paper_id": 174797974
    },
    {
      "context_text": "CDVAE [61] is proposed to generate stable crystalline materials from known materials, focusing on a generative approach using Diffusion [113] and VAE [114, 115].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (VAE, Diffusion) and a specific application (CDVAE for material generation).",
      "processing_time": 59.15234732627869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 174802445
    },
    {
      "context_text": "CDVAE [61] is proposed to generate stable crystalline materials from known materials, focusing on a generative approach using Diffusion [113] and VAE [114, 115].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (VAE, Diffusion) and a specific application (CDVAE for material generation).",
      "processing_time": 59.15234732627869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "CDVAE [61] is proposed to generate stable crystalline materials from known materials, focusing on a generative approach using Diffusion [113] and VAE [114, 115].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (VAE, Diffusion) and a specific application (CDVAE for material generation).",
      "processing_time": 59.15234732627869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "CDVAE [61] is proposed to generate stable crystalline materials from known materials, focusing on a generative approach using Diffusion [113] and VAE [114, 115].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (VAE, Diffusion) and a specific application (CDVAE for material generation).",
      "processing_time": 59.15234732627869,
      "citing_paper_id": "280010642",
      "cited_paper_id": 238634584
    },
    {
      "context_text": "Regression Transformer uses MoleculeNet [135] and TAPE (Tasks Assessing Protein Embeddings) [136] benchmarks in their evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MoleculeNet and TAPE as benchmarks used in the evaluation of Regression Transformer. However, benchmarks are excluded unless they refer to specific, downloadable datasets. Both MoleculeNet and TAPE are primarily used for score comparison and are not specific datasets.",
      "processing_time": 61.327716588974,
      "citing_paper_id": "280010642",
      "cited_paper_id": 195069360
    },
    {
      "context_text": "Regression Transformer uses MoleculeNet [135] and TAPE (Tasks Assessing Protein Embeddings) [136] benchmarks in their evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MoleculeNet and TAPE as benchmarks used in the evaluation of Regression Transformer. However, benchmarks are excluded unless they refer to specific, downloadable datasets. Both MoleculeNet and TAPE are primarily used for score comparison and are not specific datasets.",
      "processing_time": 61.327716588974,
      "citing_paper_id": "280010642",
      "cited_paper_id": 217680306
    },
    {
      "context_text": "MatScholar [154, 155] includes over five million paper abstracts in the materials science domain.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "MatScholar is mentioned as a resource containing paper abstracts in materials science, but it does not fit the criteria for a dataset as it is more of a database or repository.",
      "processing_time": 58.63455533981323,
      "citing_paper_id": "280010642",
      "cited_paper_id": 195791303
    },
    {
      "context_text": "MatScholar [154, 155] includes over five million paper abstracts in the materials science domain.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "MatScholar is mentioned as a resource containing paper abstracts in materials science, but it does not fit the criteria for a dataset as it is more of a database or repository.",
      "processing_time": 58.63455533981323,
      "citing_paper_id": "280010642",
      "cited_paper_id": 198999551
    },
    {
      "context_text": "MatScholar [154, 155] Literature text Paper abstracts > 5M T1 MatSciKB [36] Knowledge base Text ∼ 38k T1 PubChem [103] Proteins, genes, chemical structures, literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "PubChem"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used as a knowledge base for materials science text, containing structured information to support information extraction and knowledge discovery.",
        "PubChem": "Utilized for accessing a vast repository of chemical structures, proteins, genes, and literature text, supporting various tasks in materials science and chemistry."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several resources, but only 'MatSciKB' and 'PubChem' are specific, verifiable datasets. 'MatScholar' and 'MatbookQA' are not clearly identified as datasets, and 'MaScQA' is incomplete.",
      "processing_time": 73.56633996963501,
      "citing_paper_id": "280010642",
      "cited_paper_id": 195791303
    },
    {
      "context_text": "MatScholar [154, 155] Literature text Paper abstracts > 5M T1 MatSciKB [36] Knowledge base Text ∼ 38k T1 PubChem [103] Proteins, genes, chemical structures, literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "PubChem"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used as a knowledge base for materials science text, containing structured information to support information extraction and knowledge discovery.",
        "PubChem": "Utilized for accessing a vast repository of chemical structures, proteins, genes, and literature text, supporting various tasks in materials science and chemistry."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several resources, but only 'MatSciKB' and 'PubChem' are specific, verifiable datasets. 'MatScholar' and 'MatbookQA' are not clearly identified as datasets, and 'MaScQA' is incomplete.",
      "processing_time": 73.56633996963501,
      "citing_paper_id": "280010642",
      "cited_paper_id": 198999551
    },
    {
      "context_text": "MatScholar [154, 155] Literature text Paper abstracts > 5M T1 MatSciKB [36] Knowledge base Text ∼ 38k T1 PubChem [103] Proteins, genes, chemical structures, literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "PubChem"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used as a knowledge base for materials science text, containing structured information to support information extraction and knowledge discovery.",
        "PubChem": "Utilized for accessing a vast repository of chemical structures, proteins, genes, and literature text, supporting various tasks in materials science and chemistry."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several resources, but only 'MatSciKB' and 'PubChem' are specific, verifiable datasets. 'MatScholar' and 'MatbookQA' are not clearly identified as datasets, and 'MaScQA' is incomplete.",
      "processing_time": 73.56633996963501,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272367007
    },
    {
      "context_text": "MatScholar [154, 155] Literature text Paper abstracts > 5M T1 MatSciKB [36] Knowledge base Text ∼ 38k T1 PubChem [103] Proteins, genes, chemical structures, literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "PubChem"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used as a knowledge base for materials science text, containing structured information to support information extraction and knowledge discovery.",
        "PubChem": "Utilized for accessing a vast repository of chemical structures, proteins, genes, and literature text, supporting various tasks in materials science and chemistry."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several resources, but only 'MatSciKB' and 'PubChem' are specific, verifiable datasets. 'MatScholar' and 'MatbookQA' are not clearly identified as datasets, and 'MaScQA' is incomplete.",
      "processing_time": 73.56633996963501,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274140377
    },
    {
      "context_text": "ChemBERTa [100] adapts RoBERTa [102] architecture to property prediction task using SMILES data from PubChem [103].",
      "catation_intent": "research work",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "Used to provide SMILES data for molecular property prediction, focusing on the adaptation of the RoBERTa architecture to chemical data."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'SMILES data from PubChem', which is a specific, identifiable dataset. However, the primary focus is on the adaptation of the RoBERTa architecture to a property prediction task, not the use of the dataset itself.",
      "processing_time": 66.96282768249512,
      "citing_paper_id": "280010642",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "ChemBERTa [100] adapts RoBERTa [102] architecture to property prediction task using SMILES data from PubChem [103].",
      "catation_intent": "research work",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "Used to provide SMILES data for molecular property prediction, focusing on the adaptation of the RoBERTa architecture to chemical data."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'SMILES data from PubChem', which is a specific, identifiable dataset. However, the primary focus is on the adaptation of the RoBERTa architecture to a property prediction task, not the use of the dataset itself.",
      "processing_time": 66.96282768249512,
      "citing_paper_id": "280010642",
      "cited_paper_id": 224803102
    },
    {
      "context_text": "ChemBERTa [100] adapts RoBERTa [102] architecture to property prediction task using SMILES data from PubChem [103].",
      "catation_intent": "research work",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "Used to provide SMILES data for molecular property prediction, focusing on the adaptation of the RoBERTa architecture to chemical data."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'SMILES data from PubChem', which is a specific, identifiable dataset. However, the primary focus is on the adaptation of the RoBERTa architecture to a property prediction task, not the use of the dataset itself.",
      "processing_time": 66.96282768249512,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274140377
    },
    {
      "context_text": "Similarly, AIMNet [87] and AIMNet2 [88] introduce atom-in-molecule networks to learn atomic representations and demonstrate their applications in predicting material properties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AIMNet and AIMNet2, which are neural network models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 57.79201102256775,
      "citing_paper_id": "280010642",
      "cited_paper_id": 201754999
    },
    {
      "context_text": "Similarly, AIMNet [87] and AIMNet2 [88] introduce atom-in-molecule networks to learn atomic representations and demonstrate their applications in predicting material properties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AIMNet and AIMNet2, which are neural network models, not datasets. No specific datasets are mentioned or used in the described research.",
      "processing_time": 57.79201102256775,
      "citing_paper_id": "280010642",
      "cited_paper_id": 278220803
    },
    {
      "context_text": "In the early stage, ICSD [144] received its first record back in 1913 and was later available on the web in 2003.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ICSD"
      ],
      "dataset_descriptions": {
        "ICSD": "Mentioned as a historical reference, noting its first record in 1913 and web availability in 2003. No specific research usage or methodology described."
      },
      "confidence_score": 0.5,
      "reasoning": "The context mentions ICSD, which is a specific, verifiable dataset. However, the context does not provide information on how it is used in the research.",
      "processing_time": 66.19946622848511,
      "citing_paper_id": "280010642",
      "cited_paper_id": 204105343
    },
    {
      "context_text": "Advancements in synthesis prediction are mainly based on the BART encoder-decoder architecture [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BART, which is a model, not a dataset. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 57.12305212020874,
      "citing_paper_id": "280010642",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "BART (Bidirectional and Auto-Regressive Transformers) [48] was introduced in 2019 and is a sequence-to-sequence model consisting of a BERT-like bidirectional encoder and a GPT-like autoregressive decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the introduction of the BART model. No datasets are referenced for training or evaluation.",
      "processing_time": 57.37035942077637,
      "citing_paper_id": "280010642",
      "cited_paper_id": 204960716
    },
    {
      "context_text": "Many of those models can be grouped into GNN-based methods [89] and Transformer-based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of models (GNN-based and Transformer-based).",
      "processing_time": 56.47334885597229,
      "citing_paper_id": "280010642",
      "cited_paper_id": 206756462
    },
    {
      "context_text": "…diverse sources of materials-related information (e.g., arXiv articles, Wikipedia, datasets, textbooks); ToolHub, a suite of tools for retrieving up-to-date information; and Retriever, which extracts relevant knowledge from both MatSciKB and ToolHub using BM25 [140] and Contriever [141].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'datasets' generically without naming any specific dataset. It focuses on tools and methods for information retrieval and knowledge extraction.",
      "processing_time": 57.364787578582764,
      "citing_paper_id": "280010642",
      "cited_paper_id": 207220720
    },
    {
      "context_text": "…diverse sources of materials-related information (e.g., arXiv articles, Wikipedia, datasets, textbooks); ToolHub, a suite of tools for retrieving up-to-date information; and Retriever, which extracts relevant knowledge from both MatSciKB and ToolHub using BM25 [140] and Contriever [141].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'datasets' generically without naming any specific dataset. It focuses on tools and methods for information retrieval and knowledge extraction.",
      "processing_time": 57.364787578582764,
      "citing_paper_id": "280010642",
      "cited_paper_id": 249097975
    },
    {
      "context_text": "It is important to note that SMILES and SELFIES (Self-Referencing Embedded Strings) [90] are 2D molecular representations that omit critical information about a molecule’s 3D conformation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only molecular representation formats. No verifiable resources are identified.",
      "processing_time": 56.30021023750305,
      "citing_paper_id": "280010642",
      "cited_paper_id": 212415210
    },
    {
      "context_text": "SciDocs [164] offers a foundation for language model pretraining, focusing on scientific documents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SciDocs"
      ],
      "dataset_descriptions": {
        "SciDocs": "Used as a foundation for language model pretraining, focusing on scientific documents to enhance document-level representation learning."
      },
      "confidence_score": 1.0,
      "reasoning": "SciDocs is mentioned as a foundation for language model pretraining, specifically focusing on scientific documents. It is a specific, verifiable dataset.",
      "processing_time": 62.870023012161255,
      "citing_paper_id": "280010642",
      "cited_paper_id": 215768677
    },
    {
      "context_text": "Focusing on the evaluation of machine learning and deep learning models trained for materials science, MatBench [178] leverages materials data from the aforementioned datasets, like MP, to formulate a comprehensive set of benchmarking tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatBench"
      ],
      "dataset_descriptions": {
        "MatBench": "Used to benchmark materials property prediction models, providing a comprehensive set of tasks and datasets for evaluating machine learning and deep learning models in materials science."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'MatBench' and 'MP', which are likely datasets used for benchmarking materials property prediction methods. However, 'MP' is not fully expanded, and 'MatBench' is described as a benchmarking framework rather than a dataset.",
      "processing_time": 68.68057656288147,
      "citing_paper_id": "280010642",
      "cited_paper_id": 218487092
    },
    {
      "context_text": "…and quantized Mistral models to learn the complex relationships between atomic structures and material properties from datasets such as JARVIS-DFT [120] FlowLLM is trained on the widely used dataset of inorganic crystalline materials, derived from the Materials Project, focusing on a subset of…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "JARVIS-DFT",
        "Materials Project"
      ],
      "dataset_descriptions": {
        "JARVIS-DFT": "Used to train models to learn complex relationships between atomic structures and material properties, focusing on data-driven materials design.",
        "Materials Project": "Used to train FlowLLM on a subset of inorganic crystalline materials, focusing on the relationships between atomic structures and material properties."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'JARVIS-DFT' and 'Materials Project' as datasets used for training and learning material properties. 'JARVIS-DFT' is a specific dataset, while 'Materials Project' is referenced as a source of inorganic crystalline materials.",
      "processing_time": 74.49915623664856,
      "citing_paper_id": "280010642",
      "cited_paper_id": 220347237
    },
    {
      "context_text": "BERT-based models for property prediction include ChemBERTa [100], CatBERTa [59], SolvBERT [59], and Mol-BERT [101].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several BERT-based models but does not refer to any specific datasets. The context is focused on models and their applications, not on datasets.",
      "processing_time": 57.68146085739136,
      "citing_paper_id": "280010642",
      "cited_paper_id": 224803102
    },
    {
      "context_text": "BERT-based models for property prediction include ChemBERTa [100], CatBERTa [59], SolvBERT [59], and Mol-BERT [101].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several BERT-based models but does not refer to any specific datasets. The context is focused on models and their applications, not on datasets.",
      "processing_time": 57.68146085739136,
      "citing_paper_id": "280010642",
      "cited_paper_id": 237476761
    },
    {
      "context_text": "BERT-based models for property prediction include ChemBERTa [100], CatBERTa [59], SolvBERT [59], and Mol-BERT [101].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several BERT-based models but does not refer to any specific datasets. The context is focused on models and their applications, not on datasets.",
      "processing_time": 57.68146085739136,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265544461
    },
    {
      "context_text": "CatBERTa [59] is trained on a set of DFT calculations from Open Catalyst 2020 (OC20) [104] This challenging task requires the development of advanced AI models to learn complex material structures and synthesize novel materials.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to train CatBERTa on DFT calculations, focusing on learning complex material structures and synthesizing novel materials."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Open Catalyst 2020 (OC20)' as a dataset used for training CatBERTa. The dataset is clearly identified and relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 69.67388820648193,
      "citing_paper_id": "280010642",
      "cited_paper_id": 224803470
    },
    {
      "context_text": "CatBERTa [59] is trained on a set of DFT calculations from Open Catalyst 2020 (OC20) [104] This challenging task requires the development of advanced AI models to learn complex material structures and synthesize novel materials.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to train CatBERTa on DFT calculations, focusing on learning complex material structures and synthesizing novel materials."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Open Catalyst 2020 (OC20)' as a dataset used for training CatBERTa. The dataset is clearly identified and relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 69.67388820648193,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265544461
    },
    {
      "context_text": "…materials Crystal structures, DFT props ∼ 10k T3, T4 MPTrj [150] Trajectory data Atomic structures, props ∼ 1.5M T2, T3 Open Catalyst 2020 (OC20) [104] Catalysis structures, forces Graphs, 3D coords ∼ 1.3M T2, T3, T5 Alexandria [151] 1D, 2D, 3D materials Molecular structures, DFT calculations ∼…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to study catalysis structures and forces, focusing on graphs and 3D coordinates. The dataset contains approximately 1.3 million entries, supporting research in materials science and computational chemistry."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions multiple datasets, but only 'Open Catalyst 2020 (OC20)' is a specific, verifiable dataset with a clear identifier and is relevant to the research topic.",
      "processing_time": 71.00682187080383,
      "citing_paper_id": "280010642",
      "cited_paper_id": 224803470
    },
    {
      "context_text": "Other datasets, such as Open Catalyst 2020 (OC20) [104] and MPTrj [150], have been designed with large-scale adsorption energies and structures on catalyst surfaces, interatomic potentials, and atomic trajectory.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)",
        "MPTrj"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to design large-scale adsorption energies and structures on catalyst surfaces, focusing on interatomic potentials and atomic trajectories.",
        "MPTrj": "Used to design large-scale adsorption energies and structures on catalyst surfaces, focusing on interatomic potentials and atomic trajectories."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, OC20 and MPTrj, which are specific and verifiable. OC20 is explicitly linked to a cited paper, and both datasets are described in the context of their design for large-scale adsorption energies and structures on catalyst surfaces.",
      "processing_time": 77.00071954727173,
      "citing_paper_id": "280010642",
      "cited_paper_id": 224803470
    },
    {
      "context_text": "Other datasets, such as Open Catalyst 2020 (OC20) [104] and MPTrj [150], have been designed with large-scale adsorption energies and structures on catalyst surfaces, interatomic potentials, and atomic trajectory.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)",
        "MPTrj"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to design large-scale adsorption energies and structures on catalyst surfaces, focusing on interatomic potentials and atomic trajectories.",
        "MPTrj": "Used to design large-scale adsorption energies and structures on catalyst surfaces, focusing on interatomic potentials and atomic trajectories."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, OC20 and MPTrj, which are specific and verifiable. OC20 is explicitly linked to a cited paper, and both datasets are described in the context of their design for large-scale adsorption energies and structures on catalyst surfaces.",
      "processing_time": 77.00071954727173,
      "citing_paper_id": "280010642",
      "cited_paper_id": 261925357
    },
    {
      "context_text": "We refer interested readers to a recent survey [99] that covers 80 GNNs used for property prediction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey of GNNs used for property prediction. No verifiable datasets are referenced.",
      "processing_time": 57.478437423706055,
      "citing_paper_id": "280010642",
      "cited_paper_id": 230528669
    },
    {
      "context_text": "…impact of foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might unlock new opportunities for…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.477367639541626,
      "citing_paper_id": "280010642",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "SciTune [35] adopts a vision-language pretraining strategy, similar to CLIP [12], to align images (e.g., figures, microscopy) with captions and scientific questions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'vision-language pretraining strategy' and 'align images with captions and scientific questions', which suggests the use of a dataset for training. However, no specific dataset name is mentioned.",
      "processing_time": 59.1779351234436,
      "citing_paper_id": "280010642",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "SciTune [35] adopts a vision-language pretraining strategy, similar to CLIP [12], to align images (e.g., figures, microscopy) with captions and scientific questions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'vision-language pretraining strategy' and 'align images with captions and scientific questions', which suggests the use of a dataset for training. However, no specific dataset name is mentioned.",
      "processing_time": 59.1779351234436,
      "citing_paper_id": "280010642",
      "cited_paper_id": 259316643
    },
    {
      "context_text": "To build an API for exchanging material-related data, OPTIMADE [173] is an open-source tool for providing access to approximately 59M structures from 29 databases.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions OPTIMADE as a tool for accessing databases but does not specify any particular dataset. The mention is about the tool's capability rather than a specific dataset.",
      "processing_time": 58.545071601867676,
      "citing_paper_id": "280010642",
      "cited_paper_id": 232104726
    },
    {
      "context_text": "…foundation models (FMs) in natural language processing (NLP) (e.g., BERT [7], GPT [8, 9, 10], PaLM [11]) and computer vision (e.g., CLIP [12], DINO [13]), the materials science community is now exploring how similar large-scale and pretrained models might unlock new opportunities for research and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.79772138595581,
      "citing_paper_id": "280010642",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "…(2) recommending the modification (i.e., exchange, addition, substitution, or removal); (3) employing machine learning tools such as MatDeepLearn [142] and TorchMD-Net [143] for property prediction; (4) evaluating the outcome; and (5) repeating the process with alternative modifications if the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only machine learning tools and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.79481625556946,
      "citing_paper_id": "280010642",
      "cited_paper_id": 234207025
    },
    {
      "context_text": "Machine learning or deep learning based multi-scale modeling and fusion methods for materials research have been developed [129].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for materials research.",
      "processing_time": 56.07894158363342,
      "citing_paper_id": "280010642",
      "cited_paper_id": 235220930
    },
    {
      "context_text": "SciCap [165] is a multimodal dataset with scientific figure-caption pairs extracted from over 290,000 arXiv papers, which is suitable for training multimodal models like vision-language models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SciCap"
      ],
      "dataset_descriptions": {
        "SciCap": "Used to train multimodal vision-language models, focusing on generating captions for scientific figures extracted from arXiv papers."
      },
      "confidence_score": 1.0,
      "reasoning": "SciCap is identified as a multimodal dataset with specific characteristics, making it a clear and specific resource.",
      "processing_time": 62.72019648551941,
      "citing_paper_id": "280010642",
      "cited_paper_id": 237578994
    },
    {
      "context_text": "ChemFormer [65] shows that models pretrained using only the encoder stack are limited for sequence-to-sequence tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (ChemFormer). The context focuses on the limitations of the model for certain tasks.",
      "processing_time": 58.04403614997864,
      "citing_paper_id": "280010642",
      "cited_paper_id": 237747003
    },
    {
      "context_text": "MatSciBERT [71] is a domain-specific, BERT-based model trained on a large corpus of peer-reviewed materials science publications and establishes state-of-the-art results on downstream tasks such as named entity recognition, relation classification, and abstract classification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a 'large corpus' but does not specify a name or identifier. No other specific datasets are mentioned.",
      "processing_time": 57.10526728630066,
      "citing_paper_id": "280010642",
      "cited_paper_id": 238226735
    },
    {
      "context_text": "The authors use multiple datasets including Pistachio [132] and CheBI-20 [133] in their evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pistachio",
        "CheBI-20"
      ],
      "dataset_descriptions": {
        "Pistachio": "Used in the evaluation of the model, focusing on cross-modal molecule retrieval with natural language queries.",
        "CheBI-20": "Used in the evaluation of the model, focusing on cross-modal molecule retrieval with natural language queries."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, Pistachio and CheBI-20, which are used in the evaluation. These names appear to be specific and plausible datasets.",
      "processing_time": 70.64708542823792,
      "citing_paper_id": "280010642",
      "cited_paper_id": 243865204
    },
    {
      "context_text": "The authors use multiple datasets including Pistachio [132] and CheBI-20 [133] in their evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pistachio",
        "CheBI-20"
      ],
      "dataset_descriptions": {
        "Pistachio": "Used in the evaluation of the model, focusing on cross-modal molecule retrieval with natural language queries.",
        "CheBI-20": "Used in the evaluation of the model, focusing on cross-modal molecule retrieval with natural language queries."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, Pistachio and CheBI-20, which are used in the evaluation. These names appear to be specific and plausible datasets.",
      "processing_time": 70.64708542823792,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "…modification (i.e., exchange, addition, substitution, or removal); (3) employing machine learning tools such as MatDeepLearn [142] and TorchMD-Net [143] for property prediction; (4) evaluating the outcome; and (5) repeating the process with alternative modifications if the target property is not…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tools and methods. The cited paper titles do not introduce any datasets either.",
      "processing_time": 57.27280354499817,
      "citing_paper_id": "280010642",
      "cited_paper_id": 245960612
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258947657
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "Beyond these static foundation models operating solely within their learned parameters and pretraining data, a new paradigm has emerged to support more autonomous and flexible problem-solving: LLM agents [49, 50, 51, 52, 53, 54, 55].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a new paradigm involving LLM agents. The cited papers' titles suggest a focus on planning capabilities of LLMs but do not explicitly name datasets.",
      "processing_time": 60.01171112060547,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263890629
    },
    {
      "context_text": "For materials identification, MatBERT [70] is a BERT-based model trained for materials science, focusing on named entity recognition (NER) to extract and classify entities related to materials science into predefined labels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MatBERT, which is a BERT-based model, not a dataset. No specific datasets are mentioned or used in the context.",
      "processing_time": 57.857364654541016,
      "citing_paper_id": "280010642",
      "cited_paper_id": 248065796
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 252030240
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265601068
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270878784
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271874369
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274023350
    },
    {
      "context_text": "In recent years, machine learning models trained on curated datasets, comprising both simulated and experimental results, have begun to supplement these traditional simulations, enabling faster property prediction and the emergence of generative design capabilities [1, 2, 3, 4, 5, 6].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'curated datasets' but does not specify any particular dataset names. The term 'curated datasets' is too generic and lacks specific identifiers.",
      "processing_time": 58.48282289505005,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276855039
    },
    {
      "context_text": "For property extraction and association, MaterialsBERT [72] builds a general-purpose data extraction pipeline to automatically extract material properties from literature.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a pipeline for extracting material properties but does not refer to a specific dataset. MaterialsBERT is a method, not a dataset.",
      "processing_time": 57.55010509490967,
      "citing_paper_id": "280010642",
      "cited_paper_id": 252544920
    },
    {
      "context_text": "Open MatSci ML Toolkit [42] offers a curated interface to datasets, model architectures, and evaluation protocols, aimed at improving reproducibility and benchmarking across tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'datasets' but does not specify any particular dataset names. It focuses on the toolkit's features rather than specific datasets.",
      "processing_time": 57.67185306549072,
      "citing_paper_id": "280010642",
      "cited_paper_id": 253237094
    },
    {
      "context_text": "DePlot [74] is a tool that translates the image of a plot or chart to a linearized table.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DePlot as a tool, not a dataset. It describes the functionality of DePlot, which is to translate images of plots or charts into linearized tables.",
      "processing_time": 58.88660168647766,
      "citing_paper_id": "280010642",
      "cited_paper_id": 254877346
    },
    {
      "context_text": "Text+Chem T5 [69] is a multi-domain, multi-task Transformer based language model and provides a unified representation between natural language and chemical representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Text+Chem T5) and its capabilities. No verifiable datasets are referenced.",
      "processing_time": 57.98414063453674,
      "citing_paper_id": "280010642",
      "cited_paper_id": 256389950
    },
    {
      "context_text": "GT4SD [25] is a Python toolkit for developing and training generative models for scientific discovery.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GT4SD as a Python toolkit, which is a software tool, not a dataset. No specific dataset is mentioned or used in the context.",
      "processing_time": 58.1078999042511,
      "citing_paper_id": "280010642",
      "cited_paper_id": 256416564
    },
    {
      "context_text": "A Python-based web app framework, named Crystal Toolkit [182], is introduced to provide an interactive interface for exploring materials science information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a web app framework, Crystal Toolkit, which is not a dataset but a tool. No datasets are mentioned.",
      "processing_time": 57.09508156776428,
      "citing_paper_id": "280010642",
      "cited_paper_id": 256827363
    },
    {
      "context_text": "This decoder-based architecture forms the foundation for state-of-the-art large language models such as GPT-4 [10], Google Gemini [46], or Llama [47] models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. No dataset names are present in the citation span.",
      "processing_time": 56.553722620010376,
      "citing_paper_id": "280010642",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "This decoder-based architecture forms the foundation for state-of-the-art large language models such as GPT-4 [10], Google Gemini [46], or Llama [47] models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. No dataset names are present in the citation span.",
      "processing_time": 56.553722620010376,
      "citing_paper_id": "280010642",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "This decoder-based architecture forms the foundation for state-of-the-art large language models such as GPT-4 [10], Google Gemini [46], or Llama [47] models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. No dataset names are present in the citation span.",
      "processing_time": 56.553722620010376,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, MolScribe [73] is an image-to-graph generation algorithm to identify molecular structures from images in documents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MolScribe as an image-to-graph generation algorithm but does not refer to it as a dataset. It is described as a method or tool.",
      "processing_time": 58.112608671188354,
      "citing_paper_id": "280010642",
      "cited_paper_id": 257637220
    },
    {
      "context_text": "Regression Transformer [134] abstracts regression as a conditional sequence modeling problem and bridges sequence regression and conditional sequence generation by using a nominal-scale training objective on combinations of numerical and textual tokens.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Regression Transformer). The context focuses on the methodology and its application, not on any particular dataset.",
      "processing_time": 58.11022472381592,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258021826
    },
    {
      "context_text": "ChemCrow [41] aims to build an LLM agent for chemistry, covering several tasks including organic synthesis, drug discovery, and materials design implemented with the LangChain framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (LangChain) and a project (ChemCrow). The focus is on the capabilities and implementation of the LLM agent for chemistry tasks.",
      "processing_time": 59.853766679763794,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258059792
    },
    {
      "context_text": "…props Self-collected data T1, T3, T4 MatPilot [40] Knowledge retriever + physical workstations + LLM Tabular data, text, graphs N/A T1, T5 ChemCrow [41] CoT reasoning + LLM Literature search, web search Online T1, T4 fine-tuning framework, called MatSci-Instruct , is proposed to generate data for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to self-collected data and online sources, which are too generic and lack specific identifiers.",
      "processing_time": 58.3744797706604,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258059792
    },
    {
      "context_text": "For example, MuMMI [130] is an ensemble machine learning approach for solving protein-membrane interactions that require multiscale modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MuMMI) for solving protein-membrane interactions. No datasets are referenced for training or evaluation.",
      "processing_time": 58.55465912818909,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258213830
    },
    {
      "context_text": "CNN-based model is utilized by [131] to achieve efficient multiscale modeling of heterogeneous materials, focusing on the prediction of homogenized macroscopic stress given a microstructure input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CNN-based model) and its application to modeling heterogeneous materials.",
      "processing_time": 57.15544128417969,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258381985
    },
    {
      "context_text": "Flam-Shepherd et al. [119] demonstrate that language models trained directly on sequences derived directly from chemical file formats like XYZ files, CIFs, or Protein Data Bank (PDB) files can directly generate molecules, crystals, and protein binding sites in three dimensions.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions file formats (XYZ, CIF, PDB) but does not refer to them as datasets. These are file formats used to store molecular and crystal structures, not datasets.",
      "processing_time": 59.29973912239075,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258587994
    },
    {
      "context_text": "These works [24, 118, 119] are different from models that use graph and graph-derived string representations as they treat materials as a sequence of discretized tokens and adopt Transformer architecture, thus utilizing the capability of next-token prediction given the input sequences for material…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide clear dataset names.",
      "processing_time": 57.150460958480835,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258587994
    },
    {
      "context_text": "These works [24, 118, 119] are different from models that use graph and graph-derived string representations as they treat materials as a sequence of discretized tokens and adopt Transformer architecture, thus utilizing the capability of next-token prediction given the input sequences for material…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide clear dataset names.",
      "processing_time": 57.150460958480835,
      "citing_paper_id": "280010642",
      "cited_paper_id": 259501816
    },
    {
      "context_text": "These works [24, 118, 119] are different from models that use graph and graph-derived string representations as they treat materials as a sequence of discretized tokens and adopt Transformer architecture, thus utilizing the capability of next-token prediction given the input sequences for material…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not provide clear dataset names.",
      "processing_time": 57.150460958480835,
      "citing_paper_id": "280010642",
      "cited_paper_id": 267522772
    },
    {
      "context_text": "To evaluate language models on materials science text, MatSciNLP [156] provides a benchmark comprising seven tasks, including Named Entity Recognition (NER), relation, sentence, and paragraph classification, event argument extraction, synthesis action retrieval, and slot filling.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciNLP"
      ],
      "dataset_descriptions": {
        "MatSciNLP": "Used to evaluate language models on materials science tasks, including NER, relation, sentence, and paragraph classification, event argument extraction, synthesis action retrieval, and slot filling."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions MatSciNLP as a benchmark for evaluating language models on materials science tasks, which aligns with the topic of planning capabilities of LLMs.",
      "processing_time": 67.42780137062073,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258685532
    },
    {
      "context_text": "…books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼ 170k T1, T4, T5 LLM4Mat-Bench [157] LLM benchmark for property prediction Crystal composition, CIFs, textual…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciNLP"
      ],
      "dataset_descriptions": {
        "MatSciNLP": "Used to evaluate scientific language models on materials science language tasks, focusing on text-to-schema modeling to assess the models' ability to understand and generate structured scientific content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and benchmarks, but only 'MatSciNLP' is a specific, verifiable dataset with a clear identifier and is relevant to the topic of planning capabilities in LLMs.",
      "processing_time": 68.38052105903625,
      "citing_paper_id": "280010642",
      "cited_paper_id": 258685532
    },
    {
      "context_text": "Several datasets are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenOrca",
        "WebInstructSub",
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ"
      ],
      "dataset_descriptions": {
        "OpenOrca": "Used for training and evaluating models on general Q&A tasks, providing a diverse set of complex explanation traces.",
        "WebInstructSub": "Used for training and evaluating models on web-based instructions, enhancing the model's ability to follow and generate instructions.",
        "MathQA": "Used for training and evaluating models on math-related Q&A, focusing on problem-solving and reasoning skills.",
        "MatbookQA": "Used for training and evaluating models on material science Q&A, enhancing domain-specific knowledge and reasoning.",
        "MaScQA": "Used for training and evaluating models on materials science Q&A, focusing on complex scientific concepts and applications.",
        "ARC": "Used for training and evaluating models on advanced reasoning and commonsense Q&A, enhancing the model's ability to handle challenging problems.",
        "PIQA": "Used for training and evaluating models on physical interaction Q&A, focusing on understanding and reasoning about physical scenarios.",
        "SciQ": "Used for training and evaluating models on science-related Q&A, covering a wide range of scientific subjects and concepts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used to support IFT, which are specific and identifiable. These datasets are used for training and evaluating models on various types of questions, particularly in the domain of science and mathematics.",
      "processing_time": 100.5473244190216,
      "citing_paper_id": "280010642",
      "cited_paper_id": 259075316
    },
    {
      "context_text": "Several datasets are introduced to support IFT including general Q&A like OpenOrca [166] and WebInstructSub [167], and science-related Q&A extracted from science exams or knowledge bases of different subjects, such as MathQA [168], MatbookQA [34], MaScQA [75], ARC [169], PIQA [170], SciQ [171] and…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenOrca",
        "WebInstructSub",
        "MathQA",
        "MatbookQA",
        "MaScQA",
        "ARC",
        "PIQA",
        "SciQ"
      ],
      "dataset_descriptions": {
        "OpenOrca": "Used for training and evaluating models on general Q&A tasks, providing a diverse set of complex explanation traces.",
        "WebInstructSub": "Used for training and evaluating models on web-based instructions, enhancing the model's ability to follow and generate instructions.",
        "MathQA": "Used for training and evaluating models on math-related Q&A, focusing on problem-solving and reasoning skills.",
        "MatbookQA": "Used for training and evaluating models on material science Q&A, enhancing domain-specific knowledge and reasoning.",
        "MaScQA": "Used for training and evaluating models on materials science Q&A, focusing on complex scientific concepts and applications.",
        "ARC": "Used for training and evaluating models on advanced reasoning and commonsense Q&A, enhancing the model's ability to handle challenging problems.",
        "PIQA": "Used for training and evaluating models on physical interaction Q&A, focusing on understanding and reasoning about physical scenarios.",
        "SciQ": "Used for training and evaluating models on science-related Q&A, covering a wide range of scientific subjects and concepts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets used to support IFT, which are specific and identifiable. These datasets are used for training and evaluating models on various types of questions, particularly in the domain of science and mathematics.",
      "processing_time": 100.5473244190216,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269605607
    },
    {
      "context_text": "CrystaLLM [24] is a decoder-only Transformer-based tool for crystal structure generation trained on an extensive corpus of the CIFs representing the structures of millions of inorganic solid-state materials.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a corpus of CIFs, which is a specific type of dataset used for crystal structure representation. However, the name 'CIFs' is too generic and does not meet the criteria for a specific, verifiable dataset.",
      "processing_time": 61.262673139572144,
      "citing_paper_id": "280010642",
      "cited_paper_id": 259501816
    },
    {
      "context_text": "M 2 Hub [175] is another Python-based toolkit that provides machine learning practice for materials science.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a toolkit, not a dataset. The context does not indicate the use of a specific, verifiable dataset.",
      "processing_time": 56.877878189086914,
      "citing_paper_id": "280010642",
      "cited_paper_id": 259766332
    },
    {
      "context_text": "DiffCSP [62] utilizes the fractional coordinate system to intrinsically represent crystals and model periodicity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DiffCSP) and its application to crystal structure prediction. No verifiable datasets are referenced.",
      "processing_time": 57.9305956363678,
      "citing_paper_id": "280010642",
      "cited_paper_id": 261681740
    },
    {
      "context_text": "…Inorganic materials Crystal structures, props ∼ 118M T3, T4 SNUMAT [149] Synthesized materials Crystal structures, DFT props ∼ 10k T3, T4 MPTrj [150] Trajectory data Atomic structures, props ∼ 1.5M T2, T3 Open Catalyst 2020 (OC20) [104] Catalysis structures, forces Graphs, 3D coords ∼ 1.3M T2,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SNUMAT",
        "MPTrj",
        "Open Catalyst 2020 (OC20)"
      ],
      "dataset_descriptions": {
        "SNUMAT": "Used to train models on synthesized materials' crystal structures and properties, focusing on inorganic materials with approximately 118 million entries.",
        "MPTrj": "Used to train models on trajectory data of atomic structures and properties, focusing on synthesized materials with approximately 10,000 entries.",
        "Open Catalyst 2020 (OC20)": "Used to train models on catalysis structures and forces, focusing on graphs and 3D coordinates with approximately 1.3 million entries."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets with specific names and characteristics, which are used for training and evaluation in the field of atomistic modeling.",
      "processing_time": 81.35369348526001,
      "citing_paper_id": "280010642",
      "cited_paper_id": 261925357
    },
    {
      "context_text": "MoL-MoE utilizes SMI-TED (289M) foundation model [137] as the SMILES encoder, SELFIES-BART as the SELFIES encoder [138], and MHG-GNN [139] as the graph encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and encoders but does not refer to any specific datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 57.49990129470825,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263134037
    },
    {
      "context_text": "MoL-MoE utilizes SMI-TED (289M) foundation model [137] as the SMILES encoder, SELFIES-BART as the SELFIES encoder [138], and MHG-GNN [139] as the graph encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and encoders but does not refer to any specific datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 57.49990129470825,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271544431
    },
    {
      "context_text": "MoL-MoE utilizes SMI-TED (289M) foundation model [137] as the SMILES encoder, SELFIES-BART as the SELFIES encoder [138], and MHG-GNN [139] as the graph encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and encoders but does not refer to any specific datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 57.49990129470825,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273375070
    },
    {
      "context_text": "MatChat [125] demonstrates the effectiveness of LLMs in predicting the synthesizability of inorganic compounds and selecting suitable precursors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets used in the research. It focuses on the application of LLMs in materials science, particularly in predicting the synthesizability of inorganic compounds.",
      "processing_time": 59.24251580238342,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263835191
    },
    {
      "context_text": "Ongoing efforts in data augmentation, transfer learning, and active learning aim to address these limitations by prioritizing data diversity, especially in low-resource domains like polymers, amorphous systems, and biomaterials [29, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses general methods and approaches in data augmentation, transfer learning, and active learning without naming any particular datasets.",
      "processing_time": 58.82248640060425,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263908945
    },
    {
      "context_text": "Ongoing efforts in data augmentation, transfer learning, and active learning aim to address these limitations by prioritizing data diversity, especially in low-resource domains like polymers, amorphous systems, and biomaterials [29, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses general methods and approaches in data augmentation, transfer learning, and active learning without naming any particular datasets.",
      "processing_time": 58.82248640060425,
      "citing_paper_id": "280010642",
      "cited_paper_id": 277356221
    },
    {
      "context_text": "Current work focuses mostly on atomistic or crystal-level inputs—extending to grain boundaries, sintering behavior, fracture evolution, and large-scale failure remains underexplored [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of research that are underexplored. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.5119354724884,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263908945
    },
    {
      "context_text": "HoneyBee [56] is an early attempt to build LLMs for materials science.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HoneyBee as an early attempt to build LLMs for materials science, but does not refer to it as a dataset. It appears to be a method or model.",
      "processing_time": 59.41904807090759,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263909166
    },
    {
      "context_text": "MatSci-Instruct [56], which is proposed in HoneyBee work, is an attempt to build an instruction fine-tuning dataset specified for materials science with around 52k instructions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSci-Instruct"
      ],
      "dataset_descriptions": {
        "MatSci-Instruct": "Used to fine-tune large language models for materials science, focusing on instruction-following tasks with around 52k instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions MatSci-Instruct as a dataset for instruction fine-tuning in materials science, which is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 66.01859331130981,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263909166
    },
    {
      "context_text": "…> 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼ 170k T1, T4, T5 LLM4Mat-Bench [157] LLM benchmark for property…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatbookQA",
        "MaScQA",
        "MatSci-Instruct",
        "MatSciNLP",
        "LLM4Mat-Bench"
      ],
      "dataset_descriptions": {
        "MatbookQA": "Used for Q&A tasks from materials science books, focusing on instructional and factual questions to evaluate model understanding.",
        "MaScQA": "Used for Q&A tasks from engineering exams, assessing model performance on technical and application-based questions.",
        "MatSci-Instruct": "Used for instruction-following tasks in materials science, evaluating model ability to perform complex instructions and generate accurate responses.",
        "MatSciNLP": "Used as an NLP benchmark for materials science, assessing model performance on text and Q&A tasks related to the domain.",
        "LLM4Mat-Bench": "Used as a benchmark for large language models in materials science, evaluating model performance on property prediction and other domain-specific tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training and evaluating models in the domain of materials science. These datasets are specific and have clear identifiers.",
      "processing_time": 89.01582789421082,
      "citing_paper_id": "280010642",
      "cited_paper_id": 263909166
    },
    {
      "context_text": "FORGE [43] provides a flexible pretraining and benchmarking framework for large-scale scientific models, supporting graph-based architectures and self-supervised objectives across chemistry and materials datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'chemistry and materials datasets' but does not specify any particular dataset names. The citation is about a framework, not a specific dataset.",
      "processing_time": 57.92249774932861,
      "citing_paper_id": "280010642",
      "cited_paper_id": 264591559
    },
    {
      "context_text": "Crystalline materials are stored in standard text file formats known as CIF (Crystallographic Information File) or SLICES (Simplified Line-Input Crystal-Encoding System [117]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions file formats (CIF, SLICES) but does not refer to them as datasets, databases, or similar resources. They are described as file formats for storing crystalline materials data.",
      "processing_time": 58.49460744857788,
      "citing_paper_id": "280010642",
      "cited_paper_id": 264973960
    },
    {
      "context_text": "ATLANTIC [23] extends this idea by aligning graph-based chemical representations with textual data, supporting synthesis reasoning and property prediction through interdisciplinary learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (ATLANTIC) and its capabilities, but does not reference any dataset by name.",
      "processing_time": 58.69865655899048,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308533
    },
    {
      "context_text": "While models like nach0 and ATLANTIC explore text-structure-property fusion, most foundation models operate in unimodal spaces [20, 23], lacking the ability to simultaneously reason over images, spectra, simulation outputs, and experimental metadata.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.31358075141907,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308533
    },
    {
      "context_text": "While models like nach0 and ATLANTIC explore text-structure-property fusion, most foundation models operate in unimodal spaces [20, 23], lacking the ability to simultaneously reason over images, spectra, simulation outputs, and experimental metadata.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.31358075141907,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308911
    },
    {
      "context_text": "These successes span materials discovery, molecular dynamics, inverse design, autonomous synthesis, and literature-based information extraction, catalyzing a shift toward generalist, scalable, and efficient AI systems for scientific research [14, 15, 17, 20, 26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only broad areas of success in AI systems for scientific research. No verifiable resources are named.",
      "processing_time": 57.546953439712524,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308911
    },
    {
      "context_text": "These successes span materials discovery, molecular dynamics, inverse design, autonomous synthesis, and literature-based information extraction, catalyzing a shift toward generalist, scalable, and efficient AI systems for scientific research [14, 15, 17, 20, 26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only broad areas of success in AI systems for scientific research. No verifiable resources are named.",
      "processing_time": 57.546953439712524,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505046
    },
    {
      "context_text": "These successes span materials discovery, molecular dynamics, inverse design, autonomous synthesis, and literature-based information extraction, catalyzing a shift toward generalist, scalable, and efficient AI systems for scientific research [14, 15, 17, 20, 26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only broad areas of success in AI systems for scientific research. No verifiable resources are named.",
      "processing_time": 57.546953439712524,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505419
    },
    {
      "context_text": "These successes span materials discovery, molecular dynamics, inverse design, autonomous synthesis, and literature-based information extraction, catalyzing a shift toward generalist, scalable, and efficient AI systems for scientific research [14, 15, 17, 20, 26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only broad areas of success in AI systems for scientific research. No verifiable resources are named.",
      "processing_time": 57.546953439712524,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265698016
    },
    {
      "context_text": "These successes span materials discovery, molecular dynamics, inverse design, autonomous synthesis, and literature-based information extraction, catalyzing a shift toward generalist, scalable, and efficient AI systems for scientific research [14, 15, 17, 20, 26].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only broad areas of success in AI systems for scientific research. No verifiable resources are named.",
      "processing_time": 57.546953439712524,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "Generative models like nach0 and MatterGen can produce physically implausible results despite being syntactically valid [20, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models. The cited papers are about models, not datasets.",
      "processing_time": 56.61874794960022,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308911
    },
    {
      "context_text": "Generative models like nach0 and MatterGen can produce physically implausible results despite being syntactically valid [20, 17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models. The cited papers are about models, not datasets.",
      "processing_time": 56.61874794960022,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265698016
    },
    {
      "context_text": "Another example is nach0 [20], a multitask Transformer that unifies SMILES strings and natural language, enabling molecule generation, retrosynthesis, reaction prediction, and scientific question answering in a single model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (nach0) and its capabilities. No verifiable resources are identified.",
      "processing_time": 57.353185415267944,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265308911
    },
    {
      "context_text": "Autonomous laboratories such as A-Lab [26] further illustrate the integration of foundation models with experimental loops, enabling robotic synthesis, online decision-making, and self-improving discovery.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an autonomous laboratory system. No verifiable dataset names are present in the context.",
      "processing_time": 56.95620679855347,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505046
    },
    {
      "context_text": "They can be tuned or prompted to extract synthesis protocols, material properties, designs and recommendations, or summarize literature trends at scale [26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of models. No clear identifiers for datasets are present.",
      "processing_time": 56.431817054748535,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505046
    },
    {
      "context_text": "Foundation models are increasingly used to extract and learn from synthesis protocols, suggest reaction pathways, and optimize experiment design in autonomous or semi-autonomous labs [26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of foundation models in an autonomous laboratory setting.",
      "processing_time": 56.114694118499756,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505046
    },
    {
      "context_text": "For example, MatterSim [15] is trained on over 17M DFT-labeled structures using industrial-scale infrastructures, while GNoME [14] leverages active learning in combination with distributed DFT validation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DFT-labeled structures' but does not specify a named dataset. The focus is on the training process and infrastructure rather than a specific dataset.",
      "processing_time": 58.53889274597168,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505419
    },
    {
      "context_text": "For example, MatterSim [15] is trained on over 17M DFT-labeled structures using industrial-scale infrastructures, while GNoME [14] leverages active learning in combination with distributed DFT validation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DFT-labeled structures' but does not specify a named dataset. The focus is on the training process and infrastructure rather than a specific dataset.",
      "processing_time": 58.53889274597168,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "GNoME [14] is a graph neural network-based discovery engine and applies large ensembles and uncertainty quantification to predict material stability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GNoME) and its application. The context is about a model and its capabilities, not a dataset.",
      "processing_time": 58.77824950218201,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505419
    },
    {
      "context_text": "While models like GNoME have demonstrated generalization across broad chemical spaces, the reliability of predictions in entirely novel regimes is unclear [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (GNoME) and a general discussion about prediction reliability. No verifiable resources are identified.",
      "processing_time": 58.17965817451477,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505419
    },
    {
      "context_text": "GNoME identified over 2.2M new stable inorganic materials—an order-of-magnitude leap beyond prior efforts—and discovered over 45,000 new crystal prototypes occupying previously unexplored regions of chemical space [14].",
      "catation_intent": "findings",
      "resource_type": "discovery",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the discovery of new materials and crystal prototypes. No clear, verifiable dataset names are present.",
      "processing_time": 57.78314781188965,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265505419
    },
    {
      "context_text": "Similarly, MultiMat [21] is a contrastive learning framework that learns embeddings across crystal structures, density of states (DOS), charge densities, and natural language labels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MultiMat as a framework, not a dataset. No specific, verifiable datasets are mentioned.",
      "processing_time": 56.424872636795044,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265551641
    },
    {
      "context_text": "Generative models, including diffusion models, graph-based Transformers, and LLM-driven molecular encoders, now support property-aware molecule and crystal generation, multi-objective optimization, and structure editing [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models and their applications. The context is about the capabilities of generative models, not the use of datasets.",
      "processing_time": 58.558969497680664,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265698016
    },
    {
      "context_text": "MatterGen [17] is proposed to generate stable, diverse inorganic materials across the periodic table.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MatterGen) for generating inorganic materials.",
      "processing_time": 56.38093042373657,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265698016
    },
    {
      "context_text": "Although MatterGen can generate candidate materials optimized for electronic or mechanical properties, it cannot guarantee experimental feasibility [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a generative model called MatterGen. The context focuses on the capabilities and limitations of the model rather than the use of a dataset.",
      "processing_time": 58.892781257629395,
      "citing_paper_id": "280010642",
      "cited_paper_id": 265698016
    },
    {
      "context_text": "For example, Zaki et al. [75] present a dataset of 650 Q&A questions and evaluate the performance of Llama-2-70B, GPT-3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "dataset of 650 Q&A questions"
      ],
      "dataset_descriptions": {
        "dataset of 650 Q&A questions": "Used to evaluate the performance of Llama-2-70B and GPT-3 in answering materials science questions, focusing on the models' planning capabilities and knowledge retention."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a dataset of 650 Q&A questions used to evaluate the performance of large language models. The dataset is specific and relevant to the research topic.",
      "processing_time": 69.0617606639862,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266448819
    },
    {
      "context_text": "…literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatbookQA",
        "MaScQA",
        "MatSci-Instruct",
        "MatSciNLP"
      ],
      "dataset_descriptions": {
        "MatbookQA": "Used for Q&A from materials science books, focusing on information retrieval and factual accuracy in materials science knowledge.",
        "MaScQA": "Used for Q&A from engineering exams, assessing the materials science knowledge and reasoning capabilities of large language models.",
        "MatSci-Instruct": "Used for instruction-following tasks in materials science, evaluating the ability of models to understand and execute complex instructions.",
        "MatSciNLP": "Used as an NLP benchmark, testing the performance of models on text and Q&A tasks specific to materials science."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for training and evaluating models in the domain of materials science. These datasets are specific and have clear identifiers.",
      "processing_time": 81.75507187843323,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266448819
    },
    {
      "context_text": "Local message-passing architectures like those in MatterSim and MACE-MP-0 effectively capture short-range bonding but fall short for systems dominated by nonlocal physics [16, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the limitations of local message-passing architectures in capturing nonlocal physics.",
      "processing_time": 58.16484808921814,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266693920
    },
    {
      "context_text": "Local message-passing architectures like those in MatterSim and MACE-MP-0 effectively capture short-range bonding but fall short for systems dominated by nonlocal physics [16, 15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the limitations of local message-passing architectures in capturing nonlocal physics.",
      "processing_time": 58.16484808921814,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "In the domain of atomic-scale modeling, MatterSim [15] and MACE-MP-0 [16] represent two large-scale efforts to build universal machine-learned interatomic potentials.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MatterSim and MACE-MP-0, but these are described as models or methods rather than datasets. No specific datasets are mentioned.",
      "processing_time": 57.97862124443054,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266693920
    },
    {
      "context_text": "In the domain of atomic-scale modeling, MatterSim [15] and MACE-MP-0 [16] represent two large-scale efforts to build universal machine-learned interatomic potentials.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MatterSim and MACE-MP-0, but these are described as models or methods rather than datasets. No specific datasets are mentioned.",
      "processing_time": 57.97862124443054,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "Foundation models trained on millions of DFT-calibrated structures can now serve as general-purpose simulators across diverse chemistries and environments, offering near-DFT accuracy at a fraction of the computational cost [15, 16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the capabilities of foundation models and their applications in materials chemistry.",
      "processing_time": 57.56538510322571,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266693920
    },
    {
      "context_text": "Foundation models trained on millions of DFT-calibrated structures can now serve as general-purpose simulators across diverse chemistries and environments, offering near-DFT accuracy at a fraction of the computational cost [15, 16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the capabilities of foundation models and their applications in materials chemistry.",
      "processing_time": 57.56538510322571,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "Current models also focus primarily on equilibrium properties, while dynamic or temperature-dependent properties remain less explored [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the focus of current models.",
      "processing_time": 55.52718114852905,
      "citing_paper_id": "280010642",
      "cited_paper_id": 266693920
    },
    {
      "context_text": "DiffCSP++ [18] uses the diffusion model to generate new materials by incorporating the space group constraint.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DiffCSP++) and a concept (space group constraint).",
      "processing_time": 56.6960883140564,
      "citing_paper_id": "280010642",
      "cited_paper_id": 267500390
    },
    {
      "context_text": "Gruver et al. [118] show that fine-tuned LLMs can generate the three-dimensional structure of stable crystals as text.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of fine-tuned LLMs to generate crystal structures.",
      "processing_time": 56.50059533119202,
      "citing_paper_id": "280010642",
      "cited_paper_id": 267522772
    },
    {
      "context_text": "MatSci-LLM [128] presents an LLM-based framework for generating experimental hypotheses of real-world materials discovery.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for generating experimental hypotheses using LLMs.",
      "processing_time": 55.6652934551239,
      "citing_paper_id": "280010642",
      "cited_paper_id": 267547479
    },
    {
      "context_text": "The comparative evaluations demonstrate its superiority over several existing LLMs, such as GPT-4o, Gemini, and Claude [82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.05335569381714,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268232499
    },
    {
      "context_text": "LLM4Mol [158] LLM benchmark for molecular prediction Molecules, props ∼ 40k T4 MACBENCH [159] LLM multimodal benchmark for chemistry and MatSci Q&A 628 T1",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'Molecules, props' and 'MACBENCH' as benchmarks. However, 'Molecules, props' is too generic and lacks a clear identifier. 'MACBENCH' is a specific benchmark name but is used primarily for score comparison, not as a reusable dataset.",
      "processing_time": 62.80741906166077,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268297055
    },
    {
      "context_text": "LLM4Mol [158] introduces a benchmark aimed at evaluating the ability of LLMs to perform molecular prediction tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a benchmark for evaluating LLMs in molecular prediction tasks, but does not specify a dataset name. The term 'benchmark' is excluded as it is primarily used for score comparison.",
      "processing_time": 58.989819049835205,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268297055
    },
    {
      "context_text": "Uni-SMART (Universal Science Multimodal Analysis and Research Transformer) [81] is designed for understanding multimodal scientific literature that contain molecular structures, chemical reactions, charts, and tables, in addition to textual content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model called Uni-SMART. No verifiable resources are identified.",
      "processing_time": 56.23983812332153,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268509923
    },
    {
      "context_text": "Based on CDVAE, Con-CDVEA [116] generates crystals’ latent variables according to given properties such as formation energy or band gap, and then yields the corresponding crystal structure by decoding the latent variables.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for generating crystal structures. No verifiable dataset names are present.",
      "processing_time": 56.237290143966675,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268531713
    },
    {
      "context_text": "CrystalFormer [19] is a Transformer-based autoregressive model specifically designed to generate crystal materials that respect space group symmetries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model called CrystalFormer. The title confirms that the focus is on a method, not a dataset.",
      "processing_time": 57.57004904747009,
      "citing_paper_id": "280010642",
      "cited_paper_id": 268680784
    },
    {
      "context_text": "AtomGPT [31] leverages GPT-2 and quantized Mistral models to learn the complex relationships between atomic structures and material properties from datasets such as JARVIS-DFT [120] FlowLLM is trained on the widely used dataset of inorganic crystalline materials, derived from the Materials Project,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "JARVIS-DFT",
        "Materials Project"
      ],
      "dataset_descriptions": {
        "JARVIS-DFT": "Used to train AtomGPT to learn complex relationships between atomic structures and material properties, focusing on forward and inverse materials design.",
        "Materials Project": "Used to train FlowLLM on inorganic crystalline materials, focusing on the derivation of material properties from atomic structures."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'JARVIS-DFT' and 'Materials Project' as datasets used for training models to learn relationships between atomic structures and material properties.",
      "processing_time": 70.40838432312012,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269605576
    },
    {
      "context_text": "For molecular design, GP-MoLFormer [63] enables property-conditioned molecule generation using Transformer architectures trained on large molecular datasets.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large molecular datasets' but does not specify a named dataset. The citation is focused on the method (GP-MoLFormer) rather than a specific dataset.",
      "processing_time": 58.44772958755493,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626268
    },
    {
      "context_text": "Many datasets also suffer from compositional, elemental, and phase-type biases, with overrepresentation of stable oxides and underrepresentation of disordered or metastable materials [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions biases in datasets but does not specify any particular dataset name. The mention of 'stable oxides' and 'disordered or metastable materials' suggests a domain-specific dataset, but no specific name is provided.",
      "processing_time": 60.17534422874451,
      "citing_paper_id": "280010642",
      "cited_paper_id": 269626676
    },
    {
      "context_text": "…BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for materials…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Collection of MOF databases"
      ],
      "dataset_descriptions": {
        "Collection of MOF databases": "Used to predict and generate metal-organic frameworks, focusing on the integration of LLMs with tools and evaluators to enhance materials design and discovery."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Collection of MOF databases' which appears to be a specific dataset used in the research. No other specific datasets are mentioned.",
      "processing_time": 65.75132298469543,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270223881
    },
    {
      "context_text": "ChatMOF [38] is another agentic framework built to predict and generate metal-organic frameworks by leveraging pretrained LLMs, e.g., GPT-4, GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using LLMs for predicting and generating metal-organic frameworks.",
      "processing_time": 57.41375708580017,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270223881
    },
    {
      "context_text": "SynAsk [127] is an organic chemistry domain-specific LLM platform.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SynAsk as an LLM platform for organic chemistry, but does not refer to it as a dataset. It is described as a tool or method.",
      "processing_time": 57.73102831840515,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270358007
    },
    {
      "context_text": "LLMatDesign [37] employs a step-by-step, self-reflective design to accomplish the materials discovery task: (1) receiving human input about the chemical composition and the target property; (2) recommending the modification (i.e., exchange, addition, substitution, or removal); (3) employing machine…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called LLMatDesign. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.56503105163574,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270620677
    },
    {
      "context_text": "HoneyComb [36] BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "Collection of MOF databases"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used to integrate literature text and web search results for materials science knowledge, enhancing the capabilities of LLMs in materials discovery.",
        "Collection of MOF databases": "Utilized to provide structural and textual data for MOF (Metal-Organic Framework) research, supporting the development of LLM-based agents in materials design."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'MatSciKB' and 'Collection of MOF databases', which appear to be specific datasets or collections used in materials science research.",
      "processing_time": 72.34310746192932,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270620677
    },
    {
      "context_text": "HoneyComb [36] BM25 + Contriever + LLM Literature text, web search MatSciKB T1 LLMatDesign [37] MatDeepLearn + TorchMD-Net + LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB",
        "Collection of MOF databases"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used to integrate literature text and web search results for materials science knowledge, enhancing the capabilities of LLMs in materials discovery.",
        "Collection of MOF databases": "Utilized to provide structural and textual data for MOF (Metal-Organic Framework) research, supporting the development of LLM-based agents in materials design."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'MatSciKB' and 'Collection of MOF databases', which appear to be specific datasets or collections used in materials science research.",
      "processing_time": 72.34310746192932,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272367007
    },
    {
      "context_text": "Expanding into the broader chemistry domain, Ramos et al. [4] discuss recent advances in LLMs and agentic AI for chemistry-related tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses advances in LLMs and agentic AI for chemistry-related tasks.",
      "processing_time": 56.40769290924072,
      "citing_paper_id": "280010642",
      "cited_paper_id": 270878784
    },
    {
      "context_text": "[67] employs Llama-7B fine-tuned via LoRA [126] and utilizes a proprietary Materials String representation to encode crystal structures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a proprietary representation method for encoding crystal structures, which is not a publicly accessible dataset. No other specific datasets are mentioned.",
      "processing_time": 56.762043714523315,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271064894
    },
    {
      "context_text": "CSLLM [67] comprises three LLMs designed to predict material synthesizability, synthesis methods, and synthesis precursors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the capabilities of LLMs in predicting material synthesizability, synthesis methods, and synthesis precursors.",
      "processing_time": 58.97691106796265,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271064894
    },
    {
      "context_text": "MatterGPT [64] is another Transformer-based model trained for generating solid-state materials with given properties.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MatterGPT as a model, not a dataset. No specific dataset is referenced.",
      "processing_time": 55.4271297454834,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271865537
    },
    {
      "context_text": "Focusing specifically on crystalline materials, Wang et al. [2] present a survey of AI-accelerated approaches for crystal discovery, emphasizing four key tasks—property prediction, materials synthesis, characterization assistance, and acceleration of theoretical computations—as well as associated…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey of AI-accelerated approaches for crystal discovery. No verifiable resources are identified.",
      "processing_time": 57.45732235908508,
      "citing_paper_id": "280010642",
      "cited_paper_id": 271874369
    },
    {
      "context_text": "HoneyComb [36] provides pretrained LLM capabilities to retrieve and analyze comprehensive knowledge in the materials science domain, enhancing the quality of generated content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HoneyComb as a system providing pretrained LLM capabilities but does not refer to it as a dataset. It is described as a tool or method rather than a dataset.",
      "processing_time": 58.58700919151306,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272367007
    },
    {
      "context_text": "To build LLM-based agents for materials science, MatSciKB [36], which is part of the HoneyComb work, includes diverse sources of data related to materials science—such as arXiv papers, Wikipedia pages, textbooks, multiple-choice datasets, formulas, and GPT-generated entries—to support the task of…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MatSciKB"
      ],
      "dataset_descriptions": {
        "MatSciKB": "Used to build LLM-based agents for materials science, incorporating diverse data sources such as arXiv papers, Wikipedia pages, textbooks, multiple-choice datasets, formulas, and GPT-generated entries."
      },
      "confidence_score": 1.0,
      "reasoning": "MatSciKB is mentioned as a specific knowledge base used in the context of building LLM-based agents for materials science. It includes diverse data sources relevant to the field.",
      "processing_time": 67.16591477394104,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272367007
    },
    {
      "context_text": "…Atomic structures, props ∼ 1.5M T2, T3 Open Catalyst 2020 (OC20) [104] Catalysis structures, forces Graphs, 3D coords ∼ 1.3M T2, T3, T5 Alexandria [151] 1D, 2D, 3D materials Molecular structures, DFT calculations ∼ 5M T2, T3, T4 QM9 [97, 98] Molecules, DFT SMILES, 3D structures 134k T2, T3…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open Catalyst 2020 (OC20)",
        "Alexandria",
        "QM9"
      ],
      "dataset_descriptions": {
        "Open Catalyst 2020 (OC20)": "Used to train models on catalysis structures and forces, providing graphs and 3D coordinates for improving machine-learning models in materials science.",
        "Alexandria": "Utilized for training models on 1D, 2D, and 3D materials, including molecular structures and DFT calculations, to enhance machine-learning models in materials science.",
        "QM9": "Employed to train models on molecules with DFT calculations, using SMILES and 3D structures, to improve machine-learning models in materials science."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets with specific names and characteristics, which are used in materials science for training machine-learning models.",
      "processing_time": 81.22608160972595,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272886429
    },
    {
      "context_text": "For exploring molecular property, Alexandria dataset [151] offers an open collection of approximately 5 million DFT-calculated molecular structures.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Alexandria dataset"
      ],
      "dataset_descriptions": {
        "Alexandria dataset": "Used to explore molecular properties, specifically providing an open collection of approximately 5 million DFT-calculated molecular structures for materials science research."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Alexandria dataset, which is a specific, verifiable dataset with a clear identifier and is used for exploring molecular properties.",
      "processing_time": 64.52346849441528,
      "citing_paper_id": "280010642",
      "cited_paper_id": 272886429
    },
    {
      "context_text": "LLM4Mat-Bench [157] offers a benchmark specifically focused on evaluating LLM capabilities in property prediction, particularly for crystalline materials.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a benchmark but does not refer to a specific, downloadable dataset. It is a benchmark suite used for evaluation, which is excluded according to the instructions.",
      "processing_time": 57.9593825340271,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273798212
    },
    {
      "context_text": "…books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼ 52k T1 MatSciNLP [156] NLP benchmark Text, Q&A ∼ 170k T1, T4, T5 LLM4Mat-Bench [157] LLM benchmark for property prediction Crystal composition, CIFs, textual descriptions",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LLM4Mat-Bench"
      ],
      "dataset_descriptions": {
        "LLM4Mat-Bench": "Used to benchmark large language models for materials property prediction, focusing on crystal composition, CIFs, and textual descriptions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets and benchmarks, but only 'LLM4Mat-Bench' is a specific, verifiable dataset with a clear identifier and is relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 66.7647933959961,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273798212
    },
    {
      "context_text": "Finally, Marcato FM [27] is a cross-modal encoder-decoder model for material failure prediction, combining simulation outputs, structure, and grid-based fields for robust forecasting of fracture behavior.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Marcato FM) and its application. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.58741068840027,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273993329
    },
    {
      "context_text": "A material-focused instruction Marcato FM [27] Encoder-Decoder + Llama-3 Text, simulation grids Self-collected simulations T6",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. 'Self-collected simulations' is a generic reference and does not meet the criteria for inclusion.",
      "processing_time": 57.2295446395874,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273993329
    },
    {
      "context_text": "…of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for materials science Literature text, props Self-collected data T1, T3, T4 MatPilot [40] Knowledge retriever + physical workstations + LLM Tabular data, text, graphs N/A T1, T5 ChemCrow [41] CoT reasoning + LLM Literature search, web…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Self-collected data' and 'Tabular data, text, graphs', which are too generic and do not meet the criteria for specific, verifiable datasets. No other specific datasets are mentioned.",
      "processing_time": 59.746551513671875,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273993701
    },
    {
      "context_text": "Similar to MatAgent, MatPilot [40] also employs human-in-the-loop approach, with a particular emphasis on literature search, scientific hypothesis generation, experimental scheme design, and autonomous experimental verification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called MatPilot. The context focuses on the capabilities and approach of MatPilot, which is similar to another system called MatAgent.",
      "processing_time": 59.322346687316895,
      "citing_paper_id": "280010642",
      "cited_paper_id": 273993701
    },
    {
      "context_text": "Another related survey by Han et al. [3] reviews recent advances in AI-driven inverse materials design, organizing models by material types or model architectures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of models and material types. No verifiable resources are identified.",
      "processing_time": 56.369537115097046,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274023350
    },
    {
      "context_text": "In addition to chemical compounds, PubChem [103] also provides 53M patents and 42M research papers.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "PubChem is mentioned as a source of patents and research papers, but it does not fit the criteria for a dataset as it is a database/repository. No specific dataset is named.",
      "processing_time": 58.451075077056885,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274140377
    },
    {
      "context_text": "For example, PubChem [103] is the largest open database of chemical information including name, formula, structure, and other identifiers.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem"
      ],
      "dataset_descriptions": {
        "PubChem": "PubChem is the largest open database of chemical information, including name, formula, structure, and other identifiers. It is not directly used in the research on planning capabilities of LLMs."
      },
      "confidence_score": 0.3,
      "reasoning": "PubChem is mentioned as a database, but it is not used in the context of planning capabilities of LLMs. It is a chemical information database, which is not directly relevant to the topic.",
      "processing_time": 67.3706226348877,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274140377
    },
    {
      "context_text": "Similarly, Van Herck et al. [76] study the performance of fine-tuning three open-source LLMs (GPT-J-6B [77], Llama3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT-J-6B, Llama). The context focuses on the performance of fine-tuning these models, which is not a dataset.",
      "processing_time": 59.14074373245239,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274220905
    },
    {
      "context_text": "Similarly, Van Herck et al. [76] study the performance of fine-tuning three open-source LLMs (GPT-J-6B [77], Llama3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT-J-6B, Llama). The context focuses on the performance of fine-tuning these models, which is not a dataset.",
      "processing_time": 59.14074373245239,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "Foundation models, particularly large language models (LLMs) and multimodal Transformers, offer a generalizable framework across these varied tasks and data types with minimal retraining [1, 34, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of foundation models and LLMs. No verifiable resources are identified.",
      "processing_time": 56.68882918357849,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "Foundation models, particularly large language models (LLMs) and multimodal Transformers, offer a generalizable framework across these varied tasks and data types with minimal retraining [1, 34, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of foundation models and LLMs. No verifiable resources are identified.",
      "processing_time": 56.68882918357849,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276855039
    },
    {
      "context_text": "Foundation models, particularly large language models (LLMs) and multimodal Transformers, offer a generalizable framework across these varied tasks and data types with minimal retraining [1, 34, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of foundation models and LLMs. No verifiable resources are identified.",
      "processing_time": 56.68882918357849,
      "citing_paper_id": "280010642",
      "cited_paper_id": 277356221
    },
    {
      "context_text": "…38k T1 PubChem [103] Proteins, genes, chemical structures, literature text SMILES, crystal structures, papers, patterns > 200M T1, T3, T4 MatbookQA [34] Q&A from MatSci books IFT, Q&A ∼ 2k T1 MaScQA [75] Q&A from engineering exams IFT, Q&A ∼ 1.5k T1 MatSci-Instruct [56] MatSci IFT data IFT, Q&A ∼…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubChem",
        "MatbookQA",
        "MaScQA",
        "MatSci-Instruct"
      ],
      "dataset_descriptions": {
        "PubChem": "Used to access chemical structures and literature text, including SMILES and crystal structures, for materials research.",
        "MatbookQA": "Used to generate Q&A pairs from materials science books, focusing on information retrieval and question answering tasks.",
        "MaScQA": "Used to create Q&A pairs from engineering exams, emphasizing information retrieval and question answering in materials science.",
        "MatSci-Instruct": "Used to provide instructional data for materials science, focusing on information retrieval and question answering tasks."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions several datasets, but they are not directly related to the planning capabilities of LLMs. However, they are specific and verifiable resources.",
      "processing_time": 79.26353096961975,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "LLaMat-CIF [34] is an instruction fine-tuned model with CIF data based on the pretrained LLaMat model.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'CIF data' but does not provide enough information to determine if it is a specific, verifiable dataset. The context also mentions LLaMat, which is a model, not a dataset.",
      "processing_time": 59.38357973098755,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "Other notable efforts include LLaMat and LLaMat-Chat [34], which are based on the Llama-2 and Llama-3 architectures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LLaMat and LLaMat-Chat, which are based on Llama-2 and Llama-3 architectures. However, these are models, not datasets, and thus do not meet the criteria for inclusion.",
      "processing_time": 60.28535270690918,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "Foundation models provide scalable and generalizable interfaces for integrating these tasks into adaptive pipelines [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to foundational large language models. No verifiable resources are identified.",
      "processing_time": 56.34735989570618,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274656164
    },
    {
      "context_text": "MoL-MoE [32] is introduced as a Multi-view Mixture-of-Experts framework to predict molecular properties by integrating latent spaces derived from SMILES, SELFIES, and molecular graphs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Multi-view Mixture-of-Experts framework) and the types of molecular representations used.",
      "processing_time": 57.20020532608032,
      "citing_paper_id": "280010642",
      "cited_paper_id": 274966586
    },
    {
      "context_text": "MatterChat [22] is a versatile structure-aware multi-modal LLM that unifies material structural data and textual user queries and employs a bridging module to align a pretrained universal machine learning interatomic potential with a pretrained LLM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MatterChat) and its components. No verifiable resources are identified.",
      "processing_time": 56.82580018043518,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276422100
    },
    {
      "context_text": "[68] is a multimodal fusion model that leverages LLMs to integrate diverse representations, such as SMILES, SELFIES, text descriptions, and molecular fingerprints, for property prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a multimodal fusion model and types of data representations used.",
      "processing_time": 55.79236960411072,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276741674
    },
    {
      "context_text": "Notably, Pyzer-Knapp et al. [1] focus on foundation models for materials discovery, categorizing them into four primary tasks: data extraction, property prediction, molecular generation, and synthesis prediction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only categories of tasks for foundation models in materials discovery.",
      "processing_time": 55.388493061065674,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276855039
    },
    {
      "context_text": "A widely accepted definition describes FMs as large, pretrained models capable of learning transferable representations across diverse modalities and domains that can be reused across a range of downstream applications [29, 1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of foundation models. No verifiable resources are identified.",
      "processing_time": 56.10300040245056,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276855039
    },
    {
      "context_text": "A widely accepted definition describes FMs as large, pretrained models capable of learning transferable representations across diverse modalities and domains that can be reused across a range of downstream applications [29, 1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general description of foundation models. No verifiable resources are identified.",
      "processing_time": 56.10300040245056,
      "citing_paper_id": "280010642",
      "cited_paper_id": 277356221
    },
    {
      "context_text": "Foundation models shift this paradigm by learning generalizable representations from large, diverse datasets, enabling transfer across tasks and domains [1, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to 'large, diverse datasets' in a generic sense.",
      "processing_time": 56.86581325531006,
      "citing_paper_id": "280010642",
      "cited_paper_id": 276855039
    },
    {
      "context_text": "Foundation models shift this paradigm by learning generalizable representations from large, diverse datasets, enabling transfer across tasks and domains [1, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to 'large, diverse datasets' in a generic sense.",
      "processing_time": 56.86581325531006,
      "citing_paper_id": "280010642",
      "cited_paper_id": 277356221
    },
    {
      "context_text": "These are sometimes referred to as \"small\" and \"big\" foundation models, respectively [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to 'small' and 'big' foundation models. No verifiable resources are identified.",
      "processing_time": 57.22684979438782,
      "citing_paper_id": "280010642",
      "cited_paper_id": 277356221
    },
    {
      "context_text": "To support the deployment of an LLM-based agentic system which connects LLM to external data resources, LlamaIndex [188] is specifically introduced to provide the solution for this task.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LlamaIndex) for connecting LLMs to external data resources.",
      "processing_time": 56.0881609916687,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "For the purpose of building a data pipeline and database, Emmet [185] and Maggma [177] are developed by Materials Project to support users with data queries, data transformations, and data storage.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to tools (Emmet and Maggma) used for data management.",
      "processing_time": 56.62120294570923,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "Similarly, AutoGen [186] and CrewAI [187] are popular frameworks for building role-playing LLM agents, facilitating the development of multi-agent systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only frameworks for building role-playing LLM agents.",
      "processing_time": 55.583815813064575,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "…LLM Structure, text MP T3, T4 ChatMOF [38] LLM as agent + LLM as evaluator + Tools Structure, text Collection of MOF databases T1, T3, T4 MatAgent [39] LLM + ML models for materials science Literature text, props Self-collected data T1, T3, T4 MatPilot [40] Knowledge retriever + physical…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Collection of MOF databases"
      ],
      "dataset_descriptions": {
        "Collection of MOF databases": "Used to train and evaluate LLMs in the context of materials science, focusing on the integration of LLMs with tools and databases to enhance materials discovery and evaluation."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Collection of MOF databases' which appears to be a specific dataset used in the research. However, the name is not a multi-word proper noun, uppercase acronym, or hyphenated name with digits, making it less specific.",
      "processing_time": 69.5248122215271,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "MatAgent [39] aims to cover a sufficient number of tasks in materials science, such as property prediction, hypothesis generation, experimental data analysis, high-performance alloy and polymer discovery, data-driven experimentation, and literature review automation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (MatAgent) and its applications in materials science.",
      "processing_time": 56.05778360366821,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "USPTO [123] offers Open Data Portal (ODP) for accessing US patterns, which enables researchers to discover and extract related information to specific domains.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Open Data Portal (ODP)' but does not specify its use as a dataset. It is described as a portal for accessing US patterns, which is too generic and lacks specific identifiers.",
      "processing_time": 58.999778032302856,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "This work studies the correlations between chemical motifs in reactants, reagents, and products in the USPTO dataset [123].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "USPTO dataset"
      ],
      "dataset_descriptions": {
        "USPTO dataset": "Used to study correlations between chemical motifs in reactants, reagents, and products, focusing on reaction mechanisms and synthetic pathways."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'USPTO dataset' which is a specific, verifiable dataset used in the study of chemical reactions.",
      "processing_time": 63.15677332878113,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, Ollama [180] provides a platform to deploy LLMs in local machines, covering a diverse set of open-source pretrained models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a platform for deploying LLMs. There are no verifiable resources that meet the criteria.",
      "processing_time": 57.038352727890015,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "Toolkits like ALIGNN-FF [179] extend message-passing neural networks to force field learning, with a focus on universal atomistic simulation capabilities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a toolkit (ALIGNN-FF) which is a method for force field learning using neural networks.",
      "processing_time": 57.22079396247864,
      "citing_paper_id": "280010642",
      "cited_paper_id": null
    },
    {
      "context_text": "In terms of parsing natural language text back into structured forms, the particular task we are interested in is converting plans generated by the LLM back into plan forms that can be used by plan validator tools like [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (VAL) for plan validation. The context focuses on the task of converting plans generated by LLMs into a form usable by plan validators.",
      "processing_time": 59.48622727394104,
      "citing_paper_id": "256846992",
      "cited_paper_id": 3098522
    },
    {
      "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AQUA",
        "SVAMP",
        "CommonsenseQA",
        "StrategyQA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills.",
        "AQUA": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps.",
        "SVAMP": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning.",
        "CommonsenseQA": "Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge.",
        "StrategyQA": "Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 84.3462164402008,
      "citing_paper_id": "256846992",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AQUA",
        "SVAMP",
        "CommonsenseQA",
        "StrategyQA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills.",
        "AQUA": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps.",
        "SVAMP": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning.",
        "CommonsenseQA": "Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge.",
        "StrategyQA": "Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 84.3462164402008,
      "citing_paper_id": "256846992",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "One could also use datasets like GSM8K [6], AQUA [19], SVAMP [22], CommonsenseQA [33] and StrategyQA [10] for testing different shallow reasoning abilities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AQUA",
        "SVAMP",
        "CommonsenseQA",
        "StrategyQA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to test shallow reasoning abilities in solving grade school math problems, focusing on step-by-step problem-solving skills.",
        "AQUA": "Applied to assess reasoning in algebraic word problems, emphasizing the ability to generate and follow logical steps.",
        "SVAMP": "Utilized to evaluate simple math word problems, specifically targeting the ability to handle arithmetic operations and basic reasoning.",
        "CommonsenseQA": "Employed to test commonsense reasoning, focusing on understanding and answering questions that require everyday knowledge.",
        "StrategyQA": "Used to evaluate implicit reasoning strategies in question answering, specifically assessing the ability to infer and apply logical reasoning."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets that are used for testing shallow reasoning abilities, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 84.3462164402008,
      "citing_paper_id": "256846992",
      "cited_paper_id": 232223322
    },
    {
      "context_text": "We will show that the choice of the speciﬁc LLM (we experimented with two versions of GPT3 [3, 21] as well as BLOOM [28]), as well as ﬁne tuning seems to have little effect on this dismal performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (GPT3, BLOOM). The citation is focused on the performance of different LLMs rather than the use of a specific dataset.",
      "processing_time": 59.47473335266113,
      "citing_paper_id": "256846992",
      "cited_paper_id": 36102848
    },
    {
      "context_text": "We will show that the choice of the speciﬁc LLM (we experimented with two versions of GPT3 [3, 21] as well as BLOOM [28]), as well as ﬁne tuning seems to have little effect on this dismal performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (GPT3, BLOOM). The citation is focused on the performance of different LLMs rather than the use of a specific dataset.",
      "processing_time": 59.47473335266113,
      "citing_paper_id": "256846992",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "The current method is going to operate almost exclusively on symbolic models (specifically ones specified using PDDL [1]) and other structured inputs compatible with such representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a language (PDDL) used for specifying symbolic models. PDDL is a method, not a dataset.",
      "processing_time": 57.61156868934631,
      "citing_paper_id": "256846992",
      "cited_paper_id": 59656859
    },
    {
      "context_text": "Of particular interest to us in this paper is the thread of efforts that aim to investigate (and showcase) reasoning abilities of LLMs–including commonsense reasoning [33, 27, 10], logical reasoning [31], and even ethical reasoning [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various types of reasoning abilities being investigated in LLMs.",
      "processing_time": 55.80656862258911,
      "citing_paper_id": "256846992",
      "cited_paper_id": 230799347
    },
    {
      "context_text": "Of particular interest to us in this paper is the thread of efforts that aim to investigate (and showcase) reasoning abilities of LLMs–including commonsense reasoning [33, 27, 10], logical reasoning [31], and even ethical reasoning [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various types of reasoning abilities being investigated in LLMs.",
      "processing_time": 55.80656862258911,
      "citing_paper_id": "256846992",
      "cited_paper_id": 238857096
    },
    {
      "context_text": "In conclusion, we hope that this benchmark 6 encourages other researchers to test the capabilities of their systems across different LLM models [5, 8, 30, 37, 25, 34, 12] and even those that are ﬁne-tuned for such tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to testing capabilities of LLM models. No verifiable resources are identified.",
      "processing_time": 56.97417855262756,
      "citing_paper_id": "256846992",
      "cited_paper_id": 245353475
    },
    {
      "context_text": "In conclusion, we hope that this benchmark 6 encourages other researchers to test the capabilities of their systems across different LLM models [5, 8, 30, 37, 25, 34, 12] and even those that are ﬁne-tuned for such tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to testing capabilities of LLM models. No verifiable resources are identified.",
      "processing_time": 56.97417855262756,
      "citing_paper_id": "256846992",
      "cited_paper_id": 248496292
    },
    {
      "context_text": "Most of them, such as [14, 2] focus on commonsense domains (e.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a vague reference to 'commonsense domains'. No clear, verifiable resources are identified.",
      "processing_time": 56.79355597496033,
      "citing_paper_id": "256846992",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Some prominent existing benchmarks include, BIG-BENCH [31] and Coin Flip [35].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not specify their use in the research context. No specific datasets are named or described.",
      "processing_time": 55.620426177978516,
      "citing_paper_id": "256846992",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "The macro-tenor of the drumbeat of these works has been suggesting that LLM’s are indeed capable of doing such kinds of reasoning [18, 35, 5].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works suggesting LLM capabilities.",
      "processing_time": 55.113914012908936,
      "citing_paper_id": "256846992",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "Most of them, such as [14, 2] focus on commonsense domains (e.g. moving things in kitchens) and thus evaluate “zero shot” capabilities of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general domain of evaluation for LLMs. No clear, verifiable resource names are provided.",
      "processing_time": 57.186397314071655,
      "citing_paper_id": "256846992",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "3 Not surprisingly, the works such as SayCan [2], that actually care about executability, limit themselves to using LLMs in what we are calling “heuristic-mode\"–with the actions suggested by the LLM being vetted by the underlying sound planner or a reinforcement learner with access to a faithful…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SayCan) and a general approach to using LLMs. There are no clear identifiers for datasets.",
      "processing_time": 58.138941287994385,
      "citing_paper_id": "256846992",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "This is the question handled by works like decision transformer [4] and GATO [26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 56.851171255111694,
      "citing_paper_id": "256846992",
      "cited_paper_id": 248722148
    },
    {
      "context_text": "For the retrieval model, we utilize Mpnet-v2 (Song et al., 2020) with FAISS (Johnson et al., 2019) for efficient indexing.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and tools (Mpnet-v2, FAISS) but does not reference any specific datasets.",
      "processing_time": 55.627851724624634,
      "citing_paper_id": "279154700",
      "cited_paper_id": 215827489
    },
    {
      "context_text": "Large Language Models (LLMs) are revolutionizing Natural Language Processing (NLP), demon-* strating remarkable capabilities in tasks, such as text comprehension, explanation, and generation (Schaeffer et al., 2023; Qi et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. No verifiable resources are identified.",
      "processing_time": 56.21714472770691,
      "citing_paper_id": "279154700",
      "cited_paper_id": 258418299
    },
    {
      "context_text": "However, the issue of \"hallucination\", where LLMs generate factually inaccurate content, remains a critical challenge, especially in specialized domains (Rawte et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue of hallucination in LLMs. No verifiable resources are identified.",
      "processing_time": 57.17616248130798,
      "citing_paper_id": "279154700",
      "cited_paper_id": 261696947
    },
    {
      "context_text": "Retrieval-Augmented Generation (RAG) improves factual consistency by retrieving relevant information from external textual corpora (Asai et al., 2024; Jeong et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'external textual corpora' but does not specify any named datasets. The context focuses on the method (RAG) rather than a specific dataset.",
      "processing_time": 57.949726819992065,
      "citing_paper_id": "279154700",
      "cited_paper_id": 264288947
    },
    {
      "context_text": "To mitigate this issue, incorporating external knowledge has emerged as a key strategy for enhancing LLM reliability (Gao et al., 2023; Qi et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general strategy for enhancing LLM reliability through external knowledge.",
      "processing_time": 55.8459529876709,
      "citing_paper_id": "279154700",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "To mitigate this issue, incorporating external knowledge has emerged as a key strategy for enhancing LLM reliability (Gao et al., 2023; Qi et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general strategy for enhancing LLM reliability through external knowledge.",
      "processing_time": 55.8459529876709,
      "citing_paper_id": "279154700",
      "cited_paper_id": 268247491
    },
    {
      "context_text": "• Text RAG (Gao et al., 2023): This method treats external graphs as textual corpora and employs a retriever to extract relevant information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Text RAG) and its functionality, which is not a dataset.",
      "processing_time": 57.51178050041199,
      "citing_paper_id": "279154700",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "Motivated by this, Graph Retrieval-Augmented Generation (GraphRAG) has been proposed to explicitly model knowledge relationships during retrieval, enabling LLMs to more accurately access and leverage structured knowledge (Edge et al., 2024; Wu et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GraphRAG) and its purpose. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.95702338218689,
      "citing_paper_id": "279154700",
      "cited_paper_id": 269363075
    },
    {
      "context_text": "1-70B-Instruct ( Dubey et al., 2024), and Gemma-2-9b-it (Team et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. The titles do not provide additional context to identify datasets.",
      "processing_time": 56.32915639877319,
      "citing_paper_id": "279154700",
      "cited_paper_id": 271571434
    },
    {
      "context_text": "…retrieved by these methods are typically passed to LLMs in the form of linearized text (Mavromatis and Karypis, 2024; Liu et al., 2024a) or code (Cai et al., 2024; Skianis et al., 2024), which often results in the loss of critical structural information and weakens reasoning performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their impact on LLM reasoning performance.",
      "processing_time": 55.36144471168518,
      "citing_paper_id": "279154700",
      "cited_paper_id": 272911169
    },
    {
      "context_text": "Moreover, the inherent non-linear nature of graph structures and the linear understanding of text by language models create a natural gap (Choudhary and Reddy, 2023; Wang et al., 2024), further exacerbating semantic understanding deviations and making it difficult to ensure the accuracy and…",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only discusses a gap between graph structures and language models' linear understanding.",
      "processing_time": 56.32360601425171,
      "citing_paper_id": "279154700",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, the inherent non-linear nature of graph structures and the linear understanding of text by language models create a natural gap (Choudhary and Reddy, 2023; Wang et al., 2024), further exacerbating semantic understanding deviations and making it difficult to ensure the accuracy and…",
      "catation_intent": "research work",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only discusses a gap between graph structures and language models' linear understanding.",
      "processing_time": 56.32360601425171,
      "citing_paper_id": "279154700",
      "cited_paper_id": null
    },
    {
      "context_text": "…it still faces two major challenges: 1) Inefficient Information Aggregation : Current methods (e.g., Jin et al., 2024; Markowitz et al., 2024; Luo et al., 2024; Chen et al., 2024; Sun et al.; Jiang et al., 2023) typically rely on a single agent to extract information in a fixed pattern,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.30349016189575,
      "citing_paper_id": "279154700",
      "cited_paper_id": null
    },
    {
      "context_text": "5-72B-Instruct ( Yang et al., 2024), Llama-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.94312524795532,
      "citing_paper_id": "279154700",
      "cited_paper_id": null
    },
    {
      "context_text": "…non-linear nature of graph structures and the linear understanding of text by language models create a natural gap (Choudhary and Reddy, 2023; Wang et al., 2024), further exacerbating semantic understanding deviations and making it difficult to ensure the accuracy and consistency of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses a conceptual gap between graph structures and language models.",
      "processing_time": 55.937790870666504,
      "citing_paper_id": "279154700",
      "cited_paper_id": null
    },
    {
      "context_text": "Removing edge features or global attention re-sults in significant performance drops, especially on AQUA, emphasizing their importance for reasoning.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'AQUA' but does not provide enough information to determine if it is a dataset, benchmark, or something else. The cited paper title suggests it might be related to algebraic word problems, but there is no explicit mention of a dataset.",
      "processing_time": 61.108445167541504,
      "citing_paper_id": "276259098",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "The in-context learning (ICL) examples are adapted from OpenCompass (Contrib-utors, 2023) for GSM8K and MATH500, and from LLM-Reasoner (Hao et al., 2024) for AQUA.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH500",
        "AQUA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate in-context learning for solving grade school math problems, focusing on step-by-step reasoning and problem-solving strategies.",
        "MATH500": "Used to assess in-context learning for algebraic word problems, emphasizing the generation of rationales and solution steps.",
        "AQUA": "Used to test in-context learning for arithmetic and quantitative reasoning, focusing on multi-step problem-solving and logical deduction."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets (GSM8K, MATH500, AQUA) used for in-context learning examples. These datasets are relevant to the planning capabilities of LLMs.",
      "processing_time": 78.43106174468994,
      "citing_paper_id": "276259098",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "For mathematical reasoning, we evaluate our framework on the GSM8K (Cobbe et al., 2021), MATH500 (Hendrycks et al., 2021; Lightman et al., 2023), and AQUA (Ling et al., 2017) datasets, using 4-shot settings for GSM8K and MATH500, and a 10-shot setting for AQUA.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH500",
        "AQUA"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate mathematical reasoning capabilities with a 4-shot setting, focusing on solving algebraic word problems.",
        "MATH500": "Used to assess mathematical reasoning skills with a 4-shot setting, emphasizing problem-solving and explanation generation.",
        "AQUA": "Used to test mathematical reasoning abilities with a 10-shot setting, focusing on a variety of algebraic and quantitative problems."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for evaluating a framework on mathematical reasoning tasks, providing clear details on the shot settings used for each.",
      "processing_time": 75.6244204044342,
      "citing_paper_id": "276259098",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "Table 2 shows evaluation results on AQUA with varying tree breadths.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'AQUA' but does not provide enough information to determine if it is a dataset, method, or something else. The cited paper title does not help in disambiguating the type.",
      "processing_time": 59.252312898635864,
      "citing_paper_id": "276259098",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "AQUA AQUA (Ling et al., 2017) is a dataset comprising algebraic word problems and reasoning-based multiple-choice questions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AQUA"
      ],
      "dataset_descriptions": {
        "AQUA": "Used to train and evaluate models on solving algebraic word problems and reasoning-based multiple-choice questions, focusing on the ability to generate correct answers and explanations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly identifies AQUA as a dataset with algebraic word problems and reasoning-based multiple-choice questions, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 65.35139727592468,
      "citing_paper_id": "276259098",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "…in black-box optimization (Hart et al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning introduces distinct challenges: First, the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general contexts of black-box optimization and reinforcement learning. No verifiable resources are identified.",
      "processing_time": 57.16542387008667,
      "citing_paper_id": "276259098",
      "cited_paper_id": 33081038
    },
    {
      "context_text": "…optimization (Hart et al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning introduces distinct challenges: First, the potential sequences…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only optimization and reinforcement learning contexts. No verifiable resources are identified.",
      "processing_time": 56.14749312400818,
      "citing_paper_id": "276259098",
      "cited_paper_id": 198967841
    },
    {
      "context_text": "While these methods have proven effective in black-box optimization (Hart et al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their applications. The context is focused on the effectiveness of certain methods in black-box optimization and reinforcement learning, which are not directly related to the planning capabilities of LLMs.",
      "processing_time": 60.17750835418701,
      "citing_paper_id": "276259098",
      "cited_paper_id": 206799161
    },
    {
      "context_text": "While these methods have proven effective in black-box optimization (Hart et al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their applications. The context is focused on the effectiveness of certain methods in black-box optimization and reinforcement learning, which are not directly related to the planning capabilities of LLMs.",
      "processing_time": 60.17750835418701,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258509700
    },
    {
      "context_text": "…al., 1968; Zhai & Gao, 2022; Malherbe et al., 2022; Wang et al., 2024c; 2020) and reinforcement learning (Silver et al., 2017; Kartal et al., 2019; Grill et al., 2020) contexts, their application to LLM reasoning introduces distinct challenges: First, the potential sequences of tokens, phrases,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 55.919700145721436,
      "citing_paper_id": "276259098",
      "cited_paper_id": 220769401
    },
    {
      "context_text": "Various GT architectures have been proposed, from the initial Fully-connected Graph Transformer with basic positional encodings (Dwivedi & Bresson, 2020), to more sophisticated designs like SAN with invariant eigenvector aggregation (Kreuzer et al., 2021), and Graphormer with distance-based…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.73749256134033,
      "citing_paper_id": "276259098",
      "cited_paper_id": 229298019
    },
    {
      "context_text": "GraphTrans (Wu et al., 2021) introduces the first hybrid architecture, which combines local message passing with global attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GraphTrans) and its components. The context is focused on describing the architecture and its features.",
      "processing_time": 57.421627044677734,
      "citing_paper_id": "276259098",
      "cited_paper_id": 246210055
    },
    {
      "context_text": "Similarly, on GSM8K, MCTS uses 13.33 times the tokens of CoT, while PGTS requires just 1.29 times.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to compare token usage between MCTS and PGTS methods, focusing on efficiency in numerical reasoning tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions GSM8K, which is a known dataset for numerical reasoning tasks. However, it does not provide specific details on how the dataset is used beyond token comparisons.",
      "processing_time": 63.8766303062439,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Similarly, on GSM8K, MCTS uses 13.33 times the tokens of CoT, while PGTS requires just 1.29 times.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to compare token usage between MCTS and PGTS methods, focusing on efficiency in numerical reasoning tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions GSM8K, which is a known dataset for numerical reasoning tasks. However, it does not provide specific details on how the dataset is used beyond token comparisons.",
      "processing_time": 63.8766303062439,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Specifically, CoT aggregate outcomes from multiple chains using majority voting, whereas MCTS and PGTS utilize weighted voting based on the reward of each reasoning chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for aggregating outcomes from reasoning chains.",
      "processing_time": 54.994566440582275,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Specifically, CoT aggregate outcomes from multiple chains using majority voting, whereas MCTS and PGTS utilize weighted voting based on the reward of each reasoning chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods for aggregating outcomes from reasoning chains.",
      "processing_time": 54.994566440582275,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "1-8B, PGTS achieves a 41.00% accuracy on MATH, significantly improving upon CoT’s 34.40% while using only one-third of the tokens required by MCTS.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate numerical reasoning capabilities of PGTS, comparing its accuracy and token efficiency against CoT and MCTS methods."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'MATH' as a dataset where PGTS achieves a certain accuracy. This is likely a specific dataset used for evaluating numerical reasoning capabilities.",
      "processing_time": 62.98349738121033,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "1-8B, PGTS achieves a 41.00% accuracy on MATH, significantly improving upon CoT’s 34.40% while using only one-third of the tokens required by MCTS.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate numerical reasoning capabilities of PGTS, comparing its accuracy and token efficiency against CoT and MCTS methods."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'MATH' as a dataset where PGTS achieves a certain accuracy. This is likely a specific dataset used for evaluating numerical reasoning capabilities.",
      "processing_time": 62.98349738121033,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "In summary, this work represents a meaningful step toward improving LLM reasoning capabilities, with both ethical implications and societal benefits that warrant further exploration and thoughtful application. for t = 0 , 1 , . . . , T max do 5: Extract valid actions and construct the constraints vector c 6: Compute policy logits π ← π θ ( s t , c ) Policy Update: Update policy parameters: Value Function Update: Update value parameters: ψ ← ψ − η V ∇ ψ L V 25: end while B. Related Works LLM Reasoning In the context of LLM reasoning, Chain-of-Thought (CoT) resoning (Wei et al., 2022) serves as a foundational technique that decomposes problems into intermediate steps, facilitating step-by-step reasoning that mimics human problem-solving strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and methods. The focus is on the Chain-of-Thought reasoning technique and its application in LLMs.",
      "processing_time": 57.7123122215271,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "In summary, this work represents a meaningful step toward improving LLM reasoning capabilities, with both ethical implications and societal benefits that warrant further exploration and thoughtful application. for t = 0 , 1 , . . . , T max do 5: Extract valid actions and construct the constraints vector c 6: Compute policy logits π ← π θ ( s t , c ) Policy Update: Update policy parameters: Value Function Update: Update value parameters: ψ ← ψ − η V ∇ ψ L V 25: end while B. Related Works LLM Reasoning In the context of LLM reasoning, Chain-of-Thought (CoT) resoning (Wei et al., 2022) serves as a foundational technique that decomposes problems into intermediate steps, facilitating step-by-step reasoning that mimics human problem-solving strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and methods. The focus is on the Chain-of-Thought reasoning technique and its application in LLMs.",
      "processing_time": 57.7123122215271,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "LLM Reasoning LLM reasoning has advanced through techniques such as CoT (Wei et al., 2022), ToT (Yao et al., 2024), and programmatic reasoning paradigms (Chen et al., 2022; Sel et al., 2023), fostering structured and iterative problem-solving.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.93013286590576,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "LLM Reasoning LLM reasoning has advanced through techniques such as CoT (Wei et al., 2022), ToT (Yao et al., 2024), and programmatic reasoning paradigms (Chen et al., 2022; Sel et al., 2023), fostering structured and iterative problem-solving.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.93013286590576,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "MCTS inherently explores multiple reasoning chains by traversing different paths during search, while we enhance CoT and our PGTS with self-consistency (SC) (Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'self-consistency (SC)' and cites Wang et al. (2022), but does not mention any specific datasets. The focus is on methods and techniques.",
      "processing_time": 58.029460191726685,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "MCTS inherently explores multiple reasoning chains by traversing different paths during search, while we enhance CoT and our PGTS with self-consistency (SC) (Wang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'self-consistency (SC)' and cites Wang et al. (2022), but does not mention any specific datasets. The focus is on methods and techniques.",
      "processing_time": 58.029460191726685,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Tree-of-Thought (Yao et al., 2024) extends CoT by organizing reasoning processes into a tree structure, where each node represents a partial solution and edges signify transitions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Tree-of-Thought) that extends another method (CoT).",
      "processing_time": 56.38803005218506,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Tree-of-Thought (Yao et al., 2024) extends CoT by organizing reasoning processes into a tree structure, where each node represents a partial solution and edges signify transitions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Tree-of-Thought) that extends another method (CoT).",
      "processing_time": 56.38803005218506,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "We compare PGTS against CoT and MCTS baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PGTS, CoT, MCTS). No verifiable resources are identified.",
      "processing_time": 56.80270862579346,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "We compare PGTS against CoT and MCTS baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (PGTS, CoT, MCTS). No verifiable resources are identified.",
      "processing_time": 56.80270862579346,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Across all datasets, PGTS consistently outperforms CoT, demonstrating the effectiveness of its learned policy in guiding reasoning steps.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only a general reference to 'all datasets'. No clear, verifiable resource names are provided.",
      "processing_time": 57.25766468048096,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Across all datasets, PGTS consistently outperforms CoT, demonstrating the effectiveness of its learned policy in guiding reasoning steps.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only a general reference to 'all datasets'. No clear, verifiable resource names are provided.",
      "processing_time": 57.25766468048096,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "For instance, on MATH, MCTS requires 16.25 times more tokens than CoT, whereas PGTS achieves competitive performance with only a 5.28 times increase.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'MATH' which could be a dataset, but it lacks specificity and is not clearly identified as a dataset. No other datasets are mentioned.",
      "processing_time": 57.527915954589844,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For instance, on MATH, MCTS requires 16.25 times more tokens than CoT, whereas PGTS achieves competitive performance with only a 5.28 times increase.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'MATH' which could be a dataset, but it lacks specificity and is not clearly identified as a dataset. No other datasets are mentioned.",
      "processing_time": 57.527915954589844,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "…value parameters: ψ ← ψ − η V ∇ ψ L V 25: end while B. Related Works LLM Reasoning In the context of LLM reasoning, Chain-of-Thought (CoT) resoning (Wei et al., 2022) serves as a foundational technique that decomposes problems into intermediate steps, facilitating step-by-step reasoning that…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought reasoning).",
      "processing_time": 55.286038637161255,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "First, advanced prompting techniques like Chain-of-Thought (CoT) (Wei et al., 2022) and Least-to-Most prompting (Zhou et al., 2022) encourage step-by-step reasoning by generating intermediate steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prompting techniques. No verifiable resources are identified.",
      "processing_time": 55.3582820892334,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "First, advanced prompting techniques like Chain-of-Thought (CoT) (Wei et al., 2022) and Least-to-Most prompting (Zhou et al., 2022) encourage step-by-step reasoning by generating intermediate steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prompting techniques. No verifiable resources are identified.",
      "processing_time": 55.3582820892334,
      "citing_paper_id": "276259098",
      "cited_paper_id": 248986239
    },
    {
      "context_text": "First, advanced prompting techniques like Chain-of-Thought (CoT) (Wei et al., 2022) and Least-to-Most prompting (Zhou et al., 2022) encourage step-by-step reasoning by generating intermediate steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prompting techniques. No verifiable resources are identified.",
      "processing_time": 55.3582820892334,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Since its introduction, numerous enhancements to CoT have been proposed, including Zero-Shot-CoT (Kojima et al., 2022), Self-Consistency with CoT (Wang et al., 2022), AutoCoT (Zhang et al., 2022), and VerifyCoT (Zhao et al., 2023), among others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models related to Chain of Thought (CoT) reasoning in language models.",
      "processing_time": 56.91349148750305,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Since its introduction, numerous enhancements to CoT have been proposed, including Zero-Shot-CoT (Kojima et al., 2022), Self-Consistency with CoT (Wang et al., 2022), AutoCoT (Zhang et al., 2022), and VerifyCoT (Zhao et al., 2023), among others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models related to Chain of Thought (CoT) reasoning in language models.",
      "processing_time": 56.91349148750305,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "For instance, on the MATH dataset, PGTS improves accuracy from CoT’s 34.40% to 41.40% in the 8B setting, illustrating its ability to explore high-quality reasoning chains.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate PGTS's improvement over CoT in numerical reasoning tasks, specifically measuring accuracy in generating high-quality reasoning chains."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MATH dataset, which is used to evaluate the performance of PGTS compared to CoT in numerical reasoning tasks.",
      "processing_time": 62.56398630142212,
      "citing_paper_id": "276259098",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "For instance, on the MATH dataset, PGTS improves accuracy from CoT’s 34.40% to 41.40% in the 8B setting, illustrating its ability to explore high-quality reasoning chains.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate PGTS's improvement over CoT in numerical reasoning tasks, specifically measuring accuracy in generating high-quality reasoning chains."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MATH dataset, which is used to evaluate the performance of PGTS compared to CoT in numerical reasoning tasks.",
      "processing_time": 62.56398630142212,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Least-to-Most Prompting (Zhou et al., 2022) represents another influential paradigm that iteratively constructs solutions by dividing complex problems into simpler tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Least-to-Most Prompting'. The context focuses on the method's ability to enable complex reasoning in large language models.",
      "processing_time": 58.22183895111084,
      "citing_paper_id": "276259098",
      "cited_paper_id": 248986239
    },
    {
      "context_text": "For logical reasoning, we evaluate the PrOntoQA (Saparov & He, 2022) dataset for logical deduction in a 5-shot setting and the GPQA (Rein et al., 2023) dataset for graduate-level multiple-choice questions in a 0-shot setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PrOntoQA",
        "GPQA"
      ],
      "dataset_descriptions": {
        "PrOntoQA": "Used to evaluate logical deduction in a 5-shot setting, focusing on the ability of language models to perform step-by-step reasoning.",
        "GPQA": "Used to assess performance on graduate-level multiple-choice questions in a 0-shot setting, testing the model's ability to answer complex questions without prior exposure."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PrOntoQA and GPQA, which are used for evaluating logical reasoning capabilities of language models in different settings.",
      "processing_time": 71.08923149108887,
      "citing_paper_id": "276259098",
      "cited_paper_id": 252693237
    },
    {
      "context_text": "For logical reasoning, we evaluate the PrOntoQA (Saparov & He, 2022) dataset for logical deduction in a 5-shot setting and the GPQA (Rein et al., 2023) dataset for graduate-level multiple-choice questions in a 0-shot setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PrOntoQA",
        "GPQA"
      ],
      "dataset_descriptions": {
        "PrOntoQA": "Used to evaluate logical deduction in a 5-shot setting, focusing on the ability of language models to perform step-by-step reasoning.",
        "GPQA": "Used to assess performance on graduate-level multiple-choice questions in a 0-shot setting, testing the model's ability to answer complex questions without prior exposure."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PrOntoQA and GPQA, which are used for evaluating logical reasoning capabilities of language models in different settings.",
      "processing_time": 71.08923149108887,
      "citing_paper_id": "276259098",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "Building on this problem decomposition paradigm, Program-of-Thought (Chen et al., 2022), Chain-of-Code (Li et al., 2023), Buffer-of-Thought (Yang et al., 2024), and algorithm-of-Thought (Sel et al., 2023) incorporate programmatic reasoning steps to further enhance problem-solving.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.582775592803955,
      "citing_paper_id": "276259098",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "Building on this problem decomposition paradigm, Program-of-Thought (Chen et al., 2022), Chain-of-Code (Li et al., 2023), Buffer-of-Thought (Yang et al., 2024), and algorithm-of-Thought (Sel et al., 2023) incorporate programmatic reasoning steps to further enhance problem-solving.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.582775592803955,
      "citing_paper_id": "276259098",
      "cited_paper_id": 268230308
    },
    {
      "context_text": "To evaluate the quality of each reasoning step, a process reward model (PRM) R ( s, a ) is employed, implemented either as a pretrained reward model (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2024b) or an LLM-based evaluator (Yao et al., 2024; Gao et al., 2024; Hao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of pretrained reward models and LLM-based evaluators for evaluating reasoning steps.",
      "processing_time": 57.70196747779846,
      "citing_paper_id": "276259098",
      "cited_paper_id": 254017497
    },
    {
      "context_text": "To evaluate the quality of each reasoning step, a process reward model (PRM) R ( s, a ) is employed, implemented either as a pretrained reward model (Lightman et al., 2023; Uesato et al., 2022; Wang et al., 2024b) or an LLM-based evaluator (Yao et al., 2024; Gao et al., 2024; Hao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of pretrained reward models and LLM-based evaluators for evaluating reasoning steps.",
      "processing_time": 57.70196747779846,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "In mathematical problem-solving, for instance, while LLMs excel at direct arithmetic calculations, they frequently falter when faced with multi-step word problems that demand strategic decomposition and careful planning (Kao et al., 2024; Huang & Chang, 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general observations about the capabilities of LLMs in solving math problems.",
      "processing_time": 56.11684250831604,
      "citing_paper_id": "276259098",
      "cited_paper_id": 254877753
    },
    {
      "context_text": "In mathematical problem-solving, for instance, while LLMs excel at direct arithmetic calculations, they frequently falter when faced with multi-step word problems that demand strategic decomposition and careful planning (Kao et al., 2024; Huang & Chang, 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general observations about the capabilities of LLMs in solving math problems.",
      "processing_time": 56.11684250831604,
      "citing_paper_id": "276259098",
      "cited_paper_id": 271051190
    },
    {
      "context_text": "Depth-First Search (DFS) (Yao et al., 2024) and Breadth-First Search (BFS) (Xie et al., 2024b) offer systematic exploration strategies, while A ∗ Search (Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a) incorporate heuristics and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.67100191116333,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "Depth-First Search (DFS) (Yao et al., 2024) and Breadth-First Search (BFS) (Xie et al., 2024b) offer systematic exploration strategies, while A ∗ Search (Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a) incorporate heuristics and sampling to balance exploration with exploitation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.66713047027588,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "Depth-First Search (DFS) (Yao et al., 2024) and Breadth-First Search (BFS) (Xie et al., 2024b) offer systematic exploration strategies, while A ∗ Search (Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a) incorporate heuristics and sampling to balance exploration with exploitation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.66713047027588,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.98940968513489,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.98940968513489,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865395
    },
    {
      "context_text": "Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.98940968513489,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.98940968513489,
      "citing_paper_id": "276259098",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "Third, tree-based search methods reframe reasoning as a planning problem, using reward signals to guide the exploration of reasoning paths (Feng et al., 2023; Yao et al., 2024; Besta et al., 2024; Hao et al., 2023; Xie et al., 2024b; Khalifa et al., 2023; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.98940968513489,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "Second, the framework’s backtracking capability allows recovery from suboptimal paths, addressing a common limitation of traditional search methods, such as Depth-Frist Search (DFS) and Breadth-First Search (BFS) (Yao et al., 2024; Xie et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only search methods and algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.81723499298096,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "Second, the framework’s backtracking capability allows recovery from suboptimal paths, addressing a common limitation of traditional search methods, such as Depth-Frist Search (DFS) and Breadth-First Search (BFS) (Yao et al., 2024; Xie et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only search methods and algorithms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.81723499298096,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "The challenge becomes even more pronounced when using self-evaluation as a reward signal, making the process even more resource-intensive (Feng et al., 2023; Hao et al., 2023; Xie et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the challenges of using self-evaluation as a reward signal in the context of reasoning and planning with language models.",
      "processing_time": 57.564409255981445,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "The challenge becomes even more pronounced when using self-evaluation as a reward signal, making the process even more resource-intensive (Feng et al., 2023; Hao et al., 2023; Xie et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the challenges of using self-evaluation as a reward signal in the context of reasoning and planning with language models.",
      "processing_time": 57.564409255981445,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Recent innovations include heuristic search methods like MCTS (Feng et al., 2023; Hao et al., 2023) and A ∗ search (Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and algorithms. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.028358697891235,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Recent innovations include heuristic search methods like MCTS (Feng et al., 2023; Hao et al., 2023) and A ∗ search (Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and algorithms. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.028358697891235,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "This limitation is particularly pronounced in tasks requiring intricate planning and reasoning (Stechly et al., 2024; Huang et al., 2023; Hong et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to limitations in tasks requiring intricate planning and reasoning.",
      "processing_time": 55.92239427566528,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Recent research has also explored heuristic search techniques, such as Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Hao et al., 2023) and A ∗ Search (Wang et al., 2024a), to identify optimal reasoning chains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only heuristic search techniques and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.20266842842102,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Recent research has also explored heuristic search techniques, such as Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Hao et al., 2023) and A ∗ Search (Wang et al., 2024a), to identify optimal reasoning chains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only heuristic search techniques and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.20266842842102,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "Unlike RAP (Hao et al., 2023), which incorporates a world model to simulate environment states after each action, our approach directly focuses on reasoning in the generated text without state modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two approaches to planning capabilities in language models.",
      "processing_time": 55.69708728790283,
      "citing_paper_id": "276259098",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "To address this, Graph-of-Thought (Besta et al., 2024) generalizes ToT by modeling reasoning as a graph, allowing dynamic path selection, backtracking, and aggregation of information across multiple paths.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Graph-of-Thought. The context focuses on the methodology and its capabilities.",
      "processing_time": 56.796244621276855,
      "citing_paper_id": "276259098",
      "cited_paper_id": 261030303
    },
    {
      "context_text": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks, driven by advances in model architecture, parameter scaling, and pretraining data (Achiam et al., 2023; Team et al., 2023; Jiang et al., 2023; Dubey et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in LLMs. No verifiable resources are identified.",
      "processing_time": 56.323877573013306,
      "citing_paper_id": "276259098",
      "cited_paper_id": 263830494
    },
    {
      "context_text": "This can be attributed to the inherent complexity of GPQA, which includes diverse topics and is curated to challenge even non-expert humans.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions GPQA, which is described as a benchmark. However, it is not clear if GPQA is a specific, downloadable dataset or just a benchmark suite. Given the lack of clear evidence of it being a reusable dataset, it is excluded.",
      "processing_time": 60.598825216293335,
      "citing_paper_id": "276259098",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "Moreover, the limited training data available for PGTS, given the task complexity, makes it difficult to fully capture the intricacies of GPQA.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PGTS' and 'GPQA' but does not specify any datasets. The term 'limited training data' is too generic and lacks a specific identifier.",
      "processing_time": 58.26291584968567,
      "citing_paper_id": "276259098",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "However, one exception is the GPQA task, where PGTS underperforms MCTS.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between PGTS and MCTS on the GPQA task.",
      "processing_time": 56.43506383895874,
      "citing_paper_id": "276259098",
      "cited_paper_id": 265295009
    },
    {
      "context_text": "Meanwhile, Skeleton-of-Thought (Ning et al., 2024) reduces latency by generating a skeleton outline of answers before completing the details in parallel.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Skeleton-of-Thought'. No verifiable resources are identified.",
      "processing_time": 56.62271189689636,
      "citing_paper_id": "276259098",
      "cited_paper_id": 268230308
    },
    {
      "context_text": "2.3.2 for the formal definition of this Tree Search MDP.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formal definition of a Tree Search MDP. No verifiable resources are identified.",
      "processing_time": 57.03436207771301,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "This work aims to advance the field of reasoning with large language models (LLMs) by introducing the Policy-Guided Tree Search (PGTS) framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method or framework called Policy-Guided Tree Search (PGTS).",
      "processing_time": 57.21164655685425,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "In this work, we introduced Policy-Guided Tree Search (PGTS), a novel framework for reasoning with large language models that combines the efficiency of policy-guided exploration with the structured advantages of tree search.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Policy-Guided Tree Search).",
      "processing_time": 56.186933517456055,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "To address these challenges, we propose Policy-Guided Tree Search (PGTS), a framework that integrates reinforcement learning with structured tree exploration.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a proposed method (Policy-Guided Tree Search).",
      "processing_time": 56.773247957229614,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "In practice, actions can represent tokens, phrases, or complete sentences to improve optimization efficiency (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a; Yao et al., 2024; Wang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to actions representing tokens, phrases, or sentences in the context of improving optimization efficiency for LLMs.",
      "processing_time": 58.30895757675171,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "…Search (DFS) (Yao et al., 2024) and Breadth-First Search (BFS) (Xie et al., 2024b) offer systematic exploration strategies, while A ∗ Search (Wang et al., 2024a) and Monte Carlo Tree Search (MCTS) (Feng et al., 2023; Zhao et al., 2024; Xie et al., 2024a) incorporate heuristics and sampling…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only search algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.545676469802856,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "The tree search problem in PGTS can be formulated as a MDP, which we refer to as Tree Search MDP (TS-MDP), defined by the tuple ( S , A , T , R , γ ) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a formulation of a problem as an MDP. No verifiable resources are identified.",
      "processing_time": 57.1701717376709,
      "citing_paper_id": "276259098",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "Recent studies have sought to formalize the relationship between inference compute and performance, proposing scaling laws for inference-time optimization (Snell et al., 2024; Wu et al., 2024; Chen et al., 2024a; Brown et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and their findings. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.14859628677368,
      "citing_paper_id": "276259098",
      "cited_paper_id": 271571035
    },
    {
      "context_text": "Third, the terminate action prevents unnecessary exploration by halting the process once sufficient reasoning evidence has been gathered, significantly reducing computational overhead and addressing the “overthinking” phenomenon often observed in o1-like reasoning chains (Chen et al., 2024c).",
      "catation_intent": "findings",
      "resource_type": "finding",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to the overthinking phenomenon in LLMs.",
      "processing_time": 56.876474380493164,
      "citing_paper_id": "276259098",
      "cited_paper_id": 275133600
    },
    {
      "context_text": "Additionally, iterative bootstrapping techniques (Zelikman et al., 2022; 2024; Singh et al., 2023; Yuan et al., 2023) have been explored to progressively enhance LLM reasoning performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and methods. There are no clear identifiers for datasets.",
      "processing_time": 56.8721444606781,
      "citing_paper_id": "276259098",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, Spontaneous Thought often retrieves and replays segments of past memory, aiding in the generalization of past experiences [20, 8, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a concept (Spontaneous Thought) and its function, but does not cite any dataset.",
      "processing_time": 58.41615128517151,
      "citing_paper_id": "276782491",
      "cited_paper_id": 1680775
    },
    {
      "context_text": "Additionally, Spontaneous Thought often retrieves and replays segments of past memory, aiding in the generalization of past experiences [20, 8, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a concept (Spontaneous Thought) and its function, but does not cite any dataset.",
      "processing_time": 58.41615128517151,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "Neural network-based models for long-term memory, like Neural Turing Machines [23] and Key-Value Memory Networks [40], have been designed to theoretically store large volumes of information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. The cited papers are about methods, not datasets.",
      "processing_time": 56.74481129646301,
      "citing_paper_id": "276782491",
      "cited_paper_id": 2711679
    },
    {
      "context_text": "Neural network-based models for long-term memory, like Neural Turing Machines [23] and Key-Value Memory Networks [40], have been designed to theoretically store large volumes of information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. The cited papers are about methods, not datasets.",
      "processing_time": 56.74481129646301,
      "citing_paper_id": "276782491",
      "cited_paper_id": 15299054
    },
    {
      "context_text": "Empirical studies have shown that these functional modules are sufficient to support a multitude of cognitive processes [34, 33, 22, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to cognitive architectures and processes. No verifiable resources are identified.",
      "processing_time": 56.73636317253113,
      "citing_paper_id": "276782491",
      "cited_paper_id": 6257157
    },
    {
      "context_text": "Empirical studies have shown that these functional modules are sufficient to support a multitude of cognitive processes [34, 33, 22, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to cognitive architectures and processes. No verifiable resources are identified.",
      "processing_time": 56.73636317253113,
      "citing_paper_id": "276782491",
      "cited_paper_id": 15975768
    },
    {
      "context_text": "Empirical studies have shown that these functional modules are sufficient to support a multitude of cognitive processes [34, 33, 22, 51].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to cognitive architectures and processes. No verifiable resources are identified.",
      "processing_time": 56.73636317253113,
      "citing_paper_id": "276782491",
      "cited_paper_id": 54480269
    },
    {
      "context_text": "One promising architecture is the Global Workspace Theory (GWT), which supports macro-level collaborative computations across multiple brain regions [3, 4, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and architectures. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.35084533691406,
      "citing_paper_id": "276782491",
      "cited_paper_id": 6257157
    },
    {
      "context_text": "One promising architecture is the Global Workspace Theory (GWT), which supports macro-level collaborative computations across multiple brain regions [3, 4, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and architectures. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.35084533691406,
      "citing_paper_id": "276782491",
      "cited_paper_id": 142242370
    },
    {
      "context_text": "They typically require a procedural memory module or an equivalent function module to plan and decide on the next action [22, 2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to procedural memory modules and function modules. No verifiable resources are identified.",
      "processing_time": 57.143017292022705,
      "citing_paper_id": "276782491",
      "cited_paper_id": 6257157
    },
    {
      "context_text": "Traditional cognitive architectures, such as SOAR [33], ACT-R [51], and LIDA [22], rely on handcrafted, symbolic procedural memory modules for task planning and decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions traditional cognitive architectures but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 57.34382438659668,
      "citing_paper_id": "276782491",
      "cited_paper_id": 6257157
    },
    {
      "context_text": "Traditional cognitive architectures, such as SOAR [33], ACT-R [51], and LIDA [22], rely on handcrafted, symbolic procedural memory modules for task planning and decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions traditional cognitive architectures but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 57.34382438659668,
      "citing_paper_id": "276782491",
      "cited_paper_id": 15975768
    },
    {
      "context_text": "Traditional cognitive architectures, such as SOAR [33], ACT-R [51], and LIDA [22], rely on handcrafted, symbolic procedural memory modules for task planning and decision-making.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions traditional cognitive architectures but does not refer to any specific datasets. The names mentioned are models or methods, not datasets.",
      "processing_time": 57.34382438659668,
      "citing_paper_id": "276782491",
      "cited_paper_id": 54480269
    },
    {
      "context_text": "Studies estimate that approximately 20% of Spontaneous Thought involves future prediction and planning [27, 58], indicating that this cognitive process functions similarly to a world model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general cognitive process. No dataset names are present in the citation span.",
      "processing_time": 56.897847175598145,
      "citing_paper_id": "276782491",
      "cited_paper_id": 7498624
    },
    {
      "context_text": "In this mode, individuals adhere to a specific procedure [6, 19, 41], which limits creativity [17, 24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to procedures and their impact on creativity.",
      "processing_time": 57.131487131118774,
      "citing_paper_id": "276782491",
      "cited_paper_id": 12246493
    },
    {
      "context_text": "In the wake hour, these two modes switch frequently, forming a thought stream[11, 13].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes a phenomenon related to neural activity during undirected thought.",
      "processing_time": 57.6419403553009,
      "citing_paper_id": "276782491",
      "cited_paper_id": 12490249
    },
    {
      "context_text": "As a result, traditional cognitive architectures typically store structured and unstructured data using databases or file systems, taking advantage of existing large-scale data storage solutions [33, 51].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to general data storage solutions which are not considered datasets.",
      "processing_time": 57.14452576637268,
      "citing_paper_id": "276782491",
      "cited_paper_id": 15975768
    },
    {
      "context_text": "As a result, traditional cognitive architectures typically store structured and unstructured data using databases or file systems, taking advantage of existing large-scale data storage solutions [33, 51].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to general data storage solutions which are not considered datasets.",
      "processing_time": 57.14452576637268,
      "citing_paper_id": "276782491",
      "cited_paper_id": 54480269
    },
    {
      "context_text": "Procedural Memory-Centered Architecture: This traditional cognitive architecture relies on handcrafted procedural memory to execute specific tasks [51, 33, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to cognitive architectures. No verifiable resources are identified.",
      "processing_time": 56.57945489883423,
      "citing_paper_id": "276782491",
      "cited_paper_id": 15975768
    },
    {
      "context_text": "Procedural Memory-Centered Architecture: This traditional cognitive architecture relies on handcrafted procedural memory to execute specific tasks [51, 33, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to cognitive architectures. No verifiable resources are identified.",
      "processing_time": 56.57945489883423,
      "citing_paper_id": "276782491",
      "cited_paper_id": 54480269
    },
    {
      "context_text": "Despite its limited capacity and duration, it can store sufficient task-relevant information to support planning and decision-making, enhancing advanced cognitive functions [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to working memory and cognitive functions.",
      "processing_time": 56.22389554977417,
      "citing_paper_id": "276782491",
      "cited_paper_id": 31002043
    },
    {
      "context_text": "Psychological phenomena such as daydreaming, mind-wandering, and creative thinking [16, 12] are categorized under Spontaneous Thought.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references psychological phenomena and categorizes them under spontaneous thought.",
      "processing_time": 57.76060199737549,
      "citing_paper_id": "276782491",
      "cited_paper_id": 34088202
    },
    {
      "context_text": "Research by Dijksterhuis et al. shows that individuals who engage in more mind wandering during specific tasks exhibit greater creativity [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about mind wandering and creativity.",
      "processing_time": 56.08397936820984,
      "citing_paper_id": "276782491",
      "cited_paper_id": 34088202
    },
    {
      "context_text": "It inspires, guides, and maintains certain behaviors, providing a robust sense of self-control [50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It appears to be a general reference to motivation and emotion.",
      "processing_time": 57.59396457672119,
      "citing_paper_id": "276782491",
      "cited_paper_id": 64008149
    },
    {
      "context_text": "Upon receiving the broadcast, these modules process the information and send it back to the central processing module to support subsequent operations [3, 4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to modules processing information, which is not a verifiable resource.",
      "processing_time": 58.0396933555603,
      "citing_paper_id": "276782491",
      "cited_paper_id": 142242370
    },
    {
      "context_text": "UMM is based on the Global Workspace Theory (GWT) [3, 4] for its macro architecture.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework. No dataset names are present in the citation span.",
      "processing_time": 57.303382873535156,
      "citing_paper_id": "276782491",
      "cited_paper_id": 142242370
    },
    {
      "context_text": "Inspired by Global Workspace Theory [4, 3], UMM integrates large language models to support a wide range of human-like cognitive abilities, including perception, reasoning, planning, tool use, learning, memory, reflection, and motivation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and cognitive abilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.73250603675842,
      "citing_paper_id": "276782491",
      "cited_paper_id": 142242370
    },
    {
      "context_text": "This process is akin to transferring experience from System 2 to System 1, thereby improving task-solving efficiency [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a theoretical concept.",
      "processing_time": 57.72878074645996,
      "citing_paper_id": "276782491",
      "cited_paper_id": 145184426
    },
    {
      "context_text": "Basic biological motivations encompass food seeking, predator avoidance, and maintaining circadian rhythms such as sleep [28, 62].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general biological motivations and behaviors. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.380876302719116,
      "citing_paper_id": "276782491",
      "cited_paper_id": 213174884
    },
    {
      "context_text": "Currently, the most effective method for augmenting language models employs the same training strategy as ChatGPT and GPT-4, which utilize a combination of supervised learning and RLHF [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and training strategies. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.63251209259033,
      "citing_paper_id": "276782491",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "These models, such as ChatGPT [43, 46] and GPT-4 [45, 7], have demonstrated human-level performance in various tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.9568293094635,
      "citing_paper_id": "276782491",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "A potential solution is to directly use language-centered visual-language models, such as Flamingo [1], or the multi-modal version of GPT-4 [45], as the foundational model for MindOS.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Flamingo and multi-modal GPT-4). These are excluded as per the instructions.",
      "processing_time": 59.01866698265076,
      "citing_paper_id": "276782491",
      "cited_paper_id": 248476411
    },
    {
      "context_text": "For instance, LeCun et al. propose a cognitive architecture centered around a continually learning and adaptable world model [35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a cognitive architecture proposal. No verifiable resources are identified.",
      "processing_time": 57.57688784599304,
      "citing_paper_id": "276782491",
      "cited_paper_id": 251881108
    },
    {
      "context_text": "World Modeling-Centered Architecture: The most advanced cognitive architecture, as exemplified by LeCun’s autonomous architecture [35], centers around a world model.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a cognitive architecture. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.955023765563965,
      "citing_paper_id": "276782491",
      "cited_paper_id": 251881108
    },
    {
      "context_text": "Nonetheless, current auto-prompt approaches are limited to specific tasks and simple prompt structures [53, 74, 15].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only limitations of auto-prompt approaches. No verifiable resources are identified.",
      "processing_time": 57.792093992233276,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256459681
    },
    {
      "context_text": "Numerous investigations have evidenced the potential of language models to effectively perform intricate reasoning [67, 48, 37] and planning [71, 61, 25, 66] processes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models. No verifiable resources are identified.",
      "processing_time": 57.807159423828125,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Numerous investigations have evidenced the potential of language models to effectively perform intricate reasoning [67, 48, 37] and planning [71, 61, 25, 66] processes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models. No verifiable resources are identified.",
      "processing_time": 57.807159423828125,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256808659
    },
    {
      "context_text": "Numerous investigations have evidenced the potential of language models to effectively perform intricate reasoning [67, 48, 37] and planning [71, 61, 25, 66] processes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities of language models. No verifiable resources are identified.",
      "processing_time": 57.807159423828125,
      "citing_paper_id": "276782491",
      "cited_paper_id": 258686311
    },
    {
      "context_text": "Second, the language model’s robust task planning abilities [71, 61, 25, 66] significantly enhance the functionality of the Thought Stream module.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the task planning abilities of language models. No verifiable resources are identified.",
      "processing_time": 57.86221265792847,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256598146
    },
    {
      "context_text": "Second, the language model’s robust task planning abilities [71, 61, 25, 66] significantly enhance the functionality of the Thought Stream module.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the task planning abilities of language models. No verifiable resources are identified.",
      "processing_time": 57.86221265792847,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256808659
    },
    {
      "context_text": "In addition, the language model demonstrates a robust capacity to utilize a chain of tools for executing complex tasks [71, 64, 36, 54, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of language models to execute complex tasks using a chain of tools.",
      "processing_time": 58.06909656524658,
      "citing_paper_id": "276782491",
      "cited_paper_id": 256808659
    },
    {
      "context_text": "In addition, the language model demonstrates a robust capacity to utilize a chain of tools for executing complex tasks [71, 64, 36, 54, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of language models to execute complex tasks using a chain of tools.",
      "processing_time": 58.06909656524658,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "Several works equip the language model with tool use ability, i.e. ChatGPT Plugin [44] and HuggingGPT Despite the proliferation of autonomous agents, there remains a lack of unified guidelines for developing agents with human-like cognitive capabilities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.91807746887207,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "This improvement is attributed to the robust semantic understanding and tool utilization capabilities of the language model, enabling MindOS to dynamically select the necessary tools [44, 54].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of a language model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.60772252082825,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "This method is similar to techniques like HuggingGPT [54] and ChatGPT Plugins [44].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or tools. There are no verifiable resources that meet the criteria.",
      "processing_time": 58.21871256828308,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "To avoid confusion, we refer to these experts as tools, akin to their usage in Agent communities [52, 49, 54, 36, 44].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to the terminology used for experts in Agent communities.",
      "processing_time": 58.615966796875,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, studies show that language models excel when utilizing various tools [52, 49, 54, 36, 44].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that language models excel when using various tools. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 59.12697720527649,
      "citing_paper_id": "276782491",
      "cited_paper_id": null
    },
    {
      "context_text": "As the number of agents and the complexity of environments increase, solving MAPF instances becomes increasingly difficult [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the difficulty of solving MAPF instances.",
      "processing_time": 57.52051496505737,
      "citing_paper_id": "276742474",
      "cited_paper_id": 2316322
    },
    {
      "context_text": "Algorithmic methods include systematic search algorithms like Conflict-Based Search (CBS) [6], which struggle with large-scale scenarios due to their exponential worst-case time complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithmic methods. The context is about the performance of CBS in multi-agent pathfinding scenarios.",
      "processing_time": 58.70719814300537,
      "citing_paper_id": "276742474",
      "cited_paper_id": 6107832
    },
    {
      "context_text": "We conducted tests on four standard MAPF benchmark maps [1]: random32 and random64 are square maps of size (32x32) and (64x64), den312d is a (65x81) game map, We used success rate and average episode length as metrics.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific maps used for testing, which are part of a benchmark suite. However, since the maps are not referred to as a dataset, database, or similar, they do not meet the criteria for inclusion.",
      "processing_time": 61.570934772491455,
      "citing_paper_id": "276742474",
      "cited_paper_id": 195218865
    },
    {
      "context_text": "Multi-Agent Pathfinding (MAPF) [1] is a core challenge in multi-agent systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in multi-agent systems. No verifiable resources are identified.",
      "processing_time": 58.032373666763306,
      "citing_paper_id": "276742474",
      "cited_paper_id": 195218865
    },
    {
      "context_text": "The standard MAPF [1] problem is defined as follows: Given a set of agents A = { 1 , 2 , . . . , k } on an undirected graph G = ( V, E ) , where V represents locations and E denotes connections, the goal is to find collision-free paths from each agent’s start s ( i ) ∈ V to its target t ( i ) ∈ V .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only defines the Multi-Agent Pathfinding (MAPF) problem. No verifiable resources are identified.",
      "processing_time": 58.81162214279175,
      "citing_paper_id": "276742474",
      "cited_paper_id": 195218865
    },
    {
      "context_text": "We tested LLMDR on standard MAPF benchmark maps [1] with 4 to 64 agents.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'standard MAPF benchmark maps' but does not specify a named dataset. The term 'benchmark maps' is too generic and lacks a specific identifier.",
      "processing_time": 59.53535437583923,
      "citing_paper_id": "276742474",
      "cited_paper_id": 195218865
    },
    {
      "context_text": "The use of graph neural networks allowed agents to communicate [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of graph neural networks for communication among agents.",
      "processing_time": 57.35126543045044,
      "citing_paper_id": "276742474",
      "cited_paper_id": 209324317
    },
    {
      "context_text": "Subsequent studies have introduced the use of the A* algorithm as a key component [13], [14], [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the A* algorithm. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.06652760505676,
      "citing_paper_id": "276742474",
      "cited_paper_id": 220919878
    },
    {
      "context_text": "For instance, PRIMAL [7] combines reinforcement learning with imitation learning, while more recent approaches like DHC [8] and DCC [9] have introduced advanced communication mechanisms between agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.64100527763367,
      "citing_paper_id": "276742474",
      "cited_paper_id": 232379138
    },
    {
      "context_text": "In our experiments, we combined LLMDR with the learned models of several learning-based methods, including DCC [9], DHC [8], and EPH [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on combining LLMDR with other learning-based methods.",
      "processing_time": 58.86616230010986,
      "citing_paper_id": "276742474",
      "cited_paper_id": 232379138
    },
    {
      "context_text": "In DHC [8], pathfinding and communication strategies were trained separately, with agents communicating through a graph convolution mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on training and communication strategies in multi-agent systems.",
      "processing_time": 58.642112493515015,
      "citing_paper_id": "276742474",
      "cited_paper_id": 232379138
    },
    {
      "context_text": "Failures in MAPF policies trained through reinforcement learning are not always easy to identify [8], [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing failures in MAPF policies trained through reinforcement learning.",
      "processing_time": 58.137205362319946,
      "citing_paper_id": "276742474",
      "cited_paper_id": 232379138
    },
    {
      "context_text": "Moreover, deadlock situations often involve multiple agents, making simple or random resolution strategies ineffective [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to multi-agent systems and deadlock situations.",
      "processing_time": 56.60473990440369,
      "citing_paper_id": "276742474",
      "cited_paper_id": 269527804
    },
    {
      "context_text": "Its applications span various domains, such as autonomous vehicles [2], multi-robot systems [3], and video games [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists application domains of a technology or method.",
      "processing_time": 57.788992166519165,
      "citing_paper_id": "276742474",
      "cited_paper_id": null
    },
    {
      "context_text": "Early efforts primarily rely on SFT and imitation learning (Lai et al., 2024; Pan et al., 2024a; Furuta et al., 2024; Yao et al., 2022; Nakano et al., 2021) struggle with generalization to complex and varied web scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.528072357177734,
      "citing_paper_id": "278997255",
      "cited_paper_id": 250264533
    },
    {
      "context_text": "LLM-based web agents enable autonomous website navigation, dynamic content interpretation, and execution of user interactions (Zhou et al., 2023; Deng et al., 2023; Cheng et al., 2024; Yao et al., 2022; Jang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing LLM-based web agents. No verifiable resources are identified.",
      "processing_time": 58.529106855392456,
      "citing_paper_id": "278997255",
      "cited_paper_id": 250264533
    },
    {
      "context_text": "Large language models (LLMs) have emerged as powerful agents capable of executing complex tasks across diverse domains (Yao et al., 2023; Liu et al., 2023), particularly through integration with external environments or APIs (Song et al., 2023; Wang et al., 2024; Zhuang et al., 2025; Sun et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of LLMs and their integration with external environments or APIs.",
      "processing_time": 58.82955241203308,
      "citing_paper_id": "278997255",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Such design aims to balance structural guidance and flexibility, reducing over-fitting to specific prompt patterns and promoting robust generalization across diverse web interaction scenarios (Yao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to improve language models.",
      "processing_time": 56.69784164428711,
      "citing_paper_id": "278997255",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…of executing complex tasks across diverse domains (Yao et al., 2023; Liu et al., 2023), particularly through integration with external environments or APIs (Song et al., 2023; Wang et al., 2024; Zhuang et al., 2025; Sun et al., 2023; Liao et al., 2024; Gu et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of LLMs in various contexts.",
      "processing_time": 57.86618971824646,
      "citing_paper_id": "278997255",
      "cited_paper_id": 258947337
    },
    {
      "context_text": "…of executing complex tasks across diverse domains (Yao et al., 2023; Liu et al., 2023), particularly through integration with external environments or APIs (Song et al., 2023; Wang et al., 2024; Zhuang et al., 2025; Sun et al., 2023; Liao et al., 2024; Gu et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of LLMs in various contexts.",
      "processing_time": 57.86618971824646,
      "citing_paper_id": "278997255",
      "cited_paper_id": 267406155
    },
    {
      "context_text": "…of executing complex tasks across diverse domains (Yao et al., 2023; Liu et al., 2023), particularly through integration with external environments or APIs (Song et al., 2023; Wang et al., 2024; Zhuang et al., 2025; Sun et al., 2023; Liao et al., 2024; Gu et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of LLMs in various contexts.",
      "processing_time": 57.86618971824646,
      "citing_paper_id": "278997255",
      "cited_paper_id": 267782782
    },
    {
      "context_text": "…of executing complex tasks across diverse domains (Yao et al., 2023; Liu et al., 2023), particularly through integration with external environments or APIs (Song et al., 2023; Wang et al., 2024; Zhuang et al., 2025; Sun et al., 2023; Liao et al., 2024; Gu et al., 2024; Zhuang et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing the capabilities of LLMs in various contexts.",
      "processing_time": 57.86618971824646,
      "citing_paper_id": "278997255",
      "cited_paper_id": 276250494
    },
    {
      "context_text": "Compared to static environments with simpler state representations, HTML data make maintaining comprehensive action-observation histories across interactions prohibitively costly and inefficient (Gur et al., 2023; Ye et al., 2025).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of using HTML data in dynamic environments.",
      "processing_time": 57.23083782196045,
      "citing_paper_id": "278997255",
      "cited_paper_id": 260126067
    },
    {
      "context_text": "Unlike structured API environments, web interfaces present unique challenges, including noisy HTML structures, dynamic content, and inconsistent element identification (He et al., 2024b,a), demanding robust context-aware interactions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only challenges related to web interfaces. No verifiable resources are identified.",
      "processing_time": 57.56886005401611,
      "citing_paper_id": "278997255",
      "cited_paper_id": 273638611
    },
    {
      "context_text": "More recent studies, such as WebRL (Qi et al., 2025) and OpenWebVoy-ager (He et al., 2024b), have utilized RL frameworks with curriculum-driven or iterative feedback loops, achieving significant improvements in complex and dynamic web navigation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'OpenWebVoyager' but does not refer to it as a dataset. It is described as a method or framework for building multimodal web agents.",
      "processing_time": 59.670883893966675,
      "citing_paper_id": "278997255",
      "cited_paper_id": 273638611
    },
    {
      "context_text": "Existing web agents mainly enhance proprietary LLMs through meticulously crafted prompts or pre-defined workflows (He et al., 2024a; Zheng et al., 2024; Ma et al., 2023), which pose affordability and privacy concerns for workplace implementations (Sun et al., 2024; Li et al., 2024; Zhuang et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.9831702709198,
      "citing_paper_id": "278997255",
      "cited_paper_id": null
    },
    {
      "context_text": "Web navigation inherently involves complex planning, challenging LLMs to execute sequential decision making, retain memory across multiple turns, and dynamically adapt to environmental feed-back (He et al., 2024a,b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges of web navigation for LLMs.",
      "processing_time": 57.10430288314819,
      "citing_paper_id": "278997255",
      "cited_paper_id": null
    },
    {
      "context_text": "Until recently, LLMs were evaluated on Natural Language Understanding (NLU) tasks from benchmark collections like GLUE (Wang, 2018) and SuperGLUE (Wang et al., 2019), which included tasks like paraphrase classification and sentiment analysis.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GLUE and SuperGLUE but does not indicate they are used as datasets in the current research. They are referenced as benchmark collections, which are excluded.",
      "processing_time": 59.42042827606201,
      "citing_paper_id": "277857560",
      "cited_paper_id": 143424870
    },
    {
      "context_text": "As the performance and complexity of these models continue to grow (Chen et al., 2024b), selecting the most appropriate model for a specific application has become an increasingly challenging and costly decision (Kaplan et al., 2020; Hoffmann et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model performance and complexity. No verifiable resources are identified.",
      "processing_time": 58.013917207717896,
      "citing_paper_id": "277857560",
      "cited_paper_id": 210861095
    },
    {
      "context_text": "As the performance and complexity of these models continue to grow (Chen et al., 2024b), selecting the most appropriate model for a specific application has become an increasingly challenging and costly decision (Kaplan et al., 2020; Hoffmann et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model performance and complexity. No verifiable resources are identified.",
      "processing_time": 58.013917207717896,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "…evaluation is fundamental not only for assessing performance but also for uncovering hidden dynamics within LLMs, such as potential backdoors or biases (Schuster et al., 2020), and for evaluating their emerging reasoning capabilities (Brown et al., 2020; Sanh et al., 2022; Wei et al., 2023b;a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general evaluation methods and capabilities of LLMs. No verifiable resources are named.",
      "processing_time": 58.25259757041931,
      "citing_paper_id": "277857560",
      "cited_paper_id": 211204954
    },
    {
      "context_text": "…evaluation is fundamental not only for assessing performance but also for uncovering hidden dynamics within LLMs, such as potential backdoors or biases (Schuster et al., 2020), and for evaluating their emerging reasoning capabilities (Brown et al., 2020; Sanh et al., 2022; Wei et al., 2023b;a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general evaluation methods and capabilities of LLMs. No verifiable resources are named.",
      "processing_time": 58.25259757041931,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditionally, this scaling involves incorporating human-crafted data (Holland et al., 2018), which is resource-intensive (Hutchinson et al., 2021) and may not adequately capture the complexities of language (Mehrabi et al., 2021) and reasoning required to challenge advanced LLMs (Gudibande et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the challenges and limitations of using human-crafted data, which is too generic and not a specific dataset.",
      "processing_time": 59.36547374725342,
      "citing_paper_id": "277857560",
      "cited_paper_id": 225067460
    },
    {
      "context_text": "Additionally, the high cost and effort required to develop new benchmarks often result in outdated evaluation methods that do not keep pace with the rapid development of LLMs (Kiela et al., 2021; Vu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the issue of benchmarking and evaluation methods for LLMs.",
      "processing_time": 57.59089255332947,
      "citing_paper_id": "277857560",
      "cited_paper_id": 233444226
    },
    {
      "context_text": "• Sensitivity to Prompts : Previous work (Zheng et al., 2024; Pezeshkpour & Hruschka, 2023; Lu et al., 2022; Alzahrani et al., 2024; Wang et al., 2024a) has shown that models are sensitive to benchmark formats.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions sensitivity to benchmark formats but does not specify any particular datasets. The cited papers' titles suggest a focus on multiple-choice questions and rationality in evaluation, but no specific datasets are named.",
      "processing_time": 60.73323965072632,
      "citing_paper_id": "277857560",
      "cited_paper_id": 261064970
    },
    {
      "context_text": "• Sensitivity to Prompts : Previous work (Zheng et al., 2024; Pezeshkpour & Hruschka, 2023; Lu et al., 2022; Alzahrani et al., 2024; Wang et al., 2024a) has shown that models are sensitive to benchmark formats.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions sensitivity to benchmark formats but does not specify any particular datasets. The cited papers' titles suggest a focus on multiple-choice questions and rationality in evaluation, but no specific datasets are named.",
      "processing_time": 60.73323965072632,
      "citing_paper_id": "277857560",
      "cited_paper_id": 280272508
    },
    {
      "context_text": "• Sensitivity to Prompts : Previous work (Zheng et al., 2024; Pezeshkpour & Hruschka, 2023; Lu et al., 2022; Alzahrani et al., 2024; Wang et al., 2024a) has shown that models are sensitive to benchmark formats.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions sensitivity to benchmark formats but does not specify any particular datasets. The cited papers' titles suggest a focus on multiple-choice questions and rationality in evaluation, but no specific datasets are named.",
      "processing_time": 60.73323965072632,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) are being developed at an unprecedented pace (Zhao et al., 2024), requiring significant investment for their training and refinement (Kevin Lee, 2024; Miller, 2022; Kimball, 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about the development and investment in Large Language Models.",
      "processing_time": 57.58778214454651,
      "citing_paper_id": "277857560",
      "cited_paper_id": 261242328
    },
    {
      "context_text": "Evaluation frameworks comprising multiple games include: (i) ChatArena (Wu et al., 2023), which includes Chess, Tic-Tac-Toe, Rock-Paper-Scissors, and others, (ii) GridGames (Topsakal et al., 2024), implementing Tic-Tac-Toe, Connect Four, and Gomoku, (iii) SmartPlay (Wu et al., 2024), which, alongside simple games like Tic-Tac-Toe and Tower of Hanoi, implements more complex games like Minecraft and Messenger to test spatial reasoning.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation frameworks but does not specify any datasets. The frameworks are described as including various games, which are not considered datasets.",
      "processing_time": 58.30141019821167,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263608611
    },
    {
      "context_text": "…Rock-Paper-Scissors, and others, (ii) GridGames (Topsakal et al., 2024), implementing Tic-Tac-Toe, Connect Four, and Gomoku, (iii) SmartPlay (Wu et al., 2024), which, alongside simple games like Tic-Tac-Toe and Tower of Hanoi, implements more complex games like Minecraft and Messenger to…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SmartPlay' as a tool or benchmark for LLMs, but does not specify it as a dataset. No other specific datasets are mentioned.",
      "processing_time": 59.378366470336914,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263608611
    },
    {
      "context_text": "Another important reason for using DSPy is to make use of DSPy Assertions (Singhvi et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of DSPy and DSPy Assertions. These are tools or methods, not datasets.",
      "processing_time": 58.616451025009155,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263671701
    },
    {
      "context_text": "Another important reason for using DSPy is to make use of DSPy Assertions (Singhvi et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of DSPy and DSPy Assertions. These are tools or methods, not datasets.",
      "processing_time": 58.616451025009155,
      "citing_paper_id": "277857560",
      "cited_paper_id": 266436034
    },
    {
      "context_text": "DSPy provides abstractions that allow easily swapping and modifying strategies without depending on manually-written prompt templating.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (DSPy) for working with language models. No verifiable resources are identified.",
      "processing_time": 58.41413760185242,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263671701
    },
    {
      "context_text": "We leverage the DSPy (Khattab et al., 2023) approach to prompt abstraction in our framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DSPy' but it is described as an approach to prompt abstraction, which indicates it is a method or tool rather than a dataset.",
      "processing_time": 58.620909690856934,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263671701
    },
    {
      "context_text": "Players are given multiple tries to execute valid moves using DSPy’s feedback mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a tool (DSPy) and its feedback mechanisms. No verifiable resources are identified.",
      "processing_time": 58.091508626937866,
      "citing_paper_id": "277857560",
      "cited_paper_id": 263671701
    },
    {
      "context_text": "Many benchmarks suffer from data contamination (Yang et al., 2023), where models inadvertently train on portions of the test data (Dubey et al., 2024; Groeneveld et al., 2024), leading to inflated performance metrics.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses issues with benchmarks and data contamination, but does not mention any specific datasets by name.",
      "processing_time": 57.026296854019165,
      "citing_paper_id": "277857560",
      "cited_paper_id": 265050721
    },
    {
      "context_text": "Following recent suggestions for LLM rating systems by Boubdir et al. (2023); Chiang et al. (2023), Z ERO S UM E VAL employs the Bradley-Terry (BT) rating system, an alternative to the Elo system, to rate models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only rating systems and methods. No verifiable resources are identified.",
      "processing_time": 57.39110994338989,
      "citing_paper_id": "277857560",
      "cited_paper_id": 265498394
    },
    {
      "context_text": "Sensitivity to prompt variations (Alzahrani et al., 2024) and a lack of diversity in evaluation tasks (Laskar et al., 2024) further undermine the reliability and robustness of these benchmarks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with benchmarks and evaluation tasks. No clear, verifiable datasets are identified.",
      "processing_time": 58.081385135650635,
      "citing_paper_id": "277857560",
      "cited_paper_id": 271038835
    },
    {
      "context_text": "Training these models involves massive computational efforts (Hoffmann et al., 2022), yet the evaluation phase typically utilizes a negligible fraction of this capacity (Laskar et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general statements about training and evaluation phases of models.",
      "processing_time": 57.03960847854614,
      "citing_paper_id": "277857560",
      "cited_paper_id": 271038835
    },
    {
      "context_text": "Training these models involves massive computational efforts (Hoffmann et al., 2022), yet the evaluation phase typically utilizes a negligible fraction of this capacity (Laskar et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general statements about training and evaluation phases of models.",
      "processing_time": 57.03960847854614,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "Previous work has proposed the use of games as benchmarks (Topsakal et al., 2024), offering a promising avenue for evaluating complex reasoning (Wong et al., 2023) and decision-making abilities of LLMs (Warstadt et al., 2023; Park et al., 2023; Wang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to games as benchmarks for evaluating LLMs. No clear, verifiable datasets are identified.",
      "processing_time": 58.82354426383972,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "This choice is more suitable than the traditional Elo system, which was designed for human chess players with changing skill levels, whereas LLMs have fixed skill levels defined by their weights (Elo, 1967).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Elo system) which is not a dataset.",
      "processing_time": 57.219923973083496,
      "citing_paper_id": "277857560",
      "cited_paper_id": null
    },
    {
      "context_text": "Researches have shown that humans begin by building holistic mental modeling when solving complex problems, allowing problem solvers to form a topological framework before focusing on specific details (Spreng et al., 2009; Koban et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to human cognitive processes and mental modeling. No verifiable resources are identified.",
      "processing_time": 57.77996802330017,
      "citing_paper_id": "279154984",
      "cited_paper_id": 2069491
    },
    {
      "context_text": "SVAMP and ASDiv are two simple math problem datasets with 1000 and 2096 data respectively, each question requires 1-2 steps of reasoning and calculation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SVAMP",
        "ASDiv"
      ],
      "dataset_descriptions": {
        "SVAMP": "Used to evaluate and develop English math word problem solvers, focusing on problems requiring 1-2 steps of reasoning and calculation.",
        "ASDiv": "Used to evaluate and develop English math word problem solvers, focusing on problems requiring 1-2 steps of reasoning and calculation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, SVAMP and ASDiv, which are used for evaluating and developing English math word problem solvers. These datasets are directly relevant to the topic of planning capabilities of LLMs.",
      "processing_time": 72.59709763526917,
      "citing_paper_id": "279154984",
      "cited_paper_id": 220047831
    },
    {
      "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Game of 24",
        "GSM8K",
        "ASDiv",
        "SVAMP",
        "MATH-500"
      ],
      "dataset_descriptions": {
        "Game of 24": "Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods.",
        "GSM8K": "Used to assess RFF's ability to solve grade school math word problems, emphasizing accuracy enhancements compared to baselines.",
        "ASDiv": "Used to test RFF's effectiveness in solving diverse arithmetic word problems, highlighting accuracy gains over existing methods.",
        "SVAMP": "Used to evaluate RFF's performance in solving single-variable arithmetic word problems, demonstrating significant accuracy improvements.",
        "MATH-500": "Used to measure RFF's mathematical problem-solving capabilities, showing notable accuracy improvements over baseline approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 85.09052443504333,
      "citing_paper_id": "279154984",
      "cited_paper_id": 220047831
    },
    {
      "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Game of 24",
        "GSM8K",
        "ASDiv",
        "SVAMP",
        "MATH-500"
      ],
      "dataset_descriptions": {
        "Game of 24": "Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods.",
        "GSM8K": "Used to assess RFF's ability to solve grade school math word problems, emphasizing accuracy enhancements compared to baselines.",
        "ASDiv": "Used to test RFF's effectiveness in solving diverse arithmetic word problems, highlighting accuracy gains over existing methods.",
        "SVAMP": "Used to evaluate RFF's performance in solving single-variable arithmetic word problems, demonstrating significant accuracy improvements.",
        "MATH-500": "Used to measure RFF's mathematical problem-solving capabilities, showing notable accuracy improvements over baseline approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 85.09052443504333,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "We evaluate RFF in five datasets: Game of 24 (Yao et al., 2024), GSM8K (Cobbe et al., 2021), ASDiv (Miao et al., 2021), SVAMP (Patel et al., 2021) MATH-500 (Lightman et al., 2023), and demonstrate significant improvements in accuracy over baseline methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Game of 24",
        "GSM8K",
        "ASDiv",
        "SVAMP",
        "MATH-500"
      ],
      "dataset_descriptions": {
        "Game of 24": "Used to evaluate RFF's performance in solving arithmetic problems, focusing on accuracy improvements over baseline methods.",
        "GSM8K": "Used to assess RFF's ability to solve grade school math word problems, emphasizing accuracy enhancements compared to baselines.",
        "ASDiv": "Used to test RFF's effectiveness in solving diverse arithmetic word problems, highlighting accuracy gains over existing methods.",
        "SVAMP": "Used to evaluate RFF's performance in solving single-variable arithmetic word problems, demonstrating significant accuracy improvements.",
        "MATH-500": "Used to measure RFF's mathematical problem-solving capabilities, showing notable accuracy improvements over baseline approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for evaluating RFF, which are clearly named and relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 85.09052443504333,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "These problems rely on background knowledge, which is usually not explicitly stated in the problems. we have conducted experiments on two widely used commonsense benchmarks: CommonQA (Talmor et al., 2018) and LogiQA (Liu et al., 2020) using RFF-G.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CommonQA",
        "LogiQA"
      ],
      "dataset_descriptions": {
        "CommonQA": "Used to evaluate commonsense reasoning capabilities, focusing on background knowledge not explicitly stated in problems, employing RFF-G methodology.",
        "LogiQA": "Used to assess machine reading comprehension with logical reasoning, emphasizing the need for implicit background knowledge, utilizing RFF-G approach."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two benchmarks, CommonQA and LogiQA, which are used for evaluating commonsense reasoning. These are specific datasets used in the experiments.",
      "processing_time": 71.40195727348328,
      "citing_paper_id": "279154984",
      "cited_paper_id": 220483148
    },
    {
      "context_text": "In LogiQA, the differences shows a greater gap among these baselines: the results of Least-to-Most and Give-me-Hint are close to COT while the results of RFF and CR shows a significant improvement over COT.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LogiQA"
      ],
      "dataset_descriptions": {
        "LogiQA": "Used to evaluate various baselines in machine reading comprehension with logical reasoning, focusing on the performance differences between methods like Least-to-Most, Give-me-Hint, RFF, and CR."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'LogiQA' as a challenge dataset, which is relevant to the topic of planning capabilities of LLMs. It is used to evaluate different baselines in machine reading comprehension with logical reasoning.",
      "processing_time": 69.84501075744629,
      "citing_paper_id": "279154984",
      "cited_paper_id": 220483148
    },
    {
      "context_text": "…models (LLMs), fueled by breakthroughs in deep learning architectures and unprecedented datasets, has demonstrated remarkable potential across nat-ural language processing (NLP) and interdisciplinary applications (Lee and Toutanova, 2018; Radford, 2018; Team et al., 2023; Sel et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to 'unprecedented datasets'. No specific names or identifiers are provided.",
      "processing_time": 58.01259517669678,
      "citing_paper_id": "279154984",
      "cited_paper_id": 226096901
    },
    {
      "context_text": "…models (LLMs), fueled by breakthroughs in deep learning architectures and unprecedented datasets, has demonstrated remarkable potential across nat-ural language processing (NLP) and interdisciplinary applications (Lee and Toutanova, 2018; Radford, 2018; Team et al., 2023; Sel et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to 'unprecedented datasets'. No specific names or identifiers are provided.",
      "processing_time": 58.01259517669678,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "However, CR fails to reach the average level of other methods on simple task SVAMP with 71.2% compared to 85.1%, and demonstrates better performance on hard task MATH with 40.2% to 36.2%.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SVAMP",
        "MATH"
      ],
      "dataset_descriptions": {
        "SVAMP": "Used to evaluate performance on simple arithmetic problems, comparing the method's accuracy against other approaches.",
        "MATH": "Used to assess performance on complex mathematical problems, demonstrating better relative performance on harder tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'SVAMP' and 'MATH' as datasets used to evaluate performance on mathematical problem-solving tasks. 'SVAMP' is a specific dataset for simple arithmetic problems, while 'MATH' is used for harder problems.",
      "processing_time": 71.56576251983643,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "This task contains four datasets: GSM8K, SVAMP, AsDiv and MATH-500.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "SVAMP",
        "AsDiv",
        "MATH-500"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate the ability of LLMs to solve grade school math problems, focusing on multi-step reasoning and arithmetic operations.",
        "SVAMP": "Applied to assess the performance of LLMs on single-variable arithmetic problems, emphasizing the accuracy of numerical solutions.",
        "AsDiv": "Utilized to test the generalization of LLMs across diverse arithmetic problems, including word problems and symbolic expressions.",
        "MATH-500": "Employed to measure the mathematical problem-solving capabilities of LLMs, covering a range of difficulty levels and problem types."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions four specific datasets by name, all of which are relevant to the topic of mathematical problem-solving, which is closely related to the planning capabilities of LLMs.",
      "processing_time": 83.58297348022461,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "We carefully check the output of the CR and our RFF to and find that the detailed hints and question-answer pairs can be helpful in hard problems like MATH, but they lead to a harmful phenomenon of overthinking when facing simple problems like SVAMP.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to measure mathematical problem-solving capabilities, focusing on the impact of detailed hints and question-answer pairs on complex problems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions MATH and SVAMP, which are known datasets in the domain of mathematical problem solving. However, only MATH is referenced in a way that suggests it is a dataset used in the research.",
      "processing_time": 67.24508810043335,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "MATH-500 is a subset of 500 mathematical problem from the MATH (Hendrycks et al., 2021) benchmark, which is a much more harder benchmark than GSM8K.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH-500",
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH-500": "Used to evaluate mathematical problem-solving capabilities, focusing on a subset of 500 problems from the MATH benchmark to assess difficulty levels compared to GSM8K.",
        "MATH": "Serves as a comprehensive benchmark for evaluating mathematical problem-solving skills, providing a larger and more challenging set of problems than GSM8K."
      },
      "confidence_score": 0.9,
      "reasoning": "MATH-500 is identified as a subset of the MATH dataset, which is a benchmark for mathematical problem-solving. The context indicates it is used for evaluation, comparing difficulty with GSM8K.",
      "processing_time": 75.67839169502258,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Meanwhile, there is another an interesting phenomenon: since the methods present great improvement of accuracy to CoT on GSM8K, AS-Div and MATH datasets, contributing to the better focus on details and relations about progressive prompting methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AS-Div",
        "MATH"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to measure the accuracy improvement of Chain-of-Thought methods, focusing on detailed and relational aspects of problem-solving.",
        "AS-Div": "Used to evaluate the effectiveness of Chain-of-Thought methods, specifically in enhancing accuracy and attention to detail.",
        "MATH": "Used to assess the performance of Chain-of-Thought methods, emphasizing the improvement in solving mathematical problems and focusing on progressive prompting."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions the use of GSM8K, AS-Div, and MATH datasets to measure the improvement of accuracy in Chain-of-Thought (CoT) methods.",
      "processing_time": 77.72794914245605,
      "citing_paper_id": "279154984",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "In the study of complex reasoning tasks, Chain-of-Thought (CoT) (Wei et al., 2022; Wang et al., 2022) prompting has emerged as a pivotal technique for significantly improving the performance of large language models (LLMs) by explicitly generating intermediate reasoning steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought prompting).",
      "processing_time": 56.84116220474243,
      "citing_paper_id": "279154984",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "A seminal work in this area is Chain-of-Thought (CoT) (Wei et al., 2022), which pioneered the novel view that reasoning ability can be improved by designing reasoning prompts, paradigms, and examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Chain-of-Thought) and its impact on reasoning ability in language models.",
      "processing_time": 58.17076349258423,
      "citing_paper_id": "279154984",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "We conduct Game of 24 on Llama3-8B-Instruct with a temperature of 0.7 (consistent with the setup of CoT (Wei et al., 2022) and ToT (Yao et al., 2024)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the experimental setup using Llama3-8B-Instruct for the Game of 24.",
      "processing_time": 60.24008560180664,
      "citing_paper_id": "279154984",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "We conduct Game of 24 on Llama3-8B-Instruct with a temperature of 0.7 (consistent with the setup of CoT (Wei et al., 2022) and ToT (Yao et al., 2024)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the experimental setup using Llama3-8B-Instruct for the Game of 24.",
      "processing_time": 60.24008560180664,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "The setup of CoT is consistent with (Wei et al., 2022) and (Yang et al., 2024), who employ the intermediate calculation process as the reasoning step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach (CoT) used in the research.",
      "processing_time": 57.402087450027466,
      "citing_paper_id": "279154984",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Recent innovations also bridge reasoning with executable action, exemplified by frameworks like LATS (Language Agent Tree Search) (Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a framework (LATS) but does not refer to any specific dataset. The context focuses on the method rather than a reusable dataset.",
      "processing_time": 58.6938841342926,
      "citing_paper_id": "279154984",
      "cited_paper_id": 263829963
    },
    {
      "context_text": "AOT+ (Sel et al., 2025), as the upgrade of AOT, adds a fine-grained backtracking mechanism by labeling each steps, which further reduces the reasoning consumption on an error searching route.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (AOT+).",
      "processing_time": 56.6480393409729,
      "citing_paper_id": "279154984",
      "cited_paper_id": 275820389
    },
    {
      "context_text": "The task of Game of 24 originates from (Yao et al., 2024), where the goal is to use four numbers with basic arithmetic operations (+-*/) to obtain 24, and each number can be used only once.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task or problem setup. There are no clear identifiers for datasets, and the context is about the definition of a game task rather than the use of a dataset.",
      "processing_time": 61.016679763793945,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "In the domain of search-based reasoning for large language models, the Tree-of-Thought (ToT) (Yao et al., 2024) framework introduces backtracking capabilities within multi-path decision structures, enabling systematic exploration of diverse solu-tion trajectories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Tree-of-Thought framework).",
      "processing_time": 56.76948356628418,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "Tree-of-Thought (ToT) (Yao et al., 2024) provides a searching view to enhance the ability of complex reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Tree-of-Thought (ToT).",
      "processing_time": 57.29418754577637,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "We conduct 100 times about the middle hard 100 puzzles from 901 to 1000 (Yao et al., 2024).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide a clear, specific dataset name. The reference to 'puzzles' is too generic and lacks a specific identifier.",
      "processing_time": 58.37134122848511,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "Searching paradigms like ToT and CR achieve better scores than CoT, meanwhile, the ToT method visits more states because of blind searching.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their performance. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.087151765823364,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "We employ CoT, ToT, AoT and cumulative reasoning (CR) with different parameters as the baselines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reasoning methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 57.62312579154968,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "As for ToT and CR, we adapt the settings and prompts from (Zhang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to settings and prompts adapted from another study.",
      "processing_time": 56.844021797180176,
      "citing_paper_id": "279154984",
      "cited_paper_id": null
    },
    {
      "context_text": "N AV agent using the DA GGER (Ross, Gordon, and Bagnell 2011) algorithm with the A* oracle as the expert.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm (DAgger) and an oracle (A*). These are methods, not datasets.",
      "processing_time": 58.54678010940552,
      "citing_paper_id": "261660608",
      "cited_paper_id": 103456
    },
    {
      "context_text": "Specifically, the low-level planner is trained using the DA GGER algorithm (Ross, Gordon, and Bagnell 2011) to follow a shortest path oracle as the expert.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the DA GGER algorithm but does not refer to any specific dataset. The context is about training a low-level planner using an algorithm, not a dataset.",
      "processing_time": 58.38950800895691,
      "citing_paper_id": "261660608",
      "cited_paper_id": 103456
    },
    {
      "context_text": "We perform DA GGER dataset aggregation over two rounds.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. 'DA GGER' is a method, not a dataset.",
      "processing_time": 57.44001221656799,
      "citing_paper_id": "261660608",
      "cited_paper_id": 103456
    },
    {
      "context_text": "A GroupNorm (Wu and He 2018) ResNet18 (He et al. 2016) encodes the input RGBD image, and a 2-layer 512 hidden size gated recurrent unit (GRU) (Cho et al. 2014) combines history and sensor inputs to predict the next action.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.9635694026947,
      "citing_paper_id": "261660608",
      "cited_paper_id": 4076251
    },
    {
      "context_text": "Dataset: Most prior Embodied AI simulators such as AI2THOR (Kolve et al. 2017) or Habitat (Szot et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AI2THOR as an embodied AI simulator, but it does not specify that it is a dataset. It is more likely a simulation environment or platform.",
      "processing_time": 59.50786089897156,
      "citing_paper_id": "261660608",
      "cited_paper_id": 28328610
    },
    {
      "context_text": "We opted for the recently introduced ProcTHOR framework, which is built on top of the AI2-THOR simulator, for our experiments (Deitke et al. 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the ProcTHOR framework, which is built on AI2-THOR, but does not refer to a specific dataset. The focus is on the experimental setup using a simulation environment.",
      "processing_time": 60.59260058403015,
      "citing_paper_id": "261660608",
      "cited_paper_id": 28328610
    },
    {
      "context_text": "Dataset: Most prior Embodied AI simulators such as AI2-THOR (Kolve et al. 2017) or Habitat (Szot et al. 2021) are either based on environments with single rooms or lack of variability in size of the environment (room-types) and objects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions AI2-THOR and Habitat, but these are simulators, not datasets. No specific datasets are mentioned or used.",
      "processing_time": 58.427757024765015,
      "citing_paper_id": "261660608",
      "cited_paper_id": 28328610
    },
    {
      "context_text": "In comparison, Say-Nav, when building the scene graph using visual observations ( VO ) with either PNav or OrNav as the low-level planner, achieves a higher SR ( 60 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics (SR) between different planners in a navigation task.",
      "processing_time": 58.0240683555603,
      "citing_paper_id": "261660608",
      "cited_paper_id": 91184540
    },
    {
      "context_text": "3D scene graphs (Armeni et al. 2019; Kim et al. 2019; Rosinol et al. 2021; Hughes, Chang, and Carlone 2022; Wald et al. 2020; Wu et al. 2021) have recently emerged as powerful high-level representations of 3D large-scale environments to support real-time decisions in robotics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and representations. The context focuses on 3D scene graphs and their applications in robotics, without referencing any particular dataset.",
      "processing_time": 59.367329359054565,
      "citing_paper_id": "261660608",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "3D scene graphs (Armeni et al. 2019; Kim et al. 2019; Rosinol et al. 2021; Hughes, Chang, and Carlone 2022; Wald et al. 2020; Wu et al. 2021) have recently emerged as powerful high-level representations of 3D large-scale environments to support real-time decisions in robotics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and representations. The context focuses on 3D scene graphs and their applications in robotics, without referencing any particular dataset.",
      "processing_time": 59.367329359054565,
      "citing_paper_id": "261660608",
      "cited_paper_id": 231632715
    },
    {
      "context_text": "However, instead of generating a complete high-level plan for the entire task in the beginning (Ahn et al. 2022; Song et al. 2022), SayNav utilizes LLMs to incrementally generate a short-term plan regularly, based on current observations and the memory of previously-visited regions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the approach of using LLMs for planning.",
      "processing_time": 57.01764750480652,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "For example, works such as (Song et al. 2022; Singh et al. 2023) have focused on the AI-THOR based environment that consists of only a single room.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AI-THOR based environment' but does not specify a dataset. It describes a simulation environment rather than a dataset.",
      "processing_time": 58.362066984176636,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "First, the grounding mechanisms in these methods (Ahn et al. 2022; Song et al. 2022; Huang et al. 2022; Liu et al. 2023) are designed for small-scale environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their grounding mechanisms. The context is too generic to infer any dataset usage.",
      "processing_time": 57.996368408203125,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "First, the grounding mechanisms in these meth-ods (Ahn et al. 2022; Song et al. 2022; Huang et al. 2022; Liu et al. 2023) are designed for small-scale environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their grounding mechanisms. The context is too generic to infer any specific dataset usage.",
      "processing_time": 58.18457531929016,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Recent works in autonomy have used LLMs and demonstrated significant progress (Ahn et al. 2022; Song et al. 2022; Huang et al. 2022; Liu et al. 2023; Driess et al. 2023; Brown et al. 2020b; Ouyang et al. 2022) in incorporating human knowledge, that enables efficient training of autonomous agents for tasks such as mobile manipulation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that have used LLMs in various autonomy tasks. No clear, verifiable datasets are identified.",
      "processing_time": 59.017746925354004,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Recently, agents equipped with LLM-based planners have shown remarkable capabilities to conduct complex manipulation tasks with only a few training samples (Ahn et al. 2022; Song et al. 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLM-based planners. No clear identifiers for datasets are present.",
      "processing_time": 57.61768102645874,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Recent works in autonomy have used LLMs and demonstrated significant progress (Ahn et al. 2022; Song et al. 2022; Huang et al. 2022; Liu et al. 2023; Driess et al. 2023; Brown et al. 2020b; Ouyang et al. 2022) in incorporating human knowledge, that enables efficient training of autonomous agents…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent works and their contributions to the field of autonomy using LLMs.",
      "processing_time": 58.19387626647949,
      "citing_paper_id": "261660608",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "Recent instruction tuned models such as ChatGPT have further shown strong capabilities to follow natural instructions expressed as prompts (Chung et al. 2022; Peng et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.61061239242554,
      "citing_paper_id": "261660608",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "It will also be interesting to explore the possibility of us-ing an open-source instruction-tuned LLM, such as Vicuna (Peng et al. 2023) instead of GPT-4 and GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Vicuna, GPT-4, GPT-3). The context is about exploring the use of different models, not datasets.",
      "processing_time": 60.62105870246887,
      "citing_paper_id": "261660608",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "Future actions are decided based on current perceived scenes with the memory of previously-visited regions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It describes a general approach to decision-making in navigation.",
      "processing_time": 58.00632858276367,
      "citing_paper_id": "261660608",
      "cited_paper_id": 258060965
    },
    {
      "context_text": "The only work we found to leverage LLMs specifically for navigation tasks in unknown environments is L3MVN (Yu, Kasaei, and Cao 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions L3MVN but does not refer to any specific dataset. L3MVN is a method or model, not a dataset.",
      "processing_time": 58.730740785598755,
      "citing_paper_id": "261660608",
      "cited_paper_id": 258079021
    },
    {
      "context_text": "Encouraged by the success of imitation learning (IL) on navigation tasks under resource constraints (Ramrakhya et al. 2022, 2023; Shah et al. 2023), we investigate a sample efficient IL-based method to train the low-level planner for the agent in SayNav.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods and findings.",
      "processing_time": 56.22160887718201,
      "citing_paper_id": "261660608",
      "cited_paper_id": 259262396
    },
    {
      "context_text": "Similarly, in model-based reinforcement learning (RL), an agent aims to maximize future success by using a dynamics model to simulate state transitions (Silver et al., 2017; Ha & Schmidhuber, 2018; Anthony et al., 2017; Racani ` ere et al., 2017; Nagabandi et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model-based reinforcement learning methods and approaches. No verifiable resources are identified.",
      "processing_time": 58.34311318397522,
      "citing_paper_id": "273507611",
      "cited_paper_id": 4807711
    },
    {
      "context_text": "Similarly, in model-based reinforcement learning (RL), an agent aims to maximize future success by using a dynamics model to simulate state transitions (Silver et al., 2017; Ha & Schmidhuber, 2018; Anthony et al., 2017; Racani ` ere et al., 2017; Nagabandi et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model-based reinforcement learning methods and approaches. No verifiable resources are identified.",
      "processing_time": 58.34311318397522,
      "citing_paper_id": "273507611",
      "cited_paper_id": 19449905
    },
    {
      "context_text": "Similarly, in model-based reinforcement learning (RL), an agent aims to maximize future success by using a dynamics model to simulate state transitions (Silver et al., 2017; Ha & Schmidhuber, 2018; Anthony et al., 2017; Racani ` ere et al., 2017; Nagabandi et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model-based reinforcement learning methods and approaches. No verifiable resources are identified.",
      "processing_time": 58.34311318397522,
      "citing_paper_id": "273507611",
      "cited_paper_id": 205261034
    },
    {
      "context_text": "Similarly, in model-based reinforcement learning (RL), an agent aims to maximize future success by using a dynamics model to simulate state transitions (Silver et al., 2017; Ha & Schmidhuber, 2018; Anthony et al., 2017; Racani ` ere et al., 2017; Nagabandi et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model-based reinforcement learning methods and approaches. No verifiable resources are identified.",
      "processing_time": 58.34311318397522,
      "citing_paper_id": "273507611",
      "cited_paper_id": 206853161
    },
    {
      "context_text": "Best Solution Chain-of-Thought (Wei et al., 2022) Sample S E C ReAct (Yao et al., 2022) Sample Predictive-Decoding (Ours) Sample G S E C multiple iterations of training (Silver et al., 2014) and searching (Silver et al., 2017) to achieve optimal results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.95728302001953,
      "citing_paper_id": "273507611",
      "cited_paper_id": 13928442
    },
    {
      "context_text": "Best Solution Chain-of-Thought (Wei et al., 2022) Sample S E C ReAct (Yao et al., 2022) Sample Predictive-Decoding (Ours) Sample G S E C multiple iterations of training (Silver et al., 2014) and searching (Silver et al., 2017) to achieve optimal results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.95728302001953,
      "citing_paper_id": "273507611",
      "cited_paper_id": 205261034
    },
    {
      "context_text": "Best Solution Chain-of-Thought (Wei et al., 2022) Sample S E C ReAct (Yao et al., 2022) Sample Predictive-Decoding (Ours) Sample G S E C multiple iterations of training (Silver et al., 2014) and searching (Silver et al., 2017) to achieve optimal results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.95728302001953,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Best Solution Chain-of-Thought (Wei et al., 2022) Sample S E C ReAct (Yao et al., 2022) Sample Predictive-Decoding (Ours) Sample G S E C multiple iterations of training (Silver et al., 2014) and searching (Silver et al., 2017) to achieve optimal results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.95728302001953,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "This contrasts with reinforcement learning paradigms, which use reward-based training to teach models to anticipate long-term action consequences and learn optimal decision policies (Silver et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reinforcement learning paradigm. No verifiable resources are identified.",
      "processing_time": 57.18278956413269,
      "citing_paper_id": "273507611",
      "cited_paper_id": 13928442
    },
    {
      "context_text": "Our decision-making strategy draws inspiration from model predictive control (MPC) (Qin & Badgwell, 1997).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (model predictive control).",
      "processing_time": 56.417688608169556,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "Model Predictive Control (MPC; Camacho et al., 2007) outlines the optimality condition for LLM-like planners – sequential planning must be non-myopic, which requires the anticipation of future outcomes in addition to considering past actions, represented as p ( a t ∣ History , Future ) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MPC) and its application to LLM-like planners. No verifiable resources are identified.",
      "processing_time": 59.23545432090759,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "We can see that a good prior always results in improved performances, and the gap between iterative sampling and MPC is smaller.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses performance improvements and methods.",
      "processing_time": 56.20411515235901,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "Sequential Planning with Model Predictive Control Model Predictive Control (Qin & Badgwell, 1997) introduces a different paradigm for planning: instead of optimizing an entire action sequence a ′ 0 ∶ T at once, this method selects the best action at each timestep a ′ 0 ,... ,a ′ t , fixing each as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Model Predictive Control). The context focuses on the methodology and its application in planning.",
      "processing_time": 58.55573010444641,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "On all settings, MPC is more sample efficient than naive iterative sampling.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between MPC and naive iterative sampling.",
      "processing_time": 56.573745250701904,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "One of the fundamental problems in decision-making is whether we can solve a long-term goal step by step (Qin & Badgwell, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem in decision-making. No verifiable resources are identified.",
      "processing_time": 57.38993740081787,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "In this section, we introduce Predictive-Decoding, which follows MPC principles to reduce myopic planning in LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Predictive-Decoding. The context is about reducing myopic planning in LLMs using MPC principles.",
      "processing_time": 59.410847663879395,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "…Error: Action Cannot be executed LLM: 0.45, Human: Wrong Step 1 Step 2: Pickup cellphone … N Step Foresight Step 2: Pickup keychain … N Step Foresight To investigate the causes and extent of myopia (short-sightedness) in LLM planning, we refer to optimal control theory (Qin & Badgwell, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to optimal control theory. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.59690999984741,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "Model Predictive Control Model Predictive Control (MPC) is a widely used control strategy that involves solving an optimization problem at each time step by forecasting future results (Witkin & Kass, 1988; Qin & Badgwell, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a control strategy and optimization problems. No verifiable resources are identified.",
      "processing_time": 57.55851697921753,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "However, when the π θ is not good (e.g. using a LLM that is not well-pretrained), the sample efficiency of MPC is much better than iterative sampling.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the performance of MPC relative to iterative sampling when using a poorly pretrained LLM.",
      "processing_time": 59.02878212928772,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "Sequential Planning with Model Predictive Control Model Predictive Control (Qin & Badgwell, 1997) introduces a different paradigm for planning: instead of optimizing an entire action sequence a ′ 0 ∶ T at once, this method selects the best action at each timestep a ′ 0 ,... ,a ′ t , fixing each as it progresses and then optimizing the subsequent steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Model Predictive Control). The context focuses on the methodology and its application in planning.",
      "processing_time": 58.338449478149414,
      "citing_paper_id": "273507611",
      "cited_paper_id": 16623047
    },
    {
      "context_text": "Our method follows the sampling-importance-resampling (SIR) technique from energy-based models (EBM) (Smith & Gelfand, 1992; Ji et al., 2023) to achieve the optimization goal.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.68952536582947,
      "citing_paper_id": "273507611",
      "cited_paper_id": 40992894
    },
    {
      "context_text": "Our method follows the sampling-importance-resampling (SIR) technique from energy-based models (EBM) (Smith & Gelfand, 1992; Ji et al., 2023) to achieve the optimization goal.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 57.68952536582947,
      "citing_paper_id": "273507611",
      "cited_paper_id": 263605885
    },
    {
      "context_text": "We using FLOPS as the metric, Kaplan et al. (2020), i.e. FLOPS ≈ 6 nP , where P is the number of parameters in the LLM, and n the number of generated tokens.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formula for calculating FLOPS. No verifiable resources are identified.",
      "processing_time": 57.14854860305786,
      "citing_paper_id": "273507611",
      "cited_paper_id": 210861095
    },
    {
      "context_text": "In this work, we follow Deng et al. (2020); Ji et al. (2023) and use the sampling-importance-resampling (SIR) technique, which enables us to maintain autoregressive next-token prediction while achieving global optimality.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SIR technique).",
      "processing_time": 56.14665484428406,
      "citing_paper_id": "273507611",
      "cited_paper_id": 212945787
    },
    {
      "context_text": "In previous work, foresight in generation has been applied for controllable tasks (Deng et al., 2020; Lu et al., 2021; Qin et al., 2022), typically using token-level constraints with a short lookahead of only a few tokens.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.43005394935608,
      "citing_paper_id": "273507611",
      "cited_paper_id": 212945787
    },
    {
      "context_text": "In previous work, foresight in generation has been applied for controllable tasks (Deng et al., 2020; Lu et al., 2021; Qin et al., 2022), typically using token-level constraints with a short lookahead of only a few tokens.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.43005394935608,
      "citing_paper_id": "273507611",
      "cited_paper_id": 245218671
    },
    {
      "context_text": "In previous work, foresight in generation has been applied for controllable tasks (Deng et al., 2020; Lu et al., 2021; Qin et al., 2022), typically using token-level constraints with a short lookahead of only a few tokens.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.43005394935608,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247058662
    },
    {
      "context_text": "5-Turbo is used to evaluate trajectories on the agent task AlfWorld, while Llama3-8B evaluates steps of the GSM8K trajectory.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AlfWorld",
        "GSM8K"
      ],
      "dataset_descriptions": {
        "AlfWorld": "Used to evaluate trajectories of the 5-Turbo agent, focusing on interactive learning in embodied environments.",
        "GSM8K": "Used to evaluate steps of the Llama3-8B model, focusing on problem-solving in a text-based environment."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'AlfWorld' and 'GSM8K', which are specific environments or datasets used for evaluating AI models. However, 'Llama3-8B' is a model, not a dataset, and thus excluded.",
      "processing_time": 74.15507459640503,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For example, without using any additional supervision, we achieve a 7.2% improvement on GSM8K (Cobbe et al., 2021) and a 25.3% improvement on AlfWorld (Shridhar et al., 2021) accuracy over ReAct (Yao et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "AlfWorld"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to measure the performance improvement of the model in solving math word problems, specifically comparing accuracy gains without additional supervision.",
        "AlfWorld": "Used to evaluate the model's performance in interactive learning environments, focusing on accuracy improvements in embodied text-based tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, GSM8K and AlfWorld, which are used to measure the performance improvements of the model being discussed.",
      "processing_time": 70.38544058799744,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Benchmarks Our evaluation covers three domains: math - GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), coding - HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), and agents - two agent tasks AlfWorld (Shridhar et al., 2021) and PDDL (from Agentboard, Ma et al., 2024) to understand planning ability in closed-loop interactions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GSM8K",
        "MATH",
        "HumanEval",
        "MBPP",
        "AlfWorld",
        "PDDL"
      ],
      "dataset_descriptions": {
        "GSM8K": "Used to evaluate math problem-solving skills, focusing on multi-step reasoning and numerical accuracy.",
        "MATH": "Used to assess advanced mathematical reasoning, covering a range of topics and problem types.",
        "HumanEval": "Used to evaluate code generation and execution, focusing on functional correctness and programming challenges.",
        "MBPP": "Used to test code generation and problem-solving in programming, emphasizing practical coding tasks.",
        "AlfWorld": "Used to evaluate planning and interaction in text-based environments, focusing on embodied learning and task completion.",
        "PDDL": "Used to assess planning and decision-making in agent tasks, focusing on closed-loop interactions and problem-solving."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions specific datasets used for evaluating planning capabilities in different domains. These datasets are clearly named and used for specific research purposes.",
      "processing_time": 82.98002052307129,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Research has demonstrated that with carefully designed prompts and in-context examples, LLM autoregressive planning can achieve competitive performance in various tasks (Cobbe et al., 2021; Chen et al., 2021; Shridhar et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research works. The context focuses on the capabilities of LLMs in planning tasks through prompts and examples.",
      "processing_time": 59.11008834838867,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For the AlfWorld task, Predictive-Decoding shows a larger slope across 20 steps compared to Act and ReAct, showcasing improved reflection ability compared with Act and ReAct.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods (Predictive-Decoding, Act, ReAct) on a task (AlfWorld).",
      "processing_time": 59.57321381568909,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), coding - HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021), and agents - two agent tasks AlfWorld (Shridhar et al., 2021) and PDDL (from Agentboard, Ma et al., 2024) to understand planning ability in closed-loop interactions.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Expecting ',' delimiter: line 13 column 9 (char 1434), response: ```json\n{\n    \"reasoning\": \"The citation mentions several datasets and tasks used to evaluate planni",
      "processing_time": 88.24250817298889,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "Human annotators label each step in the AlfWorld trajectory with scores from 0 , 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a labeling process for trajectories in AlfWorld, which is a method or environment, not a dataset.",
      "processing_time": 59.41934394836426,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "For instance, in AlfWorld, when the agent tries to find an apple and place it on desk 1, the imagined state after ”open fridge 1” might incorrectly include an apple in the fridge, even if it’s not there.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a scenario within the ALFWorld environment but does not reference any dataset by name.",
      "processing_time": 58.9577739238739,
      "citing_paper_id": "273507611",
      "cited_paper_id": 222208810
    },
    {
      "context_text": "…via Sequential Sampling Sampling-based methods have been extensively used to solve combinatorial optimization efficiently (Sun & Yang, 2023; Janner et al., 2022; Qin et al., 2022; Du et al., 2024), by constructing an energy-based model where the generative probability reflects the objective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on sampling-based methods and energy-based models for combinatorial optimization.",
      "processing_time": 58.78720021247864,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247058662
    },
    {
      "context_text": "…MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various methods and models but does not refer to any specific datasets. The focus is on comparing different planning and reasoning approaches.",
      "processing_time": 58.274112939834595,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various methods and models but does not refer to any specific datasets. The focus is on comparing different planning and reasoning approaches.",
      "processing_time": 58.274112939834595,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "…MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions various methods and models but does not refer to any specific datasets. The focus is on comparing different planning and reasoning approaches.",
      "processing_time": 58.274112939834595,
      "citing_paper_id": "273507611",
      "cited_paper_id": 270620269
    },
    {
      "context_text": "Our method matches or exceeds Beam Search and, when combined with Self-Consistency, outperforms both autoregressive and beam search approaches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.452441692352295,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "However, recent advancements demonstrate that LLMs possess the capability for sequential planning (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability of LLMs. No verifiable resources are identified.",
      "processing_time": 57.51174736022949,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Models like GPT-4 (Achiam et al., 2023) have demonstrated a capacity for step-by-step reasoning and planning, breaking down complex problems into sequential steps that progressively lead to solutions (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.10762357711792,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "This method can be formalized as: This approach is widely adopted in LLM reasoning and planning, encompassing Chain-of-Thought (CoT; Wei et al. 2022), ReAct (Yao et al., 2022), and Voyager (Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.31630229949951,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "This method can be formalized as: This approach is widely adopted in LLM reasoning and planning, encompassing Chain-of-Thought (CoT; Wei et al. 2022), ReAct (Yao et al., 2022), and Voyager (Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 58.31630229949951,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "LLM-based Planning and Reasoning One major development is the emergence of LLM step-by-step reasoning abilities (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability of LLMs. No verifiable resources are identified.",
      "processing_time": 57.501788854599,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Baselines We incoporate iterative planning baselines – Self-Consistency (Wang et al., 2022), Guided-Decoding (Xie et al., 2024), MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not reference any specific datasets. The context is focused on describing various planning baselines and methods used in the research.",
      "processing_time": 59.32263803482056,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Baselines We incoporate iterative planning baselines – Self-Consistency (Wang et al., 2022), Guided-Decoding (Xie et al., 2024), MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods and models but does not reference any specific datasets. The context is focused on describing various planning baselines and methods used in the research.",
      "processing_time": 59.32263803482056,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Previous methods have implemented various techniques to enhance planning, such as repeated sampling (Wang et al., 2022) and searching algorithms (Yao et al., 2024; Xie et al., 2024; Hao et al., 2023; Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.47546148300171,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Previous methods have implemented various techniques to enhance planning, such as repeated sampling (Wang et al., 2022) and searching algorithms (Yao et al., 2024; Xie et al., 2024; Hao et al., 2023; Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.47546148300171,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Previous methods have implemented various techniques to enhance planning, such as repeated sampling (Wang et al., 2022) and searching algorithms (Yao et al., 2024; Xie et al., 2024; Hao et al., 2023; Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.47546148300171,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "…approaches to trajectory optimization follow a straightforward intuition: using large language models (LLMs) to generate diverse trajectories through sampling or search, and then selecting the best one (Wang et al., 2022; Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Shinn et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.36782455444336,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "…approaches to trajectory optimization follow a straightforward intuition: using large language models (LLMs) to generate diverse trajectories through sampling or search, and then selecting the best one (Wang et al., 2022; Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Shinn et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.36782455444336,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "…approaches to trajectory optimization follow a straightforward intuition: using large language models (LLMs) to generate diverse trajectories through sampling or search, and then selecting the best one (Wang et al., 2022; Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Shinn et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.36782455444336,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "…approaches to trajectory optimization follow a straightforward intuition: using large language models (LLMs) to generate diverse trajectories through sampling or search, and then selecting the best one (Wang et al., 2022; Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Shinn et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.36782455444336,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Traditionally, reinforcement learning approaches have relied on Self-Consistency (Wang et al., 2022) Sample G Tree-of-Thought (Yao et al., 2024) Search Guided-Decoding (Xie et al., 2024) Search Monte Carlo Tree Search (Hao et al., 2023) Search Reflexion (Shinn et al., 2024) Prompt",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and approaches but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.64902400970459,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Traditionally, reinforcement learning approaches have relied on Self-Consistency (Wang et al., 2022) Sample G Tree-of-Thought (Yao et al., 2024) Search Guided-Decoding (Xie et al., 2024) Search Monte Carlo Tree Search (Hao et al., 2023) Search Reflexion (Shinn et al., 2024) Prompt",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and approaches but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.64902400970459,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Traditionally, reinforcement learning approaches have relied on Self-Consistency (Wang et al., 2022) Sample G Tree-of-Thought (Yao et al., 2024) Search Guided-Decoding (Xie et al., 2024) Search Monte Carlo Tree Search (Hao et al., 2023) Search Reflexion (Shinn et al., 2024) Prompt",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and approaches but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.64902400970459,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Traditionally, reinforcement learning approaches have relied on Self-Consistency (Wang et al., 2022) Sample G Tree-of-Thought (Yao et al., 2024) Search Guided-Decoding (Xie et al., 2024) Search Monte Carlo Tree Search (Hao et al., 2023) Search Reflexion (Shinn et al., 2024) Prompt",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions several methods and approaches but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 58.64902400970459,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Baselines We incoporate iterative planning baselines – Self-Consistency (Wang et al., 2022), Guided-Decoding (Xie et al., 2024), MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models used as baselines for evaluation, but does not mention any specific datasets. The context is focused on comparing different planning and decoding strategies.",
      "processing_time": 59.718411922454834,
      "citing_paper_id": "273507611",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "Baselines We incoporate iterative planning baselines – Self-Consistency (Wang et al., 2022), Guided-Decoding (Xie et al., 2024), MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models used as baselines for evaluation, but does not mention any specific datasets. The context is focused on comparing different planning and decoding strategies.",
      "processing_time": 59.718411922454834,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Baselines We incoporate iterative planning baselines – Self-Consistency (Wang et al., 2022), Guided-Decoding (Xie et al., 2024), MCTS (Hao et al., 2023) and Self-Infilling (Zheng et al., 2023) – and sequential planning baselines – ReAct (Yao et al., 2022), Beam Search, PAL (Gao et al., 2023), COT (Wei et al., 2022)), Tree of Thought (Yao et al., 2024), A* Search (Zhuang et al., 2023; Wang et al., 2024b) – in our evaluation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various methods and models used as baselines for evaluation, but does not mention any specific datasets. The context is focused on comparing different planning and decoding strategies.",
      "processing_time": 59.718411922454834,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "This is in line with previous observations (Uesato et al., 2022; Lightman et al., 2023) that LLMs are more natural at evaluating complete trajectories, rather than intermediate actions.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only observations about LLM capabilities. No verifiable resources are identified.",
      "processing_time": 57.50118589401245,
      "citing_paper_id": "273507611",
      "cited_paper_id": 254017497
    },
    {
      "context_text": "This is in line with previous observations (Uesato et al., 2022; Lightman et al., 2023) that LLMs are more natural at evaluating complete trajectories, rather than intermediate actions.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only observations about LLM capabilities. No verifiable resources are identified.",
      "processing_time": 57.50118589401245,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258987659
    },
    {
      "context_text": "Combinatorial Optimization via Sequential Sampling Sampling-based methods have been extensively used to solve combinatorial optimization efficiently (Sun & Yang, 2023; Janner et al., 2022; Qin et al., 2022; Du et al., 2024), by constructing an energy-based model where the generative probability…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for solving combinatorial optimization problems.",
      "processing_time": 57.23668932914734,
      "citing_paper_id": "273507611",
      "cited_paper_id": 256900800
    },
    {
      "context_text": "This capability is further enhanced by more expressive inference algorithms like searching (Yao et al., 2022; 2024; Hao et al., 2023; Wang et al., 2023a; Zheng et al., 2023; Xie et al., 2024; Zhao et al., 2024; Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing inference algorithms and their enhancements. No verifiable resources are identified.",
      "processing_time": 58.59389638900757,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258426922
    },
    {
      "context_text": "This capability is further enhanced by more expressive inference algorithms like searching (Yao et al., 2022; 2024; Hao et al., 2023; Wang et al., 2023a; Zheng et al., 2023; Xie et al., 2024; Zhao et al., 2024; Sun et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing inference algorithms and their enhancements. No verifiable resources are identified.",
      "processing_time": 58.59389638900757,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Prompt Sensitivity Analysis Wang & Zhou (2024) proposes an intereting problem: improving LLM reasoning without prompting.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a problem related to improving LLM reasoning without prompting.",
      "processing_time": 57.45328903198242,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Reflection Analysis Shinn et al. (2024); Ma et al. (2024) emphasize the importance of reflection for decision making.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of reflection for decision making.",
      "processing_time": 57.01207709312439,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Each step in the GSM8K trajectory is labeled as either Correct or Incorrect following Lightman et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a labeling process for a trajectory. No clear, verifiable dataset name is provided.",
      "processing_time": 58.833492279052734,
      "citing_paper_id": "273507611",
      "cited_paper_id": 258987659
    },
    {
      "context_text": "To ensure fair comparisons, we use standardized prompts from Guo et al. (2024), Gao et al. (2023), Cobbe et al. (2021), and Ma et al. (2024).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to standardized prompts from various studies. No clear identifiers for datasets are present.",
      "processing_time": 58.4099235534668,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Results on Math Tasks Following (Xie et al., 2024), we use the Program Aided Language Model (PAL) format, executing generated code in Python to obtain answers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PAL format) for executing generated code. No verifiable dataset names are present in the context.",
      "processing_time": 59.30663871765137,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "5 is obtained through prompt-based self-evaluation (Xie et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for evaluating a model.",
      "processing_time": 56.93644690513611,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Specifically, on the GSM8K benchmark, Predictive-Decoding outperforms MCTS (Hao et al., 2023) by 2.4% and Guided Decoding (Xie et al., 2024) by 1.4% , while utilizing 50% of the FLOPS, demonstrating its effectiveness and efficiency.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons on a benchmark. The GSM8K benchmark is excluded as it is primarily a leaderboard and not a downloadable dataset.",
      "processing_time": 59.88188886642456,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Guided-Decoding works well with Codex but underperforms with Llama3 due to overconfident scores.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Codex, Llama3). The context focuses on the performance of guided-decoding with these models.",
      "processing_time": 59.87784719467163,
      "citing_paper_id": "273507611",
      "cited_paper_id": 267211867
    },
    {
      "context_text": "Recent studies (Shih et al., 2023; Bachmann & Nagarajan, 2024) have observed myopia in language models pretrained with next-token prediction.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about language models pretrained with next-token prediction.",
      "processing_time": 57.66101670265198,
      "citing_paper_id": "273507611",
      "cited_paper_id": 268364153
    },
    {
      "context_text": "While next-token prediction is typically greedy, evidence suggests extensive pretraining might enable LLMs to implicitly plan future tokens (Wu et al., 2024a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about LLMs and their planning capabilities.",
      "processing_time": 57.65448546409607,
      "citing_paper_id": "273507611",
      "cited_paper_id": 268819892
    },
    {
      "context_text": "Our research further reveals that even in advanced LLMs like Llama-3 (Dubey et al., 2024), more than half of the reasoning processes on popular math tasks demonstrate a lack of global awareness, with myopia being particularly pronounced in incorrect instances (§3.1).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Llama-3) and a general finding about reasoning processes. No verifiable datasets are referenced.",
      "processing_time": 59.88484477996826,
      "citing_paper_id": "273507611",
      "cited_paper_id": 271571434
    },
    {
      "context_text": "While these methods can improve performance, they often require significantly higher computational resources, resulting in sub-optimal computation-performance trade-offs (Snell et al., 2024; Wu et al., 2024b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses computational resources and performance trade-offs.",
      "processing_time": 56.980093240737915,
      "citing_paper_id": "273507611",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "This aligns with findings from studies on inference time scaling (Snell et al., 2024), which we discuss further in §5.4.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a study on inference time scaling. No verifiable resources are identified.",
      "processing_time": 58.513299226760864,
      "citing_paper_id": "273507611",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "Recent work discusses the inference time scaling law for LLM reasoning (Wu et al., 2024b; Snell et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing inference time scaling laws for LLMs.",
      "processing_time": 57.795490741729736,
      "citing_paper_id": "273507611",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "Guiding LLM Reasoning with Reward Model We use Math-Shepherd (Wang et al., 2023b) We follow setting in §5.3 using reward model Math-Shepherd.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Math-Shepherd' but does not specify it as a dataset. It is referred to as a reward model, which is excluded according to the instructions.",
      "processing_time": 60.40125226974487,
      "citing_paper_id": "273507611",
      "cited_paper_id": null
    },
    {
      "context_text": "…in various natural language understanding and instruction following tasks [19, 17, 21] motivates the use of LLMs in robotic task planning, aiming to exploit the commonsense reasoning capability of LLMs. Recent studies have demonstrated the capabilities of LLMs in task planning [8, 2, 9, 12, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and studies involving LLMs in task planning.",
      "processing_time": 58.192790031433105,
      "citing_paper_id": "271212449",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "…in various natural language understanding and instruction following tasks [19, 17, 21] motivates the use of LLMs in robotic task planning, aiming to exploit the commonsense reasoning capability of LLMs. Recent studies have demonstrated the capabilities of LLMs in task planning [8, 2, 9, 12, 22].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general capabilities and studies involving LLMs in task planning.",
      "processing_time": 58.192790031433105,
      "citing_paper_id": "271212449",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "[8] attempted to query LLMs to predict actions for task completion with several demonstrations and the task description in natural language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach using LLMs for task completion. No clear, verifiable dataset names are provided.",
      "processing_time": 59.52915334701538,
      "citing_paper_id": "271212449",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "I. b) Baselines: To demonstrate the advantages of our method, we compare with the following baselines: • Language Models as Zero-Shot Planners (LMZSP) [8] is a baseline that queries an LLM to generate a plan for a given task with few-shot examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Language Models as Zero-Shot Planners) used as a baseline.",
      "processing_time": 58.74729037284851,
      "citing_paper_id": "271212449",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Some works [12, 22] provided code-style prompts to LLMs and required LLMs to generate codes that the robot can execute directly.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of code-style prompts with LLMs for generating executable codes for robots.",
      "processing_time": 58.89858317375183,
      "citing_paper_id": "271212449",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "On the other hand, some works [12, 22] provide LLMs with code-style prompts that contain task information, hints, and few-shot examples to synthesize executable codes for robots.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for providing LLMs with code-style prompts.",
      "processing_time": 58.15494108200073,
      "citing_paper_id": "271212449",
      "cited_paper_id": 252355542
    },
    {
      "context_text": "Therefore, some researchers [15, 26] explored using LLMs to translate the task description from natural language to PDDL and leveraged the symbolic planner to solve the planning problem.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs and PDDL for planning problems.",
      "processing_time": 58.33416199684143,
      "citing_paper_id": "271212449",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Compared to the works by Liu et al. [16] and Huang et al. [9], our proposed LASP can avoid repeating the same errors after correction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to previous works. There are no clear identifiers for datasets in the context.",
      "processing_time": 58.99720120429993,
      "citing_paper_id": "271212449",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "Liu et al. [16] proposed REFLECT, a framework that utilizes LLMs to summarize robot experiences, explain the error, and predict actions to correct the error directly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called REFLECT.",
      "processing_time": 57.25595712661743,
      "citing_paper_id": "271212449",
      "cited_paper_id": 259274760
    },
    {
      "context_text": "Gragera et al. [5] proposed an automated planning approach to repair planning models with incomplete action effects, allowing symbolic planners to generate solutions for tasks that were originally unsolvable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for repairing planning models. No verifiable resources are identified.",
      "processing_time": 58.5564444065094,
      "citing_paper_id": "271212449",
      "cited_paper_id": 259601920
    },
    {
      "context_text": "Heuristics are often used to guide the search (Baier et al., 2007; Hoffmann, 2001; Helmert, 2006; Bryce and Kambhampati, 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to heuristics and planning systems. No verifiable resources are identified.",
      "processing_time": 58.99816393852234,
      "citing_paper_id": "261332464",
      "cited_paper_id": 9968823
    },
    {
      "context_text": "Since search space S is huge, approximate decoding strategies are used for tractability (Holtzman et al., 2020; Luong et al., 2015; Wiseman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and strategies for approximate decoding. No verifiable resources are identified.",
      "processing_time": 58.99524259567261,
      "citing_paper_id": "261332464",
      "cited_paper_id": 23892230
    },
    {
      "context_text": "Since search space S is huge, approximate decoding strategies are used for tractability (Holtzman et al., 2020; Luong et al., 2015; Wiseman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and strategies for approximate decoding. No verifiable resources are identified.",
      "processing_time": 58.99524259567261,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Unconditional search can be hard to scale in environments with many feasible actions and objects (Puig et al., 2018; Shridhar et al., 2020) due to large branching factors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in scaling unconditional search in complex environments.",
      "processing_time": 57.77159285545349,
      "citing_paper_id": "261332464",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "• Environment artifacts : the VH agent cannot ﬁnd or interact with objects nearby when sitting , and some • Environment complexities : when an object is not accessible, the generated assertions might not be enough.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes environment artifacts and complexities in a simulation, which are not datasets.",
      "processing_time": 59.16872954368591,
      "citing_paper_id": "261332464",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "We evaluate our method in the Virtual Home (VH) Environment (Puig et al., 2018), a deterministic simulation platform fortypicalhouseholdactivities.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Virtual Home (VH) Environment, which is a simulation platform, not a dataset. No specific dataset is mentioned.",
      "processing_time": 59.28024244308472,
      "citing_paper_id": "261332464",
      "cited_paper_id": 49317780
    },
    {
      "context_text": "For high-level planning, most works in robotics use search in a pre-deﬁned domain (Fikes and Nilsson, 1971; Jiang et al., 2018; Garrett et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in robotics planning. No verifiable resources are identified.",
      "processing_time": 59.14956450462341,
      "citing_paper_id": "261332464",
      "cited_paper_id": 123106881
    },
    {
      "context_text": "For high-level planning, most works in robotics use search in a pre-deﬁned domain (Fikes and Nilsson, 1971; Jiang et al., 2018; Garrett et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in robotics planning. No verifiable resources are identified.",
      "processing_time": 59.14956450462341,
      "citing_paper_id": "261332464",
      "cited_paper_id": 210861196
    },
    {
      "context_text": "…learning, hierarchical learning, language as planning space, learning compositional skills and more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang et al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021; Nair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022; Silver et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. The context is focused on methodologies and concepts rather than datasets.",
      "processing_time": 59.43828201293945,
      "citing_paper_id": "261332464",
      "cited_paper_id": 189998275
    },
    {
      "context_text": "Prompting LLMs to generate text useful for robot task planning is a nascent topic (Ahn et al., 2022; Jansen, 2020; Huanget al., 2022a,b; Li et al., 2022; Patel andPavlick, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No clear identifiers for datasets are present.",
      "processing_time": 58.59329128265381,
      "citing_paper_id": "261332464",
      "cited_paper_id": 222066988
    },
    {
      "context_text": "…compositional skills and more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang et al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021; Nair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022; Silver et al., 2022; Srinivas et al., 2018; Xu et al., 2018, 2019; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets within the given context.",
      "processing_time": 59.43277144432068,
      "citing_paper_id": "261332464",
      "cited_paper_id": 238259902
    },
    {
      "context_text": "…compositional skills and more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang et al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021; Nair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022; Silver et al., 2022; Srinivas et al., 2018; Xu et al., 2018, 2019; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets within the given context.",
      "processing_time": 59.43277144432068,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "…compositional skills and more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang et al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021; Nair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022; Silver et al., 2022; Srinivas et al., 2018; Xu et al., 2018, 2019; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets within the given context.",
      "processing_time": 59.43277144432068,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Cao and Lee (2023) explores generating behavior trees to study hierarchical task planning using LLMs. Skreta et al. (2023) proposes iterative error correction via a syntax veriﬁer that repeatedly prompts the LLM with previous query appended with a list of errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the methodologies used to study hierarchical task planning and error correction in LLMs.",
      "processing_time": 60.58861231803894,
      "citing_paper_id": "261332464",
      "cited_paper_id": 257219432
    },
    {
      "context_text": "…as planning space, learning compositional skills and more (Akakzia et al., 2021; Eysenbach et al., 2019; Jiang et al., 2019; Kurutach et al., 2018; Mirchandani et al., 2021; Nair and Finn, 2020; Shah et al., 2022; Sharma et al., 2022; Silver et al., 2022; Srinivas et al., 2018; Xu et al., 2018,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 60.42560362815857,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Prompt design is challenging given the lack of paired natural language instruction text with executable plans or robot action sequences (Liu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in prompt design.",
      "processing_time": 57.32346034049988,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Devising a prompt for task plan prediction can be broken down into a prompting function and an answer search strategy (Liu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to task plan prediction.",
      "processing_time": 57.9408974647522,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "ProgPrompt doesn’t tackle the issue, however, Xie et al. (2023b) shows that multiple instances of the same objects can be handled by using labels with object IDs such as “book_1, book_2”.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling multiple instances of the same objects using labels with object IDs.",
      "processing_time": 59.23757767677307,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "A recent work Xie et al. (2023b) uses LLMs to generate PDDL goals, however, it requires full domain speciﬁcation for a given environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using LLMs to generate PDDL goals.",
      "processing_time": 58.86022877693176,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Some recent works Xie et al. (2023a), Capitanelli and Mastrogiovanni (2023) use PDDL as the translation language instead of code, and use the LLM to generate either a PDDL plan or the goal.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of PDDL as a translation language and LLMs for generating plans or goals.",
      "processing_time": 59.68310761451721,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "LangPrompt is similar to the prompts built by Huang et al. (2022a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (LangPrompt) and a reference to another work. There are no verifiable resources or datasets mentioned.",
      "processing_time": 60.530174255371094,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "…world knowledge and programming language understanding to generate situated task plans that can be directly executed sible action plans in context of robotic task planning (Ahn et al., 2022; Huang et al., 2022b,a; Zeng et al., 2022) by either scoring next steps or generating new steps directly.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works that discuss task planning in robotics. There are no clear identifiers for datasets.",
      "processing_time": 59.95554232597351,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "First, ProgPrompt (rows 3–6) outperforms prior work (Huang et al., 2022a) (row 8) by a substantial margin on all metrics using the same large language model backbone.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics between ProgPrompt and prior work.",
      "processing_time": 58.61690640449524,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Inner-Monologue (Huang et al., 2022b) introduces environment feedback and state monitoring, but still found that LLM planners proposed actions involving objects not present in the scene.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Inner-Monologue' but does not refer to it as a dataset. It is described as a method or approach that introduces environment feedback and state monitoring.",
      "processing_time": 59.933754444122314,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Closest to our work, Huang et al. (2022a) generates open-domain plans using LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using LLMs for generating open-domain plans.",
      "processing_time": 58.820929765701294,
      "citing_paper_id": "261332464",
      "cited_paper_id": null
    },
    {
      "context_text": "Such programs can be thought of as a direct generalization of line plans that we have considered in the rest of the paper [30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a concept discussed in another paper, which is not a reusable resource.",
      "processing_time": 59.92734217643738,
      "citing_paper_id": "249889477",
      "cited_paper_id": 634360
    },
    {
      "context_text": "In terms of parsing natural language text back into structured forms, the particular task we are interested in is converting plans generated by the LLM back into plan forms that can be used by plan validator tools like [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (VAL) for plan validation. The context focuses on the task of converting LLM-generated plans into a form usable by such tools.",
      "processing_time": 60.45352602005005,
      "citing_paper_id": "249889477",
      "cited_paper_id": 3098522
    },
    {
      "context_text": "Some prominent existing reasoning benchmarks include, BIG-BENCH [29], GSM8K [3], AQUA [18], SVAMP [23], CommonsenseQA [31] and StrategyQA [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not specify their usage in the research. These benchmarks are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 60.08701515197754,
      "citing_paper_id": "249889477",
      "cited_paper_id": 12777818
    },
    {
      "context_text": "Some prominent existing reasoning benchmarks include, BIG-BENCH [29], GSM8K [3], AQUA [18], SVAMP [23], CommonsenseQA [31] and StrategyQA [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not specify their usage in the research. These benchmarks are excluded as they are primarily used for score comparison rather than as reusable datasets.",
      "processing_time": 60.08701515197754,
      "citing_paper_id": "249889477",
      "cited_paper_id": 232223322
    },
    {
      "context_text": "In conclusion, we hope that PlanBench encourages other researchers to test the capabilities of their systems across different LLM models [2, 4, 28, 36, 24, 32, 9] and even those that are finetuned for such tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general encouragement to test LLM capabilities. No verifiable resources are identified.",
      "processing_time": 58.86134076118469,
      "citing_paper_id": "249889477",
      "cited_paper_id": 245353475
    },
    {
      "context_text": "In conclusion, we hope that PlanBench encourages other researchers to test the capabilities of their systems across different LLM models [2, 4, 28, 36, 24, 32, 9] and even those that are finetuned for such tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general encouragement to test LLM capabilities. No verifiable resources are identified.",
      "processing_time": 58.86134076118469,
      "citing_paper_id": "249889477",
      "cited_paper_id": 246411325
    },
    {
      "context_text": "In conclusion, we hope that PlanBench encourages other researchers to test the capabilities of their systems across different LLM models [2, 4, 28, 36, 24, 32, 9] and even those that are finetuned for such tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general encouragement to test LLM capabilities. No verifiable resources are identified.",
      "processing_time": 58.86134076118469,
      "citing_paper_id": "249889477",
      "cited_paper_id": 247778764
    },
    {
      "context_text": "These roles range from generating plans [11, 33] and heuristics [33, 1] to extracting planning knowledge [7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only roles of language models in planning. No clear identifiers for datasets are present.",
      "processing_time": 59.0137414932251,
      "citing_paper_id": "249889477",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Additionally, LLMs are also being made to use feedback from users or the environment to improve their planning performance [12, 35, 25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing LLMs and planning performance.",
      "processing_time": 58.15681719779968,
      "citing_paper_id": "249889477",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "In many settings, LLMs have proven more effective at translating natural language into formal representations rather than performing the planning itself, as noted in works such as Alford et al. (2009); Helmert (2009); Xie et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research works. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 59.41533398628235,
      "citing_paper_id": "276647180",
      "cited_paper_id": 5160783
    },
    {
      "context_text": "In many settings, LLMs have proven more effective at translating natural language into formal representations rather than performing the planning itself, as noted in works such as Alford et al. (2009); Helmert (2009); Xie et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research works. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 59.41533398628235,
      "citing_paper_id": "276647180",
      "cited_paper_id": 9377590
    },
    {
      "context_text": "Building on this momentum, several strategies have been proposed to map user instructions into PDDL problems Pallagani et al. (2023); Liu et al. (2023a); Dagan et al. (2023); Gestrin et al. (2024); Zhang et al. (2024), without however providing conclusive evidence for the feasibility of the task in…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing strategies for mapping user instructions into PDDL problems. No verifiable resources are identified.",
      "processing_time": 59.57187461853027,
      "citing_paper_id": "276647180",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Nonetheless, while LLMs have demonstrated planning capabilities Huang et al. (2024), they continue to struggle with long-horizon planning, uncertainty in generated plans, and generalisation to unseen domains Sermanet et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the planning capabilities and limitations of LLMs.",
      "processing_time": 58.0468475818634,
      "citing_paper_id": "276647180",
      "cited_paper_id": 267411892
    },
    {
      "context_text": "…has spurred further research into the underlying reasoning capabilities of LLMs and their potential role in executing complete planning tasks Advancement in LLMs fuelled recent efforts that looked into how these multi-billion parameter models can be best employed as agents Huang et al. (2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advancements in LLMs and their planning capabilities.",
      "processing_time": 57.868963956832886,
      "citing_paper_id": "276647180",
      "cited_paper_id": 267411892
    },
    {
      "context_text": "…in the PDDL language, focusing on their ability to parse, generate, and reason with PDDL. Speciﬁ-cally, we leverage the Planetarium benchmark Zuo et al. (2024) alongside the dataset introduced by Oswald et al. (2024) to assess how well these models understand and generate actions, problems,…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Planetarium benchmark",
        "dataset introduced by Oswald et al. (2024)"
      ],
      "dataset_descriptions": {
        "Planetarium benchmark": "Used to assess the ability of models to translate text into structured planning languages, focusing on parsing, generation, and reasoning with PDDL.",
        "dataset introduced by Oswald et al. (2024)": "Used to evaluate how well models understand and generate actions and problems in PDDL, complementing the Planetarium benchmark."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions two specific resources: 'Planetarium benchmark' and a dataset by Oswald et al. (2024). Both are used to assess the performance of models in understanding and generating actions and problems in PDDL.",
      "processing_time": 76.47663187980652,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "…at about 32%, its performance –and that of all off-the-shelf models– remains inadequate for this task, suggesting that further techniques such as few-shot prompting, ﬁne-tuning, or other enhancements are necessary to assist PDDL problem generation, as explored by Zuo et al. (2024) for instance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the performance of models and suggests techniques for improvement. No verifiable resources are identified.",
      "processing_time": 58.832674503326416,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "Novel benchmarks such as PlanBench Valmeekam et al. (2023), AutoPlanBench Stein et al. (2024), Planetarium Zuo et al. (2024), and the domain benchmark from Oswald et al. (2024) have been introduced to assess LLMs’ planning capabilities using PDDL.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Planetarium"
      ],
      "dataset_descriptions": {
        "Planetarium": "Used to assess LLMs’ planning capabilities by translating text to structured planning languages, focusing on rigorous benchmarking and evaluation of PDDL-based planning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks, but only 'Planetarium' is a specific, verifiable dataset with a clear reference to a published paper.",
      "processing_time": 65.92764735221863,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "Novel benchmarks such as PlanBench Valmeekam et al. (2023), AutoPlanBench Stein et al. (2024), Planetarium Zuo et al. (2024), and the domain benchmark from Oswald et al. (2024) have been introduced to assess LLMs’ planning capabilities using PDDL.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Planetarium"
      ],
      "dataset_descriptions": {
        "Planetarium": "Used to assess LLMs’ planning capabilities by translating text to structured planning languages, focusing on rigorous benchmarking and evaluation of PDDL-based planning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks, but only 'Planetarium' is a specific, verifiable dataset with a clear reference to a published paper.",
      "processing_time": 65.92764735221863,
      "citing_paper_id": "276647180",
      "cited_paper_id": null
    },
    {
      "context_text": "Novel benchmarks such as PlanBench Valmeekam et al. (2023), AutoPlanBench Stein et al. (2024), Planetarium Zuo et al. (2024), and the domain benchmark from Oswald et al. (2024) have been introduced to assess LLMs’ planning capabilities using PDDL.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Planetarium"
      ],
      "dataset_descriptions": {
        "Planetarium": "Used to assess LLMs’ planning capabilities by translating text to structured planning languages, focusing on rigorous benchmarking and evaluation of PDDL-based planning tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks, but only 'Planetarium' is a specific, verifiable dataset with a clear reference to a published paper.",
      "processing_time": 65.92764735221863,
      "citing_paper_id": "276647180",
      "cited_paper_id": null
    },
    {
      "context_text": "Task Signature = [input: NL instruction, PDDL domain; output: PDDL problem ] We choose the Planetarium benchmark to evaluate the problem generation capabilities of models Zuo et al. (2024).",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Planetarium benchmark' which is a specific benchmark used to evaluate the problem generation capabilities of models. However, it is excluded because it is primarily a benchmark for score comparison rather than a reusable dataset.",
      "processing_time": 61.382474422454834,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "Early studies by Zuo et al. (2024) and Oswald et al. (2024) demonstrated that LLMs are capable of translating natural language descriptions into syntactically valid PDDL representations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that LLMs can translate natural language to PDDL. No verifiable resources are named.",
      "processing_time": 58.802446603775024,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "In this study, we step back to examine the ﬂuency of twenty LLMs from seven major families in the PDDL language, focusing on their ability to parse, generate, and reason with PDDL. Speciﬁ-cally, we leverage the Planetarium benchmark Zuo et al. (2024) alongside the dataset introduced by Oswald et al. (2024) to assess how well these models understand and generate actions, problems, and plans.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Planetarium benchmark",
        "dataset introduced by Oswald et al. (2024)"
      ],
      "dataset_descriptions": {
        "Planetarium benchmark": "Used to assess LLMs' ability to translate text into structured planning languages, focusing on parsing, generating, and reasoning with PDDL.",
        "dataset introduced by Oswald et al. (2024)": "Used to evaluate LLMs' understanding and generation of actions, problems, and plans in PDDL, complementing the Planetarium benchmark."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two specific resources: 'Planetarium benchmark' and 'dataset introduced by Oswald et al. (2024)'. Both are used to assess LLMs' understanding and generation capabilities in PDDL.",
      "processing_time": 76.63175892829895,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "For this purpose, we selected domain-problem pairs from the Planetarium benchmark to prompt the models for plan generation.",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Planetarium benchmark' which is a specific benchmark. However, it is excluded because it is primarily used for score comparison rather than as a reusable dataset.",
      "processing_time": 59.991299867630005,
      "citing_paper_id": "276647180",
      "cited_paper_id": 270923717
    },
    {
      "context_text": "As described in Chen et al. (2024), long-horizon planning tasks involve “extended sequences of actions” or span “over a prolonged period of time”.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes the nature of long-horizon planning tasks.",
      "processing_time": 57.523313999176025,
      "citing_paper_id": "276647180",
      "cited_paper_id": 271693407
    },
    {
      "context_text": "The position articulated in Kambhampati et al. (2024) underscores that, although LLMs may not inherently plan effectively, they can nonetheless play a signiﬁcant supportive role in LLM-modulo planning frameworks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a position on the capabilities of LLMs in planning frameworks.",
      "processing_time": 57.857994079589844,
      "citing_paper_id": "276647180",
      "cited_paper_id": 272330427
    },
    {
      "context_text": "…the PDDL language, focusing on their ability to parse, generate, and reason with PDDL. Speciﬁ-cally, we leverage the Planetarium benchmark Zuo et al. (2024) alongside the dataset introduced by Oswald et al. (2024) to assess how well these models understand and generate actions, problems, and plans.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "dataset introduced by Oswald et al. (2024)"
      ],
      "dataset_descriptions": {
        "dataset introduced by Oswald et al. (2024)": "Used to assess LLMs' understanding and generation of PDDL actions, problems, and plans, focusing on parsing, generating, and reasoning capabilities."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions two resources: 'Planetarium benchmark' and a dataset by Oswald et al. (2024). The Planetarium benchmark is excluded as it is a challenge/leaderboard. The dataset by Oswald et al. (2024) is included as it is a specific, verifiable dataset.",
      "processing_time": 75.09574103355408,
      "citing_paper_id": "276647180",
      "cited_paper_id": null
    },
    {
      "context_text": "…on this momentum, several strategies have been proposed to map user instructions into PDDL problems Pallagani et al. (2023); Liu et al. (2023a); Dagan et al. (2023); Gestrin et al. (2024); Zhang et al. (2024), without however providing conclusive evidence for the feasibility of the task in the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing strategies for mapping user instructions into PDDL problems. No verifiable resources are identified.",
      "processing_time": 59.46430778503418,
      "citing_paper_id": "276647180",
      "cited_paper_id": null
    },
    {
      "context_text": "While there are datasets available from previous games [22, 23], GTB provides many different sized levels with deceptive paths to traverse.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GTB"
      ],
      "dataset_descriptions": {
        "GTB": "Used to study model-free planning in environments with deceptive paths, focusing on the traversal of different sized levels to understand planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'GTB' as a dataset providing levels with deceptive paths, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 65.01372694969177,
      "citing_paper_id": "273233132",
      "cited_paper_id": 57825680
    },
    {
      "context_text": "As a result, the accurate evaluation of LLMs has become a key focus for researchers [6–8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general focus on evaluating LLMs.",
      "processing_time": 57.84694457054138,
      "citing_paper_id": "273233132",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "In this context, Hendrycks et al. [6] introduced the Massive Multitask Language Understanding (MMLU) benchmark which consists of 57 tasks spanning subjects such as elementary mathematics, computer science, and law.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MMLU"
      ],
      "dataset_descriptions": {
        "MMLU": "Used to evaluate multitask language understanding across 57 diverse subjects, including elementary mathematics, computer science, and law, focusing on comprehensive language capabilities."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions the MMLU benchmark but does not specify its use as a dataset. It is described as a benchmark for evaluating language understanding across multiple tasks.",
      "processing_time": 66.43489670753479,
      "citing_paper_id": "273233132",
      "cited_paper_id": 221516475
    },
    {
      "context_text": "For example, SocKET [28] assesses social knowledge, TRUSTGPT [29] focuses on ethics, MATH [30] evaluates mathematical problem-solving, APPS [30] and HumanEval [31] test code generation abilities, FreshQA [32] and TRUTHFULQA [33] examine question-answering capabilities, and SafetyBench [34]…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MATH"
      ],
      "dataset_descriptions": {
        "MATH": "Used to evaluate mathematical problem-solving abilities, specifically assessing the performance of models on a range of mathematical problems and equations."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions several benchmarks and challenges, but none of them are specific datasets used for training or evaluation. MATH is the only exception, which is a dataset for evaluating mathematical problem-solving.",
      "processing_time": 66.69045901298523,
      "citing_paper_id": "273233132",
      "cited_paper_id": 232134851
    },
    {
      "context_text": "Large language models, built atop the transformer architecture [1], are widely influential in the field of natural language processing [2], and have shown great promise in a variety of applications that were not originally envisioned as target domains, thereby, hinting towards more general…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general statements about large language models and their applications.",
      "processing_time": 57.81657004356384,
      "citing_paper_id": "273233132",
      "cited_paper_id": 236493269
    },
    {
      "context_text": "While prior evaluations of planning abilities have been conducted in natural language environments [16, 17], our work introduces a task represented as a string of characters that depicts a 2D grid-based map.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to prior evaluations. No clear, verifiable resource names are provided.",
      "processing_time": 58.74588394165039,
      "citing_paper_id": "273233132",
      "cited_paper_id": 249889477
    },
    {
      "context_text": "This behaviour is also noticed in PlanBench [16].",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "PlanBench is mentioned as a benchmark, but it is not a specific, downloadable dataset. It is a benchmark suite used for evaluating models, which is excluded according to the instructions.",
      "processing_time": 60.72134256362915,
      "citing_paper_id": "273233132",
      "cited_paper_id": 249889477
    },
    {
      "context_text": "The most closely related works are PlanBench [16] and AutoPlanBench [17] which uses Planning Domain Description Language (PDDL) [36] and convert them into natural language to evaluate the planning abilities of LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PlanBench and AutoPlanBench, but these are not datasets. PDDL is a language, not a dataset. No specific, verifiable datasets are mentioned.",
      "processing_time": 61.28020691871643,
      "citing_paper_id": "273233132",
      "cited_paper_id": 249889477
    },
    {
      "context_text": "The best-performing LLMs, such as GPT-4-Turbo and Claude-3-Opus , are among the state-of-the-art LLMs as well when it comes to benchmarks for natural language understanding tasks, code generation tasks, as well as on PlanBench [16].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PlanBench"
      ],
      "dataset_descriptions": {
        "PlanBench": "Used to evaluate the performance of large language models on planning and reasoning tasks, specifically assessing their capabilities in understanding and generating plans."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions PlanBench, which is a benchmark for evaluating LLMs on planning and reasoning tasks. It is used to assess the performance of LLMs in these specific areas.",
      "processing_time": 67.63089179992676,
      "citing_paper_id": "273233132",
      "cited_paper_id": 249889477
    },
    {
      "context_text": "Planning may include tasks in games [11, 12], robot control [13], questioning answering [14], or visual programming [15].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general domains where planning capabilities might be applied. No verifiable resources are identified.",
      "processing_time": 59.40480899810791,
      "citing_paper_id": "273233132",
      "cited_paper_id": 253734854
    },
    {
      "context_text": "…(teal), non-walkable tiles (purple), and the position of objectives (numbered). environment Bottom: Character representation of the map provided as input to the LLM. Previous works have shown that LLMs can generate 2D grid-based maps with fine-tuning and training [18–20], and without [21, 12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the environment setup and references previous works on LLMs generating 2D grid-based maps.",
      "processing_time": 59.957510232925415,
      "citing_paper_id": "273233132",
      "cited_paper_id": 258960371
    },
    {
      "context_text": "…the transformer architecture [1], are widely influential in the field of natural language processing [2], and have shown great promise in a variety of applications that were not originally envisioned as target domains, thereby, hinting towards more general Artificial Intelligence models [3, 4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to the influence and applications of transformer architectures. No verifiable resources are identified.",
      "processing_time": 59.39931392669678,
      "citing_paper_id": "273233132",
      "cited_paper_id": 259064000
    },
    {
      "context_text": "…[29] focuses on ethics, MATH [30] evaluates mathematical problem-solving, APPS [30] and HumanEval [31] test code generation abilities, FreshQA [32] and TRUTHFULQA [33] examine question-answering capabilities, and SafetyBench [34] evaluates the safety performance of LLMs. Benchmarks…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FreshQA",
        "TRUTHFULQA"
      ],
      "dataset_descriptions": {
        "FreshQA": "Used to evaluate question-answering capabilities of LLMs, focusing on freshness and relevance of responses.",
        "TRUTHFULQA": "Used to assess the truthfulness and accuracy of LLM-generated answers, emphasizing factual correctness."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation mentions several benchmarks and datasets, but they are primarily used for score comparison rather than as reusable datasets. Only FreshQA and TRUTHFULQA are mentioned as specific datasets.",
      "processing_time": 71.90739893913269,
      "citing_paper_id": "273233132",
      "cited_paper_id": 263672149
    },
    {
      "context_text": "LLMs are trained to predict the next token based on their current context, but an ongoing debate is whether these next-token predictors are capable of planning [9, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about the capabilities of LLMs in planning.",
      "processing_time": 58.88418006896973,
      "citing_paper_id": "273233132",
      "cited_paper_id": 267413178
    },
    {
      "context_text": "Other evaluations, such as Chatbot Arena [27] and MT-Bench [8], focus on measuring the general capabilities of chatbots.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Chatbot Arena and MT-Bench, but neither is a dataset. They are platforms or benchmarks for evaluating chatbots, which are excluded based on the instructions.",
      "processing_time": 60.51526737213135,
      "citing_paper_id": "273233132",
      "cited_paper_id": 268264163
    },
    {
      "context_text": "In recent years, numerous LLMs have emerged, each competing for state-of-the-art performance by continuously expanding the limits of their capabilities [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in LLM development.",
      "processing_time": 57.844953775405884,
      "citing_paper_id": "273233132",
      "cited_paper_id": null
    },
    {
      "context_text": "…best of our knowledge, the study of “Agent”, “Autonomous Agent”, “AI Agent\" and “Multi-Agent” has been a central part of AI research for decades [6–11], aimed at understanding and building intelligent and autonomous systems, but there is currently no standardized definition for AI Agents,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and terms related to multi-agent systems. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 60.87774157524109,
      "citing_paper_id": "260681466",
      "cited_paper_id": 19706
    },
    {
      "context_text": "…changing the real environment through interaction with tools in the physical world, LLM can also utilize software tools such as search engines [59–67], mobile [68, 69], Microsoft Office [70, 71], calculators [72–74], deep models [19, 75–79, 13, 80, 81] and other versatile APIs [82, 5, 83, 84,…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various software tools and APIs. The cited paper title 'HotpotQA' is a dataset, but it is not mentioned in the citation context.",
      "processing_time": 61.447043657302856,
      "citing_paper_id": "260681466",
      "cited_paper_id": 52822214
    },
    {
      "context_text": "Step 1 Step 2 Step 3 our agents ability is especially evident in the tasks of few-shot [2] and zero-shot [3] learning, where LLMs can perform well with minimal or even no fine-tuning to adapt to a new task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of LLMs in few-shot and zero-shot learning scenarios.",
      "processing_time": 58.99905228614807,
      "citing_paper_id": "260681466",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Simultaneously, the advancement of fine-tuning [34–38] and in-context learning [39, 40] technology has offered robust support to LLM in addressing increasingly intricate challenges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in fine-tuning and in-context learning technologies. No verifiable resources are identified.",
      "processing_time": 59.62919044494629,
      "citing_paper_id": "260681466",
      "cited_paper_id": 232269696
    },
    {
      "context_text": "Simultaneously, the advancement of fine-tuning [34–38] and in-context learning [39, 40] technology has offered robust support to LLM in addressing increasingly intricate challenges.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in fine-tuning and in-context learning technologies. No verifiable resources are identified.",
      "processing_time": 59.62919044494629,
      "citing_paper_id": "260681466",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "[27] Traditional deep learning approaches exhibit limitations in terms of comprehension of tool functionality and user intentions, and common sense reasoning abilities.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general limitations of deep learning approaches.",
      "processing_time": 58.06628918647766,
      "citing_paper_id": "260681466",
      "cited_paper_id": 237091588
    },
    {
      "context_text": "…the physical world, LLM can also utilize software tools such as search engines [59–67], mobile [68, 69], Microsoft Office [70, 71], calculators [72–74], deep models [19, 75–79, 13, 80, 81] and other versatile APIs [82, 5, 83, 84, 20, 85] to enhance model performance or complete complex workflows…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various software tools and APIs. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 59.64848828315735,
      "citing_paper_id": "260681466",
      "cited_paper_id": 239998651
    },
    {
      "context_text": "WebGPT [66] and WebCPM [64] use network search to assist in implementing Question Answering tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions WebGPT and WebCPM, which are models or methods, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 59.89093565940857,
      "citing_paper_id": "260681466",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "As for Inner Monologue [48], LLM incorporates diverse environmental feedback to construct inner monologues, thereby formulating effective robot control strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'Inner Monologue'. No verifiable datasets are referenced.",
      "processing_time": 59.13418364524841,
      "citing_paper_id": "260681466",
      "cited_paper_id": 250451569
    },
    {
      "context_text": "Palm-E [50] introduced a multimodal language model which seamlessly integrates sensor data into its framework, enabling efficient planning of robot actions and task completion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (PaLM-E). The context focuses on the capabilities of the model rather than the use of a dataset.",
      "processing_time": 60.46159768104553,
      "citing_paper_id": "260681466",
      "cited_paper_id": 257364842
    },
    {
      "context_text": "Large Language Model (LLM) [1] is a recent breakthrough in natural language processing (NLP) research.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to Large Language Models.",
      "processing_time": 57.869091272354126,
      "citing_paper_id": "260681466",
      "cited_paper_id": 257900969
    },
    {
      "context_text": "EVAPORATE [94] involves the synthesis of multiple functions, which are subsequently utilized at a large scale to efficiently process documents and generate structured views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for processing documents and generating structured views.",
      "processing_time": 58.43301057815552,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258212828
    },
    {
      "context_text": "This could include databases for information retrieval [12], APIs for interacting with external systems [5], other AI models specialized for tasks such as image recognition or sentiment analysis [13], or even non-AI tools and resources such as web scraping tools or data visualization libraries [14].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general categories of resources and tools.",
      "processing_time": 58.603280544281006,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258291425
    },
    {
      "context_text": "This could include databases for information retrieval [12], APIs for interacting with external systems [5], other AI models specialized for tasks such as image recognition or sentiment analysis [13], or even non-AI tools and resources such as web scraping tools or data visualization libraries [14].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to general categories of resources and tools.",
      "processing_time": 58.603280544281006,
      "citing_paper_id": "260681466",
      "cited_paper_id": 264146554
    },
    {
      "context_text": "…software tools such as search engines [59–67], mobile [68, 69], Microsoft Office [70, 71], calculators [72–74], deep models [19, 75–79, 13, 80, 81] and other versatile APIs [82, 5, 83, 84, 20, 85] to enhance model performance or complete complex workflows through flexible control of the software.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various software tools and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 59.23631501197815,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258823133
    },
    {
      "context_text": "LLMs encompass a broad spectrum of common sense cognitive capabilities and exhibit remarkable proficiencies in natural language processing, reasoning, and interactive decision-making [28–32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLMs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 59.453118085861206,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258832478
    },
    {
      "context_text": "ASH [62] utilizes LLM for sequence hierarchical decision-making to achieve web navigation tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for web navigation tasks.",
      "processing_time": 58.269688844680786,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258841249
    },
    {
      "context_text": "Secondly, looking at the current work on tool-augmented LLMs, such as T-Bench [16] and ToolBench [17], we see that only a handful of tools are launched and executed in a single scenario.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions T-Bench and ToolBench, but these are benchmarks or frameworks rather than datasets. No specific datasets are mentioned.",
      "processing_time": 59.26516318321228,
      "citing_paper_id": "260681466",
      "cited_paper_id": 258947425
    },
    {
      "context_text": "Secondly, looking at the current work on tool-augmented LLMs, such as T-Bench [16] and ToolBench [17], we see that only a handful of tools are launched and executed in a single scenario.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions T-Bench and ToolBench, but these are benchmarks or frameworks rather than datasets. No specific datasets are mentioned.",
      "processing_time": 59.26516318321228,
      "citing_paper_id": "260681466",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "…software tools such as search engines [59–67], mobile [68, 69], Microsoft Office [70, 71], calculators [72–74], deep models [19, 75–79, 13, 80, 81] and other versatile APIs [82, 5, 83, 84, 20, 85] to enhance model performance or complete complex workflows through flexible control of the…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only software tools and models. There are no clear identifiers for datasets.",
      "processing_time": 58.54112887382507,
      "citing_paper_id": "260681466",
      "cited_paper_id": 264591439
    },
    {
      "context_text": "ART [86] leverages CoT [26] and In-context Learning [81, 41] techniques to automatically generate multi-step reasoning processes for new tasks, while also selecting and utilizing the most appropriate available tool at each step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 59.107184171676636,
      "citing_paper_id": "260681466",
      "cited_paper_id": 264591439
    },
    {
      "context_text": "Meanwhile, API-Bank [18], in a single scenario, typically dispatches only one API tool and awaits its response.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark called API-Bank, which is excluded as per instructions.",
      "processing_time": 58.73888850212097,
      "citing_paper_id": "260681466",
      "cited_paper_id": 276344352
    },
    {
      "context_text": "In this context, we utilize publicly available large language models such as ChatGPT, GPT-4 [4], InterLM [15], and others.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions large language models but does not refer to any specific datasets. The context is about using these models, not datasets.",
      "processing_time": 59.10395669937134,
      "citing_paper_id": "260681466",
      "cited_paper_id": null
    },
    {
      "context_text": "Valid Seen and Valid Unseen consist of 242 and 85 tasks [23], respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation mentions 'tasks' but does not specify a dataset name. The context is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 58.669081926345825,
      "citing_paper_id": "272693849",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Many simulators abstract and integrate key components of the embodied everyday task into their environments, such as MINI-BEHAVIOR [13] and ALFRED [23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MINI-BEHAVIOR' and 'ALFRED', but neither are datasets. They are simulators or benchmarks, which are excluded according to the instructions.",
      "processing_time": 60.57884097099304,
      "citing_paper_id": "272693849",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "For ALFRED, we utilize tools from ALFWORLD, which include interactive TextWorld environments [7] that mirror embodied worlds in the ALFRED dataset [23], to extract instance-level labels of objects observed in the image directly in front of the robot in the environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFRED dataset"
      ],
      "dataset_descriptions": {
        "ALFRED dataset": "Used to extract instance-level labels of objects observed in the environment, supporting the development of embodied agents capable of interpreting grounded instructions for everyday tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ALFRED dataset, which is used to extract instance-level labels of objects observed in the environment. The ALFRED dataset is relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 68.97656917572021,
      "citing_paper_id": "272693849",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "For ALFRED, we utilize tools from ALFWORLD, which include interactive TextWorld environments [7] that mirror embodied worlds in the ALFRED dataset [23], to extract instance-level labels of objects observed in the image directly in front of the robot in the environment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFRED dataset"
      ],
      "dataset_descriptions": {
        "ALFRED dataset": "Used to extract instance-level labels of objects observed in the environment, supporting the development of embodied agents capable of interpreting grounded instructions for everyday tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ALFRED dataset, which is used to extract instance-level labels of objects observed in the environment. The ALFRED dataset is relevant to the research topic of planning capabilities in LLMs.",
      "processing_time": 68.97656917572021,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "We select two datasets, MINI-BEHAVIOR [13] and AL-FRED [23], for our experiments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MINI-BEHAVIOR",
        "AL-FRED"
      ],
      "dataset_descriptions": {
        "MINI-BEHAVIOR": "Used to evaluate planning capabilities in a controlled environment, focusing on behavior execution and task completion in simulated settings.",
        "AL-FRED": "Used to assess the interpretation of grounded instructions for everyday tasks, emphasizing the ability to understand and execute complex, real-world instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, MINI-BEHAVIOR and AL-FRED, which are used for experiments. Both names are specific and plausible, fitting the criteria for inclusion.",
      "processing_time": 72.68230128288269,
      "citing_paper_id": "272693849",
      "cited_paper_id": 208617407
    },
    {
      "context_text": "Due to the increasing demand for human-computer interaction, tasks oriented towards ordinary users are becoming more popular, giving rise to some new tasks such as language-conditional navigation and manipulation [4, 8, 16, 26, 40, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only new tasks and areas of research. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.59663414955139,
      "citing_paper_id": "272693849",
      "cited_paper_id": 247500268
    },
    {
      "context_text": "Due to the increasing demand for human-computer interaction, tasks oriented towards ordinary users are becoming more popular, giving rise to some new tasks such as language-conditional navigation and manipulation [4, 8, 16, 26, 40, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only new tasks and areas of research. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.59663414955139,
      "citing_paper_id": "272693849",
      "cited_paper_id": 255341638
    },
    {
      "context_text": "Due to the increasing demand for human-computer interaction, tasks oriented towards ordinary users are becoming more popular, giving rise to some new tasks such as language-conditional navigation and manipulation [4, 8, 16, 26, 40, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only new tasks and areas of research. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.59663414955139,
      "citing_paper_id": "272693849",
      "cited_paper_id": 268520018
    },
    {
      "context_text": "Recently, main-stream approaches to manipulation tasks can roughly be divided into three categories as following: 1) learning-based approaches which needs to train agents in simulation environments [19, 33, 37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general categories of approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.72914671897888,
      "citing_paper_id": "272693849",
      "cited_paper_id": 247618840
    },
    {
      "context_text": "We first evaluate the ALFRED dataset to compare the success rates (SR) of state-of-the-art methods [2, 3, 20, 27, 28, 39] with our method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ALFRED"
      ],
      "dataset_descriptions": {
        "ALFRED": "Used to evaluate and compare the success rates of state-of-the-art methods with the proposed method in the context of grounding language in robotic affordances."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ALFRED dataset, which is a specific, verifiable dataset used for evaluating success rates of methods in robotic affordances.",
      "processing_time": 66.2291111946106,
      "citing_paper_id": "272693849",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Another group including HLSM [3], LLM-Planer [28] and Saycan [2] a star (*) utilize step-by-step instructions rather than goal instructions, such as Saycan [2] and E.T. [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the use of step-by-step instructions in various models, which are not considered datasets.",
      "processing_time": 60.39404535293579,
      "citing_paper_id": "272693849",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "It is widely used in various NLP tasks, such as narrative generation [34], abstract generation [5, 6, 32], and code generation [35, 38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks. No dataset names are present in the text.",
      "processing_time": 58.139376401901245,
      "citing_paper_id": "272693849",
      "cited_paper_id": 249097545
    },
    {
      "context_text": "It is widely used in various NLP tasks, such as narrative generation [34], abstract generation [5, 6, 32], and code generation [35, 38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks. No dataset names are present in the text.",
      "processing_time": 58.139376401901245,
      "citing_paper_id": "272693849",
      "cited_paper_id": 256358492
    },
    {
      "context_text": "It is widely used in various NLP tasks, such as narrative generation [34], abstract generation [5, 6, 32], and code generation [35, 38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks. No dataset names are present in the text.",
      "processing_time": 58.139376401901245,
      "citing_paper_id": "272693849",
      "cited_paper_id": 266176989
    },
    {
      "context_text": "It is widely used in various NLP tasks, such as narrative generation [34], abstract generation [5, 6, 32], and code generation [35, 38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks. No dataset names are present in the text.",
      "processing_time": 58.139376401901245,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "Unlike common text-only tasks, robotic tasks require more prior knowledge and an understanding of interactions with the physical world [10, 12, 18, 25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about the requirements for robotic tasks. No verifiable resources are identified.",
      "processing_time": 58.84100794792175,
      "citing_paper_id": "272693849",
      "cited_paper_id": 252681067
    },
    {
      "context_text": "Unlike common text-only tasks, robotic tasks require more prior knowledge and an understanding of interactions with the physical world [10, 12, 18, 25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about the requirements for robotic tasks. No verifiable resources are identified.",
      "processing_time": 58.84100794792175,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "2) LLM-based approaches which leverage the vast knowledge embedded within large language models [9, 14, 17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of a reusable resource being used.",
      "processing_time": 58.63165354728699,
      "citing_paper_id": "272693849",
      "cited_paper_id": 265715696
    },
    {
      "context_text": "Previous works [9, 29] have mostly focused on prompt design, which incorporates general commonsense knowledge and addresses the issue of understanding language instructions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on prompt design and understanding language instructions.",
      "processing_time": 57.56957030296326,
      "citing_paper_id": "272693849",
      "cited_paper_id": 265715696
    },
    {
      "context_text": "Embodied Everyday Task aims to establish human-centered AI [16] that serves human needs, goals, and values by simulating daily human tasks like navigation and manipulation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a benchmark which is excluded as per instructions. The citation is more about the research work and its goals.",
      "processing_time": 59.33826732635498,
      "citing_paper_id": "272693849",
      "cited_paper_id": 268520018
    },
    {
      "context_text": "Specifically, FMM is a numerical technique designed to tackle boundary value problems arising from the Eikonal equation [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a numerical technique called FMM. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.313931941986084,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, the “navigation” high-level action is decomposed using the Fast Marching Method (FMM) [21].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (Fast Marching Method) rather than a dataset. No specific dataset is referenced in the context.",
      "processing_time": 57.82203483581543,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "Initially introduced by [15], this primitive direct approach later became known as Naive RAG (Retrieval-Augmented Generation).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'Naive RAG'. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.99600601196289,
      "citing_paper_id": "272693849",
      "cited_paper_id": null
    },
    {
      "context_text": "Many representations are proposed, ranging from a faithful 3D recontruction [12] of the environment, to more object-centric ones [13], [14], such as object detection bounding boxes [15] and 3D bounding boxes [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various representation methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.99009561538696,
      "citing_paper_id": "252383216",
      "cited_paper_id": 13753671
    },
    {
      "context_text": "Many representations are proposed, ranging from a faithful 3D recontruction [12] of the environment, to more object-centric ones [13], [14], such as object detection bounding boxes [15] and 3D bounding boxes [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various representation methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.99009561538696,
      "citing_paper_id": "252383216",
      "cited_paper_id": 52099638
    },
    {
      "context_text": "Recently, topological maps [17], [18] and scene graphs [19], [20] emerge as an effective discrete representation of scenes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or representations such as topological maps and scene graphs.",
      "processing_time": 57.43305730819702,
      "citing_paper_id": "252383216",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "In task and motion planning, scene representations are often composed of predicates compatible with symbolic planners [32], [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to symbolic planners and their integration with blackbox samplers.",
      "processing_time": 57.41324830055237,
      "citing_paper_id": "252383216",
      "cited_paper_id": 210861196
    },
    {
      "context_text": "Other algorithms [24]–[29] do not require a map and can decide where to go based directly on the current observations and memories, without maintaining a global representation of the environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and their capabilities. There are no clear identifiers for datasets in the text.",
      "processing_time": 58.13197064399719,
      "citing_paper_id": "252383216",
      "cited_paper_id": 220845633
    },
    {
      "context_text": "Recent progresses attempt to build a symbolic and geometric scene graph to facilitate task and motion planning [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving symbolic and geometric scene graphs.",
      "processing_time": 57.24365758895874,
      "citing_paper_id": "252383216",
      "cited_paper_id": 229156165
    },
    {
      "context_text": "On the other hand, CLIP embedding gives us better results on uncommon objects but is less robust for basic objects.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between CLIP embedding and another unspecified method. No verifiable resources are identified.",
      "processing_time": 58.307353019714355,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In our setup we leverage CLIP [3] and ViLD [4]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models (CLIP and ViLD). These are excluded as per the instructions.",
      "processing_time": 58.119733572006226,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "This is achieved by finding CLIP nearest neighbor of object names.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method involving CLIP and object names.",
      "processing_time": 56.66712832450867,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "As shown in Table III, ViLD and CLIP embedding alone achieves a very low success rate in both environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (ViLD, CLIP). No verifiable resources are identified.",
      "processing_time": 57.931824684143066,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "M [3], [4] to extract image embeddings φi = [Φj(Ii)| j ∈ 1.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method for extracting image embeddings. No dataset names are present in the context.",
      "processing_time": 58.05088973045349,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Additionally CLIP embeddings better captures features of text and signs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about CLIP embeddings. No verifiable resources are identified.",
      "processing_time": 57.862427949905396,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Weablateusingdifferent features to retrieval RoIs with natural language and found there are unique failure cases with either CLIP or ViLD features, while maximum ensemble of features provide the best results. planning in a generative way.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses features and models but does not reference any named datasets.",
      "processing_time": 57.895108222961426,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "CoW [32] performs zero-shot object goal navigation by leveraging CLIP.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (CLIP) used for zero-shot object goal navigation.",
      "processing_time": 57.71087074279785,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We use the maximum ensemble of both CLIP and ViLD for the metric D defined below: Here we use both CLIP embedding and ViLD embedding because the former detects out-of-distribution objects better while the latter is more robust to common objects as shown in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (CLIP and ViLD) which are excluded according to the instructions.",
      "processing_time": 58.070252656936646,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In our setup we leverage CLIP [3] and ViLD [4] as visual encoders where image-text-alignment is scored with inner product of image feature and CLIP text feature.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (CLIP and ViLD) which are excluded according to the instructions.",
      "processing_time": 57.938127517700195,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We can directly take the maximum over the two inner products because both of them are normalized vectors designed to be queried by the inner product CLIP text encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method involving normalized vectors and the CLIP text encoder.",
      "processing_time": 57.16347789764404,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "C ONCLUSIONS We integrate NLMap, a flexible and queryable spatial semantic representation based on visual-language models including ViLD and CLIP with SayCan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on integrating NLMap with visual-language models.",
      "processing_time": 58.22313213348389,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In this work, we introduce Natural-Language Map (NLMap), a flexible and language-queryable spatial semantic representation based on visual-language models including ViLD and CLIP and integrate with SayCan.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper title also does not provide additional dataset information.",
      "processing_time": 58.37719988822937,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Contrastive Language-Image Pre-training (CLIP) [3] models are trained on image-language associations and can provide open-vocabulary image understanding and object detection [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (CLIP) and its capabilities. No verifiable resources are identified.",
      "processing_time": 58.37176513671875,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Our method will bind both of them to the closest manipulation policy “pick up coke can” with CLIP.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (CLIP) and a hypothetical example ('pick up coke can').",
      "processing_time": 59.13683104515076,
      "citing_paper_id": "252383216",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "This nearest neighbor query is similar to that used with BERT in [5].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) which is excluded. The context is too vague to infer any dataset usage.",
      "processing_time": 58.658133029937744,
      "citing_paper_id": "252383216",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "For example, [5] pioneers in using LLMs for planning, but it has no grounding in environmental context.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to using LLMs for planning.",
      "processing_time": 57.246562480926514,
      "citing_paper_id": "252383216",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Recent progress in large language models (LLMs), has shown impressive few-shot performance in language comprehension, semantic understanding, and reasoning, as well as application to robotics problems like planning [5]–[7] and instruction following [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of large language models. No verifiable resources are named.",
      "processing_time": 58.04580307006836,
      "citing_paper_id": "252383216",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Recent progress in large language models (LLMs), has shown impressive few-shot performance in language comprehension, semantic understanding, and reasoning, as well as application to robotics problems like planning [5]–[7] and instruction following [8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of large language models. No verifiable resources are named.",
      "processing_time": 58.04580307006836,
      "citing_paper_id": "252383216",
      "cited_paper_id": 250426345
    },
    {
      "context_text": "Therefore, the novel object names in this experiment are either used for navigation only, or for describing objects that are visually similar to training objects in [6].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to training objects, which is too generic.",
      "processing_time": 57.082990407943726,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "We note that manipulation policies used in this project are still limited to be with the objects that are visually similar to training objects in [6] and rely on the generalization to slightly out-of-distribution data.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation regarding the visual similarity of training objects. No clear, verifiable dataset names are provided.",
      "processing_time": 58.646281003952026,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "In contrast, SayCan [6] showed how value functions of learned skills can provide such a grounding through selecting options scored highly by a language model and an affordance model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of value functions and language models, which are not datasets.",
      "processing_time": 58.96365237236023,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Recently LLM-based planners are more flexible [6], [7], [36] and do not require handcrafting predicates, however, they do not handle the complexity of open-vocabulary object proposal and require defining a set of objects involved in planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities and limitations of LLM-based planners.",
      "processing_time": 57.27648878097534,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Previously, SayCan [6] presents a framework that allows robots to plan and execute in the real world following human instructions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework called SayCan. The context does not provide information about datasets used.",
      "processing_time": 57.399916887283325,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Recent work, SayCan [6], has shown how large language models can be applied to such problems through world-grounding affordance functions, allowing LLMs to understand what a robot can do from a state.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SayCan) and its application to grounding language in robotic affordances.",
      "processing_time": 58.19158601760864,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "1) Generate executable options: Vanilla SayCan [6] provides a list of skills associated with either 1) navigation policies to hard-coded locations 2) manipulation policies (pick and place) of objects, specified by object names.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context is about generating executable options using a method called Vanilla SayCan.",
      "processing_time": 58.53864121437073,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "NLMap +SayCan shows comparable performance as SayCan on instructions from [6] while enabling new tasks SayCan cannot do before due to its lack of contextual grounding.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance between two systems. No verifiable resources are identified.",
      "processing_time": 57.93434691429138,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Similar to Say-Can, we use a set of manipulation policies trained from imitation learning and PaLM 540B [38] as the LLM for all experiments, due to its good performance on new tasks with few-shot prompting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is about using PaLM 540B as the LLM for experiments, which is a model, not a dataset.",
      "processing_time": 60.70549035072327,
      "citing_paper_id": "252383216",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "Different from previous work [8] that uses LLM to extract names from a sentence, our object proposal is much more demanding in four different ways as we will discuss in Sec.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to previous work. There are no clear identifiers for datasets in the given context.",
      "processing_time": 58.336167097091675,
      "citing_paper_id": "252383216",
      "cited_paper_id": 250426345
    },
    {
      "context_text": "LM-Nav [8] uses three pretrained model to perform visual language navigation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'three pretrained models' but does not specify any datasets. The context does not provide enough information to identify specific datasets.",
      "processing_time": 58.14244627952576,
      "citing_paper_id": "252383216",
      "cited_paper_id": 250426345
    },
    {
      "context_text": "1) Infer objects from implication of the instruction: In previous work [8] that use LLM to extract object names from language, all object names are nouns that are directly present in the language input.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'previous work' without naming any datasets.",
      "processing_time": 57.39549016952515,
      "citing_paper_id": "252383216",
      "cited_paper_id": 250426345
    },
    {
      "context_text": "Concurrent work VLMaps [21] also explores this concept, by fusing pretrained visual-language model features into a geometric reconstruction of the scene.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called VLMaps. The context focuses on the integration of visual-language model features into geometric reconstructions.",
      "processing_time": 58.95747947692871,
      "citing_paper_id": "252383216",
      "cited_paper_id": 252846548
    },
    {
      "context_text": "A representative application is traffic signal control, which is typically formulated as a Markov decision process [142, 179, 183, 184].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general application area (traffic signal control). No verifiable resources are identified.",
      "processing_time": 57.97467923164368,
      "citing_paper_id": "280047877",
      "cited_paper_id": 1563120
    },
    {
      "context_text": "A representative application is traffic signal control, which is typically formulated as a Markov decision process [142, 179, 183, 184].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general application area (traffic signal control). No verifiable resources are identified.",
      "processing_time": 57.97467923164368,
      "citing_paper_id": "280047877",
      "cited_paper_id": 14709421
    },
    {
      "context_text": "A representative application is traffic signal control, which is typically formulated as a Markov decision process [142, 179, 183, 184].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general application area (traffic signal control). No verifiable resources are identified.",
      "processing_time": 57.97467923164368,
      "citing_paper_id": "280047877",
      "cited_paper_id": 153312312
    },
    {
      "context_text": "A representative application is traffic signal control, which is typically formulated as a Markov decision process [142, 179, 183, 184].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general application area (traffic signal control). No verifiable resources are identified.",
      "processing_time": 57.97467923164368,
      "citing_paper_id": "280047877",
      "cited_paper_id": 260548557
    },
    {
      "context_text": "Lin et al. [96] proposed a contextual multi-agent RL system for fleet management that incorporates both geographic and collaborative contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fleet management using multi-agent reinforcement learning.",
      "processing_time": 57.20770883560181,
      "citing_paper_id": "280047877",
      "cited_paper_id": 3334421
    },
    {
      "context_text": "Similarly, inspired by the Theory of Needs [116], D2A [175] introduces a desire-driven autonomous agent through an activity generation workflow constrained with a dynamic value system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach inspired by a theory. No verifiable resources are identified.",
      "processing_time": 57.9669451713562,
      "citing_paper_id": "280047877",
      "cited_paper_id": 4635503
    },
    {
      "context_text": "Such data enables agents to understand mobility behaviors and supports tasks such as transportation planning [187], last-mile logistics [187], and crowd monitoring [211].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.964884757995605,
      "citing_paper_id": "280047877",
      "cited_paper_id": 9082946
    },
    {
      "context_text": "Such data enables agents to understand mobility behaviors and supports tasks such as transportation planning [187], last-mile logistics [187], and crowd monitoring [211].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.964884757995605,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271954939
    },
    {
      "context_text": "(3) Trustworthiness : Evaluates the agent’s reliability, safety, ethical compliance, and transparency, especially in applications that directly affect public stakeholders [7, 14] [10] have shown the feasibility of building high-fidelity replicas of cities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and findings. No verifiable resources are identified.",
      "processing_time": 57.16260838508606,
      "citing_paper_id": "280047877",
      "cited_paper_id": 10242377
    },
    {
      "context_text": "(3) Trustworthiness : Evaluates the agent’s reliability, safety, ethical compliance, and transparency, especially in applications that directly affect public stakeholders [7, 14] [10] have shown the feasibility of building high-fidelity replicas of cities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and findings. No verifiable resources are identified.",
      "processing_time": 57.16260838508606,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268722652
    },
    {
      "context_text": "In some cases, post-hoc rationalization techniques [138] can generate simplified explanations of agent behavior based on execution histories, aiding human understanding and debugging.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique for generating explanations of agent behavior.",
      "processing_time": 56.664687156677246,
      "citing_paper_id": "280047877",
      "cited_paper_id": 13029170
    },
    {
      "context_text": "To address this, systems like ST-Hadoop [6], GeoSpark [205], and LocationSpark [159] integrate spatio-temporal range queries with big data frameworks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems (ST-Hadoop, GeoSpark, LocationSpark) but does not refer to any specific datasets. These are tools or frameworks, not datasets.",
      "processing_time": 58.71815371513367,
      "citing_paper_id": "280047877",
      "cited_paper_id": 13241655
    },
    {
      "context_text": "To address this, systems like ST-Hadoop [6], GeoSpark [205], and LocationSpark [159] integrate spatio-temporal range queries with big data frameworks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems (ST-Hadoop, GeoSpark, LocationSpark) but does not refer to any specific datasets. These are tools or frameworks, not datasets.",
      "processing_time": 58.71815371513367,
      "citing_paper_id": "280047877",
      "cited_paper_id": 53026976
    },
    {
      "context_text": "To address this, systems like ST-Hadoop [6], GeoSpark [205], and LocationSpark [159] integrate spatio-temporal range queries with big data frameworks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems (ST-Hadoop, GeoSpark, LocationSpark) but does not refer to any specific datasets. These are tools or frameworks, not datasets.",
      "processing_time": 58.71815371513367,
      "citing_paper_id": "280047877",
      "cited_paper_id": 207231872
    },
    {
      "context_text": "Modern LLMs are built upon the Transformer architecture [166], which facilitates efficient modeling of long-range dependencies in text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the Transformer architecture. No verifiable resources are identified.",
      "processing_time": 57.149985551834106,
      "citing_paper_id": "280047877",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "• Textual input : Unstructured textual content, such as policy documents, regulatory codes, and social media posts, provides high-level semantic and social context [17, 217].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to unstructured textual content in general, which is too generic.",
      "processing_time": 57.87620663642883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 15551372
    },
    {
      "context_text": "• Textual input : Unstructured textual content, such as policy documents, regulatory codes, and social media posts, provides high-level semantic and social context [17, 217].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to unstructured textual content in general, which is too generic.",
      "processing_time": 57.87620663642883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 233592247
    },
    {
      "context_text": "Moreover, Cucchiara et al. [22] proposed VTTS, a vision-based traffic monitoring system that assesses traffic conditions through rule-based reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VTTS) and its application. The context focuses on the system's capabilities rather than data usage.",
      "processing_time": 59.11419868469238,
      "citing_paper_id": "280047877",
      "cited_paper_id": 18595016
    },
    {
      "context_text": "First, adversarial attacks [154] can exploit spatial and temporal dependencies in city-scale data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to city-scale data. No clear, verifiable dataset names are present.",
      "processing_time": 58.274457931518555,
      "citing_paper_id": "280047877",
      "cited_paper_id": 57189241
    },
    {
      "context_text": "In parallel, NoSQL-based approaches [61] employ multi-level indexing schemes and key-encoding strategies ( e.g. , space-filling curves) to store and retrieve spatio-temporal data at scale.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses NoSQL-based approaches and their use in storing and retrieving spatio-temporal data, but no named datasets are referenced.",
      "processing_time": 60.14154672622681,
      "citing_paper_id": "280047877",
      "cited_paper_id": 60121883
    },
    {
      "context_text": "In the transportation domain, rule-based adaptive traffic signal systems such as SCOOT [62] and SCATS [108] were designed to adjust signal timings in real time based on preconfigured logic tied to sensor inputs, and have been widely implemented in cities worldwide.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SCOOT and SCATS as rule-based adaptive traffic signal systems but does not refer to them as datasets. They are described as methods or systems for traffic signal optimization.",
      "processing_time": 59.52290916442871,
      "citing_paper_id": "280047877",
      "cited_paper_id": 109965942
    },
    {
      "context_text": "In the transportation domain, rule-based adaptive traffic signal systems such as SCOOT [62] and SCATS [108] were designed to adjust signal timings in real time based on preconfigured logic tied to sensor inputs, and have been widely implemented in cities worldwide.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SCOOT and SCATS as rule-based adaptive traffic signal systems but does not refer to them as datasets. They are described as methods or systems for traffic signal optimization.",
      "processing_time": 59.52290916442871,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In recent years, breakthroughs in Machine Learning (ML) have driven progress toward the vision of the intelligent city [12, 219], which seeks to address these urban challenges through data-driven analytics [68] and decision-making [164] with minimal human assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals related to ML and smart cities.",
      "processing_time": 57.204970836639404,
      "citing_paper_id": "280047877",
      "cited_paper_id": 114735598
    },
    {
      "context_text": "In recent years, breakthroughs in Machine Learning (ML) have driven progress toward the vision of the intelligent city [12, 219], which seeks to address these urban challenges through data-driven analytics [68] and decision-making [164] with minimal human assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals related to ML and smart cities.",
      "processing_time": 57.204970836639404,
      "citing_paper_id": "280047877",
      "cited_paper_id": 215791332
    },
    {
      "context_text": "In recent years, breakthroughs in Machine Learning (ML) have driven progress toward the vision of the intelligent city [12, 219], which seeks to address these urban challenges through data-driven analytics [68] and decision-making [164] with minimal human assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals related to ML and smart cities.",
      "processing_time": 57.204970836639404,
      "citing_paper_id": "280047877",
      "cited_paper_id": 257767087
    },
    {
      "context_text": "In recent years, breakthroughs in Machine Learning (ML) have driven progress toward the vision of the intelligent city [12, 219], which seeks to address these urban challenges through data-driven analytics [68] and decision-making [164] with minimal human assistance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and goals related to ML and smart cities.",
      "processing_time": 57.204970836639404,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For urban LLM agents, geovector data provides essential spatial context for tasks such as routing [100], land-use simulation [131], and infrastructure analysis [115].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'geovector data' but does not specify a named dataset. The term is too generic and lacks a specific identifier.",
      "processing_time": 58.31523776054382,
      "citing_paper_id": "280047877",
      "cited_paper_id": 130096094
    },
    {
      "context_text": "For urban LLM agents, geovector data provides essential spatial context for tasks such as routing [100], land-use simulation [131], and infrastructure analysis [115].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'geovector data' but does not specify a named dataset. The term is too generic and lacks a specific identifier.",
      "processing_time": 58.31523776054382,
      "citing_paper_id": "280047877",
      "cited_paper_id": 231877730
    },
    {
      "context_text": "For urban LLM agents, geovector data provides essential spatial context for tasks such as routing [100], land-use simulation [131], and infrastructure analysis [115].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'geovector data' but does not specify a named dataset. The term is too generic and lacks a specific identifier.",
      "processing_time": 58.31523776054382,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "This is known as the many hands problem [27, 162], i.e. , when multiple entities contribute to an outcome, it becomes unclear who is accountable for specific decisions or failures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses a conceptual issue (the many hands problem) rather than a reusable resource.",
      "processing_time": 58.71834421157837,
      "citing_paper_id": "280047877",
      "cited_paper_id": 143983583
    },
    {
      "context_text": "This is known as the many hands problem [27, 162], i.e. , when multiple entities contribute to an outcome, it becomes unclear who is accountable for specific decisions or failures.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses a conceptual issue (the many hands problem) rather than a reusable resource.",
      "processing_time": 58.71834421157837,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276569762
    },
    {
      "context_text": "Building on this, CoLight [179] introduced graph attention networks to enable dynamic communication between intersections, further improving coordination and responsiveness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CoLight) and its improvements using graph attention networks.",
      "processing_time": 57.648350954055786,
      "citing_paper_id": "280047877",
      "cited_paper_id": 153312312
    },
    {
      "context_text": "Tasks such as ambulance routing [64] or housing evaluation [214] often require access to mobility patterns, health records, or financial data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'mobility patterns, health records, or financial data' but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 59.69122266769409,
      "citing_paper_id": "280047877",
      "cited_paper_id": 155295771
    },
    {
      "context_text": "Tasks such as ambulance routing [64] or housing evaluation [214] often require access to mobility patterns, health records, or financial data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context mentions 'mobility patterns, health records, or financial data' but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 59.69122266769409,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In urban planning, a representative example is ESSAS [53], an expert system for site analysis and selection, developed to assist planners in evaluating land parcel suitability by integrating spatial data with planning regulations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ESSAS as an expert system for site analysis and selection, which is a tool or method rather than a dataset. No specific dataset is mentioned.",
      "processing_time": 59.08770751953125,
      "citing_paper_id": "280047877",
      "cited_paper_id": 155298218
    },
    {
      "context_text": "This equips them with broad linguistic fluency and implicit knowledge about the world [135, 136].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of language models.",
      "processing_time": 56.31760382652283,
      "citing_paper_id": "280047877",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "This equips them with broad linguistic fluency and implicit knowledge about the world [135, 136].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of language models.",
      "processing_time": 56.31760382652283,
      "citing_paper_id": "280047877",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "…ML technologies into urban systems, intelligent city services allow stakeholders to efficiently analyze massive and heterogeneous urban data, empowering applications like traffic optimization [216], energy management [210], and policy formulation [140] at various spatial and temporal scales.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses general applications of machine learning in urban systems.",
      "processing_time": 57.681291580200195,
      "citing_paper_id": "280047877",
      "cited_paper_id": 189762063
    },
    {
      "context_text": "…ML technologies into urban systems, intelligent city services allow stakeholders to efficiently analyze massive and heterogeneous urban data, empowering applications like traffic optimization [216], energy management [210], and policy formulation [140] at various spatial and temporal scales.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses general applications of machine learning in urban systems.",
      "processing_time": 57.681291580200195,
      "citing_paper_id": "280047877",
      "cited_paper_id": 251518408
    },
    {
      "context_text": "…ML technologies into urban systems, intelligent city services allow stakeholders to efficiently analyze massive and heterogeneous urban data, empowering applications like traffic optimization [216], energy management [210], and policy formulation [140] at various spatial and temporal scales.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses general applications of machine learning in urban systems.",
      "processing_time": 57.681291580200195,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "One promising approach is to compute geospatial equity scores [196], which quantify how equitably an agent’s decisions affect different regions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing geospatial equity scores. No verifiable dataset names are present in the citation context.",
      "processing_time": 58.690003633499146,
      "citing_paper_id": "280047877",
      "cited_paper_id": 195847962
    },
    {
      "context_text": "For instance, Hu et al. [58] developed a DQN-based valve scheduling system for pollution isolation in water distribution networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DQN-based valve scheduling system).",
      "processing_time": 57.20930814743042,
      "citing_paper_id": "280047877",
      "cited_paper_id": 204142413
    },
    {
      "context_text": "Urban LLM agents operate in environments rich with real-time data, much of which contains sensitive personal or community-level information [212].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to real-time data in smart city applications.",
      "processing_time": 56.60711598396301,
      "citing_paper_id": "280047877",
      "cited_paper_id": 206456257
    },
    {
      "context_text": "Typical examples include traffic flows [51], air quality [49, 50], energy consumption [202, 227], and noise levels [224].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of data (traffic flows, air quality, energy consumption, noise levels) but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.60268020629883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 207215142
    },
    {
      "context_text": "Typical examples include traffic flows [51], air quality [49, 50], energy consumption [202, 227], and noise levels [224].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of data (traffic flows, air quality, energy consumption, noise levels) but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.60268020629883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 229156802
    },
    {
      "context_text": "Typical examples include traffic flows [51], air quality [49, 50], energy consumption [202, 227], and noise levels [224].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of data (traffic flows, air quality, energy consumption, noise levels) but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.60268020629883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 229923103
    },
    {
      "context_text": "Typical examples include traffic flows [51], air quality [49, 50], energy consumption [202, 227], and noise levels [224].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of data (traffic flows, air quality, energy consumption, noise levels) but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.60268020629883,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268724471
    },
    {
      "context_text": "Typical examples include traffic flows [51], air quality [49, 50], energy consumption [202, 227], and noise levels [224].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions various types of data (traffic flows, air quality, energy consumption, noise levels) but does not specify any named datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 60.60268020629883,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "They are designed to interact with the cyber-physical-social systems of cities [228], where digital infrastructure ( e.g. , IoT devices, control systems), physical systems ( e.g. , traffic networks, energy grids), and human behavior ( e.g. , mobility patterns, social norms) are deeply intertwined.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a general concept of cyber-physical-social systems without referencing any particular dataset.",
      "processing_time": 58.28702354431152,
      "citing_paper_id": "280047877",
      "cited_paper_id": 212705090
    },
    {
      "context_text": "In vehicle routing, Lu et al. [110] introduced the L2I framework, where an RL agent selects among multiple local search operators to iteratively improve routing plans, aiming to minimize total travel distance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (L2I framework) for solving vehicle routing problems using reinforcement learning.",
      "processing_time": 57.925631523132324,
      "citing_paper_id": "280047877",
      "cited_paper_id": 213389645
    },
    {
      "context_text": "When domain knowledge or functional capabilities are limited, retrieval-augmented generation (RAG) enables models to incorporate relevant external documents during inference [80].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RAG) for incorporating external documents. No verifiable datasets are referenced.",
      "processing_time": 57.922566413879395,
      "citing_paper_id": "280047877",
      "cited_paper_id": 218869575
    },
    {
      "context_text": "Recently, JUST [87] further advances this line by supporting online updates in a scalable retrieval engine.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or system called JUST. The context focuses on the capabilities of JUST rather than a dataset.",
      "processing_time": 58.52466678619385,
      "citing_paper_id": "280047877",
      "cited_paper_id": 218907828
    },
    {
      "context_text": "Building on the aforementioned technologies, LLMs have emerged as intelligent agents capable of reasoning, planning, memorizing, and interacting with external tools [129, 141, 186], and have increasingly been applied to urban domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No verifiable resources are identified.",
      "processing_time": 57.50137782096863,
      "citing_paper_id": "280047877",
      "cited_paper_id": 221342993
    },
    {
      "context_text": "Building on the aforementioned technologies, LLMs have emerged as intelligent agents capable of reasoning, planning, memorizing, and interacting with external tools [129, 141, 186], and have increasingly been applied to urban domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No verifiable resources are identified.",
      "processing_time": 57.50137782096863,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Building on the aforementioned technologies, LLMs have emerged as intelligent agents capable of reasoning, planning, memorizing, and interacting with external tools [129, 141, 186], and have increasingly been applied to urban domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLMs. No verifiable resources are identified.",
      "processing_time": 57.50137782096863,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "We refer to Zheng et al. [219] and Alam et al. [4] for more details on spatio-temporal retrieval.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works for more details on spatio-temporal retrieval.",
      "processing_time": 57.36734223365784,
      "citing_paper_id": "280047877",
      "cited_paper_id": 232269781
    },
    {
      "context_text": "We refer to Zheng et al. [219] and Alam et al. [4] for more details on spatio-temporal retrieval.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works for more details on spatio-temporal retrieval.",
      "processing_time": 57.36734223365784,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "First, LLMs can easily generalize to new tasks simply by conditioning on provided examples in the prompt [180] through their powerful in-context learning ability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the in-context learning ability of LLMs.",
      "processing_time": 56.988356590270996,
      "citing_paper_id": "280047877",
      "cited_paper_id": 237416585
    },
    {
      "context_text": "…are commonly used: instruction tuning, where the model is fine-tuned to follow natural language instructions using curated prompt-response examples [180]; and reinforcement learning from human feedback (RLHF), which trains a reward model based on human evaluations to guide the LLM’s outputs toward…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on instruction tuning and reinforcement learning from human feedback, which are methodologies rather than datasets.",
      "processing_time": 59.82260823249817,
      "citing_paper_id": "280047877",
      "cited_paper_id": 237416585
    },
    {
      "context_text": "Early attempts like ClimateBERT [178] adapt general-purpose language models ( e.g. , BERT) through continued training on climate-related corpora to enhance performance on climate-specific NLP tasks such as classification, fact-checking, and claim generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'climate-related corpora' but does not specify a named dataset. The citation is about adapting a model, not using a specific dataset.",
      "processing_time": 58.68592143058777,
      "citing_paper_id": "280047877",
      "cited_paper_id": 239768231
    },
    {
      "context_text": "Additionally, frameworks like DiMA [122] demonstrate the use of LLM agents for ClimateBERT [178] Text - - Single-Agent - ClimateGPT [163] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data ChatClimate [165] Text Vector Database Spatio-Temporal Single-Agent - ClimaQA [114] Text Vector…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several frameworks and models but does not specify any datasets. The names mentioned are either models or methods, which are excluded according to the instructions.",
      "processing_time": 58.86983585357666,
      "citing_paper_id": "280047877",
      "cited_paper_id": 239768231
    },
    {
      "context_text": "Additionally, frameworks like DiMA [122] demonstrate the use of LLM agents for ClimateBERT [178] Text - - Single-Agent - ClimateGPT [163] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data ChatClimate [165] Text Vector Database Spatio-Temporal Single-Agent - ClimaQA [114] Text Vector…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several frameworks and models but does not specify any datasets. The names mentioned are either models or methods, which are excluded according to the instructions.",
      "processing_time": 58.86983585357666,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267034871
    },
    {
      "context_text": "Additionally, frameworks like DiMA [122] demonstrate the use of LLM agents for ClimateBERT [178] Text - - Single-Agent - ClimateGPT [163] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data ChatClimate [165] Text Vector Database Spatio-Temporal Single-Agent - ClimaQA [114] Text Vector…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several frameworks and models but does not specify any datasets. The names mentioned are either models or methods, which are excluded according to the instructions.",
      "processing_time": 58.86983585357666,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276885258
    },
    {
      "context_text": "For example, coordinated anomalies across districts may signal system-wide manipulation [28, 150].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of anomalies in spatiotemporal data. The cited papers' titles suggest the use of spatiotemporal data but do not specify datasets.",
      "processing_time": 61.31716823577881,
      "citing_paper_id": "280047877",
      "cited_paper_id": 243476338
    },
    {
      "context_text": "For example, coordinated anomalies across districts may signal system-wide manipulation [28, 150].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of anomalies in spatiotemporal data. The cited papers' titles suggest the use of spatiotemporal data but do not specify datasets.",
      "processing_time": 61.31716823577881,
      "citing_paper_id": "280047877",
      "cited_paper_id": 264615596
    },
    {
      "context_text": "…the model is fine-tuned to follow natural language instructions using curated prompt-response examples [180]; and reinforcement learning from human feedback (RLHF), which trains a reward model based on human evaluations to guide the LLM’s outputs toward helpful, safe, and aligned responses [126].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches such as fine-tuning and reinforcement learning from human feedback.",
      "processing_time": 57.82569980621338,
      "citing_paper_id": "280047877",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Techniques such as privacy labeling, red-teaming tests [132], and consent enforcement mechanisms can add additional layers of protection.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. The cited paper title suggests 'red-teaming' as a method, not a dataset.",
      "processing_time": 59.11139678955078,
      "citing_paper_id": "280047877",
      "cited_paper_id": 246634238
    },
    {
      "context_text": "Another is counterfactual analysis [111], where synthetic inputs from underrepresented areas are introduced to test whether the agent responds fairly under more balanced conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or approach (counterfactual analysis) rather than a dataset.",
      "processing_time": 58.85102033615112,
      "citing_paper_id": "280047877",
      "cited_paper_id": 246863881
    },
    {
      "context_text": "Rehearsal learning [231] provides a practical alternative by enabling agents to simulate possible interventions and outcomes without relying on complete causal models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Rehearsal learning'. No verifiable resources are identified.",
      "processing_time": 58.04679560661316,
      "citing_paper_id": "280047877",
      "cited_paper_id": 250596535
    },
    {
      "context_text": "This requires mechanisms for detecting and responding to distribution shifts, especially those that are localized in space or time [48, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the need for mechanisms to detect and respond to distribution shifts. No verifiable resources are identified.",
      "processing_time": 59.154433727264404,
      "citing_paper_id": "280047877",
      "cited_paper_id": 251518264
    },
    {
      "context_text": "This requires mechanisms for detecting and responding to distribution shifts, especially those that are localized in space or time [48, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the need for mechanisms to detect and respond to distribution shifts. No verifiable resources are identified.",
      "processing_time": 59.154433727264404,
      "citing_paper_id": "280047877",
      "cited_paper_id": 260499637
    },
    {
      "context_text": "At the algorithmic level, techniques such as differential privacy [2] and federated learning [201] reduce risk by avoiding direct access to raw data and minimizing the retention of individual-level information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques like differential privacy and federated learning. These are methods, not datasets.",
      "processing_time": 58.21774649620056,
      "citing_paper_id": "280047877",
      "cited_paper_id": 251719382
    },
    {
      "context_text": "At the algorithmic level, techniques such as differential privacy [2] and federated learning [201] reduce risk by avoiding direct access to raw data and minimizing the retention of individual-level information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques like differential privacy and federated learning. These are methods, not datasets.",
      "processing_time": 58.21774649620056,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For long-term deployments, mechanisms like privacy budgets [151] can track cumulative exposure over time, ensuring that agents remain within acceptable usage limits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a mechanism for tracking privacy exposure over time.",
      "processing_time": 56.88197612762451,
      "citing_paper_id": "280047877",
      "cited_paper_id": 252683292
    },
    {
      "context_text": "This includes a broad range of behaviors such as generating textual outputs ( e.g. , reports or recommendations), executing code for simulation or computation, calling APIs or external services, or issuing control commands to affect real-world systems ( e.g. , switching traffic signals) [92, 143].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a range of behaviors and capabilities of language models. No verifiable resources are identified.",
      "processing_time": 58.61931109428406,
      "citing_paper_id": "280047877",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Perception involves parsing, summarizing, or transforming raw input into a format suitable for reasoning [143].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of perception involving parsing, summarizing, or transforming raw input.",
      "processing_time": 58.38729166984558,
      "citing_paper_id": "280047877",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Additionally, access to external tools such as code execution environments and domain-specific APIs can further enhance the output reliability of LLMs [143].",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only external tools and APIs. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.87532114982605,
      "citing_paper_id": "280047877",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "The recent revolution in Large Language Models (LLMs), such as GPT-4 [1] and DeepSeek-R1 [46], offers new opportunities to rethink the development of urban intelligence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their potential applications. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.38510084152222,
      "citing_paper_id": "280047877",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "The recent revolution in Large Language Models (LLMs), such as GPT-4 [1] and DeepSeek-R1 [46], offers new opportunities to rethink the development of urban intelligence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their potential applications. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.38510084152222,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Traditional KG-based systems, such as UrbanKG [104] and UUKG [124], rely on pre-defined schemas and manual annotation pipelines to extract entities and relations from urban data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions UrbanKG and UUKG but does not indicate they are datasets. They are described as systems, which suggests they are methods or tools rather than datasets.",
      "processing_time": 59.94339609146118,
      "citing_paper_id": "280047877",
      "cited_paper_id": 257632892
    },
    {
      "context_text": "For example, UrbanKGent [104] synthesizes tool-use instructions to guide the construction of urban knowledge graphs, enabling LLM agents to interface with spatial databases through structured queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'UrbanKGent' which is part of the cited paper's title 'UrbanKG: An Urban Knowledge Graph System'. However, it does not refer to a dataset but rather a system or method for constructing urban knowledge graphs.",
      "processing_time": 61.99514627456665,
      "citing_paper_id": "280047877",
      "cited_paper_id": 257632892
    },
    {
      "context_text": "• Advanced reasoning augmentation: In addition to the vanilla method, existing studies also invoke external tools or multi-turn agent discussion to obtain more reliable, diverse reasoning traces [112].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on advanced reasoning augmentation techniques.",
      "processing_time": 57.582855224609375,
      "citing_paper_id": "280047877",
      "cited_paper_id": 257900871
    },
    {
      "context_text": "Effective memory enables continuity in reasoning, retrieval of prior knowledge, and support for personalized or historical context [91, 129].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of effective memory. No verifiable resources are identified.",
      "processing_time": 57.78508472442627,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Effective memory enables continuity in reasoning, retrieval of prior knowledge, and support for personalized or historical context [91, 129].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of effective memory. No verifiable resources are identified.",
      "processing_time": 57.78508472442627,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266933252
    },
    {
      "context_text": "For example, Park et al. [129] create an interactive simulation of complex human behaviors in a game environment, based on generative agents consisting of modules including planning and reacting, memory and retrieval, and active reflection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for creating interactive simulations of human behavior using generative agents, but does not mention any specific datasets.",
      "processing_time": 58.14287781715393,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Generative Agents [129] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Humanoid Agents [177] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - D2A [175] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Trajectory,Text Vector Database Spatio-Temporal Multi-Agent' multiple times, but it is not a specific, identifiable dataset. The names 'Generative Agents', 'Humanoid Agents', and 'D2A' are mentioned, but they are likely referring to methods or platforms rather than datasets.",
      "processing_time": 64.37832903862,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "Generative Agents [129] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Humanoid Agents [177] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - D2A [175] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Trajectory,Text Vector Database Spatio-Temporal Multi-Agent' multiple times, but it is not a specific, identifiable dataset. The names 'Generative Agents', 'Humanoid Agents', and 'D2A' are mentioned, but they are likely referring to methods or platforms rather than datasets.",
      "processing_time": 64.37832903862,
      "citing_paper_id": "280047877",
      "cited_paper_id": 263830637
    },
    {
      "context_text": "At the core of LLM-powered agents lies a modular architecture designed to emulate goal-oriented behavior [47, 129, 171].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a modular architecture for LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 58.3714063167572,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258040990
    },
    {
      "context_text": "At the core of LLM-powered agents lies a modular architecture designed to emulate goal-oriented behavior [47, 129, 171].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a modular architecture for LLM-powered agents. No verifiable resources are identified.",
      "processing_time": 58.3714063167572,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "This includes fine-tuning model weights, updating memories, or refining decision-making strategies through self-reflection [147].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general processes such as fine-tuning model weights, updating memories, or refining decision-making strategies. No verifiable resources are identified.",
      "processing_time": 59.88931083679199,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "(2) Efficiency : Assesses the agent’s resource consumption ( e.g. , computational cost, response time, and communication overhead) when operating under real-world urban constraints [147].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of efficiency in agents. No clear identifiers for datasets are present.",
      "processing_time": 58.14511179924011,
      "citing_paper_id": "280047877",
      "cited_paper_id": 258833055
    },
    {
      "context_text": "Third, prompt injection attacks [103] exploit the decentralized and asynchronous nature of urban information flows.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of attack. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.91227388381958,
      "citing_paper_id": "280047877",
      "cited_paper_id": 259129807
    },
    {
      "context_text": "For instance, injecting subtle noise into traffic flows near hospitals can mislead routing systems, potentially delaying emergency vehicles in surrounding areas [97, 98].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a hypothetical scenario involving traffic flows near hospitals. No verifiable resources are identified.",
      "processing_time": 58.39333248138428,
      "citing_paper_id": "280047877",
      "cited_paper_id": 259251823
    },
    {
      "context_text": "For instance, injecting subtle noise into traffic flows near hospitals can mislead routing systems, potentially delaying emergency vehicles in surrounding areas [97, 98].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a hypothetical scenario involving traffic flows near hospitals. No verifiable resources are identified.",
      "processing_time": 58.39333248138428,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270379543
    },
    {
      "context_text": "When applied to previously unseen urban contexts ( e.g. , new cities or emerging services), these models struggle to provide reliable analysis and decision-making results due to significant disparities in data distribution [72, 156].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with data distribution across different urban contexts. No verifiable resources are named.",
      "processing_time": 58.65546154975891,
      "citing_paper_id": "280047877",
      "cited_paper_id": 260499706
    },
    {
      "context_text": "When applied to previously unseen urban contexts ( e.g. , new cities or emerging services), these models struggle to provide reliable analysis and decision-making results due to significant disparities in data distribution [72, 156].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general issues with data distribution across different urban contexts. No verifiable resources are named.",
      "processing_time": 58.65546154975891,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271955275
    },
    {
      "context_text": "Wiering [184] proposed an early multi-agent RL framework that incorporated information from neighboring intersections to estimate cumulative waiting times.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for traffic light control using multi-agent reinforcement learning.",
      "processing_time": 57.49785137176514,
      "citing_paper_id": "280047877",
      "cited_paper_id": 260548557
    },
    {
      "context_text": "Unlike general-purpose LLMs, which are primarily scrutinized for demographic bias in language outputs [107, 155], urban agents interact with spatially distributed data and influence diverse communities.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'spatially distributed data'. No clear, verifiable dataset names are present.",
      "processing_time": 59.033183574676514,
      "citing_paper_id": "280047877",
      "cited_paper_id": 260775522
    },
    {
      "context_text": "Compared to traditional multi-agent systems, LLM-based agents communicate more naturally, adapt to broader contexts with minimal domain-specific tuning, and demonstrate emergent cooperative behavior through shared prompts and memory [47, 171].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of LLM-based agents compared to traditional multi-agent systems.",
      "processing_time": 58.42063355445862,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "As a result, LLMs are increasingly deployed as autonomous agents that can respond and adapt to rapidly changing environments [171, 189].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the deployment of LLMs as autonomous agents.",
      "processing_time": 58.741512298583984,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "As a result, LLMs are increasingly deployed as autonomous agents that can respond and adapt to rapidly changing environments [171, 189].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the deployment of LLMs as autonomous agents.",
      "processing_time": 58.741512298583984,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "[171, 189] and embodied agents [99, 102].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to embodied agents. No verifiable resources are identified.",
      "processing_time": 58.418500661849976,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "[171, 189] and embodied agents [99, 102].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to embodied agents. No verifiable resources are identified.",
      "processing_time": 58.418500661849976,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "[171, 189] and embodied agents [99, 102].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to embodied agents. No verifiable resources are identified.",
      "processing_time": 58.418500661849976,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271064200
    },
    {
      "context_text": "[171, 189] and embodied agents [99, 102].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to embodied agents. No verifiable resources are identified.",
      "processing_time": 58.418500661849976,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276333529
    },
    {
      "context_text": "For instance, Wang et al. [171] and Xi et al. [189] provide comprehensive reviews of general LLM agents, covering their architectures, planning capabilities, and applications, while Guo et al. [47] focuses specifically on the progress and challenges of LLM-powered multi-agent systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reviews and surveys of LLM agents and their planning capabilities.",
      "processing_time": 58.15339207649231,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261064713
    },
    {
      "context_text": "For instance, Wang et al. [171] and Xi et al. [189] provide comprehensive reviews of general LLM agents, covering their architectures, planning capabilities, and applications, while Guo et al. [47] focuses specifically on the progress and challenges of LLM-powered multi-agent systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reviews and surveys of LLM agents and their planning capabilities.",
      "processing_time": 58.15339207649231,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "LLM-Mob [173] introduces a novel framework that formats mobility data into historical and contextual stays, capturing both long-term and short-term dependencies while enabling time-aware Trajectory Operational State Spatio-Temporal Single-Agent - TrajLLM [74] Trajectory Operational State…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It focuses on a method or framework for processing mobility data.",
      "processing_time": 57.893842458724976,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261276812
    },
    {
      "context_text": "…[74] Trajectory Operational State Spatio-Temporal Single-Agent - CoPB [145] Trajectory Operational State Spatio-Temporal Single-Agent - LLM-Mob [173] Trajectory - Spatio-Temporal Single-Agent - AgentMove [39] Trajectory Operational State Spatio-Temporal Single-Agent - TrajAgent [29] Trajectory…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CoPB",
        "LLM-Mob",
        "AgentMove",
        "TrajAgent"
      ],
      "dataset_descriptions": {
        "CoPB": "Used to evaluate LLMs' ability to predict human mobility, focusing on spatio-temporal single-agent trajectories.",
        "LLM-Mob": "Used to assess LLMs' predictive accuracy in human mobility, emphasizing operational state and spatio-temporal single-agent trajectories.",
        "AgentMove": "Used to test LLMs' planning capabilities in planning tasks, focusing on spatiotemporal single-agent trajectories.",
        "TrajAgent": "Used to evaluate LLMs' performance in predicting human mobility, specifically with spatio-temporal single-agent trajectories."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several trajectory datasets, which are relevant to the research topic of planning capabilities of LLMs. These datasets are used to evaluate the predictive performance of LLMs in human mobility scenarios.",
      "processing_time": 72.17802000045776,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261276812
    },
    {
      "context_text": "These tools act as extensions of the agent’s perception capability, enabling advanced operations like spatial transformations [3] and traffic simulations [213].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.008803606033325,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "These tools act as extensions of the agent’s perception capability, enabling advanced operations like spatial transformations [3] and traffic simulations [213].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tools and capabilities. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 59.008803606033325,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273822178
    },
    {
      "context_text": "TrafficGPT [213] processes road networks through simulation tools, enabling visualizations and diagnostic analysis in traffic scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TrafficGPT as a tool for processing road networks, but does not refer to any specific dataset. The context focuses on the capabilities of the tool rather than a reusable dataset.",
      "processing_time": 60.477885007858276,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "…[15] supports personalized trip planning via an interactive loop where users iteratively refine travel preferences and constraints, while TrafficGPT [213] analyzes traffic data and delegates final decision-making to human operators, thereby incorporating expert oversight into the process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes systems and methods but does not reference any datasets by name.",
      "processing_time": 58.61540675163269,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "For instance, traffic management agents such as TrafficGPT [213], OpenTI [24], and TP-GPT [169] leverage LLMs to retrieve and analyze traffic data, provide interactive decision support, and generate real-time reports.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions traffic management agents leveraging LLMs but does not specify any datasets. The cited papers' titles do not indicate the use of specific datasets.",
      "processing_time": 59.38438415527344,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "For instance, traffic management agents such as TrafficGPT [213], OpenTI [24], and TP-GPT [169] leverage LLMs to retrieve and analyze traffic data, provide interactive decision support, and generate real-time reports.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions traffic management agents leveraging LLMs but does not specify any datasets. The cited papers' titles do not indicate the use of specific datasets.",
      "processing_time": 59.38438415527344,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266693228
    },
    {
      "context_text": "For instance, traffic management agents such as TrafficGPT [213], OpenTI [24], and TP-GPT [169] leverage LLMs to retrieve and analyze traffic data, provide interactive decision support, and generate real-time reports.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions traffic management agents leveraging LLMs but does not specify any datasets. The cited papers' titles do not indicate the use of specific datasets.",
      "processing_time": 59.38438415527344,
      "citing_paper_id": "280047877",
      "cited_paper_id": 269605040
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267750165
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268032947
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Zhou et al. [229], Otal et al. [125], TravelAgent [15], TrafficGPT [213] Post-HocEvaluation UrbanKGent [123], UrbanLLM [65] Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UrbanKGent",
        "UrbanLLM"
      ],
      "dataset_descriptions": {
        "UrbanKGent": "Used to support urban planning tasks, likely providing structured data for knowledge graph-based applications in the context of LLMs.",
        "UrbanLLM": "Utilized to enhance participatory urban planning, potentially serving as a training or evaluation dataset for LLMs in urban planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'UrbanKGent' and 'UrbanLLM', which appear to be specific datasets or resources used in the context of urban planning and LLMs. However, the context does not provide explicit details on their usage or the specific research questions they address.",
      "processing_time": 69.93405032157898,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "CityGPT (IoT) [45], AgentMove [39], CoLLMLight [208], TrafficGPT [213], TravelAgent [15], UrbanLLM [65] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models or frameworks, none of which are datasets. The context does not provide any specific dataset names or verifiable resources.",
      "processing_time": 58.98092722892761,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "CityGPT (IoT) [45], AgentMove [39], CoLLMLight [208], TrafficGPT [213], TravelAgent [15], UrbanLLM [65] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models or frameworks, none of which are datasets. The context does not provide any specific dataset names or verifiable resources.",
      "processing_time": 58.98092722892761,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "CityGPT (IoT) [45], AgentMove [39], CoLLMLight [208], TrafficGPT [213], TravelAgent [15], UrbanLLM [65] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models or frameworks, none of which are datasets. The context does not provide any specific dataset names or verifiable resources.",
      "processing_time": 58.98092722892761,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "CityGPT (IoT) [45], AgentMove [39], CoLLMLight [208], TrafficGPT [213], TravelAgent [15], UrbanLLM [65] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models or frameworks, none of which are datasets. The context does not provide any specific dataset names or verifiable resources.",
      "processing_time": 58.98092722892761,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "CityGPT (IoT) [45], AgentMove [39], CoLLMLight [208], TrafficGPT [213], TravelAgent [15], UrbanLLM [65] Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models or frameworks, none of which are datasets. The context does not provide any specific dataset names or verifiable resources.",
      "processing_time": 58.98092722892761,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "TrafficGPT [213] enhances ChatGPT with traffic simulators and predictive models, enabling it to analyze and interpret numerical traffic data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (TrafficGPT) that processes traffic data. No verifiable dataset names are provided.",
      "processing_time": 59.40736389160156,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "Similarly, TrafficGPT [213] and CityGPT-IoT [45] enable users to interact with transportation data through natural language.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TrafficGPT' and 'CityGPT-IoT', which are tools or models, not datasets. No specific datasets are mentioned.",
      "processing_time": 59.55869102478027,
      "citing_paper_id": "280047877",
      "cited_paper_id": 261705716
    },
    {
      "context_text": "Similarly, TrafficGPT [213] and CityGPT-IoT [45] enable users to interact with transportation data through natural language.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TrafficGPT' and 'CityGPT-IoT', which are tools or models, not datasets. No specific datasets are mentioned.",
      "processing_time": 59.55869102478027,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "At the reasoning level, symbolic planning diagrams [42] can visualize goal decomposition and tool usage.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool for visualizing symbolic planning diagrams.",
      "processing_time": 57.67764186859131,
      "citing_paper_id": "280047877",
      "cited_paper_id": 262269888
    },
    {
      "context_text": "Similarly, Liu et al. [106] develop an LLM-powered system for delivery route optimization, reducing last-mile costs by semantically analyzing urban road network constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method or system developed by Liu et al. for delivery route optimization using LLMs.",
      "processing_time": 59.53438758850098,
      "citing_paper_id": "280047877",
      "cited_paper_id": 263322443
    },
    {
      "context_text": "Based on these agents, Humanoid Agents [177] supplement the System 1 thinking process featuring intuitive and instantaneous desires, embracing basic needs for survival, emotions, and the closeness of social relationships, which further consolidate the authenticity of the simulation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a platform for simulating human-like generative agents. No verifiable dataset names are present.",
      "processing_time": 58.89042782783508,
      "citing_paper_id": "280047877",
      "cited_paper_id": 263830637
    },
    {
      "context_text": "For example, LLM4POI [86] translates raw trajectories into prompt sequences using predefined templates, while LLMTime [44] tokenizes time series inputs digit-by-digit to improve forecasting performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models (LLM4POI and LLMTime). The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.10152554512024,
      "citing_paper_id": "280047877",
      "cited_paper_id": 263908782
    },
    {
      "context_text": "For example, LLM4POI [86] translates raw trajectories into prompt sequences using predefined templates, while LLMTime [44] tokenizes time series inputs digit-by-digit to improve forecasting performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or models (LLM4POI and LLMTime). The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.10152554512024,
      "citing_paper_id": "280047877",
      "cited_paper_id": 269449477
    },
    {
      "context_text": "The modality-to-text translation can be achieved through hand-crafted templates [9, 86, 113] or specialized tokenization strategies [44].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for modality-to-text translation.",
      "processing_time": 57.20182156562805,
      "citing_paper_id": "280047877",
      "cited_paper_id": 263908782
    },
    {
      "context_text": "The modality-to-text translation can be achieved through hand-crafted templates [9, 86, 113] or specialized tokenization strategies [44].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for modality-to-text translation.",
      "processing_time": 57.20182156562805,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268560147
    },
    {
      "context_text": "The modality-to-text translation can be achieved through hand-crafted templates [9, 86, 113] or specialized tokenization strategies [44].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for modality-to-text translation.",
      "processing_time": 57.20182156562805,
      "citing_paper_id": "280047877",
      "cited_paper_id": 269449477
    },
    {
      "context_text": "The modality-to-text translation can be achieved through hand-crafted templates [9, 86, 113] or specialized tokenization strategies [44].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for modality-to-text translation.",
      "processing_time": 57.20182156562805,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270199466
    },
    {
      "context_text": "EconAgent [85] utilizes LLM agents to create a simulation of macroeconomic activities, highlighting labor supply and consumption agent decisions intertwined with dynamics of financial markets and government taxation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLM agents in simulating macroeconomic activities.",
      "processing_time": 57.90932846069336,
      "citing_paper_id": "280047877",
      "cited_paper_id": 264146527
    },
    {
      "context_text": "…- D2A [175] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text -…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "EconAgent"
      ],
      "dataset_descriptions": {
        "EconAgent": "Used to simulate macroeconomic activities with large language model-empowered agents, focusing on operational state and text data in a multi-agent environment."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several multi-agent datasets, but only 'EconAgent' is associated with a specific paper title, which helps disambiguate it as a dataset.",
      "processing_time": 65.76686811447144,
      "citing_paper_id": "280047877",
      "cited_paper_id": 264146527
    },
    {
      "context_text": "For example, UrbanCLIP [197] aligns satellite imagery with textual descriptions using contrastive learning to enhance geographic understanding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'UrbanCLIP' but does not refer to it as a dataset. It is described as a method for aligning satellite imagery with textual descriptions using contrastive learning.",
      "processing_time": 60.13834738731384,
      "citing_paper_id": "280047877",
      "cited_paper_id": 264590333
    },
    {
      "context_text": "However, processing visual inputs requires robust computer vision pipelines and, more importantly, effective cross-modal alignment to connect visual content with spatial coordinates and textual semantics [55, 197].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the need for robust computer vision pipelines and cross-modal alignment. No verifiable resources are named.",
      "processing_time": 59.15161752700806,
      "citing_paper_id": "280047877",
      "cited_paper_id": 264590333
    },
    {
      "context_text": "However, processing visual inputs requires robust computer vision pipelines and, more importantly, effective cross-modal alignment to connect visual content with spatial coordinates and textual semantics [55, 197].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the need for robust computer vision pipelines and cross-modal alignment. No verifiable resources are named.",
      "processing_time": 59.15161752700806,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268680708
    },
    {
      "context_text": "Other studies focus on applying LLMs to analyze public sentiment and deliver information during and after disasters, including earthquakes [52], typhoons [190], floods [157], and fires [30], to enhance public safety and situational awareness.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in disaster scenarios. No clear, verifiable datasets are identified.",
      "processing_time": 59.305975675582886,
      "citing_paper_id": "280047877",
      "cited_paper_id": 265609836
    },
    {
      "context_text": "Other studies focus on applying LLMs to analyze public sentiment and deliver information during and after disasters, including earthquakes [52], typhoons [190], floods [157], and fires [30], to enhance public safety and situational awareness.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in disaster scenarios. No clear, verifiable datasets are identified.",
      "processing_time": 59.305975675582886,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270081134
    },
    {
      "context_text": "Other studies focus on applying LLMs to analyze public sentiment and deliver information during and after disasters, including earthquakes [52], typhoons [190], floods [157], and fires [30], to enhance public safety and situational awareness.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in disaster scenarios. No clear, verifiable datasets are identified.",
      "processing_time": 59.305975675582886,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Other studies focus on applying LLMs to analyze public sentiment and deliver information during and after disasters, including earthquakes [52], typhoons [190], floods [157], and fires [30], to enhance public safety and situational awareness.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in disaster scenarios. No clear, verifiable datasets are identified.",
      "processing_time": 59.305975675582886,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": 265609836
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270081134
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276885142
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, the [31] Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data [52], [190], [157], [30] Text - - Single-Agent - LLMAir system [32] employs a multi-agent LLM architecture to analyze air quality data during events such as wildfires.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Geovector, Text Vector Database Spatial Multi-Agent Synthetic Data' and 'Text - - Single-Agent - LLMAir system'. However, these do not appear to be specific, verifiable datasets but rather systems or methods. No other specific datasets are mentioned.",
      "processing_time": 63.22332835197449,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, UGI [193] places LLM agents in a simulated city where each agent acts independently based on local observations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'simulated city' but does not specify a dataset name. The title confirms it's about a platform for agents in a city environment, not a dataset.",
      "processing_time": 59.922019958496094,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266362729
    },
    {
      "context_text": "UGI [193], for instance, employs a curriculum learning strategy where agents are trained on progressively more complex tasks, starting with single-step execution and gradually advancing to multi-agent coordination in urban environments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach (curriculum learning) used in training agents.",
      "processing_time": 58.454320192337036,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266362729
    },
    {
      "context_text": "Implicit Co-ordination UGI [193], OpenCity [198], AgentSociety [133] Explicit Co-ordination CoLLMLight [208], CityGPT (IoT) [45] Human-AgentCollaboration",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and frameworks, but none of them are clearly identified as datasets. They appear to be platforms or systems used for simulation and coordination.",
      "processing_time": 59.66311264038086,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266362729
    },
    {
      "context_text": "Implicit Co-ordination UGI [193], OpenCity [198], AgentSociety [133] Explicit Co-ordination CoLLMLight [208], CityGPT (IoT) [45] Human-AgentCollaboration",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and frameworks, but none of them are clearly identified as datasets. They appear to be platforms or systems used for simulation and coordination.",
      "processing_time": 59.66311264038086,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273662115
    },
    {
      "context_text": "Implicit Co-ordination UGI [193], OpenCity [198], AgentSociety [133] Explicit Co-ordination CoLLMLight [208], CityGPT (IoT) [45] Human-AgentCollaboration",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and frameworks, but none of them are clearly identified as datasets. They appear to be platforms or systems used for simulation and coordination.",
      "processing_time": 59.66311264038086,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276317785
    },
    {
      "context_text": "Implicit Co-ordination UGI [193], OpenCity [198], AgentSociety [133] Explicit Co-ordination CoLLMLight [208], CityGPT (IoT) [45] Human-AgentCollaboration",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several platforms and frameworks, but none of them are clearly identified as datasets. They appear to be platforms or systems used for simulation and coordination.",
      "processing_time": 59.66311264038086,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266362729
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267682348
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271051324
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273654966
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276885258
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Generation PlanGPT [232], UrbanLLM [65], CoPB [145] AdvancedReasoningAugmentation UrbanKGent [123], UGI [193], TrajAgent [29] Learning from Environmental Feedback Simulation-Based Feedback iLLM-TSC [128], LLMLight [79], CoLLMLight [208], DiMA [122] Real-WorldFeedback WatchOverGPT [144] Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 58.57007884979248,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Verma et al. [167] demonstrate this potential by simulating urban citizens’ perceptions of city services, including waste management, underscoring the role of LLMs in understanding and potentially improving public satisfaction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the use of LLMs in simulating urban citizens' perceptions but does not reference a particular dataset.",
      "processing_time": 60.04470467567444,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266375217
    },
    {
      "context_text": "Additionally, Verma et al. [167] investigates LLM-powered generative agents that interact with urban street view imagery to simulate navigation toward defined destinations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'urban street view imagery' but does not specify a named dataset. The citation is more about the method and findings rather than a reusable dataset.",
      "processing_time": 59.88242197036743,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266375217
    },
    {
      "context_text": "Beyond feature enhancement, augmented LLM frameworks, such as Open-TI [24], are emerging to support advanced traffic analysis, including simulations and external tool integration for tasks like demand optimization and traffic signal control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an augmented LLM framework called Open-TI. The context focuses on the capabilities and applications of the framework rather than the use of a specific dataset.",
      "processing_time": 60.90567755699158,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266693228
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266693228
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271051324
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271571077
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "PlanGPT [232], UrbanLLM [65], LAMP [8], ITINERA [161], TravelAgent [15], LLM-Find [121], UrbanKGent [123] Hardware Control LLMLight [79], CoLLMLight [208], Open-TI [24], iLLM-TSC [128]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 59.14188361167908,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Open-TI [24] adopts a hierarchical structure where a central agent oversees global planning while local agents handle low-level control. iLLM-TSC [128] combines Reinforcement Learning (RL) with LLM-based verification: actions proposed by the RL policy are reviewed and approved by an LLM agent…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and systems. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.13941264152527,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266693228
    },
    {
      "context_text": "Open-TI [24] adopts a hierarchical structure where a central agent oversees global planning while local agents handle low-level control. iLLM-TSC [128] combines Reinforcement Learning (RL) with LLM-based verification: actions proposed by the RL policy are reviewed and approved by an LLM agent…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and systems. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 59.13941264152527,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271051324
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266844108
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267069344
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271571077
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274822996
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276617519
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Li et al. [82], SpaRC [139], CoS [59], VoT [188], SpatialVLM [16], Yang et al. [199] Urban Scenarios LLM-Find [121], Spatial-RAG [204], ITIN-ERA [161], LAMP [8], LLMLight [79], CityGPT [38]",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. The names mentioned are primarily models, methods, or tools, which are excluded according to the instructions.",
      "processing_time": 59.689191818237305,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Structured prompting techniques [82, 139] help organize spatial relationships into interpretable templates, enabling more systematic reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only structured prompting techniques. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 58.89089488983154,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266844108
    },
    {
      "context_text": "Structured prompting techniques [82, 139] help organize spatial relationships into interpretable templates, enabling more systematic reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only structured prompting techniques. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 58.89089488983154,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Meanwhile, Li et al. [91] discuss the design of lightweight, personalized LLM agents, with emphasis on efficiency, long-term memory, and user-specific customization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the design of lightweight, personalized LLM agents.",
      "processing_time": 58.149208307266235,
      "citing_paper_id": "280047877",
      "cited_paper_id": 266933252
    },
    {
      "context_text": "Along this line, more recent efforts have introduced specialized models and frameworks to further improve the climate reasoning capabilities of LLMs. ClimateGPT [163] is trained on extensive scientific and climate datasets using the Llama-2 architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'extensive scientific and climate datasets' but does not provide specific names. The citation is focused on the model (ClimateGPT) rather than a specific dataset.",
      "processing_time": 60.5159273147583,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267034871
    },
    {
      "context_text": "Beyond pure language, SpatialVLM [16] integrates textual inputs with visual and 3D spatial information, providing enhanced spatial awareness.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SpatialVLM) that integrates textual, visual, and 3D spatial information.",
      "processing_time": 59.429338455200195,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267069344
    },
    {
      "context_text": "LLMLight [79] employs a fine-tuned LLM to optimize signal phase timings at intersections through APIs in a simulation environment, aiming to reduce vehicle waiting times.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LLMLight) and its application in traffic signal control. No verifiable datasets are referenced.",
      "processing_time": 59.57932710647583,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "For example, LLMLight [79] obtains the real-time traffic conditions at target intersections, encoding them into readable textual format to guide traffic signal control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (LLMLight) and its application in traffic signal control.",
      "processing_time": 59.11395764350891,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "LLMLight [79], CoLLMLight [208], and LA-Light [172] apply LLMs to traffic signal control, reasoning over real-time observations and selecting signal phases aimed at improving traffic efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their application to traffic signal control. No verifiable resources are identified.",
      "processing_time": 58.73288011550903,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "LLMLight [79], CoLLMLight [208], and LA-Light [172] apply LLMs to traffic signal control, reasoning over real-time observations and selecting signal phases aimed at improving traffic efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their application to traffic signal control. No verifiable resources are identified.",
      "processing_time": 58.73288011550903,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268379562
    },
    {
      "context_text": "Agents such as iLLM-TSC [128] and LLMLight [79] integrate LLMs with traffic simulators to iteratively refine control policies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions agents integrating LLMs with traffic simulators but does not specify any datasets. The cited papers' titles do not indicate the use of specific datasets.",
      "processing_time": 59.71408128738403,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Agents such as iLLM-TSC [128] and LLMLight [79] integrate LLMs with traffic simulators to iteratively refine control policies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions agents integrating LLMs with traffic simulators but does not specify any datasets. The cited papers' titles do not indicate the use of specific datasets.",
      "processing_time": 59.71408128738403,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271051324
    },
    {
      "context_text": "For example, LLMLight [79] trains agents via reinforcement learning in traffic simulators, enhancing their ability to adaptively control traffic signals.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions training agents using reinforcement learning in traffic simulators, but does not specify any dataset names. The context is about the method and findings rather than a reusable dataset.",
      "processing_time": 60.16618728637695,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Third, instruction tuning in simulated environments offers a way to inject spatial knowledge directly into LLM agents [8, 38, 79].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving instruction tuning in simulated environments.",
      "processing_time": 57.98358201980591,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Third, instruction tuning in simulated environments offers a way to inject spatial knowledge directly into LLM agents [8, 38, 79].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving instruction tuning in simulated environments.",
      "processing_time": 57.98358201980591,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, LLMLight [79] demonstrates the feasibility of using LLMs as independent decision-making agents, interpreting traffic data through prompts and employing Chain-of-Thought reasoning to select signal phases, with a tailored LLM agent fine-tuned for this purpose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'traffic data' but does not specify a named dataset. The focus is on the method and findings of using LLMs for traffic signal control.",
      "processing_time": 60.01323390007019,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267617080
    },
    {
      "context_text": "Travel agents like LAMP [8], ITINERA [161], and TravelAgent [15] provide personalized itineraries by integrating user preferences with transportation constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems like LAMP, ITINERA, and TravelAgent, which are tools or methods, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 60.33059763908386,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "Travel agents like LAMP [8], ITINERA [161], and TravelAgent [15] provide personalized itineraries by integrating user preferences with transportation constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems like LAMP, ITINERA, and TravelAgent, which are tools or methods, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 60.33059763908386,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Travel agents like LAMP [8], ITINERA [161], and TravelAgent [15] provide personalized itineraries by integrating user preferences with transportation constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems like LAMP, ITINERA, and TravelAgent, which are tools or methods, not datasets. No specific datasets are mentioned or used in the context.",
      "processing_time": 60.33059763908386,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "ITINERA [161] constructs a POI-level memory from user travel blogs, using LLMs to extract descriptions and encode them into dense embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ITINERA, which is a system or method, not a dataset. No specific dataset is named or described in the citation span.",
      "processing_time": 59.38336539268494,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "For instance, ITINERA [161] combines LLM-generated plans with spatial solvers to ensure routes are contextually appropriate and spatially feasible.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ITINERA' but does not refer to it as a dataset. It is described as a method combining LLM-generated plans with spatial solvers. No specific dataset is mentioned.",
      "processing_time": 60.93920969963074,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "Second, tool-augmented agents extend LLMs with access to spatial tools for solving real-world tasks such as travel planning [73, 123, 161].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of tool-augmented agents using spatial tools for tasks like travel planning. No verifiable resources are named.",
      "processing_time": 59.998812675476074,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "Second, tool-augmented agents extend LLMs with access to spatial tools for solving real-world tasks such as travel planning [73, 123, 161].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of tool-augmented agents using spatial tools for tasks like travel planning. No verifiable resources are named.",
      "processing_time": 59.998812675476074,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273680557
    },
    {
      "context_text": "Second, tool-augmented agents extend LLMs with access to spatial tools for solving real-world tasks such as travel planning [73, 123, 161].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of tool-augmented agents using spatial tools for tasks like travel planning. No verifiable resources are named.",
      "processing_time": 59.998812675476074,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, ITINERA [161] adopts a preference-aware POI retrieval module, where user requests are decomposed into fine-grained intents and encoded into embedding vectors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. The reference is to a method or system (ITINERA) rather than a dataset.",
      "processing_time": 59.379533529281616,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267627170
    },
    {
      "context_text": "…Trajectory Operational State Spatio-Temporal Single-Agent - TrajLLM [74] Trajectory Operational State Spatio-Temporal Single-Agent - CoPB [145] Trajectory Operational State Spatio-Temporal Single-Agent - LLM-Mob [173] Trajectory - Spatio-Temporal Single-Agent - AgentMove [39] Trajectory…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods or models (TrajLLM, CoPB, LLM-Mob, AgentMove) but does not refer to any specific datasets. The context is focused on trajectory and spatio-temporal single-agent models, which are not datasets.",
      "processing_time": 63.13974142074585,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267682348
    },
    {
      "context_text": "For longer-term planning tasks, CoPB [145] decomposes decision-making into a chain of intention-driven steps, allowing the agent to simulate human-like planning behavior across spatial and temporal contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CoPB) for decomposing decision-making in long-term planning tasks.",
      "processing_time": 58.79044508934021,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267682348
    },
    {
      "context_text": "Drawing from behavioral theories, CoPB [145] explicitly models the interweaving of attitudes, subjective norms, and perceived behavioral control within the reasoning process of agents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (CoPB). The context focuses on the theoretical framework and modeling aspects.",
      "processing_time": 59.04723000526428,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267682348
    },
    {
      "context_text": "Otal et al. [125] introduce an LLM agent for emergency response that communicates with citizens and dispatchers, while being overseen by human officials to ensure clarity and appropriateness in high-stakes scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of an LLM agent for emergency response. No verifiable resources are identified.",
      "processing_time": 58.837899923324585,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267750165
    },
    {
      "context_text": "These models can classify emergency events from 911 calls or social media, assist dispatchers with real-time recommendations, and alert relevant agencies when systems are overwhelmed [125].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of models in crisis management. No verifiable resources are identified.",
      "processing_time": 58.63099455833435,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267750165
    },
    {
      "context_text": "LLMob [67] is one of the pioneering studies, which capitalizes on LLM agents to generate individual mobility patterns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework using LLM agents for generating mobility patterns.",
      "processing_time": 58.285837173461914,
      "citing_paper_id": "280047877",
      "cited_paper_id": 267782436
    },
    {
      "context_text": "For instance, Zhou et al. [229] introduce a participatory planning framework where an LLM \"planner\" interacts with multiple LLM \"residents\" agents that simulate community feedback.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework involving LLMs for participatory urban planning.",
      "processing_time": 58.40741229057312,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268032947
    },
    {
      "context_text": "Zhou et al. [229] Geo-Vector, Image - Spatial Multi-Agent - Singla et al. [148] Geo-Vector, Image - Spatial Multi-Agent - Ni et al. [119] Geo-Vector, Image Operational State Spatio-Temporal Multi-Agent -",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model types and research areas. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 59.144221782684326,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268032947
    },
    {
      "context_text": "Zhou et al. [229] Geo-Vector, Image - Spatial Multi-Agent - Singla et al. [148] Geo-Vector, Image - Spatial Multi-Agent - Ni et al. [119] Geo-Vector, Image Operational State Spatio-Temporal Multi-Agent -",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model types and research areas. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 59.144221782684326,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274964904
    },
    {
      "context_text": "Zhou et al. [229] Geo-Vector, Image - Spatial Multi-Agent - Singla et al. [148] Geo-Vector, Image - Spatial Multi-Agent - Ni et al. [119] Geo-Vector, Image Operational State Spatio-Temporal Multi-Agent -",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model types and research areas. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 59.144221782684326,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275133196
    },
    {
      "context_text": "For example, Zhou et al. [229] propose a participatory land use planning framework empowered by LLMs. Specifically, it crafts LLM agents to emulate the planner and residents, and engage them in a multi-agent discussion to balance their divergent needs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework using LLMs for participatory urban planning.",
      "processing_time": 58.481372594833374,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268032947
    },
    {
      "context_text": "Advanced frameworks like CAMEL [84] demonstrate how agents with assigned roles can engage in multi-turn dialogues to collaboratively solve complex tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or framework (CAMEL) used for collaborative task-solving.",
      "processing_time": 58.47696876525879,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268042527
    },
    {
      "context_text": "For example, PlanGPT [232] introduces a hierarchical retrieval framework that combines keyword indexing with cross-attention-based re-ranking, enabling LLMs to locate semantically relevant textual chunks from large-scale planning archives.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale planning archives' but does not specify a named dataset. The term 'planning archives' is too generic and lacks a specific identifier.",
      "processing_time": 60.06347918510437,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "[222] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data PlanGPT [232] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data City-LEO [66] Text Vector Database Spatio-Temporal Single-Agent - Kalyuzhnaya et al. [76] Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 639), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Text Vector Database', 'Spatio-Temporal Single-Age",
      "processing_time": 72.4706380367279,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "[222] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data PlanGPT [232] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data City-LEO [66] Text Vector Database Spatio-Temporal Single-Agent - Kalyuzhnaya et al. [76] Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 639), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Text Vector Database', 'Spatio-Temporal Single-Age",
      "processing_time": 72.4706380367279,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275936018
    },
    {
      "context_text": "[222] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data PlanGPT [232] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data City-LEO [66] Text Vector Database Spatio-Temporal Single-Agent - Kalyuzhnaya et al. [76] Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 639), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Text Vector Database', 'Spatio-Temporal Single-Age",
      "processing_time": 72.4706380367279,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "[222] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data PlanGPT [232] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data City-LEO [66] Text Vector Database Spatio-Temporal Single-Agent - Kalyuzhnaya et al. [76] Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 639), response: ```json\n{\n    \"reasoning\": \"The context mentions 'Text Vector Database', 'Spatio-Temporal Single-Age",
      "processing_time": 72.4706380367279,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Similarly, PlanGPT [232] utilizes retrieved documents to inform constraint-aware urban planning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PlanGPT) and its application in urban planning.",
      "processing_time": 58.65056085586548,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "In the domain of urban planning and governance, PlanGPT [232] integrates local databases and tool-calling capabilities to support policy drafting and zoning evaluation, while UrbanLLM [65] decomposes urban service queries into subtasks and coordinates external AI models to enable autonomous urban…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to local databases and tool-calling capabilities, which are too generic and not named.",
      "processing_time": 59.70366597175598,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "To generate, translate, or evaluate professional urban planning documents, PlanGPT [232] crafts an LLM agent that can strategically utilize various data sources representing the latest urban information.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'various data sources representing the latest urban information' but does not specify any named datasets. The reference is too generic and lacks specific identifiers.",
      "processing_time": 59.87479305267334,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "PlanGPT [232], on the other hand, develops a domain-specific vector database by using Plan-Emb, a custom embedding model pre-trained on general Chinese corpora and then fine-tuned on curated urban planning documents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a custom embedding model and a domain-specific vector database, but does not specify a named, verifiable dataset. The focus is on the method and the model rather than a specific dataset.",
      "processing_time": 61.023157835006714,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "PlanGPT [232] exemplifies this approach by fine-tuning LLMs on instruction-style data extracted from these materials, allowing the agent to internalize the logic and structure of urban planning policies.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'instruction-style data' but does not specify a named dataset. The term 'materials' is too generic and lacks a specific identifier.",
      "processing_time": 59.6929669380188,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "For example, PlanGPT [232] generates structured planning reports to support land use and city development decisions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PlanGPT) and its application in urban planning.",
      "processing_time": 58.929261445999146,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268063792
    },
    {
      "context_text": "Urban LLM agents interact with a wide range of sensing modalities that span the spatial, temporal, and social dimensions of urban life [233].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general interaction with sensing modalities in urban computing.",
      "processing_time": 58.099364280700684,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268091158
    },
    {
      "context_text": "Emerging research suggests that LLMs can be applied to tasks such as crime classification and generating explanations for crime occurrences [182].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the potential application of LLMs in digital forensics. No clear, verifiable datasets are identified.",
      "processing_time": 59.9141743183136,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268091311
    },
    {
      "context_text": "For example, LA-Light [172] introduces a hybrid framework where the LLM serves as a central reasoning engine, coordinating specialized tools to gather data and make human-like decisions, particularly for rare events such as sensor failures or emergency vehicles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called LA-Light. The context focuses on the capabilities and applications of the framework rather than the use of a dataset.",
      "processing_time": 60.81746482849121,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268379562
    },
    {
      "context_text": "Recent works further extend cross-modal alignment to other modalities, including POIs [192], street-view imagery [55], time series [93, 225], and trajectories [194], improving the agent’s ability to sense multimodal urban data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to various modalities used in recent works, which are not datasets themselves.",
      "processing_time": 59.90776014328003,
      "citing_paper_id": "280047877",
      "cited_paper_id": 268680708
    },
    {
      "context_text": "Recent works further extend cross-modal alignment to other modalities, including POIs [192], street-view imagery [55], time series [93, 225], and trajectories [194], improving the agent’s ability to sense multimodal urban data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to various modalities used in recent works, which are not datasets themselves.",
      "processing_time": 59.90776014328003,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271961973
    },
    {
      "context_text": "Recent works further extend cross-modal alignment to other modalities, including POIs [192], street-view imagery [55], time series [93, 225], and trajectories [194], improving the agent’s ability to sense multimodal urban data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to various modalities used in recent works, which are not datasets themselves.",
      "processing_time": 59.90776014328003,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Another line of research focuses on symbolic abstraction [59, 83], which represents spatial concepts in a compact format that supports step-by-step reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a line of research focusing on symbolic abstraction. No verifiable resources are identified.",
      "processing_time": 59.15484976768494,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270045529
    },
    {
      "context_text": "Another line of research focuses on symbolic abstraction [59, 83], which represents spatial concepts in a compact format that supports step-by-step reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a line of research focusing on symbolic abstraction. No verifiable resources are identified.",
      "processing_time": 59.15484976768494,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "One is to enforce spatial and temporal consistency constraints during training and fine-tuning, helping agents align with realistic urban behaviors [97].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training and fine-tuning agents.",
      "processing_time": 58.097290992736816,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270379543
    },
    {
      "context_text": "TempReason [158], Test of time [37], TRAM [176], TimeBench [21], Timo [153], CoTempQA [152], TimeLlaMA [206] Urban Scenarios Time-LLM [69], Wang et al. [174], ChatTime [170] Spatial Reasoning",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several names, but they are primarily models, methods, or benchmarks, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 59.678595542907715,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270440657
    },
    {
      "context_text": "TempReason [158], Test of time [37], TRAM [176], TimeBench [21], Timo [153], CoTempQA [152], TimeLlaMA [206] Urban Scenarios Time-LLM [69], Wang et al. [174], ChatTime [170] Spatial Reasoning",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several names, but they are primarily models, methods, or benchmarks, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 59.678595542907715,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270440703
    },
    {
      "context_text": "TempReason [158], Test of time [37], TRAM [176], TimeBench [21], Timo [153], CoTempQA [152], TimeLlaMA [206] Urban Scenarios Time-LLM [69], Wang et al. [174], ChatTime [170] Spatial Reasoning",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several names, but they are primarily models, methods, or benchmarks, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 59.678595542907715,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270619481
    },
    {
      "context_text": "TempReason [158], Test of time [37], TRAM [176], TimeBench [21], Timo [153], CoTempQA [152], TimeLlaMA [206] Urban Scenarios Time-LLM [69], Wang et al. [174], ChatTime [170] Spatial Reasoning",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several names, but they are primarily models, methods, or benchmarks, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 59.678595542907715,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273404602
    },
    {
      "context_text": "TempReason [158], Test of time [37], TRAM [176], TimeBench [21], Timo [153], CoTempQA [152], TimeLlaMA [206] Urban Scenarios Time-LLM [69], Wang et al. [174], ChatTime [170] Spatial Reasoning",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several names, but they are primarily models, methods, or benchmarks, not datasets. No specific, verifiable datasets are mentioned.",
      "processing_time": 59.678595542907715,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274776648
    },
    {
      "context_text": "• In general scenarios , recent benchmarks [21, 37, 158, 176] have revealed that while LLMs show promising results in basic tasks like event ordering, they often struggle with more complex reasoning involving temporal logic and implicit relations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions benchmarks but does not specify any particular dataset names. It focuses on the performance of LLMs in temporal reasoning tasks.",
      "processing_time": 59.129865407943726,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270440657
    },
    {
      "context_text": "For instance, Timo [153] augments LLMs with mathematical knowledge for arithmetic-based temporal reasoning, while CoTempQA [152] focuses on improving reasoning capabilities on overlapping and co-occurring events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CoTempQA' and 'Timo', but these are methods or systems rather than datasets. No specific datasets are mentioned.",
      "processing_time": 59.548121213912964,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270440703
    },
    {
      "context_text": "For instance, Timo [153] augments LLMs with mathematical knowledge for arithmetic-based temporal reasoning, while CoTempQA [152] focuses on improving reasoning capabilities on overlapping and co-occurring events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CoTempQA' and 'Timo', but these are methods or systems rather than datasets. No specific datasets are mentioned.",
      "processing_time": 59.548121213912964,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270619481
    },
    {
      "context_text": "Simulation-based audits using urban digital twins [209] can reveal hidden biases.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or approach (simulation-based audits using urban digital twins) rather than a dataset.",
      "processing_time": 60.18822145462036,
      "citing_paper_id": "280047877",
      "cited_paper_id": 270619971
    },
    {
      "context_text": "Another approach, iLLM-TSC [128], combines LLMs with reinforcement learning (RL), using the LLM as a supervisor to evaluate and refine RL-generated decisions, addressing RL’s limitations in scenarios with imperfect observations or rare events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining LLMs with reinforcement learning for traffic signal control.",
      "processing_time": 58.75506567955017,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271051324
    },
    {
      "context_text": "TraveLLM [35], for instance, introduces an LLM-driven framework that generates alternative transit plans through conversational interactions, providing real-time support during service disruptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called TraveLLM. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.020036935806274,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271329051
    },
    {
      "context_text": "Based on LLMob, to incorporate fine-grained collective patterns into mobility generation, MobAgent [89] applies agent clustering according to individual profiles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MobAgent) and a concept (LLMob). No verifiable resources are identified.",
      "processing_time": 59.7436637878418,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271533766
    },
    {
      "context_text": "First, retrieval-augmented approaches such as LLM-Find [121] and Spatial-RAG [204] integrate external geospatial knowledge bases to retrieve spatial facts and constraints, resulting in higher precision and interpretability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'geospatial knowledge bases' but does not specify any particular dataset. The cited papers focus on methods and frameworks rather than specific datasets.",
      "processing_time": 59.85005521774292,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271571077
    },
    {
      "context_text": "First, retrieval-augmented approaches such as LLM-Find [121] and Spatial-RAG [204] integrate external geospatial knowledge bases to retrieve spatial facts and constraints, resulting in higher precision and interpretability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'geospatial knowledge bases' but does not specify any particular dataset. The cited papers focus on methods and frameworks rather than specific datasets.",
      "processing_time": 59.85005521774292,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276617519
    },
    {
      "context_text": "Additionally, LLM-Find [121] enables agents to retrieve geographic data by writing and executing code, while UrbanKGent [123] automatically constructs urban knowledge graphs by extracting entities and relationships from heterogeneous sources such as text and maps.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LLM-Find' and 'UrbanKGent', but neither are datasets. They are frameworks or systems. No specific datasets are mentioned.",
      "processing_time": 60.3613395690918,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271571077
    },
    {
      "context_text": "Additionally, LLM-Find [121] enables agents to retrieve geographic data by writing and executing code, while UrbanKGent [123] automatically constructs urban knowledge graphs by extracting entities and relationships from heterogeneous sources such as text and maps.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'LLM-Find' and 'UrbanKGent', but neither are datasets. They are frameworks or systems. No specific datasets are mentioned.",
      "processing_time": 60.3613395690918,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "To further improve output reliability and problem-solving depth, inference-time computation techniques such as best-of-N sampling, majority voting, and Monte Carlo Tree Search (MCTS) explore diverse reasoning paths and select robust answers [149].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques for improving model performance during inference.",
      "processing_time": 58.17592096328735,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271719990
    },
    {
      "context_text": "Inspired by recent progress in autonomous science [109], we envision agents that can support the entire workflows of urban scientific discovery.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a vision for agents supporting scientific discovery workflows.",
      "processing_time": 58.331610918045044,
      "citing_paper_id": "280047877",
      "cited_paper_id": 271854887
    },
    {
      "context_text": "When combined with geovector data, trajectories can help agents infer causality, identify anomalies [230], and anticipate future trends [93].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'geovector data' and 'trajectories', which are generic terms without specific identifiers.",
      "processing_time": 59.98466968536377,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273229366
    },
    {
      "context_text": "Subsequently, Wang et al. [174] integrate textual event knowledge with temporal data streams, enriching the semantic context of time series forecasting in domains such as transportation and energy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of textual event knowledge with temporal data streams. No clear, verifiable dataset names are provided.",
      "processing_time": 60.17301058769226,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273404602
    },
    {
      "context_text": "…for ClimateBERT [178] Text - - Single-Agent - ClimateGPT [163] Text Vector Database Spatio-Temporal Single-Agent Synthetic Data ChatClimate [165] Text Vector Database Spatio-Temporal Single-Agent - ClimaQA [114] Text Vector Database Spatio-Temporal Single-Agent - city-scale ride-hailing services.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It lists various models and frameworks, which are not considered datasets according to the extraction rules.",
      "processing_time": 59.76404404640198,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273506998
    },
    {
      "context_text": "Complementing these model developments, ClimaQA [114] introduces an automated evaluation framework designed to systematically assess LLMs’ understanding of climate science.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ClimaQA as an evaluation framework, which is not a dataset but a method or tool. No specific dataset is mentioned in the context.",
      "processing_time": 60.322123289108276,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273506998
    },
    {
      "context_text": "Further advancing the field, TrajAgent [29] unifies trajectory modeling tasks under a single LLM-based agentic framework, integrating data augmentation and parameter optimization to adapt models dynamically.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called TrajAgent. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 60.383148431777954,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273654966
    },
    {
      "context_text": "TrajAgent [29] derives a self-reflection mechanism to generate a large-scale dataset of agent-environment interactions under urban mobility constraints, which is used to train LLMs to generate feasible trajectories and respond to changing urban contexts.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "agent-environment interactions dataset"
      ],
      "dataset_descriptions": {
        "agent-environment interactions dataset": "Used to train LLMs to generate feasible trajectories and respond to changing urban contexts, focusing on urban mobility constraints."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions a large-scale dataset of agent-environment interactions generated by TrajAgent, which is used to train LLMs for trajectory generation and urban context response.",
      "processing_time": 64.12696671485901,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273654966
    },
    {
      "context_text": "…Single-Agent - CoPB [145] Trajectory Operational State Spatio-Temporal Single-Agent - LLM-Mob [173] Trajectory - Spatio-Temporal Single-Agent - AgentMove [39] Trajectory Operational State Spatio-Temporal Single-Agent - TrajAgent [29] Trajectory Operational State Spatio-Temporal Multi-Agent -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several frameworks and methods, but does not specify any datasets. The names 'CoPB', 'LLM-Mob', 'AgentMove', and 'TrajAgent' are not datasets but rather frameworks or methods.",
      "processing_time": 62.042476654052734,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273654966
    },
    {
      "context_text": "…Single-Agent - CoPB [145] Trajectory Operational State Spatio-Temporal Single-Agent - LLM-Mob [173] Trajectory - Spatio-Temporal Single-Agent - AgentMove [39] Trajectory Operational State Spatio-Temporal Single-Agent - TrajAgent [29] Trajectory Operational State Spatio-Temporal Multi-Agent -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several frameworks and methods, but does not specify any datasets. The names 'CoPB', 'LLM-Mob', 'AgentMove', and 'TrajAgent' are not datasets but rather frameworks or methods.",
      "processing_time": 62.042476654052734,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "In mobility applications, TrajAgent [29] stores diverse trajectories and user interaction history in a unified memory structure for trajectory-related tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'TrajAgent' which is described as storing diverse trajectories and user interaction history. However, it is not clear if 'TrajAgent' itself is a dataset or a framework. The title of the cited paper suggests it is a framework, not a dataset.",
      "processing_time": 62.734768867492676,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273654966
    },
    {
      "context_text": "OpenCity [198] scales this idea by allowing thousands of agents to operate in a shared environment, where local memory and feedback mechanisms guide behavior.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a platform called OpenCity but does not refer to it as a dataset or provide details on data usage.",
      "processing_time": 60.66228890419006,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273662115
    },
    {
      "context_text": "Furthermore, OpenCity [198] introduces a group-and-distill strategy that implements a prototype learning paradigm, discovering agents with similar profiles for batch simulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or strategy for simulation. The context focuses on the approach rather than a reusable dataset.",
      "processing_time": 59.68375086784363,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273662115
    },
    {
      "context_text": "…- Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text - Spatio-Temporal Multi-Agent - OpenCity [198] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenCity"
      ],
      "dataset_descriptions": {
        "OpenCity": "Used to simulate urban activities with massive LLM agents, focusing on spatio-temporal multi-agent trajectories and text vectors for planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'OpenCity' which is a platform for simulating urban activities with LLM agents. It is used as a database for spatio-temporal multi-agent trajectories and text vectors.",
      "processing_time": 63.99904251098633,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273662115
    },
    {
      "context_text": "…- Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text - Spatio-Temporal Multi-Agent - OpenCity [198] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent -",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenCity"
      ],
      "dataset_descriptions": {
        "OpenCity": "Used to simulate urban activities with massive LLM agents, focusing on spatio-temporal multi-agent trajectories and text vectors for planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'OpenCity' which is a platform for simulating urban activities with LLM agents. It is used as a database for spatio-temporal multi-agent trajectories and text vectors.",
      "processing_time": 63.99904251098633,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, to ensure adaptable parking facilities that accommodate various types of vehicles and urban settings, Jin et al. [73] devise an LLM-based parking planning agent to evaluate and optimize current parking infrastructures in an efficient and flexible way.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or application of LLMs in parking planning. No verifiable resources are identified.",
      "processing_time": 59.73366713523865,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273680557
    },
    {
      "context_text": "Similarly, EF-LLM [134] is a framework designed to address challenges in load, photovoltaic (PV), and wind power forecasting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework (EF-LLM) for energy forecasting. No verifiable datasets are referenced.",
      "processing_time": 59.888962268829346,
      "citing_paper_id": "280047877",
      "cited_paper_id": 273811763
    },
    {
      "context_text": "Further, Park et al. [130] apply qualitative interviews to the agents to mirror their attitudes and behaviors during realistic lives, which are demonstrated to accurately replicate human participants’ authentic responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method involving qualitative interviews applied to agents in simulations.",
      "processing_time": 59.205605030059814,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274117080
    },
    {
      "context_text": "Bhandari et al. [11] Text - Spatio-Temporal Multi-Agent - Park et al. [130] Text - Spatio-Temporal Multi-Agent - Yang et al. [200] Text - Spatio-Temporal Multi-Agent - predictions through carefully designed prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to spatio-temporal multi-agent systems. No clear, verifiable datasets are identified.",
      "processing_time": 60.060155391693115,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274117080
    },
    {
      "context_text": "Bhandari et al. [11] Text - Spatio-Temporal Multi-Agent - Park et al. [130] Text - Spatio-Temporal Multi-Agent - Yang et al. [200] Text - Spatio-Temporal Multi-Agent - predictions through carefully designed prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to spatio-temporal multi-agent systems. No clear, verifiable datasets are identified.",
      "processing_time": 60.060155391693115,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Bhandari et al. [11] Text - Spatio-Temporal Multi-Agent - Park et al. [130] Text - Spatio-Temporal Multi-Agent - Yang et al. [200] Text - Spatio-Temporal Multi-Agent - predictions through carefully designed prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to spatio-temporal multi-agent systems. No clear, verifiable datasets are identified.",
      "processing_time": 60.060155391693115,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Unlike standard multi-agent reinforcement learning [220], urban LLM agents should account for asymmetric information, fragmented governance, and real-world regulatory boundaries.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to multi-agent reinforcement learning. No verifiable resources are identified.",
      "processing_time": 59.15192890167236,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274198497
    },
    {
      "context_text": "Beyond the analysis of human mobility data, social-economic activity emphasizes more complex and diverse interactive behaviors in social and economic scenarios [117].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'human mobility data'. No clear, verifiable dataset names are provided.",
      "processing_time": 59.674455404281616,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274464979
    },
    {
      "context_text": "Recent work, GARLIC [54], focuses on optimizing vehicle dispatching, a critical task in ride-hailing operations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GARLIC) for optimizing vehicle dispatching. No verifiable datasets are referenced.",
      "processing_time": 60.063005685806274,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274656234
    },
    {
      "context_text": "To further enhance the LLM’s ability to reason over complex urban signals, ChatTime [170] proposes to scale and quan-tize numerical time series, converting them into discrete tokens that can be directly processed alongside text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for processing time series data in LLMs but does not reference a named dataset.",
      "processing_time": 60.24864053726196,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274776648
    },
    {
      "context_text": "Yang et al. [199] further demonstrate that generating cognitive maps can facilitate complex spatial reasoning, particularly in video-based benchmarks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'video-based benchmarks' but does not specify any particular dataset. The term 'benchmarks' is excluded as it likely refers to a suite of tasks rather than a specific, downloadable dataset.",
      "processing_time": 60.99303722381592,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274822996
    },
    {
      "context_text": "To achieve balanced and area-specific land use layouts, Singla et al. [148] harness four specialized LLM agents to manage the development of different sub-areas, targeting the local-regional land-use requirements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using LLM agents for urban planning.",
      "processing_time": 58.52749466896057,
      "citing_paper_id": "280047877",
      "cited_paper_id": 274964904
    },
    {
      "context_text": "A promising research direction lies in developing hybrid approaches that combine the adaptability of emergent coordination with the controllability of structured interaction, particularly for tasks demanding both social fidelity and operational robustness [119].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a research direction and a cited paper title that does not provide additional context.",
      "processing_time": 58.860151529312134,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275133196
    },
    {
      "context_text": "Furthermore, to provide a fine-grained assessment of the urban plans and facilitate continuous plan enhancement, Ni et al. [119] propose the cyclical urban planning paradigm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework for cyclical urban planning.",
      "processing_time": 57.90582776069641,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275133196
    },
    {
      "context_text": "Besides, Kalyuzhnaya et al. [76] design a multi-agent system that integrates retrieval-augmented generation (RAG) approaches, demonstrating superior accuracy in answering queries regarding urban management.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (multi-agent system with RAG).",
      "processing_time": 58.127702474594116,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275936018
    },
    {
      "context_text": "In urban waste management, LLMs can improve operational efficiency by processing waste pickup requests, managing collection schedules, and optimizing resource allocation [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of LLMs in urban waste management.",
      "processing_time": 58.285276651382446,
      "citing_paper_id": "280047877",
      "cited_paper_id": 275936018
    },
    {
      "context_text": "Similarly, AgentMove [39] divides mobility prediction into individual behaviors, spatial distributions, and shared movement patterns, with each handled by a dedicated module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AgentMove' but does not refer to it as a dataset. It appears to be a method or framework for predicting human mobility.",
      "processing_time": 59.60783410072327,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "Further, AgentMove [39] proposes a systematic agentic framework that decomposes mobility prediction into subtasks, including individual pattern mining, urban structure modeling, and collective knowledge extraction, achieving superior performance across diverse datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'diverse datasets' but does not specify any particular dataset names. The citation is focused on the method and its performance rather than a specific dataset.",
      "processing_time": 60.17024779319763,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "Similarly, AgentMove [39] develops a temporal memory to capture users’ recent and long-term mobility patterns in key-value pairs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'AgentMove' but does not refer to it as a dataset. It appears to be a method or framework for predicting human mobility.",
      "processing_time": 59.504234075546265,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "For example, AgentMove [39] integrates personalized trajectory histories into the prompt for destination prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'AgentMove' but does not refer to it as a dataset. It is described as a method or framework for predicting human mobility using LLMs.",
      "processing_time": 59.97646975517273,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276250403
    },
    {
      "context_text": "…Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - AgentTorch [19] Text - Spatio-Temporal Multi-Agent - OpenCity [198]…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions several multi-agent systems and databases, but none of them are clearly identified as datasets. The names appear to be tools, frameworks, or methods rather than specific, verifiable datasets.",
      "processing_time": 60.518500089645386,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276317785
    },
    {
      "context_text": "AgentSociety [133] proposes a large-scale simulator, integrating LLM agents, a realistic society environment, and a powerful simulation engine.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a simulator and a general environment. No clear, verifiable datasets are identified.",
      "processing_time": 59.18557572364807,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276317785
    },
    {
      "context_text": "AgentSociety [133] further enhances realism by equip-ping agents with internal traits such as beliefs, goals, and emotions, facilitating emergent social behaviors like norm formation and group polarization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'AgentSociety'. The context focuses on the capabilities of the agents within the simulation, not on any particular dataset.",
      "processing_time": 60.42355036735535,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276317785
    },
    {
      "context_text": "Spatial-RAG [204], for instance, integrates maps as spatial databases and supports compositional queries like “Find a bar within walking distance from my office—must have live jazz.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (Spatial-RAG) and its capabilities but does not reference any dataset.",
      "processing_time": 59.37791848182678,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276617519
    },
    {
      "context_text": "Spatial-RAG [204] achieves this by unifying sparse spatial retrieval ( i.e. , SQL queries over spatial databases) with dense semantic retrieval based on text embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Spatial-RAG) that combines spatial and semantic retrieval. No verifiable datasets are referenced.",
      "processing_time": 59.22289824485779,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276617519
    },
    {
      "context_text": "The CriX framework [137] integrates retrieval-augmented generation with the Mistral AI LLM. CriX dynamically retrieves socio-economic indicators ( e.g. , literacy rates, income levels) and maps them to crime hotspots, producing human-readable explanations that link criminal patterns to underlying…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'socio-economic indicators' but does not specify a named dataset. The information is too generic and lacks a clear, identifiable dataset name.",
      "processing_time": 59.413013219833374,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276708938
    },
    {
      "context_text": "Although LLMs show strong performance in the semantic analysis of air quality data, studies indicate that their numerical forecasting capabilities remain limited, requiring further research to improve predictive accuracy [41].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to air quality data. No clear, verifiable dataset names are provided.",
      "processing_time": 58.85359811782837,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276742424
    },
    {
      "context_text": "For example, RE’EM [31] proposes using LLMs to analyze large-scale public text data, such as social media, to uncover subtle public opinions and social divisions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large-scale public text data, such as social media' but does not specify a named dataset. The reference is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 59.530505895614624,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276885142
    },
    {
      "context_text": "DiMA [122] presents a ride-hailing assistant trained through continual fine-tuning in a simulated role-playing environment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called DiMA. The context focuses on the training process and environment, which is not a verifiable dataset.",
      "processing_time": 59.43634629249573,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276885258
    },
    {
      "context_text": "By improving the simulation capabilities of agent-based models, LLMs can support the intricate decision-making processes involved in coordinating emergency responses and mitigating the impact of urban disasters [56].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the improvement of simulation capabilities in agent-based models using LLMs.",
      "processing_time": 58.107189893722534,
      "citing_paper_id": "280047877",
      "cited_paper_id": 276993547
    },
    {
      "context_text": "GAIA [18] exemplifies this by supporting human operators in tasks such as operation adjustment, monitoring, and handling complex black start scenarios.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a system (GAIA) and its capabilities. The context is about the application of a large language model in power dispatch, which does not align with the extraction criteria.",
      "processing_time": 59.81826686859131,
      "citing_paper_id": "280047877",
      "cited_paper_id": 277042081
    },
    {
      "context_text": "Finally, a paradigm shift is underway in which urban LLM agents are not only interpreters of data but also capable of actively generating and contributing new data ( e.g. , crowd-sourced citizen feedback) [57, 94], i.e. , LLM agents as sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of LLM agents as sensors. There are no clear, verifiable resources mentioned.",
      "processing_time": 58.661569595336914,
      "citing_paper_id": "280047877",
      "cited_paper_id": 277104104
    },
    {
      "context_text": "Finally, a paradigm shift is underway in which urban LLM agents are not only interpreters of data but also capable of actively generating and contributing new data ( e.g. , crowd-sourced citizen feedback) [57, 94], i.e. , LLM agents as sensors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of LLM agents as sensors. There are no clear, verifiable resources mentioned.",
      "processing_time": 58.661569595336914,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In the urban domain, Zhang et al. [215], Liang et al. [94], and Fang et al. [36] surveyed urban and spatio-temporal foundation models, in which LLMs are regarded as a promising approach for generalizing across heterogeneous spatio-temporal inputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only surveys and reviews of spatio-temporal foundation models. No clear, verifiable datasets are identified.",
      "processing_time": 58.658013105392456,
      "citing_paper_id": "280047877",
      "cited_paper_id": 277104104
    },
    {
      "context_text": "In the urban domain, Zhang et al. [215], Liang et al. [94], and Fang et al. [36] surveyed urban and spatio-temporal foundation models, in which LLMs are regarded as a promising approach for generalizing across heterogeneous spatio-temporal inputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only surveys and reviews of spatio-temporal foundation models. No clear, verifiable datasets are identified.",
      "processing_time": 58.658013105392456,
      "citing_paper_id": "280047877",
      "cited_paper_id": 279074991
    },
    {
      "context_text": "In the urban domain, Zhang et al. [215], Liang et al. [94], and Fang et al. [36] surveyed urban and spatio-temporal foundation models, in which LLMs are regarded as a promising approach for generalizing across heterogeneous spatio-temporal inputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only surveys and reviews of spatio-temporal foundation models. No clear, verifiable datasets are identified.",
      "processing_time": 58.658013105392456,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "TP-RAG [118] proposes a spatio-temporal-aware travel planning method, supporting city-scale travel plan generation services.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for spatio-temporal-aware travel planning.",
      "processing_time": 57.61549782752991,
      "citing_paper_id": "280047877",
      "cited_paper_id": 277740771
    },
    {
      "context_text": "Additionally, LLMs are applied to process complex, unstructured geospatial data for tasks like city-wide delivery demand estimation and forecasting [120].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to the application of LLMs to complex, unstructured geospatial data without naming any particular dataset.",
      "processing_time": 59.20478963851929,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "• Visual input : Urban environments are increasingly equipped with visual sensors, including satellites, smartphones, and vehicle-mounted cameras [63].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to general types of visual sensors in urban environments.",
      "processing_time": 58.001795053482056,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Techniques such as COT and self-refinement can enhance this process by supporting step-by-step reasoning and iterative plan improvement [46, 181].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques for enhancing reasoning and plan improvement.",
      "processing_time": 56.91091012954712,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Iterative self-refinement strategies, exemplified by DeepSeek-R1, allow models to critique and revise their outputs, improving reasoning quality [46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (DeepSeek-R1).",
      "processing_time": 57.4323992729187,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "To address this, UrbanKGent [123] proposes an LLM-powered agent for open-domain KG construction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called UrbanKGent. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.57447862625122,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Ur-banKGent [123] constructs urban knowledge graphs using LLM agents, with human reviewers verifying extracted relationships and facts to safeguard the accuracy of the knowledge base.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for constructing urban knowledge graphs using LLM agents.",
      "processing_time": 57.42870283126831,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "…Vector Database Spatio-Temporal Multi-Agent - D2A [175] Trajectory,Text Vector Database Spatio-Temporal Multi-Agent - Williams et al. [185] Text - Spatio-Temporal Multi-Agent - EconAgent [85] Text Operational State - Multi-Agent - AgentSociety [133] Trajectory,Text Vector Database…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions several multi-agent datasets, which are relevant to the planning capabilities of LLMs. However, the names are not clearly specified as datasets and lack clear identifiers.",
      "processing_time": 58.721604108810425,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, Williams et al. [185] adopt LLM agents for epidemic modeling, showcasing that the generative agents can mimic realistic quarantining and self-isolation behaviors during the COVID-19 pandemic.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLM agents for epidemic modeling. No verifiable resource names are provided.",
      "processing_time": 57.99000382423401,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "…and analyze opinions of the masses, potentially promoting urban policy As an alternative to traditional travel survey methods, Bhandari et al. [11] leverage LLM agents to generate surveyed mobility data, representing the daily movements of people, which avoid privacy concerns, participant…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'surveyed mobility data' generated by LLM agents, but does not specify a named dataset. The reference is too generic and lacks a clear identifier.",
      "processing_time": 58.62793946266174,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "UrbanLLM [65] responds to user queries ( e.g. , finding a parking spot) by producing activity plans based on spatio-temporal contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UrbanLLM) and its application. There are no clear identifiers for datasets.",
      "processing_time": 57.74250793457031,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "UrbanLLM [65] solves complex urban problems by decomposing them into tractable subtasks, selecting tailored spatio-temporal models for handling subtasks, and synthesizing their results into coherent outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UrbanLLM) and its capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.01286697387695,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In contrast, UrbanLLM [65] focuses on autonomous activity planning and orchestration rather than primarily learning policy logic from formal documents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (UrbanLLM) and its focus area. No verifiable resources are identified.",
      "processing_time": 57.6385133266449,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "…urban planning and governance, PlanGPT [232] integrates local databases and tool-calling capabilities to support policy drafting and zoning evaluation, while UrbanLLM [65] decomposes urban service queries into subtasks and coordinates external AI models to enable autonomous urban activity planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to local databases and tool-calling capabilities, which are not specific enough to be considered datasets.",
      "processing_time": 58.18261432647705,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Similarly, UrbanLLM [65] engages domain experts to evaluate AI-generated urban activity plans, identifying omissions and inconsistencies relative to professional standards.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving domain experts to evaluate AI-generated urban activity plans.",
      "processing_time": 57.16649007797241,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, WatchOverGPT [144] processes data from surveillance sensors, citizen reports, and external alerts to monitor and respond to emergency events.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a system (WatchOverGPT) that processes various types of data, but no named datasets are referenced.",
      "processing_time": 57.99937391281128,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In addition, WatchOverGPT [144] demonstrates the potential of LLMs in real-time crime detection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called WatchOverGPT. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.681570529937744,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "We categorize them into five representative types: geovector, time series, trajectory, visual, and textual inputs [215], each contributing to different aspects of the agents’ perceptual capabilities: • Geovector input : Static spatial representations such as points, lines, and polygons serve as the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes types of inputs used in agents' perceptual capabilities.",
      "processing_time": 57.31101083755493,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "One critical application is traffic signal control, a traditionally challenging task often managed by rule-based algorithms or reinforcement learning techniques [77].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application area and techniques.",
      "processing_time": 55.665021896362305,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent pioneering work such as AutoUrbanCI [191] demonstrates this potential by developing LLM-powered agents for urban causal inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called AutoUrbanCI. There are no clear identifiers for datasets in the given context.",
      "processing_time": 57.76179623603821,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing works [14, 95] typically evaluate agent performance from three core dimensions: (1) Utility : Measures the agent’s ability to achieve intended urban goals, such as minimizing traffic congestion or optimizing public service delivery, relative to predefined success criteria [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general evaluation criteria for agent performance.",
      "processing_time": 56.51931548118591,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Malicious actors may embed common urban terms like “green corridor” or “historic preservation” as semantic triggers to sway outcomes toward specific interests [5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to urban terms but does not cite a dataset or a specific resource.",
      "processing_time": 57.227152824401855,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, ensuring data provenance [5], which tracks the origin, transformation, and credibility of inputs, can strengthen the trustworthiness of real-time data pipelines.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the concept of data provenance.",
      "processing_time": 57.03972053527832,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "• Trajectory input : Trajectory data records the movement of individuals and vehicles, often sourced from GPS traces, mobile apps, and transit logs [218].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'trajectory data' but does not specify a named, verifiable dataset. It describes a general type of data rather than a specific dataset.",
      "processing_time": 57.58080554008484,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "At the causal level, recent work in causal auditing [146] offers ways to trace how specific inputs lead to specific outcomes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called 'causal auditing'. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.583982706069946,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "In opposition to these works, Yang et al. [200] investigate the voting behaviors of LLMs in response to various urban projects, which exhibit the limitations of LLM agents in simulating diverse and unbiased viewpoints.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the behavior of LLMs in a specific context. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.62216114997864,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, UrbanPlanBench [222] lays the foundational investigation into the acquisition of planning knowledge among LLMs, spanning various aspects of the urban planning task, including fundamental principles, professional knowledge, and management regulations, which showcases the proficiency of…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'UrbanPlanBench' as a foundational investigation into the acquisition of planning knowledge among LLMs, but it does not specify that it is a dataset. It appears to be a benchmark or challenge, which is excluded.",
      "processing_time": 58.83713245391846,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Second, backdoor attacks [90] introduce hidden triggers into the agent’s decision process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a type of attack. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.91158723831177,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Zhan et al. [210] proposed MORE, an RL-based controller for thermal power generation units that maximizes combustion efficiency while reducing pollutant emissions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MORE) and its application. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.528698205947876,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "These signals are critical for reasoning about urban dynamics but pose significant challenges due to non-stationarity [33], complex periodic structures [34], and spatial correlations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses general challenges related to urban dynamics signals.",
      "processing_time": 56.747113943099976,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "TravelAgent [15] supports personalized trip planning via an interactive loop where users iteratively refine travel preferences and constraints, while TrafficGPT [213] analyzes traffic data and delegates final decision-making to human operators, thereby incorporating expert oversight into the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes systems (TravelAgent and TrafficGPT) and their functionalities, but does not reference any datasets used.",
      "processing_time": 57.45982551574707,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "TravelAgent [15] utilizes LLMs to reason about time, distance, and scheduling constraints, then employs APIs and arithmetic tools to generate feasible travel plans.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs and APIs for generating travel plans.",
      "processing_time": 56.4018657207489,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "To foster city management, City-LEO [66] synergizes LLMs’ logical reasoning abilities to effectively scope down the prior knowledge, which aims to customize the user requirements, and an end-to-end optimizer to derive decisions under uncertain environments.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a system called City-LEO that integrates LLMs for city management. No verifiable datasets are referenced.",
      "processing_time": 57.197396755218506,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Early studies [23, 160] propose diverse strategies for utilizing LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to early studies proposing strategies for utilizing LLMs.",
      "processing_time": 56.60497808456421,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, CityGPT (IoT) [45] splits user queries into separate spatial and temporal components, assigns them to specialized agents, and merges the outputs using a coordination module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CityGPT) and its functionality. No verifiable resources are identified.",
      "processing_time": 56.60185790061951,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "CityGPT [45] also employs explicit coordination, albeit in a modular fashion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called CityGPT. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.68070125579834,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "However, these data sources are typically uneven [219].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets or resources. It only refers to 'data sources' in a generic manner.",
      "processing_time": 56.50719594955444,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Unlike traditional data pipelines [219], these agents operate as adaptive observers, selectively interacting with urban environments based on task requirements.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only contrasts traditional data pipelines with adaptive observer agents.",
      "processing_time": 56.324475049972534,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Data sources include points of interest (POIs), road networks, and land use [219].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions general data sources without specific, identifiable datasets. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 56.727468490600586,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "To solve this issue, Chopra et al. [19] propose a scalable framework, AgentTorch, which creates archetypes representing unique agent characteristics and avoids the redundant simulation for similar agent behaviors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or framework called AgentTorch. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.47038698196411,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For location-based recommendations, LAMP [8] fine-tunes LLMs with city-specific geospatial knowledge, enabling accurate and context-aware conversational recommendations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (LAMP) for fine-tuning LLMs with city-specific geospatial knowledge.",
      "processing_time": 57.07144355773926,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, Chain-of-Symbol (CoS) prompting [59] encodes textually formatted spatial relations as concise, discrete symbols, improving both interpretability and reasoning efficiency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Chain-of-Symbol (CoS) prompting. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.52375388145447,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "Malicious instructions embedded in routine inputs, such as a fake outage notice, may remain dormant until activated by specific conditions, like a weather event or system overload [105].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It describes a scenario involving malicious instructions but does not reference any verifiable resource.",
      "processing_time": 56.273228883743286,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, TransitGPT [25] translates natural language queries into data requests, thus democratizing transit information access.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called TransitGPT. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.268919229507446,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "For instance, LLM4DistReconfig [20] fine-tunes LLMs to optimize network configurations in near real-time, reducing system losses while adhering to operational constraints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LLM4DistReconfig) and its application. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.68827724456787,
      "citing_paper_id": "280047877",
      "cited_paper_id": null
    },
    {
      "context_text": "This model creation is labor-intensive, error-prone, and demands deep semantic modeling expertise [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenges of model creation.",
      "processing_time": 55.007771492004395,
      "citing_paper_id": "279391683",
      "cited_paper_id": 270391544
    },
    {
      "context_text": "Xia et al. present in [6] an agent-based framework for LLM-controlled industrial automation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an agent-based framework for LLM-controlled industrial automation.",
      "processing_time": 55.565967082977295,
      "citing_paper_id": "279391683",
      "cited_paper_id": 272910918
    },
    {
      "context_text": "The use of LLMs in multi-robot systems is reviewed by Li et al. in [9], covering applications in task allocation and coordination.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a review of LLM applications in multi-robot systems.",
      "processing_time": 55.652995109558105,
      "citing_paper_id": "279391683",
      "cited_paper_id": 276161350
    },
    {
      "context_text": "A similar direction is pursued in the LLMAPM framework introduced in [7], which targets manufacturing process planning in Industry 5.0.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework (LLMAPM) for manufacturing process planning using large language models in Industry 5.0.",
      "processing_time": 56.50058889389038,
      "citing_paper_id": "279391683",
      "cited_paper_id": 276625983
    },
    {
      "context_text": "A recent survey in [5] outlines MCP’s core concepts and its potential to expose external tools to LLMs in a standardized way.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a survey outlining core concepts and potential of MCP.",
      "processing_time": 55.210450172424316,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "[5] highlights opportunities for using MCP in planning and control within smart factories, but it does not present concrete implementations or evaluate the feasibility of using LLMs for tool selection or task execution in physical automation systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses the potential of MCP in smart factories without providing concrete implementations or evaluations.",
      "processing_time": 56.2407488822937,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "While the authors of [5] outline the conceptual potential of the new MCP for exposing tools to LLMs, other recent studies focus more directly on how LLMs can be applied to planning and control tasks in industrial automation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a conceptual framework and application areas. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.237175941467285,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "Each Capability is a function declaration in one of the programming frameworks provided by [4].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only programming frameworks. There are no verifiable resources that meet the criteria.",
      "processing_time": 55.64404273033142,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "To integrate external functionalities with LLMs, Model Context Protocol (MCP) was recently introduced [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a protocol (MCP) for integrating external functionalities with LLMs.",
      "processing_time": 55.978697538375854,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "Model Context Protocol (MCP) , introduced in [4], is an open standard for connecting LLMs to external resources, prompt templates and tools, i.e., executable functions, in a structured and unified way.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a protocol for connecting LLMs to external resources. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.47257709503174,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "[4] One of MCP’s key strengths is the decoupling of models and external functions: LLMs do not need to be trained with specific tool knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.785900354385376,
      "citing_paper_id": "279391683",
      "cited_paper_id": null
    },
    {
      "context_text": "However, thorough planning, effective communication, and collaboration can ensure successful outcomes (Lester, 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is a general statement about planning, communication, and collaboration.",
      "processing_time": 56.034836292266846,
      "citing_paper_id": "270063287",
      "cited_paper_id": 56552072
    },
    {
      "context_text": "From the results we demonstrate that even with small open-source LLM as the planner in PMC, its performance still surpasses GPT-4, with ReAct or CoT by a large margin.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between different models and methods.",
      "processing_time": 55.12278747558594,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "It achieved an average success rate of about 42% on TravelPlanner, a significant improvement from the initial 0.6%, and outperformed GPT-4 with ReAct by 14% on API-Bank.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TravelPlanner"
      ],
      "dataset_descriptions": {
        "TravelPlanner": "Used to evaluate the success rate of a model in real-world planning tasks, specifically comparing performance against GPT models with a focus on planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'TravelPlanner' as a benchmark for evaluating success rates. It is used to measure the performance of a model in real-world planning tasks.",
      "processing_time": 58.776572465896606,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "It achieved an average success rate of about 42% on TravelPlanner, a significant improvement from the initial 0.6%, and outperformed GPT-4 with ReAct by 14% on API-Bank.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TravelPlanner"
      ],
      "dataset_descriptions": {
        "TravelPlanner": "Used to evaluate the success rate of a model in real-world planning tasks, specifically comparing performance against GPT models with a focus on planning capabilities."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'TravelPlanner' as a benchmark for evaluating success rates. It is used to measure the performance of a model in real-world planning tasks.",
      "processing_time": 58.776572465896606,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "As our method consists of tool-use and planning (two-stage), we compare our method with the two-stage baseline, ReAct from (Xie et al., 2024) using GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ReAct' from Xie et al., 2024, but it is used as a method or model rather than a dataset. No specific datasets are mentioned.",
      "processing_time": 56.825013160705566,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "As our method consists of tool-use and planning (two-stage), we compare our method with the two-stage baseline, ReAct from (Xie et al., 2024) using GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ReAct' from Xie et al., 2024, but it is used as a method or model rather than a dataset. No specific datasets are mentioned.",
      "processing_time": 56.825013160705566,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "The complexity of each sub-task is significantly reduced as it is now a specific, well-defined task with clear requirements and local constraints, making the off-the-shelf planning method directly applicable, e.g., ReAct (Yao et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ReAct). The context focuses on the application of the method to reduce task complexity.",
      "processing_time": 55.94889307022095,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "…Masterman et al., 2024), demonstrating significant intelligence in reasoning (Kojima et al., 2022; Wei et al., 2023; Wang et al., 2023b), planning (Yao et al., 2023b,a; Besta et al., 2024), instruction-following (Xu et al., 2023; Wang et al., 2023c; Ren et al., 2023), and tool-usage (Schick et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various capabilities of language models. No verifiable resources are identified.",
      "processing_time": 55.70035433769226,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "The executor may utilize off-the-shelf planning techniques, like Re-Act (Yao et al., 2023b), to facilitate sub-task accomplishment.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Re-Act) which is not a dataset.",
      "processing_time": 55.361101388931274,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Table 3 presents the detailed pass rates for individual constraints, indicating that PMC significantly outperforms GPT4+ReAct+CoT in terms of pass rates across all constraints.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance between different models. No verifiable resources are identified.",
      "processing_time": 55.42412734031677,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Then, the lo-cal constraints C l for sub-task T i identified by the manager will be given as the auxiliary information together with the refined T i to the executor agent A i , which will utilize the planning method, e.g., ReAct, to accomplish T i by decoding T i into a sequence of actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ReAct) and a general description of a planning process. No verifiable resources are identified.",
      "processing_time": 56.07363700866699,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "It also surpasses GPT-4 with ReAct on API-Bank (Li et al., 2023) by 13 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'API-Bank' but does not provide enough information to determine if it is a dataset or a method. The cited paper title does not help in disambiguating the type.",
      "processing_time": 56.64073419570923,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "This includes an off-the-shelf planning algorithm such as ReAct (Yao et al., 2023b), which is used to translate the sub-task into a series of executable function calls required to accomplish the assigned sub-task. each edge E ij ∈ E delineates the dependencies be-tween sub-tasks T i and T j , where i, j ∈ S ( K ) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ReAct) and general concepts about task dependencies. No verifiable resources are identified.",
      "processing_time": 55.821439027786255,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Some work (Wei et al., 2023; Yao et al., 2023b; Chen et al., 2023; Wang et al., 2023a) focuses on task decomposition, aiming to solve complex tasks in a divide-and-conquer manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to task decomposition in various works. No verifiable resources are identified.",
      "processing_time": 55.657042026519775,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "Some work (Wei et al., 2023; Yao et al., 2023b; Chen et al., 2023; Wang et al., 2023a) focuses on task decomposition, aiming to solve complex tasks in a divide-and-conquer manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to task decomposition in various works. No verifiable resources are identified.",
      "processing_time": 55.657042026519775,
      "citing_paper_id": "270063287",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "In contrast, the step-by-step methods (Wei et al., 2022; Yao et al., 2023b; Chen et al., 2023; Wu et al., 2023; Gao et al., 2023b) in-terleave planning and execution, where each action is determined based on previous outcomes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.901652574539185,
      "citing_paper_id": "270063287",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "In contrast, the step-by-step methods (Wei et al., 2022; Yao et al., 2023b; Chen et al., 2023; Wu et al., 2023; Gao et al., 2023b) in-terleave planning and execution, where each action is determined based on previous outcomes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.901652574539185,
      "citing_paper_id": "270063287",
      "cited_paper_id": 253801709
    },
    {
      "context_text": "In contrast, the step-by-step methods (Wei et al., 2022; Yao et al., 2023b; Chen et al., 2023; Wu et al., 2023; Gao et al., 2023b) in-terleave planning and execution, where each action is determined based on previous outcomes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.901652574539185,
      "citing_paper_id": "270063287",
      "cited_paper_id": 257404891
    },
    {
      "context_text": "…(Kojima et al., 2022; Wei et al., 2023; Wang et al., 2023b), planning (Yao et al., 2023b,a; Besta et al., 2024), instruction-following (Xu et al., 2023; Wang et al., 2023c; Ren et al., 2023), and tool-usage (Schick et al., 2023; Yang et al., 2023b; Shen et al., 2024) across various domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and capabilities of language models. No verifiable resources are identified.",
      "processing_time": 55.500081062316895,
      "citing_paper_id": "270063287",
      "cited_paper_id": 256697342
    },
    {
      "context_text": "Recent studies (Shinn et al., 2023; Madaan et al., 2023; Huang et al., 2022; Gou et al., 2024) also explore to enhance LLM’s planning ability via reflection and refinement strategies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies exploring LLM's planning ability. No verifiable resources are identified.",
      "processing_time": 55.666001081466675,
      "citing_paper_id": "270063287",
      "cited_paper_id": 258823123
    },
    {
      "context_text": "Recent studies (Shinn et al., 2023; Madaan et al., 2023; Huang et al., 2022; Gou et al., 2024) also explore to enhance LLM’s planning ability via reflection and refinement strategies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies exploring LLM's planning ability. No verifiable resources are identified.",
      "processing_time": 55.666001081466675,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Web-agents (Yao et al., 2022; Deng et al., 2023; Gur et al., 2024; Furuta et al., 2024) explore the interaction between LLM and web-environment by simulating human’s web-browsing behaviors via RL-based planning or trajectory planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing web-agents and their interactions with LLMs. No clear, verifiable datasets are identified.",
      "processing_time": 56.19888496398926,
      "citing_paper_id": "270063287",
      "cited_paper_id": 259129428
    },
    {
      "context_text": "To improve planning stability and performance, recent studies (Dagan et al., 2023; Guan et al., 2023; Yang et al., 2023c) integrate LLMs with external planning tools requiring task descriptions in specific formats, e.g., first-order logic (Barwise, 1977).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.4462194442749,
      "citing_paper_id": "270063287",
      "cited_paper_id": 259859069
    },
    {
      "context_text": "To improve planning stability and performance, recent studies (Dagan et al., 2023; Guan et al., 2023; Yang et al., 2023c) integrate LLMs with external planning tools requiring task descriptions in specific formats, e.g., first-order logic (Barwise, 1977).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to methods and approaches. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.4462194442749,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "…introduces ex-ternal planners to aid the planning procedure of LLMs. Numerous strategies have been developed to harness the potentials of LLMs for specific agent planning (Xi et al., 2023), whose effectiveness and accuracy of planning significantly determine the agent’s robustness and usability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general strategies and methods for enhancing LLM-based agents' planning capabilities.",
      "processing_time": 55.1946759223938,
      "citing_paper_id": "270063287",
      "cited_paper_id": 261817592
    },
    {
      "context_text": "Other task-specific agents focus on designing sophisticated planning strategies, such as tree search (Zhou et al., 2023) and Bayesian adaptive MDPs (Liu et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.02772760391235,
      "citing_paper_id": "270063287",
      "cited_paper_id": 263310943
    },
    {
      "context_text": "TravelPlanner (Xie et al., 2024).",
      "catation_intent": "reusable resource",
      "resource_type": "benchmark",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'TravelPlanner' which is a benchmark, but it does not refer to a specific, downloadable dataset. It is more likely a benchmark or challenge suite used for evaluation.",
      "processing_time": 56.34719705581665,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "In TravelPlanner, the Deliv-erer Agent is responsible for the \"Inference Role\", which its job is to deliver a plan given the execution results following the same format as experiments in the original paper (Xie et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a role within a system. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.590675830841064,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "In this scenario, PMC significantly improves with an average final pass rate of 22.4%, surpassing the best-reported baseline result in (Xie et al., 2024).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance improvement over a baseline. The cited paper title suggests a benchmark, but no specific dataset is named.",
      "processing_time": 55.89340305328369,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "In TravelPlanner , users specify their origin, destination, and individual requirements.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a system called TravelPlanner. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.7425057888031,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "To maintain the integrity of the experiment and stay true to the objectives of the original TravelPlanner paper, we conduct a separate experiment that excludes this external knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to excluding external knowledge in an experiment. No clear, verifiable datasets are identified.",
      "processing_time": 55.19528818130493,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "However, based on our observation and paper (Xie et al., 2024), there are some common problems that the language models may suffer in tool execution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about language models and tool execution.",
      "processing_time": 54.696911573410034,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "68% success rate on TravelPlanner (Xie et al., 2024), a significant increase from GPT-4 ( 2 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TravelPlanner"
      ],
      "dataset_descriptions": {
        "TravelPlanner": "Used to evaluate planning capabilities of language models, focusing on planning capabilities and real-world task execution. The benchmark assesses success rates in complex planning scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "TravelPlanner is mentioned as a benchmark, but it is not clear if it is a specific, downloadable dataset or just a leaderboard. However, given the context, it seems to be used as a testbed for evaluating planning capabilities.",
      "processing_time": 58.53074860572815,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "For example, TravelPlanner only takes one global constraint into account for evaluation despite there are many other global constraints in actual trip planning.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark called TravelPlanner, which is excluded as per instructions.",
      "processing_time": 55.187437295913696,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "The design of prompt for the deliverer agent requires a fair amount of effort as different task, e.g, TravelPlanner and APIBank, requires the Deliverer Agent to perform different primary role.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks or benchmarks which are excluded according to the instructions.",
      "processing_time": 54.55299997329712,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "The results for TravelPlanner and API-Bank are given in the last column in Table 2 and Table 4.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'TravelPlanner' and 'API-Bank', but these are likely benchmarks or tools rather than datasets. No specific datasets are mentioned.",
      "processing_time": 55.49832844734192,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "Specifically, we examine its efficacy in the domains of itinerary planning (Xie et al., 2024) and daily tools using (Li et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only domains of application. The titles of the cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.12210702896118,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "In the absence of hints, the setting replicates that described in (Xie et al., 2024), where the highest final pass rate for baseline models stands at 0.56%, consistent with the original study’s findings.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is excluded as per instructions.",
      "processing_time": 54.30803680419922,
      "citing_paper_id": "270063287",
      "cited_paper_id": 267406800
    },
    {
      "context_text": "LLM-powered * These authors contributed equally to this work. agents, known for their strong logical skills and strategic planning, are considered a promising path toward achieving artificial general intelligence (AGI) (Wang et al., 2024b; You et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the capabilities of LLM-powered agents and their potential towards AGI.",
      "processing_time": 54.67346739768982,
      "citing_paper_id": "270063287",
      "cited_paper_id": 269790943
    },
    {
      "context_text": "Multi-agent systems (Chen et al., 2024; Hong et al., 2024; Gong et al., 2023; Mei et al., 2024) seek to solve more complex real-world tasks by combining multiple powerful LLM-based agents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multi-agent systems and LLM-based agents. There are no clear identifiers for datasets, and the context is focused on describing the research area rather than using specific datasets.",
      "processing_time": 56.11570906639099,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Multi-agent systems (Chen et al., 2024; Hong et al., 2024; Gong et al., 2023; Mei et al., 2024) seek to solve more complex real-world tasks by combining multiple powerful LLM-based agents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to multi-agent systems and LLM-based agents. There are no clear identifiers for datasets, and the context is focused on describing the research area rather than using specific datasets.",
      "processing_time": 56.11570906639099,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing solutions mainly focus on tackling the complexities inherent in integrating heterogeneous agents with different capabilities and specializations (Mei et al., 2024), while the planning strategies among these agents are overlooked.",
      "catation_intent": "gap",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the gap in existing research regarding planning strategies among heterogeneous agents.",
      "processing_time": 54.98088979721069,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "The plan selection methods (Yao et al., 2023a; Besta et al., 2024; Wang et al., 2023b; Xiao and Wang, 2023) elicit LLMs to generate various alternative plans for a task following by a search algorithm for optimal plan selection and execution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for plan selection in LLMs.",
      "processing_time": 54.49955654144287,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "…demonstrating significant intelligence in reasoning (Kojima et al., 2022; Wei et al., 2023; Wang et al., 2023b), planning (Yao et al., 2023b,a; Besta et al., 2024), instruction-following (Xu et al., 2023; Wang et al., 2023c; Ren et al., 2023), and tool-usage (Schick et al., 2023; Yang et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various capabilities of models. No verifiable resources are identified.",
      "processing_time": 54.737614154815674,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Planning acts as an essential capability to interact with external environments, which involves organizing thought trajectories, setting objectives, and determining steps to accomplish the objectives (Mattar and Lengyel, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about planning capabilities. No verifiable resources are identified.",
      "processing_time": 54.463918924331665,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "To assess PMC, we move beyond the existing planning methods that largely focus on simplistic tasks (Singh et al., 2023) or puzzles (Ahn et al., 2024) irrelevant to practical applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing planning methods and tasks. No verifiable resources are identified.",
      "processing_time": 54.53582549095154,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Distinct from the toy tasks (Singh et al., 2023) or puzzles (Ahn et al., 2024) commonly used in existing planning methods, we evaluate PMC on two real-world applications: itinerary planning and daily tool usage.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to 'toy tasks' and 'puzzles' which are not specific datasets but rather types of tasks used in planning methods.",
      "processing_time": 55.48949480056763,
      "citing_paper_id": "270063287",
      "cited_paper_id": null
    },
    {
      "context_text": "Accordingly, a model’s ability to intelligently prioritize goals, a fundamental aspect of both reasonable and necessary planning strategies, has been a critical focus of classical planning research [29, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on classical planning research and goal prioritization.",
      "processing_time": 54.202585220336914,
      "citing_paper_id": "267750873",
      "cited_paper_id": 2014543
    },
    {
      "context_text": "Classical planning domains, known for structuring real-world problems into algorithmically analyzable formats [8, 9, 10], serve as an ideal testbed for testing the planning capabilities of LLMs [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only classical planning domains which are not considered datasets according to the rules.",
      "processing_time": 54.449127197265625,
      "citing_paper_id": "267750873",
      "cited_paper_id": 2014543
    },
    {
      "context_text": "Classical planning domains, known for structuring real-world problems into algorithmically analyzable formats [8, 9, 10], serve as an ideal testbed for testing the planning capabilities of LLMs [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only classical planning domains which are not considered datasets according to the rules.",
      "processing_time": 54.449127197265625,
      "citing_paper_id": "267750873",
      "cited_paper_id": 11518222
    },
    {
      "context_text": "Classical planning domains, known for structuring real-world problems into algorithmically analyzable formats [8, 9, 10], serve as an ideal testbed for testing the planning capabilities of LLMs [11, 12, 13, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only classical planning domains which are not considered datasets according to the rules.",
      "processing_time": 54.449127197265625,
      "citing_paper_id": "267750873",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Firstly, we adopt the in-batch negative sampling technique, as described by [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique. No dataset names are present in the context.",
      "processing_time": 54.28007221221924,
      "citing_paper_id": "267750873",
      "cited_paper_id": 2449317
    },
    {
      "context_text": "As such, they are often used to research large language models reasoning capabilities [11, 12, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research on large language models' reasoning capabilities.",
      "processing_time": 53.98431754112244,
      "citing_paper_id": "267750873",
      "cited_paper_id": 49552345
    },
    {
      "context_text": "As such, they are often used to research large language models reasoning capabilities [11, 12, 28, 14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research on large language models' reasoning capabilities.",
      "processing_time": 53.98431754112244,
      "citing_paper_id": "267750873",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Then, the set of applicable actions is scored based on their similarity with the state and goals representation, using the late-interaction architecture of ColBERT [31]. plans that result in states that have been previously visited are not inserted into the queue.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ColBERT) which is not a dataset. The citation is about using ColBERT for scoring actions based on similarity, not about a dataset.",
      "processing_time": 55.38728737831116,
      "citing_paper_id": "267750873",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "Our ranking architecture incorporates the late interaction schema, as introduced by ColBert [31], which was originally designed to handle search problems efficiently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ColBERT but does not indicate it is used as a dataset. It is referenced as a method or approach.",
      "processing_time": 54.38436269760132,
      "citing_paper_id": "267750873",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "With the success of large language models (LLMs) in various natural language processing tasks, there has been an increasing interest in utilizing them for planning and reasoning applications, including web agents [1, 2, 3], embodied agents [3, 4, 5] or open-world games [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in planning and reasoning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.0765221118927,
      "citing_paper_id": "267750873",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "With the success of large language models (LLMs) in various natural language processing tasks, there has been an increasing interest in utilizing them for planning and reasoning applications, including web agents [1, 2, 3], embodied agents [3, 4, 5] or open-world games [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in planning and reasoning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.0765221118927,
      "citing_paper_id": "267750873",
      "cited_paper_id": 254408960
    },
    {
      "context_text": "With the success of large language models (LLMs) in various natural language processing tasks, there has been an increasing interest in utilizing them for planning and reasoning applications, including web agents [1, 2, 3], embodied agents [3, 4, 5] or open-world games [6, 7].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of LLMs in planning and reasoning. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.0765221118927,
      "citing_paper_id": "267750873",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "To assess the effectiveness of our proposed method, we adopt generalized planning—a subfield of classical planning that provides a more suitable framework for evaluating the capabilities of LLMs, particularly in adapting to varied problem instances [34, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a subfield of classical planning called 'generalized planning'. No verifiable datasets are referenced.",
      "processing_time": 54.563822746276855,
      "citing_paper_id": "267750873",
      "cited_paper_id": 248366629
    },
    {
      "context_text": "Analyzing the Plansformer baseline, we find that fine-tuning on simple configurations is beneficial for managing unseen problems within these settings, in line with the observations of Pallagani et al. [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Plansformer) and a general finding about fine-tuning. No verifiable resources are identified.",
      "processing_time": 54.74920320510864,
      "citing_paper_id": "267750873",
      "cited_paper_id": 254854675
    },
    {
      "context_text": "Pallagani et al. [36] introduced Plansformer, where a classic planner is used to create a large dataset of solved problems, which was then used to fine-tune code models for planning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'large dataset of solved problems' created by a classic planner, but does not provide a specific name for this dataset. The name is too generic and lacks a clear identifier.",
      "processing_time": 55.23541593551636,
      "citing_paper_id": "267750873",
      "cited_paper_id": 254854675
    },
    {
      "context_text": "• Fine-tuning: Code-llama-7b-instruct model, trained for code instruction tasks, was fine-tuned using the approach proposed by Plansformer [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions fine-tuning a model using an approach proposed by Plansformer, but does not mention any specific datasets. The cited paper title 'Plansformer: Generating Symbolic Plans using Transformers' does not provide additional information about datasets.",
      "processing_time": 55.628854274749756,
      "citing_paper_id": "267750873",
      "cited_paper_id": 254854675
    },
    {
      "context_text": "The following fine-tuning settings described is based on [36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only fine-tuning settings. The title 'Plansformer: Generating Symbolic Plans using Transformers' does not provide additional context to identify a dataset.",
      "processing_time": 55.111244201660156,
      "citing_paper_id": "267750873",
      "cited_paper_id": 254854675
    },
    {
      "context_text": "Hao et al. [14] have the LLM describe changes in state after each action, an approach closer to ours.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is too limited to infer any dataset usage.",
      "processing_time": 54.325265884399414,
      "citing_paper_id": "267750873",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "AutoPlanBench [20] prompts LLM to employ a step-by-step reasoning approach.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AutoPlanBench but does not indicate it is a dataset. It appears to be a benchmark or challenge, which is excluded unless it refers to a specific, downloadable dataset.",
      "processing_time": 55.03869676589966,
      "citing_paper_id": "267750873",
      "cited_paper_id": 265221159
    },
    {
      "context_text": "Recent studies [16, 17, 20] have highlighted significant limitations in the planning capabilities of LLMs, indicating a critical area for improvement.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies highlighting limitations in LLM planning capabilities.",
      "processing_time": 54.05429530143738,
      "citing_paper_id": "267750873",
      "cited_paper_id": 265221159
    },
    {
      "context_text": "V 0-2, M IXTRAL -8 X 7B-V 0-1, and GPT-4 T URBO , chosen for their varying capacities and approaches in handling complex tasks [21, 22, 23, 24, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.27928066253662,
      "citing_paper_id": "267750873",
      "cited_paper_id": null
    },
    {
      "context_text": "V 0-2, M IXTRAL -8 X 7B-V 0-1, and GPT-4 T URBO , chosen for their varying capacities and approaches in handling complex tasks [21, 22, 23, 24, 25, 26, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.27928066253662,
      "citing_paper_id": "267750873",
      "cited_paper_id": null
    },
    {
      "context_text": "As described in the footnotes, the SayCan repository provides two files: one providing tasks and environment states, and the other providing tasks and corresponding plans, but they do not completely match.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'SayCan repository' which appears to be a dataset or collection of files used for robotic task planning. However, it does not meet the criteria for a verifiable resource as it lacks a clear, specific identifier.",
      "processing_time": 55.19816470146179,
      "citing_paper_id": "270257748",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "In addressing the issue of grounded planning, various approaches have been explored in fields like robot controlling (Ahn et al., 2022; Wang et al., 2023b) and tool use (Qin et al., 2023; Li et al., 2023; Tang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and fields. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 54.36583352088928,
      "citing_paper_id": "270257748",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "In addressing the issue of grounded planning, various approaches have been explored in fields like robot controlling (Ahn et al., 2022; Wang et al., 2023b) and tool use (Qin et al., 2023; Li et al., 2023; Tang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and fields. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 54.36583352088928,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "In addressing the issue of grounded planning, various approaches have been explored in fields like robot controlling (Ahn et al., 2022; Wang et al., 2023b) and tool use (Qin et al., 2023; Li et al., 2023; Tang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and fields. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 54.36583352088928,
      "citing_paper_id": "270257748",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "In addressing the issue of grounded planning, various approaches have been explored in fields like robot controlling (Ahn et al., 2022; Wang et al., 2023b) and tool use (Qin et al., 2023; Li et al., 2023; Tang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches and fields. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 54.36583352088928,
      "citing_paper_id": "270257748",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "Robot There exists some research related to grounded planning in robotics(Yoshida et al., 2023; Brohan et al., 2023; Ahn et al., 2022), but there is still a lot of room for development.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas. No clear identifiers for datasets are present.",
      "processing_time": 54.02482843399048,
      "citing_paper_id": "270257748",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Conversely, other methodologies employ iterative interactive approaches as the primary means of plan generation to adapt to changes in the environment and conditions (Ahn et al., 2022; Wang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and approaches. No verifiable resources are identified.",
      "processing_time": 54.02132177352905,
      "citing_paper_id": "270257748",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Conversely, other methodologies employ iterative interactive approaches as the primary means of plan generation to adapt to changes in the environment and conditions (Ahn et al., 2022; Wang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and approaches. No verifiable resources are identified.",
      "processing_time": 54.02132177352905,
      "citing_paper_id": "270257748",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "We have converted datasets proposed in VirtualHome (Huang et al., 2022) and SayCan (Ahn et al., 2022) and merged all executable actions as a candidate action set 2 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets proposed in VirtualHome' and 'SayCan', which are specific resources used in the research. However, the names do not follow the required format (e.g., no multi-word proper nouns, acronyms, or hyphenated names).",
      "processing_time": 55.97827076911926,
      "citing_paper_id": "270257748",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Some prior research has delved into the planning ability of Large Language Models (LLMs) and found that LLMs can engage in planning to some extent using their internal knowledge through common-sense reasoning (Zhao et al., 2023; Brown et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general findings about LLMs' planning abilities.",
      "processing_time": 53.786264419555664,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "Large language models are trained on data containing extensive common knowledge and exhibit certain planning and common-sense reasoning abilities (Zhao et al., 2023; Brown et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general training data for large language models. No clear identifiers or specific datasets are provided.",
      "processing_time": 54.347148418426514,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258179056
    },
    {
      "context_text": "However, these methods can only enable models to perform planning on a limited set of actions for specific domain tasks (Lin et al., 2023; Wu et al., 2023; Hao et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and their limitations.",
      "processing_time": 53.47101974487305,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258865812
    },
    {
      "context_text": "Here is an example from GPT4Tools.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper about teaching large language models to use tools.",
      "processing_time": 53.727057456970215,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "We have collected open-source data relevant to tool usage by LLMs, including contributions from ToolAlpaca (Tang et al., 2023), API-Bank (Li et al., 1 https://www.wikihow.com/ 2023), and GPT4Tools (Yang et al., 2023).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'open-source data' but does not specify any named datasets. The sources mentioned (ToolAlpaca, API-Bank, GPT4Tools) are tools or methods, not datasets.",
      "processing_time": 55.08504509925842,
      "citing_paper_id": "270257748",
      "cited_paper_id": 258967184
    },
    {
      "context_text": "5-16k (Zheng et al., 2023b) for experiments.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a range of tokens used for experiments. No clear, verifiable resource is identified.",
      "processing_time": 54.108619689941406,
      "citing_paper_id": "270257748",
      "cited_paper_id": 259129398
    },
    {
      "context_text": "In addition to this, we fine-tuned Llama-2-7B (Touvron et al., 2023) to check the performance of the SFT model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Llama-2-7B) which is not included as per instructions.",
      "processing_time": 54.2591609954834,
      "citing_paper_id": "270257748",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "To maintain consistency with other tasks, we only retain the API name and description without including API parameters, as this would require additional training, and many works have already explored this kind of capability(Qin et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to capabilities explored in other works. No clear, verifiable resource is identified.",
      "processing_time": 54.13449287414551,
      "citing_paper_id": "270257748",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "ToolLLM (Qin et al., 2023) proposed a DFS-based method to improve.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DFS-based) for improving LLM capabilities.",
      "processing_time": 53.69385385513306,
      "citing_paper_id": "270257748",
      "cited_paper_id": 260334759
    },
    {
      "context_text": "As the task domain becomes broad, the action space becomes vast and open, these grounded planning methods appear too restricted to handle the steeply increased complexity (Wang et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the limitations of grounded planning methods in handling complex action spaces.",
      "processing_time": 53.82124876976013,
      "citing_paper_id": "270257748",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "Due to many reported issues with ChatGPT as an evaluator, such as position bias, length preference, and style partiality (Koo et al., 2023; Wu and Aji, 2023; Zheng et al., 2023a), we employ various methods to mitigate those biases.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only issues with ChatGPT as an evaluator. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.27529525756836,
      "citing_paper_id": "270257748",
      "cited_paper_id": null
    },
    {
      "context_text": "Typically, model fine-tuning is applied for performance improvement in certain restricted scenarios (Song et al., 2023; Shen et al., 2023; Yuan et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for model fine-tuning. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 54.15792775154114,
      "citing_paper_id": "270257748",
      "cited_paper_id": null
    },
    {
      "context_text": "Some approaches opt for a global planning strategy based on the task, aiming to directly generate plans that can be grounded to the execution environment in a single step (Song et al., 2023; Shen et al., 2023; Yuan et al., 2023; Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers discussing global planning strategies.",
      "processing_time": 53.540250301361084,
      "citing_paper_id": "270257748",
      "cited_paper_id": null
    },
    {
      "context_text": "We mixed it with the Alpaca dataset (Taori et al., 2023) and fine-tuned the model with 3 epochs to improve the generalization ability.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Alpaca dataset"
      ],
      "dataset_descriptions": {
        "Alpaca dataset": "Used to evaluate the planning capabilities of LLMs by mixing with another dataset and fine-tuning the model for 3 epochs to improve generalization ability."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'Alpaca dataset' which is a specific, named dataset used for fine-tuning a model. The dataset is clearly identified and its usage is described.",
      "processing_time": 58.12598466873169,
      "citing_paper_id": "270257748",
      "cited_paper_id": null
    },
    {
      "context_text": "Embodied AI refers to artificial intelligence systems integrated into physical entities, such as mobile robots, that interact with the environment through sensors and actuators [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of embodied AI. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 54.03013873100281,
      "citing_paper_id": "270878115",
      "cited_paper_id": 7866026
    },
    {
      "context_text": "Traditional path planning methods include algorithms such as A* [10], D* [11], and potential field methods [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions traditional path planning methods but does not refer to any specific datasets. The cited papers are about methods and algorithms, not datasets.",
      "processing_time": 54.14454746246338,
      "citing_paper_id": "270878115",
      "cited_paper_id": 11243437
    },
    {
      "context_text": "Traditional path planning methods include algorithms such as A* [10], D* [11], and potential field methods [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions traditional path planning methods but does not refer to any specific datasets. The cited papers are about methods and algorithms, not datasets.",
      "processing_time": 54.14454746246338,
      "citing_paper_id": "270878115",
      "cited_paper_id": 18116614
    },
    {
      "context_text": "Inspired by Success weighted Path Length (SPL) from [36], we will refer to the following measure as CPL, short for C overage weighted by (normalized inverse) P ath L ength: where N means the number of test episodes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (SPL) which is not a dataset. The context is about evaluating navigation agents, but no datasets are explicitly named.",
      "processing_time": 54.37049746513367,
      "citing_paper_id": "270878115",
      "cited_paper_id": 49865049
    },
    {
      "context_text": "We referenced the metrics from [36] and [37], including success rate, average distance, and coverage rate.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics but does not refer to any specific datasets. The context is about evaluating embodied navigation agents, which is relevant to planning capabilities of LLMs, but no datasets are explicitly named.",
      "processing_time": 54.5417857170105,
      "citing_paper_id": "270878115",
      "cited_paper_id": 49865049
    },
    {
      "context_text": "In addition to the A* and D* algorithms mentioned in the previous chapter, path planning algorithms include heuristic optimization methods based on pre-trained weights, such as genetic algorithms [26], particle swarm optimization [27], and deep reinforcement learning [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to algorithms and methods.",
      "processing_time": 53.51048684120178,
      "citing_paper_id": "270878115",
      "cited_paper_id": 70219313
    },
    {
      "context_text": "Classically, decomposing a given map based on topological rules and then applying a repeatable coverage pattern is a common way to solve this issue following the divide-and-conquer algorithm [30]–[32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or approaches for solving coverage issues in robotics.",
      "processing_time": 53.62556552886963,
      "citing_paper_id": "270878115",
      "cited_paper_id": 215998752
    },
    {
      "context_text": "Coverage path planning is a typical method employed in various research areas, such as ocean seabed mapping [7], terrain reconstruction [8], and lawn mowing [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only references methods and applications in different research areas.",
      "processing_time": 53.687281370162964,
      "citing_paper_id": "270878115",
      "cited_paper_id": 244352923
    },
    {
      "context_text": "Singh et al. describe a programmatic LLM prompt structure that enables the generation of plans functional across different situated environments, robot capabilities, and tasks [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating plans using LLMs.",
      "processing_time": 53.3052077293396,
      "citing_paper_id": "270878115",
      "cited_paper_id": 252519594
    },
    {
      "context_text": "ReAct utilizes LLMs to generate interleaved reasoning traces and task-specific actions [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs in a method called ReAct.",
      "processing_time": 53.54885816574097,
      "citing_paper_id": "270878115",
      "cited_paper_id": 252762395
    },
    {
      "context_text": "We use OpenAI GPT-4o services [34], a multimodal efficient model for inference and reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a service (GPT-4) rather than a dataset. No specific dataset is referenced.",
      "processing_time": 53.482133865356445,
      "citing_paper_id": "270878115",
      "cited_paper_id": 257532815
    },
    {
      "context_text": "LLMs have shown promise in processing and analyzing massive datasets, enabling them to uncover patterns, forecast future occurrences, and identify abnormal behaviour in a wide range of fields [17].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of LLMs. No clear, verifiable resource names are provided.",
      "processing_time": 53.8888578414917,
      "citing_paper_id": "270878115",
      "cited_paper_id": 267740683
    },
    {
      "context_text": "LLMs demonstrate their ability to solve mathematical problems collaboratively [13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the ability of LLMs to solve mathematical problems collaboratively. No verifiable resources are identified.",
      "processing_time": 53.999136209487915,
      "citing_paper_id": "270878115",
      "cited_paper_id": 268554279
    },
    {
      "context_text": "5-turbo chatbot in providing real-time, adaptive, and accurate path-planning algorithms compared to state-of-the-art methods like Rapidly Exploring Random Tree (RRT) and A* in various simulated scenarios [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on comparing path-planning algorithms in simulated scenarios.",
      "processing_time": 53.65455150604248,
      "citing_paper_id": "270878115",
      "cited_paper_id": 268723699
    },
    {
      "context_text": "Low-level control connects algorithms to different types of system agents, such as UAVs, UGVs, or UUVs [25].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of system agents. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.48187232017517,
      "citing_paper_id": "270878115",
      "cited_paper_id": null
    },
    {
      "context_text": "There is another class of planning algorithms for large POMDPs, called as direct search algorithms [11,12,13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to planning algorithms for POMDPs.",
      "processing_time": 53.09705090522766,
      "citing_paper_id": "271040200",
      "cited_paper_id": 791679
    },
    {
      "context_text": "The problem environment – ξ, is modelled as deterministic Partially Observable Markov Decision Process (POMDP) [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model or method (POMDP).",
      "processing_time": 53.09323978424072,
      "citing_paper_id": "271040200",
      "cited_paper_id": 1934251
    },
    {
      "context_text": "Monte-carlo tree search [1] is such an effective method that can tackle very large state spaces.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Monte-Carlo tree search).",
      "processing_time": 53.08968114852905,
      "citing_paper_id": "271040200",
      "cited_paper_id": 18941952
    },
    {
      "context_text": "Each state is assigned a value augmented with an exploration term, similar to one used in UCB1 [14].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UCB1) which is not a dataset.",
      "processing_time": 53.035871267318726,
      "citing_paper_id": "271040200",
      "cited_paper_id": 207609497
    },
    {
      "context_text": "We tested our method in Scienceworld [2] environment.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Scienceworld' as an environment where the method was tested. However, it does not specify that it is a dataset, and the title suggests it is an environment or platform rather than a dataset.",
      "processing_time": 54.2105598449707,
      "citing_paper_id": "271040200",
      "cited_paper_id": 247451124
    },
    {
      "context_text": "We opted for ScienceWorld [2], an interactive text-based environment that demands intricate interactive reasoning processes for resolving a multitude of science-theory-based tasks across various classes, such as thermodynamics, genetics, friction, and more.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "ScienceWorld is mentioned as an interactive text-based environment used for complex reasoning tasks, but it does not fit the criteria for a dataset as it is more of an environment or platform.",
      "processing_time": 53.52898907661438,
      "citing_paper_id": "271040200",
      "cited_paper_id": 247451124
    },
    {
      "context_text": "LLM based agents like SayCan [6], ReAct [7], Reflexion [8], CLIN [9] exploits the large pretraining of an LLM with real-world semantic knowledge and uses it to suggest next best actions without further significant training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the capabilities of LLM-based agents in suggesting actions.",
      "processing_time": 53.3139853477478,
      "citing_paper_id": "271040200",
      "cited_paper_id": 247939706
    },
    {
      "context_text": "Heuristics play a critical role in accelerating both search-based [40], [41] and sampling-based [42], [43] planners.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to heuristics in planning algorithms. No verifiable resources are identified.",
      "processing_time": 53.251466512680054,
      "citing_paper_id": "262045061",
      "cited_paper_id": 700325
    },
    {
      "context_text": "Heuristics play a critical role in accelerating both search-based [40], [41] and sampling-based [42], [43] planners.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to heuristics in planning algorithms. No verifiable resources are identified.",
      "processing_time": 53.251466512680054,
      "citing_paper_id": "262045061",
      "cited_paper_id": 1842081
    },
    {
      "context_text": "A word ℓ 1: T is accepted by M ϕ µ , i.e., ℓ 1: T | = ϕ µ , if and only if the state q T +1 obtained after transitions q t +1 = T ( q t , ℓ t ) for t = 1 , . . . , T is one of the ﬁnal states F .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a formal definition of a model checking process. No verifiable resources are identified.",
      "processing_time": 53.25283670425415,
      "citing_paper_id": "262045061",
      "cited_paper_id": 4300572
    },
    {
      "context_text": "A word ℓ 1: T is accepted by M ϕ µ , i.e., ℓ 1: T | = ϕ µ , if and only if the state q T +1 obtained after transitions q t +1 = T ( q t , ℓ t ) for t = 1 , . . . , T is one of the ﬁnal states F .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a formal definition of a model checking process. No verifiable resources are identified.",
      "processing_time": 53.25283670425415,
      "citing_paper_id": "262045061",
      "cited_paper_id": 13202755
    },
    {
      "context_text": "Beyond temporal logics, other expressive robot task representations include the planning domain definition language [32], Petri nets [33], [34], and process algebra [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools for robot task representation.",
      "processing_time": 52.794676065444946,
      "citing_paper_id": "262045061",
      "cited_paper_id": 7188883
    },
    {
      "context_text": "In works closely related to ours, Chen et al. [25] show that natural language tasks in 2D maps encoded as sets of landmarks can be converted to signal temporal logic [26] via LLM re-prompting and automatic syntax correction, enabling the use of existing temporal logic planners [27]–[31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and tools. The focus is on converting natural language tasks to temporal logic using LLMs.",
      "processing_time": 53.40476584434509,
      "citing_paper_id": "262045061",
      "cited_paper_id": 7846914
    },
    {
      "context_text": "Seminal papers [23], [24] in the early 2000s established formal logics and automata as powerful representations of robot tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to seminal papers establishing formal logics and automata for robot tasks.",
      "processing_time": 53.13101077079773,
      "citing_paper_id": "262045061",
      "cited_paper_id": 16900887
    },
    {
      "context_text": "We focus on LTL [50] with syntax in Table.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to LTL syntax. No verifiable resources are identified.",
      "processing_time": 53.073434829711914,
      "citing_paper_id": "262045061",
      "cited_paper_id": 27294004
    },
    {
      "context_text": "Beyond temporal logics, other expressive robot task representations include the planning domain deﬁnition language [32], Petri nets [33], [34], and process algebra [35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various methods and languages for robot task representation.",
      "processing_time": 52.78168749809265,
      "citing_paper_id": "262045061",
      "cited_paper_id": 61897015
    },
    {
      "context_text": "A scene graph [11] is a prominent example that models buildings, ﬂoors, rooms, objects, and occupancy in a uniﬁed hierarchical representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or structure (scene graph) rather than a dataset.",
      "processing_time": 53.340285539627075,
      "citing_paper_id": "262045061",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "Ding et al. [49] use an LLM to create both symbolic and geometric spatial relationships among tableware objects to assist task and motion planning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It focuses on the use of an LLM for creating spatial relationships among objects for planning.",
      "processing_time": 53.21251606941223,
      "citing_paper_id": "262045061",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "To evaluate our method, we use Allensville (1-ﬂoor), Benevolence (3-ﬂoor) and Collierville (3-ﬂoor) from the 3D Scene Graph dataset [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Allensville (1-ﬂoor)",
        "BitterDB (not used for the target topic)"
      ],
      "dataset_descriptions": {
        "Allensville (1-ﬂoor)": "Used to evaluate the method, focusing on a single-floor environment to test planning capabilities in a simplified setting.",
        "Benevolence (3-ﬂoor)": "Used to evaluate the method, focusing on a multi-floor environment to test planning capabilities across different levels.",
        "Collierville (3-ﬂoor)": "Used to evaluate the method, focusing on a multi-floor environment to test planning capabilities across different levels."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific locations from the 3D Scene Graph dataset, which is used to evaluate the method. The dataset is clearly identified and used for evaluation purposes.",
      "processing_time": 59.516133069992065,
      "citing_paper_id": "262045061",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "Beyond single-level maps, hierarchical models encode topological relations among local maps and semantic elements [10], [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to hierarchical models and their encoding capabilities. No verifiable resources are identified.",
      "processing_time": 53.047078132629395,
      "citing_paper_id": "262045061",
      "cited_paper_id": 203837042
    },
    {
      "context_text": "enabled metric-semantic mapping [1]–[9], offering rich information in support of robot autonomy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for semantic mapping.",
      "processing_time": 52.5921094417572,
      "citing_paper_id": "262045061",
      "cited_paper_id": 206852649
    },
    {
      "context_text": "The metric, semantic, and topological elements of such models offer the building blocks for robots to execute semantic tasks [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general elements of models. No verifiable resources are identified.",
      "processing_time": 52.95159316062927,
      "citing_paper_id": "262045061",
      "cited_paper_id": 214429751
    },
    {
      "context_text": "Large language models (LLMs), such as GPT-3 [16], BERT [17], and LLaMA [18], offer a possible resolution with their ability to relate environment entities to concepts in natural language.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 303), response: ```json\n{\n    \"reasoning\": \"The context does not mention any specific datasets, only models and thei",
      "processing_time": 55.566932916641235,
      "citing_paper_id": "262045061",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Large language models (LLMs), such as GPT-3 [16], BERT [17], and LLaMA [18], offer a possible resolution with their ability to relate environment entities to concepts in natural language.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 303), response: ```json\n{\n    \"reasoning\": \"The context does not mention any specific datasets, only models and thei",
      "processing_time": 55.566932916641235,
      "citing_paper_id": "262045061",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "[21] use GPT-3 to parse text instructions to landmarks and the contrastive language image pre-training (CLIP) model [22] to infer a joint landmark-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT-3 and CLIP). The context focuses on the use of these models for parsing text instructions and inferring landmarks.",
      "processing_time": 53.473816871643066,
      "citing_paper_id": "262045061",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Scene graph construction can be done from streaming sensor data [5], [12], [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to scene graph construction from streaming sensor data, which is a method or process, not a dataset.",
      "processing_time": 53.33233451843262,
      "citing_paper_id": "262045061",
      "cited_paper_id": 248913107
    },
    {
      "context_text": "Chen et al. [19], [20] use LLMs for scene graph labeling, showing their capability of high-level understanding of indoor scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LLMs for scene graph labeling. No verifiable resources are identified.",
      "processing_time": 53.04408288002014,
      "citing_paper_id": "262045061",
      "cited_paper_id": 252212141
    },
    {
      "context_text": "The LLM model returns a sequence of function calls [53].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sequence of function calls. There is no indication of a reusable resource or dataset being used.",
      "processing_time": 52.89951539039612,
      "citing_paper_id": "262045061",
      "cited_paper_id": 259141622
    },
    {
      "context_text": "Expanding the memory pool with a broader and more diverse set of building experiences could improve retrieval efficiency and boost the agent’s performance and success rates in constructing complex structures(Wang et al. 2023b).",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 313), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only a general conc",
      "processing_time": 55.87446737289429,
      "citing_paper_id": "274281617",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "Current research predominantly concentrates on single agents performing straightforward tasks aligned with Minecraft’s technology tree, such as tool crafting, mining, and item productions(Wang et al. 2023b; Yu et al. 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks and research focus areas.",
      "processing_time": 52.35064244270325,
      "citing_paper_id": "274281617",
      "cited_paper_id": 265129059
    },
    {
      "context_text": "In recent years, autonomous agents within the Minecraft environment have become a focal point of research, with meth-ods like reinforcement learning(Baker et al. 2022) and large language models (LLMs) playing a central role(Wang et al. 2024; Fan et al. 2022; Yuan et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No verifiable resources are identified.",
      "processing_time": 52.5711464881897,
      "citing_paper_id": "274281617",
      "cited_paper_id": 265609805
    },
    {
      "context_text": "To address this, we leverage advanced Vision-Language Models (VLMs) such as GPT-4o, renowned for their capabilities in vision-language reasoning and interpretation, to automate the evaluation of our agent’s constructed architectures(Zhang et al. 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on using VLMs for evaluating agent performance.",
      "processing_time": 52.71134877204895,
      "citing_paper_id": "274281617",
      "cited_paper_id": 265609805
    },
    {
      "context_text": "In cases of unsuccessful executions, the agent employs visual inspection and self-reflection to identify and correct errors, iteratively refining its construction plans in a closed-loop process. tures(Zhang et al. 2023; Xie et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or processes. The context focuses on the agent's error correction and iterative refinement, which does not indicate the use of a specific dataset.",
      "processing_time": 53.26159977912903,
      "citing_paper_id": "274281617",
      "cited_paper_id": 265609805
    },
    {
      "context_text": "In contrast, without memory, the implementation of reflection alone improved performance by 12.8%, suggesting that while reflection yields moderate gains(Huang et al. 2024), memory remains the more influential factor in enhancing agent capabilities.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the impact of reflection and memory on agent performance.",
      "processing_time": 52.2958550453186,
      "citing_paper_id": "274281617",
      "cited_paper_id": null
    },
    {
      "context_text": "We collect publicly available protocols from Pro-tocols.io (Teytelman et al., 2016), a platform for developing and sharing reproducible methods.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Protocols.io' as a platform for collecting publicly available protocols, which is a reusable resource but does not qualify as a dataset according to the criteria.",
      "processing_time": 53.02236986160278,
      "citing_paper_id": "264172681",
      "cited_paper_id": 4959317
    },
    {
      "context_text": "We collect publicly available protocols from Pro-tocols.io (Teytelman et al., 2016), a platform for developing and sharing reproducible methods.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Protocols.io' as a platform for collecting publicly available protocols, which is a reusable resource but does not qualify as a dataset according to the criteria.",
      "processing_time": 53.02236986160278,
      "citing_paper_id": "264172681",
      "cited_paper_id": 5803269
    },
    {
      "context_text": "io (Teytelman et al., 2016), a platform for developing and sharing reproducible methods.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation mentions 'io', which is a platform for developing and sharing reproducible methods, but does not refer to a specific dataset. The context does not provide enough information to identify a verifiable dataset.",
      "processing_time": 53.27151107788086,
      "citing_paper_id": "264172681",
      "cited_paper_id": 4959317
    },
    {
      "context_text": "Our approach is inspired by robotic planning, in which a closed set of admissible actions is provided to a controller agent (Jiménez et al., 2019; Ahn et al., 2022; Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to robotic planning and controller agents. No verifiable resources are identified.",
      "processing_time": 52.42459440231323,
      "citing_paper_id": "264172681",
      "cited_paper_id": 53691145
    },
    {
      "context_text": "Our approach is inspired by robotic planning, in which a closed set of admissible actions is provided to a controller agent (Jiménez et al., 2019; Ahn et al., 2022; Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to robotic planning and controller agents. No verifiable resources are identified.",
      "processing_time": 52.42459440231323,
      "citing_paper_id": "264172681",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "Work such as SciBERT, BioGPT, Galactica and others have also shown the utility of pretraining an LLM on a corpus of biomedical (Gu et al., 2021; Lewis et al., 2020; Luo et al., 2022; Lee et al., 2020; Shin et al., 2020) or general scientific text (Beltagy et al., 2019; Taylor et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pretraining on a corpus of biomedical or general scientific text, but does not specify a named dataset. The names mentioned (SciBERT, BioGPT, Galactica) are models, not datasets.",
      "processing_time": 53.2639102935791,
      "citing_paper_id": "264172681",
      "cited_paper_id": 202558505
    },
    {
      "context_text": "Work such as SciBERT, BioGPT, Galactica and others have also shown the utility of pretraining an LLM on a corpus of biomedical (Gu et al., 2021; Lewis et al., 2020; Luo et al., 2022; Lee et al., 2020; Shin et al., 2020) or general scientific text (Beltagy et al., 2019; Taylor et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pretraining on a corpus of biomedical or general scientific text, but does not specify a named dataset. The names mentioned (SciBERT, BioGPT, Galactica) are models, not datasets.",
      "processing_time": 53.2639102935791,
      "citing_paper_id": "264172681",
      "cited_paper_id": 252542956
    },
    {
      "context_text": "We observe the biggest gap in the Levenshtein distance score metric, where GPT-4 significantly outperforms GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance comparison between GPT-4 and GPT-3 using the Levenshtein distance score metric.",
      "processing_time": 52.836875438690186,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Use of paid API The GPT-4 and GPT-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT-4 and GPT-3). These are excluded as per the instructions.",
      "processing_time": 52.74093198776245,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We found that Llama-2 significantly underperforms GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models, Llama-2 and GPT-3.",
      "processing_time": 52.36715793609619,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We found that Llama-2 significantly underperforms GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models, Llama-2 and GPT-3.",
      "processing_time": 52.36715793609619,
      "citing_paper_id": "264172681",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "More recently, pre-trained gener-alist LLMs such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) have shown to be capable of tasks such as searching for chemical compounds similar to a given one (OpenAI, 2023) or drug editing (Liu et al., 2023c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 52.17193841934204,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We evaluate GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evaluation of GPT-3.",
      "processing_time": 51.862799882888794,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "5 in both the prediction of the correct next step, whereas GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models. No verifiable resources are identified.",
      "processing_time": 52.061283588409424,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Mean-while, GPT-4 and GPT-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models (GPT-4 and GPT-3). No verifiable resources are identified.",
      "processing_time": 52.435418367385864,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "This paradigm allows us to rapidly measure the protocol knowledge of GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to measuring protocol knowledge of GPT-3.",
      "processing_time": 52.20290732383728,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We explore the performance of GPT-3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the performance of GPT-3. There are no verifiable resources or datasets mentioned.",
      "processing_time": 52.26003384590149,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We see that GPT-4 consistently outperforms GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between GPT-4 and GPT-3.",
      "processing_time": 51.89917612075806,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "We see that GPT-4 outperforms GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between GPT-4 and GPT-3.",
      "processing_time": 51.987464904785156,
      "citing_paper_id": "264172681",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "…protocols against ground truths, metrics that rely on n-gram overlaps such as BLEU (Papineni et al., 2002) or contextual embeddings such as BERTScore (Zhang et al., 2019) might not capture small differences, such as the order of actions, or relation between substances (Bhandari et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and methods. The context focuses on evaluation metrics in text summarization, which are excluded according to the instructions.",
      "processing_time": 52.67026877403259,
      "citing_paper_id": "264172681",
      "cited_paper_id": 222341867
    },
    {
      "context_text": "…space, both through the use of restricted action space (Ahn et al., 2022; Driess et al., 2023), function/tool search (Wang et al., 2023a; Schick et al., 2023; Shen et al., 2023; Bran et al., 2023; Boiko et al., 2023) and translation of plans into admissible action space (Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers are referenced for their methodologies, not for providing datasets.",
      "processing_time": 52.2564160823822,
      "citing_paper_id": "264172681",
      "cited_paper_id": 246035276
    },
    {
      "context_text": "However, decomposing complex tasks into subtasks in frameworks such as Chain-of-Thought reasoning (Wei et al., 2022; Zhang et al., 2023), and its variants such as Least-to-Most (Zhou et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and frameworks. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 51.98415780067444,
      "citing_paper_id": "264172681",
      "cited_paper_id": 256504063
    },
    {
      "context_text": "However, decomposing complex tasks into sub-tasks in frameworks such as Chain-of-Thought reasoning (Wei et al., 2022; Zhang et al., 2023), and its variants such as Least-to-Most (Zhou et al., 2022) and Tree of Thought reasoning (Yao et al., 2023) improves performance in multi-step reasoning…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the improvement of multi-step reasoning through various reasoning frameworks.",
      "processing_time": 52.10668587684631,
      "citing_paper_id": "264172681",
      "cited_paper_id": 256504063
    },
    {
      "context_text": "…of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al., 2023) can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (Bubeck et al., 2023; Bran et al., 2023; Chiang et al., 2023; Peng et al., 2023; Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models and methods. The context focuses on the use of GPT-4 for self-evaluation in various studies.",
      "processing_time": 52.40906810760498,
      "citing_paper_id": "264172681",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "…of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al., 2023) can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (Bubeck et al., 2023; Bran et al., 2023; Chiang et al., 2023; Peng et al., 2023; Zhou et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to models and methods. The context focuses on the use of GPT-4 for self-evaluation in various studies.",
      "processing_time": 52.40906810760498,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258822910
    },
    {
      "context_text": "We make use of a one-shot example prompt, and an automatic feedback loop (Liu et al., 2023a) that provides error signals if: the generated code is not valid Python pseudocode; no pseudofunctions are defined; the pseudocode or pseudofunctions do not have arguments; any numerical parameters in the…",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of a one-shot example prompt and an automatic feedback loop, which are not datasets.",
      "processing_time": 52.396292209625244,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "Furthermore, interactions with simulators and debuggers can be used to improve both plans (Liu et al., 2023a) and value functions that determine the appropriateness of action calls (Ahn et al., 2022; Driess et al., 2023; Mu et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods and findings. No verifiable resources are identified.",
      "processing_time": 52.01701855659485,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258298051
    },
    {
      "context_text": "However, evaluating an LLM’s performance on more open-ended tasks, such as healthcare support (Dash et al., 2023) or chemical synthesis planning (Bran et al., 2023) is done manually.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only open-ended tasks in healthcare and chemical synthesis planning. No verifiable resources are named.",
      "processing_time": 51.81210112571716,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258331653
    },
    {
      "context_text": "However, these have been found to contradict human evaluation (Bran et al., 2023) or be systematically biased (Wang et al., 2023b), where the order of the provided responses affects the predicted ranking.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues with evaluation methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 51.894436836242676,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "However, these have been found to contradict human evaluation (Bran et al., 2023) or be systematically biased (Wang et al., 2023b), where the order of the provided responses affects the predicted ranking.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only issues with evaluation methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 51.894436836242676,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258960339
    },
    {
      "context_text": "Automatic Evaluation of LLMs While evaluation of the performance of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to evaluating LLMs in games and planning in PDDL domains. No clear, verifiable datasets are identified.",
      "processing_time": 52.51567983627319,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Automatic Evaluation of LLMs While evaluation of the performance of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al., 2023) can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (Bubeck et al., 2023; Bran et al., 2023;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers evaluating LLMs in various contexts. No clear, verifiable datasets are identified.",
      "processing_time": 51.974833726882935,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "LLM planners can also learn to create their own training curriculum and refine their function use (Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability of LLM planners. No verifiable resources are identified.",
      "processing_time": 51.368276596069336,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": ", 2023), function/tool search (Wang et al., 2023a; Schick et al., 2023; Shen et al., 2023; Bran et al., 2023; Boiko et al., 2023) and translation of plans into admissible action space (Huang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets, and the context is focused on methods and tools rather than data.",
      "processing_time": 51.93797159194946,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "…in simulated and real embodied space, both through the use of restricted action space (Ahn et al., 2022; Driess et al., 2023), function/tool search (Wang et al., 2023a; Schick et al., 2023; Shen et al., 2023; Bran et al., 2023; Boiko et al., 2023) and translation of plans into admissible action…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of restricted action space and function/tool search in embodied agents.",
      "processing_time": 51.533018350601196,
      "citing_paper_id": "264172681",
      "cited_paper_id": 258887849
    },
    {
      "context_text": "Task decomposition has also been combined with self-verification through deductive reasoning to improve step-by-step reasoning accuracy (Ling et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on task decomposition and self-verification through deductive reasoning.",
      "processing_time": 51.49220418930054,
      "citing_paper_id": "264172681",
      "cited_paper_id": 259089245
    },
    {
      "context_text": "We also ran Llama-2 on the next step prediction task, but we found that the model was unable to complete this task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the performance of Llama-2 on a next step prediction task.",
      "processing_time": 51.44528245925903,
      "citing_paper_id": "264172681",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "To benchmark performance on open-source models, we also conducted a run of our experimental evaluation tasks on Llama-2 (Touvron et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of Llama-2, which is a model, not a dataset.",
      "processing_time": 51.14921522140503,
      "citing_paper_id": "264172681",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "As part of our evaluation on Llama-2 we observe that, when using feedback, the model is distracted and does not attempt to re-write code.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Llama-2) and its behavior during evaluation. No verifiable resources are identified.",
      "processing_time": 51.36249852180481,
      "citing_paper_id": "264172681",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "• HotpotQA [Yang et al., 2018].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HotpotQA",
        "ARC"
      ],
      "dataset_descriptions": {
        "HotpotQA": "Used to evaluate multi-hop reasoning capabilities in LLMs, focusing on diverse and explainable question answering tasks."
      },
      "confidence_score": 0.9,
      "reasoning": "HotpotQA is a specific dataset designed for multi-hop question answering, which is relevant to the planning capabilities of LLMs.",
      "processing_time": 56.237507343292236,
      "citing_paper_id": "274822434",
      "cited_paper_id": 52822214
    },
    {
      "context_text": "We also implemente the ROMAS system using two leading application frameworks, LangChain and AgentScope, and select code volume, average QA time, and task success rate as the evaluation metrics [Feng et al., 2020].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and application frameworks. The context is about implementing a system and evaluating it, not using a dataset.",
      "processing_time": 51.39770221710205,
      "citing_paper_id": "274822434",
      "cited_paper_id": 211171605
    },
    {
      "context_text": "This process not only ensures that each difference point effectively corrects the errors from the old strategy’s execution but also aims to achieve the best possible solution with the minimum modification cost [Zhang et al., 2020].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on a research concept rather than a reusable resource.",
      "processing_time": 50.95939612388611,
      "citing_paper_id": "274822434",
      "cited_paper_id": 216284142
    },
    {
      "context_text": "Beyond pre-training, the integration of advanced techniques such as continual learning, including continual pre-training [Jiang et al., 2023, 2024], and prompt learning [Wang et al., 2022, Xue et al., 2023b], offers opportunities to improve the system’s adaptability and performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 51.08126211166382,
      "citing_paper_id": "274822434",
      "cited_paper_id": 245218925
    },
    {
      "context_text": "Beyond pre-training, the integration of advanced techniques such as continual learning, including continual pre-training [Jiang et al., 2023, 2024], and prompt learning [Wang et al., 2022, Xue et al., 2023b], offers opportunities to improve the system’s adaptability and performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 51.08126211166382,
      "citing_paper_id": "274822434",
      "cited_paper_id": 263829588
    },
    {
      "context_text": "In terms of structural design, traditional MAS often rely on static task allocation and predefined processes, such as Chain of Thought (CoT) [Wei et al., 2022], Self-consistent CoT (CoT-SC) [Wang et al., 2023c] and Tree of Thought (ToT) [Yao et al., 2023a].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods/models (Chain of Thought, Self-consistent CoT, Tree of Thought) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 51.79120445251465,
      "citing_paper_id": "274822434",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "In terms of structural design, traditional MAS often rely on static task allocation and predefined processes, such as Chain of Thought (CoT) [Wei et al., 2022], Self-consistent CoT (CoT-SC) [Wang et al., 2023c] and Tree of Thought (ToT) [Yao et al., 2023a].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods/models (Chain of Thought, Self-consistent CoT, Tree of Thought) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 51.79120445251465,
      "citing_paper_id": "274822434",
      "cited_paper_id": 247595263
    },
    {
      "context_text": "In terms of structural design, traditional MAS often rely on static task allocation and predefined processes, such as Chain of Thought (CoT) [Wei et al., 2022], Self-consistent CoT (CoT-SC) [Wang et al., 2023c] and Tree of Thought (ToT) [Yao et al., 2023a].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods/models (Chain of Thought, Self-consistent CoT, Tree of Thought) but does not reference any specific datasets. The cited papers' titles confirm these are methods, not datasets.",
      "processing_time": 51.79120445251465,
      "citing_paper_id": "274822434",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Agent list AL is a list of agents with a predefined call order, structured like a tree [Yao et al., 2023a]. tasker serves as the control center for each entire process including main process, planning and managing the workflow of each agent.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or structure for organizing agents and managing workflows.",
      "processing_time": 50.57041931152344,
      "citing_paper_id": "274822434",
      "cited_paper_id": 258762525
    },
    {
      "context_text": "Interactive MAS, such as ReAct [Yao et al., 2023b] , ChatCoT [Chen et al., 2023] and Voyager [Wang et al., 2023a], facilitate dynamic feedback and self-correction but struggle with high correction costs and the inability to rectify previously executed subtasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the capabilities and limitations of interactive MAS systems.",
      "processing_time": 50.66444230079651,
      "citing_paper_id": "274822434",
      "cited_paper_id": 258841374
    },
    {
      "context_text": "LLM evaluation and Human evaluation are used to assess the accuracy, coherence, completeness, and logic of the descriptions [Wang et al., 2023b].",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (LLM evaluation and Human evaluation) used to assess certain qualities of descriptions.",
      "processing_time": 50.548351764678955,
      "citing_paper_id": "274822434",
      "cited_paper_id": 258960339
    },
    {
      "context_text": "We select success rate, LLM evaluation [Wang et al., 2023b], and Human evaluation as metrics.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics used for evaluation. The cited paper title suggests a focus on evaluation methods rather than datasets.",
      "processing_time": 50.479626417160034,
      "citing_paper_id": "274822434",
      "cited_paper_id": 258960339
    },
    {
      "context_text": "We focus on the financial data analysis scenario, a critical application area for generative language models [Xue et al., 2023c, Wu et al., 2023b], to evaluate the capabilities of ROMAS.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general application area (financial data analysis). No specific, verifiable datasets are named.",
      "processing_time": 50.43557572364807,
      "citing_paper_id": "274822434",
      "cited_paper_id": 260775975
    },
    {
      "context_text": "…expect systems not only to perform analyses but also to deliver advanced computational capabilities, such as generating predictive insights [Jin et al., 2023, Xue et al., 2024c] and facilitating decision-making based on historical data [Xue et al., 2022a, Pan et al., 2023, Zhou et al., 2024a].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing advanced computational capabilities and decision-making based on historical data.",
      "processing_time": 50.171170234680176,
      "citing_paper_id": "274822434",
      "cited_paper_id": 263830033
    },
    {
      "context_text": "Self-planning process [Huang et al., 2024].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a self-planning process discussed in the cited paper.",
      "processing_time": 50.12469959259033,
      "citing_paper_id": "274822434",
      "cited_paper_id": 267411892
    }
  ],
  "filtering_stats": {
    "original_papers_count": 132,
    "filtered_papers_count": 127,
    "filtered_percentage": "96.2%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 3306,
    "total_citation_instances": 4549,
    "successful_extractions": 393,
    "failed_extractions": 4156,
    "total_processing_time": 205.25112295150757
  }
}