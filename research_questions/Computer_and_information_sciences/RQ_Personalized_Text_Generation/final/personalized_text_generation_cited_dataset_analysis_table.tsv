Name (extracted)	Citing Article	Citied Article	Features
GoEmotions	https://doi.org/10.1109/ICDMW65004.2024.00071 (2024)	https://doi.org/10.18653/v1/2020.acl-main.372 (2020)	The GoEmotions dataset is primarily used for fine-grained emotion classification and recognition in text. Researchers employ it to compare model performance, evaluate personalized fine-tuning methods, and study the challenges of capturing subtle emotional nuances. The dataset's wide range of emotion labels enables detailed analysis and enhances the evaluation of models in recognizing nuanced emotional expressions.
PsyQA	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2021.findings-acl.130 (2021)	The PsyQA dataset is used to generate and evaluate long counseling texts for mental health support, focusing on the quality, appropriateness, and effectiveness of AI-generated responses. It is employed to compare different selection methods (e.g., top-1 vs. random) and to assess the performance of large language models like ChatGLM and BELLE. The dataset enables researchers to measure helpfulness, empathy, and win rates in generating supportive texts.
Yelp	https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.681 (2021), https://doi.org/10.18653/v1/2023.acl-long.4 (2023) (+1)	https://doi.org/10.18653/v1/2021.findings-acl.129 (2021)	The Yelp dataset is primarily used for personalized text generation and sentiment prediction research, focusing on user-specific styles, preferences, and sentiment patterns in restaurant reviews. It enables the evaluation of methods like UserAdapter for sentiment analysis and machine translation, assessing BLEU scores and personalized product rating predictions. The dataset's rich user reviews facilitate these tasks by providing diverse and detailed textual data.
EMH	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The EMH dataset is used to evaluate the performance of AI models, particularly large language models (LLMs), in generating empathetic and appropriate responses for mental health support. Research focuses on comparing different models (e.g., Vicuna, ChatGPT) and selection methods (top-1 vs. random) to assess the quality, helpfulness, and empathy of text-based mental health support. The dataset enables researchers to systematically analyze and improve the effectiveness of AI in providing emotional and mental health assistance.
CODAH	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The CODAH dataset is primarily used for commonsense reasoning in dialogues, focusing on generating and evaluating coherent, contextually appropriate, and culturally aware responses. It is employed in tasks such as detecting and generating commonsense knowledge, evaluating dialogue systems, and predicting plausible continuations of scenarios. This dataset enables researchers to assess and improve the contextual understanding and coherence of conversational agents.
HellaSWAG	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The HellaSWAG dataset is primarily used for next situation prediction, extending the SWAG dataset with more complex and diverse scenarios. It assesses models' commonsense reasoning by evaluating their ability to generate plausible continuations of given contexts. This dataset is employed to test and improve the text generation capabilities of models, particularly in understanding and predicting the most likely outcomes in complex scenarios.
PENS	https://doi.org/10.48550/arXiv.2503.02614 (2025), https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.18653/v1/2021.acl-long.7 (2021)	The PENS dataset is used to develop and evaluate personalized news headline generation models, focusing on integrating user preferences and interests. It employs user click history data from Microsoft News to enhance personalization and improve user engagement through tailored headlines. The dataset serves as a benchmark for constructing systems that generate more realistic and engaging personalized news headlines.
Reddit	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.48550/arXiv.2308.07968 (2023), https://doi.org/10.1145/3404835.3462828 (2021) (+1)	https://doi.org/10.18653/v1/D16-1127 (2016)	The Reddit dataset is used to study and enhance conversational patterns in dialogue systems, focusing on natural and engaging interactions. It is employed to pre-train word embeddings, improving semantic and syntactic understanding in English language tasks. The dataset, drawn from 45 subreddits, provides diverse user-generated content, enabling the training and evaluation of personalized dialogue agents that incorporate user attributes and utterances.
IMDB	https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.18653/v1/2022.naacl-main.252 (2021)	https://doi.org/10.18653/v1/2021.findings-acl.129 (2021)	The IMDB dataset is primarily used for personalized sentiment prediction and sentiment analysis, focusing on user-specific sentiment patterns in movie reviews. Researchers employ it to explore personalized product rating predictions and conduct comparison and ablation studies, evaluating methods like UserAdapter for performance on movie reviews. This dataset enables detailed analysis of individual user sentiments, enhancing the accuracy of sentiment prediction models.
Amazon Reviews Dataset	https://doi.org/10.48550/arXiv.2407.11016 (2024), https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/D19-1018 (2019)	The Amazon Reviews Dataset is primarily used to train models for personalized text generation, leveraging 150 million user reviews and ratings. It enhances recommendation justifications by capturing diverse user preferences and feedback. Additionally, the dataset provides sentiment and opinion data, which is crucial for training models to understand and generate contextually appropriate and personalized content.
PersonalDialog	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://www.semanticscholar.org/paper/a3ce3004a0eade48a3ae652dbf5c04a60c2416aa (2019)	The PersonalDialog dataset is used to develop and enhance personalized dialogue systems, focusing on maintaining coherent and engaging conversations over multiple turns. It is employed to generate personalized dialogues with diversified traits, specifically to improve conversational diversity and personalization in chatbot interactions. Derived from Weibo, the dataset supports research aimed at incorporating varied traits into conversational responses, thereby enriching user engagement and interaction quality.
VATEX	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2019.00468 (2019)	The VATEX dataset is used to collect multilingual video captions, enhancing the diversity and quality of training data for video-and-language research. It focuses on high-quality annotations across multiple languages, supporting the development of models that can generate personalized text and improve cross-lingual understanding in video captioning tasks.
LSMDC	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2019.00468 (2019)	The LSMDC dataset is primarily used to collect caption sentences for video-and-language research, contributing to a diverse corpus of movie descriptions. It is utilized to train models for generating personalized text, enhancing the dataset's utility in creating rich, contextually relevant captions from various sources.
SWAG	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The SWAG dataset is primarily used for next situation prediction and commonsense reasoning tasks, focusing on generating plausible continuations of everyday scenarios. Researchers employ it to evaluate models' ability to predict likely outcomes, emphasizing situational understanding. This dataset enables the assessment of models' reasoning capabilities in practical, real-world contexts.
CelebV-Text	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.48550/arXiv.2207.12393 (2022)	The CelebV-Text dataset is used to generate text descriptions from facial emotions in videos, demonstrating improvements in linguistic naturalness and complexity through grammar structures and synonym replacement. It is also used to compare with other face video datasets, focusing on the representation and scale of facial attributes, and to highlight the diversity, complexity, and naturalness of texts, emphasizing the quality and characteristics of the textual content.
PERSONA-CHAT	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.18653/v1/N19-1170 (2019)	https://doi.org/10.18653/v1/P18-1205 (2018)	The PERSONA-CHAT dataset is used to train and evaluate dialogue systems that generate personalized responses, focusing on incorporating persona information into conversations. It supports the development of dialogue agents for chitchat interactions, enhancing engagement by optimizing control over repetition, specificity, and question-asking across multiple turns. This dataset enables researchers to improve the naturalness and coherence of conversational agents in both human-human and human-bot settings.
Weibo dataset	https://doi.org/10.1145/3404835.3462828 (2021), https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.1145/3404835.3463239 (2020)	The Weibo dataset is used as a subset of PChatbotW to train and evaluate personalized chatbots, focusing on social media interactions and conversations from Weibo over a one-year period. It enhances contextual understanding and personalization by incorporating user attributes and utterances, improving conversational relevance in chatbot responses.
Unhealthy Conversations	https://doi.org/10.1109/ICDMW65004.2024.00071 (2024)	https://doi.org/10.18653/v1/2020.acl-main.372 (2020)	The 'Unhealthy Conversations' dataset is used to assess and evaluate the performance of personalized fine-tuning methods in identifying and handling attributes of unhealthy conversations. Research focuses on measuring performance gains from personalization, specifically in the context of unhealthy interactions. This dataset enables researchers to test and improve models that can better recognize and manage problematic conversational elements.
ConvAI2	https://doi.org/10.48550/arXiv.2305.11482 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The ConvAI2 dataset is primarily used for fine-tuning pre-trained RoBERTa models to enhance dialogue natural language inference, focusing on consistency and coherence in dialogues. It is also utilized to generate personalized dialogues by incorporating rich personal information, enabling character-based conversations with specific personal facts. This dataset supports research in improving the quality and personalization of conversational agents.
CelebAMask-HQ	https://doi.org/10.48550/arXiv.2312.06116 (2023)	https://doi.org/10.1109/cvpr42600.2020.00559 (2019)	The CelebAMask-HQ dataset is used to enhance personalized text-to-image generation by providing high-quality, well-curated facial image data. It supports the training and fine-tuning of models, particularly the DTI module, by supplying foreground-masked identity images. The dataset's rich meta-annotations enable the selection of 400 unique human identities, which are paired with prompts to improve the evaluation of personalized text-to-image systems.
ActivityNet	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2017.83 (2017)	The ActivityNet dataset is primarily used to collect event captions from videos, serving as a rich source of contextual information. This data is utilized in personalized text generation research, where the captions provide detailed descriptions of events, enhancing the context and relevance of generated text. The dataset's extensive video content and diverse event types enable researchers to develop and evaluate models that can generate more accurate and contextually appropriate text.
SNLI 2	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2017.83 (2017)	The SNLI 2 dataset is used to enrich corpora with logical and semantic relationships through natural language inference, specifically to enhance personalized text generation. This enrichment allows researchers to incorporate deeper contextual understanding into text generation models, improving the coherence and relevance of generated text.
MM-Vox	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The MM-Vox dataset is used to generate personalized text descriptions of faces by focusing on 36 facial attributes. Researchers employ Probabilistic Context-Free Grammar (PCFG) to explore the relationship between visual and textual data, enabling the creation of detailed and accurate text descriptions that reflect specific facial characteristics. This approach supports studies in personalized text generation, particularly in enhancing the precision and relevance of text outputs based on visual inputs.
CelebA	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The CelebA dataset is primarily used to train models on face attributes, focusing on 40 distinct appearance features. This training enhances the capabilities of models in understanding and generating personalized text related to facial characteristics. The dataset's rich attribute annotations enable researchers to develop and refine algorithms that can accurately recognize and describe facial features, thereby improving personalized text generation applications.
VoxCeleb	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The VoxCeleb dataset is primarily used for speaker identification and facial attribute labeling in research. It serves as a large-scale source of face videos, enabling the development and evaluation of models that can accurately identify speakers and label facial attributes. This dataset facilitates the creation of robust multimodal systems by providing diverse and extensive video data.
CelebV-HQ	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.48550/arXiv.2207.12393 (2022)	The CelebV-HQ dataset is used to enhance personalized text generation models by providing high-quality face video data with detailed annotations of facial attributes, including appearance, movement, and emotion. It includes timestamps of dynamic facial attributes, which support the temporal aspect of these models, enabling more nuanced and contextually accurate text generation.
Twitter dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/D16-1127 (2016)	The Twitter dataset is utilized to analyze short-form, informal conversations, enhancing models' capabilities to handle real-world social media interactions. Researchers employ this dataset to improve the understanding and processing of natural, unstructured language, focusing on the nuances of social media communication. This enables more effective modeling of user interactions and conversational dynamics in online platforms.
Dialog NLI dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Dialog NLI dataset is used to train sequence classification and NLI models for evaluating persona consistency between user comments and generated sentences. It focuses on natural language inference annotations in dialogues, enabling researchers to assess the consistency and coherence of generated text in conversational contexts.
Persona dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Persona dataset is used to create the Dialog NLI dataset, which contains persona descriptions and dialogue utterances annotated with Natural Language Inference (NLI) labels. This enables researchers to study personalized text generation and persona consistency through NLI annotations, focusing on how personas are represented and maintained in dialogues.
PChatbotW	https://doi.org/10.1145/3404835.3462828 (2021)	https://doi.org/10.1145/3404835.3463239 (2020)	The PChatbotW dataset is used for training personalized chatbots, specifically to enhance conversational AI systems. Sourced from Weibo and covering a one-year period, this large-scale dataset is employed to improve the performance and personalization of chatbot interactions. The dataset's extensive temporal coverage and scale enable researchers to develop more sophisticated and contextually aware conversational models.
MSCOCO	https://doi.org/10.1109/ICASSP48485.2024.10447048 (2024)	https://doi.org/10.1162/tacl_a_00166 (2014)	The MSCOCO dataset is primarily used for enhancing the training of models in personalized text generation by providing a large set of annotated images. This dataset enables researchers to apply regularization techniques, improving model performance and robustness in generating personalized text. The extensive annotations and diverse image content are crucial for training models to understand context and generate more accurate and contextually relevant text.
Flicker30K	https://doi.org/10.1109/ICASSP48485.2024.10447048 (2024)	https://doi.org/10.1162/tacl_a_00166 (2014)	The Flicker30K dataset is used to enhance the robustness of personalized text generation models by providing a diverse set of images with captions. It serves as a regularization tool, improving model performance through exposure to varied image-caption pairs. This dataset enables researchers to address challenges in generating coherent and contextually appropriate text by leveraging its rich and diverse content.
C HAT dataset	https://doi.org/10.18653/v1/2020.acl-main.131 (2020)	https://doi.org/10.18653/v1/P18-1205 (2018)	The C HAT dataset is used to train and evaluate personalized dialogue agents, particularly in multi-turn dialogues conditioned on personas. It supports research into conversational AI with personalization, focusing on both automatic metrics and human evaluations to assess the performance and superiority of personalized dialogue systems over baseline models.
PersonalSum	https://doi.org/10.48550/arXiv.2502.14289 (2025)	https://doi.org/10.1037/0033-295X.84.3.231 (1977)	The PersonalSum dataset is used to develop and evaluate personalized summarization techniques for large language models. It focuses on enhancing summarization quality through user-subjective guidance, enabling researchers to assess how well these models can generate summaries that align with individual user preferences.
DUTIR	https://doi.org/10.1145/3613904.3642899 (2024)	https://doi.org/10.18653/v1/D17-1059 (2017)	The DUTIR dataset is used to extract and tally emotional tokens in Chinese texts, focusing on the nuances of indirect emotional expressions in Eastern cultures. It is specifically applied in the context of personalized text generation, where the dataset helps researchers understand and model subtle emotional cues in text. This enables more culturally sensitive and emotionally nuanced text generation systems.
OpenWebText	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The OpenWebText dataset is used to train models on web-scraped text, enhancing the diversity of training data. It is specifically employed for personalized text generation, providing a rich and varied set of web-based textual content. This dataset enables researchers to develop models that can generate more diverse and contextually relevant text by leveraging its extensive and varied content.
ELI5	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The ELI5 dataset is used to train and evaluate models on generating explanatory answers, particularly focusing on simplifying complex concepts and ensuring clarity and detail in responses. This dataset, derived from the Explain Like I'm 5 subreddit, is utilized in research to enhance the ability of models to produce clear and understandable explanations, making it valuable for improving the comprehensibility of automated responses in educational and informational contexts.
D4	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.48550/arXiv.2205.11764 (2022)	Dataset D4 is re-organized for depression-diagnosis-oriented chat, specifically focusing on the emotional status of users in Chinese dialogues. It is used to identify empathic comfort utterances within these dialogues. The dataset enables researchers to analyze and understand the nuances of emotional support in mental health contexts, facilitating the development of more effective chatbot interventions.
IMDB movie reviews	https://www.semanticscholar.org/paper/e04a80263d252a3d8a382ba37a249b9345620570 (2019)	https://www.semanticscholar.org/paper/1c61f9ef06fe74505775a833ff849185757199e7 (2011)	The IMDB movie reviews dataset is primarily used for training and evaluating sentiment classifiers, focusing on sentiment analysis in movie reviews. Researchers employ this dataset to assess the performance of different methods in controlling sentiment attributes, using an external sentiment classifier to evaluate attribute control. This enables the development and refinement of techniques for managing sentiment in text data.
Wikipedia Tables	https://doi.org/10.18653/v1/D17-1239 (2017)	https://doi.org/10.18653/v1/D16-1128 (2016)	The Wikipedia Tables dataset is used to generate short biographies by conditioning language models on structured data. Researchers extract structured information from the tables to form coherent narratives, focusing on creating personalized text that accurately represents the subjects of the biographies. This approach leverages the dataset's structured format to enhance the coherence and relevance of the generated text.
CelebA-HQ-256	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The CelebA-HQ-256 dataset is primarily used to pretrain denoising diffusion probabilistic models, specifically focusing on high-resolution images of celebrity faces. This pretraining enhances the capabilities of models in generating personalized text, leveraging the high-quality and diverse facial images to improve text generation accuracy and relevance.
LSUN-Church-256	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN-Church-256 dataset is used to pretrain denoising diffusion probabilistic models, specifically focusing on church images. This pretraining helps diversify the visual contexts in personalized text generation tasks. The dataset's large collection of high-resolution church images enables researchers to enhance the visual diversity and quality of generated text, contributing to more nuanced and contextually rich outputs.
KIT Motion-Language MoCap dataset	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://www.semanticscholar.org/paper/d92514b811d767369c61c66e71aa66c639f6d158 (2020)	The KIT Motion-Language MoCap dataset is used to synthesize human motions from textual descriptions, emphasizing the diversity and quality of generated motions. Researchers employ the TEMOS model to achieve this, focusing on accurate motion synthesis. This dataset enables the evaluation of motion generation models by providing paired text and motion data, facilitating advancements in natural and realistic motion synthesis from text.
AMASS	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://doi.org/10.1109/CVPR46437.2021.00078 (2021)	The AMASS dataset is re-annotated with English language labels to generate diverse and natural 3D human motions from text. It enhances personalized text-to-motion synthesis, focusing on creating realistic and contextually appropriate movements. This dataset supports research in text-driven motion generation, improving the naturalness and diversity of synthesized human motions.
Baidu PersonaChat	https://doi.org/10.48550/arXiv.2305.11482 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Baidu PersonaChat dataset is used to fine-tune a pre-trained RoBERTa model for dialogue natural language inference, specifically to enhance consistency and coherence in dialogues. This application leverages the dataset's dialogue structures to improve the model's ability to understand and generate contextually appropriate responses.
VGGFace2	https://doi.org/10.1109/ICCV51070.2023.00202 (2023)	https://doi.org/10.1109/FG.2018.00020 (2017)	The VGGFace2 dataset is primarily used to train DreamBooth models for personalized text generation, focusing on generating high-quality, correct-identity images for 50 identities. This enhances recognition accuracy across various poses and ages, leveraging the dataset's extensive and diverse facial image collection.
the Pile dataset	https://doi.org/10.48550/arXiv.2402.04914 (2024)	https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e (2020)	The Pile dataset, a 825GB English corpus from 22 diverse sources including academic writing, internet, prose, dialogue, and miscellaneous, is primarily used for pretraining models. This extensive and varied content enables researchers to develop robust language models capable of handling a wide range of textual data, enhancing their performance in various natural language processing tasks.
IMDb62	https://doi.org/10.48550/arXiv.2402.04914 (2024)	https://doi.org/10.1162/COLI_a_00173 (2014)	The IMDb62 dataset is used to study authorship attribution by analyzing the writing styles of 62 prolific IMDb reviewers, each of whom has written 1,000 movie reviews. Topic models are employed to capture and compare stylistic elements, enabling researchers to address questions related to author identification and stylistic consistency. The large, consistent review count per author enhances the reliability of the analysis.
News dataset	https://doi.org/10.18653/v1/2020.emnlp-main.698 (2020)	https://doi.org/10.18653/v1/N16-1014 (2015)	The News dataset is used to evaluate the diversity of generated text in neural conversation models. Researchers focus on comparing Dist-n scores between model-generated and human-generated text, assessing the model's ability to produce varied and natural responses. This dataset enables the quantitative analysis of text diversity, crucial for improving conversational AI systems.
Stellar	https://doi.org/10.48550/arXiv.2312.06116 (2023)	https://doi.org/10.1109/cvpr42600.2020.00559 (2019)	The Stellar dataset is used in research for generating imaginary human-centric depictions through multimodal prompts. It employs publicly available images of celebrities for training and evaluation. This dataset enables researchers to explore the integration of visual and textual data in creating imaginative content, focusing on the development of models that can generate coherent and contextually relevant text based on visual inputs.
360PanoI	https://doi.org/10.1109/WACV57701.2024.00486 (2023)	https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092 (2021)	The 360PanoI dataset is used for fine-tuning models with paired image-text data, specifically to enhance personalized text generation capabilities. This involves training models to generate text that is more contextually relevant and personalized based on the input images. The dataset's paired image-text data is crucial for improving the model's ability to understand and generate text that aligns with visual content.
Citation Network Dataset (V14)	https://doi.org/10.48550/arXiv.2407.11016 (2024)	https://doi.org/10.1145/1401890.1402008 (2008)	The Citation Network Dataset (V14) is used to generate data samples for personalized text generation, utilizing 5,259,858 papers and 29 features per paper. This comprehensive dataset enables researchers to create robust models by providing a rich set of attributes for each paper, enhancing the accuracy and relevance of generated text.
OMCS corpus	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1007/3-540-36124-3_77 (2002)	The OMCS corpus is used to enhance personalized text generation by retrieving relevant propositions as distant rationales in training data. This methodology incorporates common sense knowledge, improving the model's ability to generate more contextually appropriate and coherent text. The dataset's rich common sense information enables researchers to address the challenge of generating personalized content that aligns with user-specific contexts and preferences.
Movie-DiC	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://www.semanticscholar.org/paper/e2efefbba8bf3e76605db24da0ba15df7b0adc9e (2012)	The Movie-DiC dataset is used to develop and evaluate dialogue generation systems, particularly for movie dialogues. It provides a rich source of conversational data, supporting research in personalized text generation. The dataset enables researchers to create more contextually appropriate and engaging dialogues by leveraging its extensive collection of movie conversations.
SogouCS&CA corpus (2008 version)	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://doi.org/10.1145/1367497.1367560 (2008)	The SogouCS&CA corpus (2008 version) is primarily used for training word2vec models to construct word vectors for Chinese text analysis. This dataset enables researchers to perform natural language processing tasks by providing a large, structured collection of Chinese text data, facilitating the development and evaluation of algorithms that require robust word embeddings.
SogouCS&CA corpus	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f (2013)	The SogouCS&CA corpus is used to train word embeddings for Chinese text analysis, particularly focusing on the 2008 version. This dataset enables the generation of distributed representations of words and phrases, facilitating research in natural language processing and enhancing the understanding of semantic relationships in Chinese text.
personal knowledge base	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://doi.org/10.1109/35021BIGCOMP.2015.7072837 (2015)	The 'personal knowledge base' dataset is used to enhance the personalization of dialogue interactions in open-domain conversation systems. It employs example-based approaches to improve responses ranking, ensuring that the generated dialogues are more contextually relevant and personalized. This dataset enables researchers to focus on improving the quality and naturalness of conversational agents by leveraging user-specific data.
Modified MUG	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ACCESS.2020.3017881 (2020)	The Modified MUG dataset is used to generate text descriptions from facial emotions in a closed-world text-video dataset containing 1,039 videos. It focuses on different emotions, enabling researchers to explore the relationship between facial expressions and corresponding textual descriptions. This dataset facilitates the development and evaluation of models that can accurately translate visual emotional cues into text, enhancing applications in affective computing and emotion recognition.
Amazon Electronics	https://doi.org/10.18653/v1/P18-2112 (2018)	https://doi.org/10.1145/2766462.2767755 (2015)	The Amazon Electronics dataset is used to evaluate models on real-world data, specifically for generating personalized recommendations for electronic products. Researchers focus on user interactions and preferences to assess the effectiveness of recommendation algorithms. This dataset enables the testing and validation of personalized recommendation systems in the electronics domain.
FoCus	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The FoCus dataset is used in research focused on conversational information-seeking scenarios, specifically analyzing user queries and information retrieval within personalized dialogue systems. It enables researchers to study how users formulate queries and seek information in interactive dialogues, enhancing the understanding of user behavior and improving the effectiveness of personalized dialogue systems.
LiveChat	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The LiveChat dataset is used to study live streaming interactions, focusing on real-time user engagement and response patterns. It is employed in research to analyze how users engage in real-time conversations, with a specific emphasis on personalized text generation. This dataset enables researchers to explore the dynamics of immediate user responses and interaction patterns, providing insights into the effectiveness of personalized messaging in live streaming environments.
Pchatbot	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The Pchatbot dataset, compiled from Weibo and judicial forums, is used to explore diverse conversational contexts and legal information in chatbot applications. Researchers employ this dataset to analyze and enhance chatbot interactions by integrating real-world conversational data and legal content, focusing on improving the contextual relevance and accuracy of chatbot responses in various scenarios.
Ultrafeedback	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The Ultrafeedback dataset is used to develop a personalized reward model, specifically focusing on incorporating user feedback to dynamically adjust preferences. This approach enables researchers to refine and personalize text generation models by continuously aligning them with user preferences, enhancing the relevance and satisfaction of generated content.
HelpSteer2	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The HelpSteer2 dataset is utilized in the development of a personalized reward model, focusing on enhancing user guidance and interaction. This involves employing methodologies that emphasize user feedback and engagement to refine the reward system. The dataset enables researchers to address specific research questions related to improving user experience and interaction in personalized systems.
SafeRLHF	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The SafeRLHF dataset is used to ensure safety in personalized reward models by incorporating human feedback into reinforcement learning. It focuses on enhancing the safety of generated outputs, addressing research questions related to aligning AI-generated content with human values and ethical standards. This dataset enables researchers to train models that are more reliable and trustworthy in personalized text generation tasks.
Rewards-in-Context	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The 'Rewards-in-Context' dataset is used in research to align foundation models with multi-objective goals and dynamic preferences. It enables the adjustment of model outputs to better reflect user-specific or context-specific requirements, enhancing the adaptability and alignment of AI systems. This dataset supports methodologies focused on improving the flexibility and responsiveness of foundation models to varying objectives and preferences.
Chinese Weibo	https://doi.org/10.48550/arXiv.2210.08753 (2022)	https://doi.org/10.18653/V1/2020.ACL-DEMOS.30 (2019)	The Chinese Weibo dataset is used to conduct experiments on personalized text generation, specifically focusing on social media content from a Chinese platform. Researchers employ this dataset to develop and test models that can generate text tailored to individual users, leveraging the unique linguistic and cultural nuances present in the Weibo data. This enables the exploration of how personalized content can be effectively created and adapted for Chinese social media users.
Pushshift Reddit Dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.5281/ZENODO.3608135 (2020)	The Pushshift Reddit Dataset is used to provide raw data for preprocessing user attributes, focusing on the reproducibility of models and scripts. It enables researchers to ensure that their methodologies and analyses can be replicated, enhancing transparency and reliability in their studies.
English Reddit	https://doi.org/10.48550/arXiv.2210.08753 (2022)	https://doi.org/10.18653/V1/2020.ACL-DEMOS.30 (2019)	The English Reddit dataset is used to conduct experiments on personalized text generation, specifically focusing on social media content from an English-speaking platform. Researchers employ this dataset to explore and develop models that can generate text tailored to individual users, leveraging the rich and diverse content available on Reddit. This dataset enables the examination of user-specific language patterns and preferences, enhancing the personalization of generated text.
WMT15-WMT17	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT15-WMT17 dataset is used to collect multilingual data with English as the target language, primarily for evaluating machine translation systems. It focuses on assessing these systems' ability to generate personalized text, employing methodologies that involve comparing translations against human-generated references to measure accuracy and personalization.
WMT15	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT15 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset supports research aimed at improving the accuracy and naturalness of machine-translated text, specifically in the German language.
WMT16	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT16 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset enables researchers to address specific challenges in personalized translation, improving the overall performance and naturalness of generated text.
WMT17	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT17 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset enables researchers to address specific challenges in personalized translation, improving the overall performance and naturalness of generated text.
MIND	https://doi.org/10.48550/arXiv.2305.06566 (2023)	https://doi.org/10.18653/v1/2020.acl-main.331 (2020)	The MIND dataset is used to analyze news articles for personalized text generation, specifically focusing on extracting and categorizing main ideas and content types such as 'guidance' or 'instructions'. This involves methodologies that parse and interpret textual data to understand and generate personalized content. The dataset's rich textual information and structured content types enable researchers to develop and refine algorithms for generating tailored text based on article analysis.
Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages	https://doi.org/10.18653/v1/P18-1080 (2018)	https://www.semanticscholar.org/paper/42d3c35a2a10a71b8dddb9fde15a3fee245c2bcc (2018)	The dataset of Facebook posts from all 412 current members of the United States Senate and House is used to analyze top-level comments, focusing on public engagement and response patterns. Researchers employ this dataset to study how the public interacts with political figures, examining comment content and engagement metrics. This analysis helps understand the dynamics of online political communication and public sentiment.
CustomConcept101	https://doi.org/10.48550/arXiv.2402.09368 (2024)	https://doi.org/10.1109/CVPR52729.2023.00192 (2022)	The CustomConcept101 dataset is used to validate the VCD framework by providing a diverse representation of animals and objects. It supports multi-concept customization in text-to-image diffusion models, enabling researchers to test and refine the framework's ability to generate images based on complex textual inputs. This dataset's diversity and multi-concept nature are crucial for enhancing the robustness and versatility of text-to-image models.
Yelp (restaurant)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp (restaurant) dataset is used to evaluate explainable recommendation systems, focusing on user reviews and ratings for restaurants. Researchers employ this dataset to analyze and enhance the explainability of recommendations, leveraging the rich textual and numerical data to understand user preferences and improve system transparency.
Amazon (movies & TV)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021)	https://doi.org/10.1145/3340531.3411992 (2020)	The Amazon (movies & TV) dataset is used to evaluate explainable recommendation systems, focusing on user reviews and ratings for movies and TV shows. Researchers employ this dataset to analyze and enhance the transparency and interpretability of recommendation algorithms, ensuring that users understand why certain items are recommended. This involves methodologies that assess the effectiveness of explanations derived from user reviews and ratings, contributing to more user-centric and trustworthy recommendation systems.
C HAT unique	https://doi.org/10.18653/v1/2020.acl-main.131 (2020)	https://doi.org/10.18653/v1/P18-1205 (2018)	The C HAT unique dataset is used to train chatbots with configurable and persistent personalities. It focuses on the explicit description of interlocutor personas using profile sentences, enabling researchers to develop chatbots that can maintain consistent and distinct personality traits throughout interactions. This dataset supports the creation of more engaging and human-like conversational agents by providing structured persona data.
ImageNet	https://doi.org/10.48550/arXiv.2403.03206 (2024)	https://doi.org/10.1007/s11263-015-0816-y (2014)	The ImageNet dataset is primarily used for enhancing text-to-image models by converting images into a format that includes captions like "a photo of a 〈 class name 〉." This conversion enriches the dataset, making it suitable for personalized text generation tasks. The dataset's extensive class labels and high-quality images enable researchers to train models that generate more accurate and contextually relevant images from textual descriptions.
PRISM	https://doi.org/10.48550/arXiv.2502.14289 (2025)	https://doi.org/10.48550/arXiv.2404.16019 (2024)	The PRISM dataset is used to collect preference annotations from conversations involving over a thousand users, focusing on the subjective and multicultural alignment of large language models. Despite limitations in annotation quantity per user, the dataset enables researchers to evaluate and improve the cultural and personal relevance of language models through user feedback.
Common Crawl	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.3115/v1/D14-1162 (2014)	The Common Crawl dataset is primarily used as a source for pretrained GloVe embeddings, which focus on word representation in natural language processing. This dataset enables researchers to develop and refine word embeddings, enhancing the accuracy and effectiveness of NLP models by providing a large corpus of web text.
ForumSum	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/N19-1260 (2018)	The ForumSum dataset is used to summarize forum discussions, focusing on the effectiveness of multi-level memory networks in capturing conversational nuances. This involves applying these networks to analyze and condense lengthy forum threads, enabling researchers to evaluate how well such models can preserve the context and subtleties of online conversations.
Reddit TIFU-long	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/N19-1260 (2018)	The Reddit TIFU-long dataset is used to generate summaries from long Reddit posts, focusing on maintaining coherence and relevance in longer texts. Researchers employ this dataset to evaluate summarization techniques, specifically addressing the challenge of preserving context and meaning over extended content. This dataset enables the development and testing of algorithms designed to handle and condense large volumes of textual data effectively.
Avocado Research Email Collection	https://doi.org/10.48550/arXiv.2308.07968 (2023)		The Avocado Research Email Collection is used to study email communication patterns within an IT company, specifically focusing on personalization in email content and attachment usage. Researchers analyze the dataset to understand how personalization affects communication dynamics, employing methods that examine the frequency and context of personalized elements and attachments. This enables insights into effective communication strategies in professional settings.
Partiprompts set	https://doi.org/10.48550/arXiv.2403.03206 (2024)	https://doi.org/10.48550/arXiv.2206.10789 (2022)	The Partiprompts set is used to conduct human preference studies, specifically evaluating captions generated by models. In these studies, approximately 128 captions are assessed with around three voters per prompt, comparing different outputs. This dataset enables researchers to gather qualitative feedback on the effectiveness and quality of generated text, facilitating the refinement of text generation models through direct user evaluation.
R EDDIT	https://doi.org/10.18653/v1/D18-1298 (2018)	https://www.semanticscholar.org/paper/be3a65ef15f79ebb8296e6a0e8d1a9cb5c0f3638 (2015)	The R EDDIT dataset is used to train and evaluate dialog systems, specifically focusing on enhancing the quality of generated responses. Researchers employ a large-scale comment dataset to assess system performance, ensuring that the dialogues are coherent and contextually appropriate. This dataset enables the development of more effective and natural conversational agents by providing a rich source of human interaction data.
Wikipedia	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The Wikipedia dataset is used to train models on multilingual text, encompassing English, German, Spanish, and French versions. This approach leverages the extensive and diverse content of Wikipedia to develop models capable of handling multiple languages. The dataset's multilingual nature enables research focused on improving cross-lingual understanding and translation capabilities.
Project Gutenberg	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The Project Gutenberg dataset is used to train models on a large collection of public domain books. This enables researchers to develop and test algorithms that can process and understand extensive textual content, focusing on natural language processing tasks. The dataset's vast corpus of literary works supports the training of robust language models, enhancing their ability to generate and analyze text.
news data	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The 'news data' dataset is used to train models on news articles from diverse sources, including those curated by Hermann et al., Barrault et al., Sandhaus, and Grusky et al. This training focuses on leveraging large, varied corpora to enhance model performance in understanding and generating coherent news content. The dataset's broad source base ensures exposure to different writing styles and topics, crucial for robust model training.
UN data	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The UN data dataset is used to train models on multilingual United Nations documents, focusing on enhancing the models' understanding of formal and diplomatic language. This involves employing machine learning techniques to process and analyze the linguistic nuances present in these documents. The dataset's multilingual nature and formal content are crucial for improving the model's performance in handling complex, official texts.
DeepFashion	https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.1109/CVPR.2016.124 (2016)	The DeepFashion dataset is primarily used to enhance clothes recognition and retrieval through rich annotations. It supports personalized text generation for fashion-related content by providing robust data for training models to recognize and describe clothing items accurately. This dataset enables researchers to develop more effective and contextually relevant fashion recommendation systems and content generation tools.
SHHQ	https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.1109/CVPR.2016.124 (2016)	The SHHQ dataset is utilized for developing personalized text generation models by providing high-quality 2D human images. These images support the models in generating text that requires detailed visual data, enhancing the accuracy and relevance of the generated content. The dataset's high-resolution images are crucial for training models to understand and describe complex visual scenes effectively.
LAION-5B	https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.1109/CVPR.2016.124 (2016)	The LAION-5B dataset is leveraged as a large-scale multimodal resource, primarily providing diverse 2D human images. It is used to train and improve personalized text generation models by enhancing the models' ability to generate text that is contextually relevant and personalized. The dataset's extensive size and variety of images enable researchers to develop more robust and versatile text generation systems.
LAION-400M	https://doi.org/10.1109/CVPRW63382.2024.00100 (2024)	https://doi.org/10.48550/arXiv.2303.09319 (2023)	The LAION-400M dataset is used to train multimodal latent diffusion models that combine text and image inputs. This approach focuses on generating images conditioned on both textual and visual subjects, enabling research in joint subject and text conditional image generation. The dataset's large-scale multimodal content facilitates the development of models capable of producing high-quality, contextually relevant images.
news data (Hermann et al., 2015)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Hermann et al., 2015)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and their summaries. This dataset enables researchers to develop and refine algorithms that can generate coherent and contextually relevant summaries, enhancing the personalization of news content delivery.
news data (Barrault et al., 2019)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Barrault et al., 2019)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.
news data (Sandhaus, 2008)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Sandhaus, 2008)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and summaries. This involves employing machine learning techniques to generate tailored content, enhancing the relevance and engagement of news for individual users. The dataset's extensive collection of news articles and summaries enables researchers to develop and refine algorithms that can produce personalized text outputs.
news data (Grusky et al., 2018)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Grusky et al., 2018)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.
Europarl	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The Europarl dataset is primarily used to train models for personalized text generation, with a focus on multilingual parliamentary proceedings. It enables researchers to develop and evaluate models that can generate text reflecting the style and content of parliamentary debates in multiple languages. The dataset's extensive multilingual corpus is crucial for training these models effectively.
UN data from WMT (En-De, En-Es, En-Fr)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The UN data from WMT (En-De, En-Es, En-Fr) is primarily used to train models for personalized text generation, specifically focusing on multilingual United Nations documents. This dataset enables researchers to develop and refine algorithms that can generate text tailored to individual users, leveraging the rich linguistic diversity and formal structure of UN documents.
EmpatheticDialogues	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/P19-1534 (2018)	The EmpatheticDialogues dataset is used to evaluate empathetic open-domain conversation models, specifically focusing on the test set to assess model performance in generating empathetic responses. This dataset enables researchers to measure how well models can produce emotionally appropriate and contextually relevant dialogues, emphasizing the importance of empathy in conversational AI.
10k (post, response) pairs	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The '10k (post, response) pairs' dataset is used to train and evaluate models focused on expressing empathy in text-based mental health support. It emphasizes emotional reactions, interpretations, and explorations. The dataset enables researchers to develop and assess models that can effectively convey empathy, enhancing the quality of automated mental health support systems.
ED	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The ED dataset is used to evaluate AI models' performance in generating empathetic dialogues for text-based mental health support. Research focuses on assessing the models' ability to express empathy, using the dataset to test and refine empathetic dialogue generation techniques. This enables researchers to improve the quality and effectiveness of AI-driven mental health support systems.
Empa	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.48550/arXiv.2205.11764 (2022)	The Empa dataset is used to re-organize empathetic dialogues, focusing on the user's emotional status. While specific methodological details are not provided, the dataset enables researchers to address how empathetic responses can be structured in dialogue systems. This supports the development of more emotionally attuned conversational agents.
TED gender-annotated data	https://doi.org/10.18653/V1/E17-1101 (2016)	https://doi.org/10.18653/v1/D15-1130 (2015)	The TED gender-annotated data is used to train and evaluate personality-aware machine translation models, specifically focusing on gender annotations to enhance personalization in generated text. This dataset enables researchers to explore how gender information can be effectively integrated into translation models to produce more contextually appropriate and personalized translations.
One-Billion-Word	https://doi.org/10.18653/v1/2021.emnlp-main.681 (2021)		The One-Billion-Word dataset is used to train and evaluate personalized text generation models, with a focus on large-scale language modeling and adaptation techniques. Researchers employ this dataset to develop and refine models that can generate text tailored to individual users, leveraging its extensive size and diverse content to enhance model performance and adaptability.
CelebA-HQ	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The CelebA-HQ dataset is primarily used to test personalized text generation models, specifically focusing on high-quality celebrity images. Researchers employ this dataset to evaluate the visual-textual alignment, ensuring that generated text accurately reflects the visual content of the images. This application leverages the dataset's high-resolution and diverse celebrity images to enhance the robustness and accuracy of visual-textual models.
ConvAI2 personalized dialogue corpus	https://doi.org/10.5220/0010812500003116 (2021)	https://doi.org/10.1007/978-3-030-29135-8_7 (2019)	The ConvAI2 personalized dialogue corpus is used to evaluate personalized dialogue systems, specifically focusing on their ability to generate contextually appropriate and user-specific responses. Researchers employ this dataset to assess the performance of dialogue models in maintaining coherent and personalized conversations, leveraging its annotated dialogues to measure contextual relevance and personalization accuracy.
LSUN-Church	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN-Church dataset is used to test personalized text generation models, specifically focusing on church images. Researchers employ this dataset to evaluate the model's ability to generate text for diverse visual contexts, ensuring the generated text accurately reflects the unique characteristics of church imagery. This application helps in assessing the model's performance in context-specific text generation tasks.
WIKIBIO	https://doi.org/10.18653/v1/D17-1239 (2017)	https://doi.org/10.18653/v1/D16-1128 (2016)	The WIKIBIO dataset is used to generate biographical text from structured data, emphasizing the scalability of neural text generation models. Researchers focus on increasing the number of tokens and record types to enhance model performance. This dataset enables the exploration of methods to efficiently convert structured information into coherent biographical narratives, addressing challenges in data scaling and text coherence.
Huffington Post articles	https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.48550/arXiv.2209.11429 (2022)	The Huffington Post articles dataset is used to create a categorized dataset for personalized text generation, specifically focusing on classifying news articles into distinct topics. This involves employing methodologies that categorize and label articles, enabling research into how personalized content can be generated based on topic-specific data.
News Categorization dataset	https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.48550/arXiv.2209.11429 (2022)	The News Categorization dataset is primarily used to construct a dataset for personalized text generation, leveraging news articles categorized from the HuffPost website. This dataset enables researchers to develop and test algorithms that can generate personalized text content, focusing on the categorization and contextual relevance of news articles.
Yelp 3	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The Yelp 3 dataset is used to analyze restaurant reviews, focusing on sentiment and personalization in user feedback. Researchers employ methodologies that examine the emotional tone and individualized aspects of reviews. This dataset enables studies to address research questions related to customer satisfaction and personalized experiences, leveraging the rich textual data and user-specific attributes.
TripAdvisor 4	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The TripAdvisor 4 dataset is used in research to analyze hotel reviews, focusing on user preferences and personalized recommendations. Studies employ this dataset to understand how user-specific data can enhance recommendation systems, leveraging the rich textual content and user ratings to tailor suggestions more effectively. This dataset enables researchers to explore methodologies for improving the personalization of travel recommendations.
Amazon Movies and TV	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The Amazon Movies and TV dataset is utilized to study movie and TV reviews, focusing on user ratings and personalized content suggestions. Researchers employ this dataset to investigate how user feedback and ratings can inform and enhance personalized recommendations. The dataset's extensive review data and rating scores enable detailed analyses of user preferences and behaviors, supporting the development of more effective recommendation systems.
BABEL	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://doi.org/10.1109/CVPR46437.2021.00078 (2021)	The BABEL dataset is used to train the MotionDiffuse model, which integrates motion data with English labels to enhance personalized text generation. It focuses on bodies, actions, and behaviors, enabling researchers to explore the relationship between physical movements and textual content. This dataset facilitates the development of more contextually rich and dynamic text generation models by incorporating detailed motion data.
TripAdvisor dataset	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.3115/1073083.1073135 (2002)	The TripAdvisor dataset is used to evaluate the performance of methods in improving BLEU scores, focusing on the automatic evaluation of machine translation. It provides a benchmark for assessing translation quality, enabling researchers to compare different machine translation techniques and measure their effectiveness in generating accurate translations.
Amazon (cell phones)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The Amazon (cell phones) dataset is used to generate neural template explanations for cell phone recommendations, leveraging user reviews and product attributes. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich user review content and detailed product attributes enable researchers to create more transparent and user-friendly recommendation explanations.
Yelp (restaurants)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp (restaurants) dataset is used to generate neural template explanations for restaurant recommendations, leveraging user reviews and dining experiences. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich textual content and user feedback enable researchers to create more transparent and user-friendly recommendation explanations.
TripAdvisor (hotels)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The TripAdvisor (hotels) dataset is used to generate neural template explanations for hotel recommendations, leveraging user reviews and accommodation features. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich textual content and feature details enable researchers to create more transparent and user-friendly explanations for hotel recommendations.
Amazon 5-core	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.1145/2766462.2767755 (2015)	The Amazon 5-core dataset is used to construct datasets for personalized text generation, specifically focusing on user-generated reviews and metadata from May 1996 to July 2014. It ensures no duplicated records, providing a clean, extensive resource for generating personalized texts based on historical review data.
Electronics dataset	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.3115/1073083.1073135 (2002)	The Electronics dataset is used to evaluate the performance of text generation models on electronics-related content. Researchers focus on measuring the quality of generated sentences using ROUGE scores, which assess the overlap between machine-generated and reference texts. This dataset enables the assessment of model accuracy and coherence in generating technical and product-related text.
Beauty dataset	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.3115/1073083.1073135 (2002)	The Beauty dataset is used to evaluate the performance of text generation models on beauty-related content. Researchers focus on measuring the quality of generated sentences using ROUGE scores, which assess the overlap between machine-generated and reference texts. This dataset enables the assessment of model effectiveness in generating coherent and contextually appropriate beauty-related text.
lexicon from Wilson et al., 2005	https://www.semanticscholar.org/paper/2a215755d7548ffc82079ce734c4ac60b62f6f56 (2017)	https://doi.org/10.3115/1220575.1220619 (2005)	The lexicon from Wilson et al., 2005 is used to provide sentiment labels for words, which enhances the emotional context in personalized text generation models. This dataset enables researchers to incorporate nuanced emotional content, improving the authenticity and relevance of generated text.
