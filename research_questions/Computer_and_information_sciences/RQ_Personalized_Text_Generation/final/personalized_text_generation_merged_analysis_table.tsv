Analysis_Source	Name (extracted)	Citing Article	Citied Article	Features	Name_Variants	Homepage_URL
cited_context	10k (post	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The '10k (post, response) pairs' dataset is used to train and evaluate models focused on expressing empathy in text-based mental health support. It emphasizes emotional reactions, interpretations, and explorations. The dataset enables researchers to develop and assess models that can effectively convey empathy, enhancing the quality of automated mental health support systems.		
cited_context	2005	https://www.semanticscholar.org/paper/2a215755d7548ffc82079ce734c4ac60b62f6f56 (2017)	https://doi.org/10.3115/1220575.1220619 (2005)	The lexicon from Wilson et al., 2005 is used to provide sentiment labels for words, which enhances the emotional context in personalized text generation models. This dataset enables researchers to incorporate nuanced emotional content, improving the authenticity and relevance of generated text.		
cited_context	2008)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Sandhaus, 2008)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and summaries. This involves employing machine learning techniques to generate tailored content, enhancing the relevance and engagement of news for individual users. The dataset's extensive collection of news articles and summaries enables researchers to develop and refine algorithms that can produce personalized text outputs.		
cited_context	2015)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Hermann et al., 2015)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and their summaries. This dataset enables researchers to develop and refine algorithms that can generate coherent and contextually relevant summaries, enhancing the personalization of news content delivery.		
cited_context	2018)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Grusky et al., 2018)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.		
cited_context	2019)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Barrault et al., 2019)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.		
cited_context	360PanoI	https://doi.org/10.1109/WACV57701.2024.00486 (2023)	https://www.semanticscholar.org/paper/a8ca46b171467ceb2d7652fbfb67fe701ad86092 (2021)	The 360PanoI dataset is used for fine-tuning models with paired image-text data, specifically to enhance personalized text generation capabilities. This involves training models to generate text that is more contextually relevant and personalized based on the input images. The dataset's paired image-text data is crucial for improving the model's ability to understand and generate text that aligns with visual content.		
cited_context	ActivityNet	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2017.83 (2017)	The ActivityNet dataset is primarily used to collect event captions from videos, serving as a rich source of contextual information. This data is utilized in personalized text generation research, where the captions provide detailed descriptions of events, enhancing the context and relevance of generated text. The dataset's extensive video content and diverse event types enable researchers to develop and evaluate models that can generate more accurate and contextually appropriate text.		
cited_context	AMASS	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://doi.org/10.1109/CVPR46437.2021.00078 (2021)	The AMASS dataset is re-annotated with English language labels to generate diverse and natural 3D human motions from text. It enhances personalized text-to-motion synthesis, focusing on creating realistic and contextually appropriate movements. This dataset supports research in text-driven motion generation, improving the naturalness and diversity of synthesized human motions.		
citing_context	Amazon	https://doi.org/10.1145/3696410.3714583 (2024)		The Amazon dataset is used to construct a dataset for personalized text generation, specifically focusing on user reviews across various product categories. This involves leveraging the textual content and user-specific information to develop models that can generate personalized text. The dataset's diverse range of product categories and rich user review data enable researchers to explore and enhance personalized text generation techniques.		
cited_context	Amazon (cell phones)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The Amazon (cell phones) dataset is used to generate neural template explanations for cell phone recommendations, leveraging user reviews and product attributes. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich user review content and detailed product attributes enable researchers to create more transparent and user-friendly recommendation explanations.		
cited_context	Amazon (movies & TV)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021)	https://doi.org/10.1145/3340531.3411992 (2020)	The Amazon (movies & TV) dataset is used to evaluate explainable recommendation systems, focusing on user reviews and ratings for movies and TV shows. Researchers employ this dataset to analyze and enhance the transparency and interpretability of recommendation algorithms, ensuring that users understand why certain items are recommended. This involves methodologies that assess the effectiveness of explanations derived from user reviews and ratings, contributing to more user-centric and trustworthy recommendation systems.		
cited_context | citing_context	Amazon 5-Core	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.1145/2766462.2767755 (2015)	The Amazon 5-core dataset is used to build datasets for personalized text generation, specifically focusing on user-generated reviews and metadata from May 1996 to July 2014, ensuring no duplicates. This dataset enables researchers to analyze and generate personalized text content based on historical review data, enhancing the understanding of user preferences and behaviors over time.; The Amazon 5-core dataset is used to construct datasets for personalized text generation, specifically focusing on user-generated reviews and metadata from May 1996 to July 2014. It ensures no duplicated records, providing a clean, extensive resource for generating personalized texts based on historical review data.	Amazon 5-core	
cited_context	Amazon Electronics	https://doi.org/10.18653/v1/P18-2112 (2018)	https://doi.org/10.1145/2766462.2767755 (2015)	The Amazon Electronics dataset is used to evaluate models on real-world data, specifically for generating personalized recommendations for electronic products. Researchers focus on user interactions and preferences to assess the effectiveness of recommendation algorithms. This dataset enables the testing and validation of personalized recommendation systems in the electronics domain.		
citing_context	Amazon Movies	https://doi.org/10.48550/arXiv.2210.15500 (2022)	https://www.semanticscholar.org/paper/113424928b19a1d7645ef04a2b53532dd426c283 (2015)	The Amazon Movies dataset is used to evaluate the fairness and utility of the DDP method by plotting results on movie reviews. This involves analyzing review data to assess how well the DDP method maintains fairness while preserving utility in the context of movie ratings and reviews. The dataset's review content and ratings are key features enabling this evaluation.		
citing_context	Amazon Movies 1	https://doi.org/10.48550/arXiv.2210.15500 (2022)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021)	The Amazon Movies 1 dataset is used to investigate explanation generation for movie recommendations using the personalized transformer model PETER. It focuses on generating explanations that help users understand why certain movies are recommended to them. This research employs the dataset to enhance the transparency and interpretability of recommendation systems, leveraging the dataset's rich user-movie interaction data.		
cited_context	Amazon Movies and TV	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The Amazon Movies and TV dataset is utilized to study movie and TV reviews, focusing on user ratings and personalized content suggestions. Researchers employ this dataset to investigate how user feedback and ratings can inform and enhance personalized recommendations. The dataset's extensive review data and rating scores enable detailed analyses of user preferences and behaviors, supporting the development of more effective recommendation systems.		
citing_context	Amazon review data	https://doi.org/10.1145/3589334.3645408 (2023)		The Amazon review data is used to generate personalized book reviews, focusing on the largest category of books to create user-specific review texts. This involves employing natural language processing techniques to tailor reviews to individual users, enhancing the relevance and personalization of the content. The dataset's extensive collection of reviews enables researchers to train models that can produce detailed and contextually appropriate reviews, specifically addressing the needs and preferences of individual users.		
citing_context	Amazon review dataset	https://doi.org/10.1145/3626772.3657821 (2024)	https://doi.org/10.1145/2766462.2767755 (2015)	The Amazon review dataset is used to gather reviews from categories such as Sports, Beauty, and Toys, focusing on user preferences and product ratings. Researchers employ this dataset to analyze consumer behavior and sentiment, using methodologies that involve categorizing and evaluating textual data. This enables studies on user preferences and product satisfaction, providing insights into consumer decision-making processes.		
cited_context | citing_context	Amazon Reviews Dataset	https://doi.org/10.48550/arXiv.2407.11016 (2024), https://doi.org/10.48550/arXiv.2501.02157 (2025), https://doi.org/10.48550/arXiv.2407.11016 (2024), https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/D19-1018 (2019)	The Amazon Reviews Dataset is primarily used for generating data samples for personalized text generation, leveraging 150 million user reviews to train models on user preferences and review content. It is also utilized to bridge language and items for retrieval and recommendation, enhancing recommendation accuracy by focusing on user-item interactions and textual reviews. This dataset's extensive size and rich textual data enable researchers to develop more contextually rich and diverse text outputs and improve recommendation systems.; The Amazon Reviews Dataset is primarily used to train models for personalized text generation, leveraging 150 million user reviews and ratings. It enhances recommendation justifications by capturing diverse user preferences and feedback. Additionally, the dataset provides sentiment and opinion data, which is crucial for training models to understand and generate contextually appropriate and personalized content.	Amazon Reviews Dataset	
citing_context	amazon.com user reviews	https://www.semanticscholar.org/paper/28a70871ef06acda945b95ac6e4a7a5fbe58528d (2014)	https://doi.org/10.1145/1341531.1341560 (2008)	The 'amazon.com user reviews' dataset is used to study user review patterns and sentiment, particularly focusing on product feedback. It is employed in research aimed at understanding and generating personalized text. The dataset's rich textual content and diverse product categories enable detailed analysis of user sentiments and feedback, facilitating the development of more nuanced and contextually appropriate text generation models.		
cited_context | citing_context	Avocado Research Email Collection	https://doi.org/10.1145/3589334.3645408 (2023), https://doi.org/10.48550/arXiv.2308.07968 (2023), https://doi.org/10.48550/arXiv.2308.07968 (2023)		The Avocado Research Email Collection is used for generating personalized emails and studying email communication patterns in an IT company. It focuses on creating contextually appropriate and user-specific email content through content and attachment analysis, enabling research in personalized text generation and communication behavior.; The Avocado Research Email Collection is used to study email communication patterns within an IT company, specifically focusing on personalization in email content and attachment usage. Researchers analyze the dataset to understand how personalization affects communication dynamics, employing methods that examine the frequency and context of personalized elements and attachments. This enables insights into effective communication strategies in professional settings.	Avocado Research Email Collection	
citing_context	b5 corpus	https://doi.org/10.1109/ICCCNT61001.2024.10724424 (2024)	https://www.semanticscholar.org/paper/547a6c2a15f8b40c015c7515abf3680a5df74d85 (2018)	The b5 corpus is used to study personality-dependent natural language understanding and generation. It includes controlled and free textbooks with diverse communicative tasks and author personality data. Researchers employ this dataset to explore how personality traits influence language use and to develop models that can generate text reflecting specific personality characteristics.		
cited_context	BABEL	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://doi.org/10.1109/CVPR46437.2021.00078 (2021)	The BABEL dataset is used to train the MotionDiffuse model, which integrates motion data with English labels to enhance personalized text generation. It focuses on bodies, actions, and behaviors, enabling researchers to explore the relationship between physical movements and textual content. This dataset facilitates the development of more contextually rich and dynamic text generation models by incorporating detailed motion data.		
cited_context	Baidu PersonaChat	https://doi.org/10.48550/arXiv.2305.11482 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Baidu PersonaChat dataset is used to fine-tune a pre-trained RoBERTa model for dialogue natural language inference, specifically to enhance consistency and coherence in dialogues. This application leverages the dataset's dialogue structures to improve the model's ability to understand and generate contextually appropriate responses.		
cited_context | citing_context	Beauty Dataset	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021), https://doi.org/10.1145/3626772.3657821 (2024), https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.3115/1073083.1073135 (2002)	The Beauty dataset is used to evaluate the performance of text generation models on beauty-related content. Research focuses on measuring the effectiveness of generated sentences using metrics like ROUGE and BLEU scores. Additionally, the dataset is employed to assess personalized text generation models, considering user preferences in beauty-related contexts. This enables researchers to refine models for more tailored and contextually appropriate text outputs.; The Beauty dataset is used to evaluate the performance of text generation models on beauty-related content. Researchers focus on measuring the quality of generated sentences using ROUGE scores, which assess the overlap between machine-generated and reference texts. This dataset enables the assessment of model effectiveness in generating coherent and contextually appropriate beauty-related text.	Beauty dataset	
citing_context	BeerAdvocate	https://doi.org/10.1145/3404835.3462939 (2021)	https://doi.org/10.1109/ICDM.2012.110 (2012)	The BeerAdvocate dataset is used to generate personalized beer reviews by leveraging user-specific attributes and attitudes extracted from multi-aspect reviews. This involves analyzing detailed review data to capture individual preferences and sentiments, enabling the creation of tailored content that reflects users' unique perspectives and experiences with different beers.		
citing_context	BookCorpus	https://doi.org/10.54254/2755-2721/97/20241406 (2024)	https://doi.org/10.1109/ICCV.2015.11 (2015)	The BookCorpus dataset is used to train models in understanding narrative structures and generating story-like text. It provides extensive textual content that enables researchers to develop and evaluate algorithms capable of producing coherent and contextually appropriate narratives. This dataset supports research focused on enhancing the narrative coherence and creativity of generated text, making it valuable for studies in natural language processing and computational creativity.		
cited_context	C HAT dataset	https://doi.org/10.18653/v1/2020.acl-main.131 (2020)	https://doi.org/10.18653/v1/P18-1205 (2018)	The C HAT dataset is used to train and evaluate personalized dialogue agents, particularly in multi-turn dialogues conditioned on personas. It supports research into conversational AI with personalization, focusing on both automatic metrics and human evaluations to assess the performance and superiority of personalized dialogue systems over baseline models.		
cited_context	C HAT unique	https://doi.org/10.18653/v1/2020.acl-main.131 (2020)	https://doi.org/10.18653/v1/P18-1205 (2018)	The C HAT unique dataset is used to train chatbots with configurable and persistent personalities. It focuses on the explicit description of interlocutor personas using profile sentences, enabling researchers to develop chatbots that can maintain consistent and distinct personality traits throughout interactions. This dataset supports the creation of more engaging and human-like conversational agents by providing structured persona data.		
cited_context	CelebA	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The CelebA dataset is primarily used to train models on face attributes, focusing on 40 distinct appearance features. This training enhances the capabilities of models in understanding and generating personalized text related to facial characteristics. The dataset's rich attribute annotations enable researchers to develop and refine algorithms that can accurately recognize and describe facial features, thereby improving personalized text generation applications.		
cited_context | citing_context	Celeba-Hq	https://doi.org/10.48550/arXiv.2309.05793 (2023), https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52 (2017), https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The CelebA-HQ dataset is used to fine-tune models on high-quality celebrity images, enhancing the visual fidelity and realism in personalized text generation. This dataset's high-resolution images enable researchers to improve the quality of generated content, focusing on the integration of realistic visual elements into text.; The CelebA-HQ dataset is primarily used to test personalized text generation models, specifically focusing on high-quality celebrity images. Researchers employ this dataset to evaluate the visual-textual alignment, ensuring that generated text accurately reflects the visual content of the images. This application leverages the dataset's high-resolution and diverse celebrity images to enhance the robustness and accuracy of visual-textual models.	CelebA-HQ	
cited_context	CelebA-HQ-256	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The CelebA-HQ-256 dataset is primarily used to pretrain denoising diffusion probabilistic models, specifically focusing on high-resolution images of celebrity faces. This pretraining enhances the capabilities of models in generating personalized text, leveraging the high-quality and diverse facial images to improve text generation accuracy and relevance.		
cited_context | citing_context	Celebamask-Hq	https://doi.org/10.48550/arXiv.2312.06116 (2023)	https://doi.org/10.1109/cvpr42600.2020.00559 (2019)	The CelebAMask-HQ dataset is used to enhance personalized text-to-image generation by providing high-quality, well-curated facial image data. It is utilized to train and fine-tune models, particularly the DTI module, by supplying foreground-masked identity images and rich meta-annotations. This enables the generation of textual embeddings for facial image manipulation and improves the evaluation of personalized text-to-image systems.; The CelebAMask-HQ dataset is used to enhance personalized text-to-image generation by providing high-quality, well-curated facial image data. It supports the training and fine-tuning of models, particularly the DTI module, by supplying foreground-masked identity images. The dataset's rich meta-annotations enable the selection of 400 unique human identities, which are paired with prompts to improve the evaluation of personalized text-to-image systems.	CelebAMask-HQ	
cited_context	CelebV-HQ	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.48550/arXiv.2207.12393 (2022)	The CelebV-HQ dataset is used to enhance personalized text generation models by providing high-quality face video data with detailed annotations of facial attributes, including appearance, movement, and emotion. It includes timestamps of dynamic facial attributes, which support the temporal aspect of these models, enabling more nuanced and contextually accurate text generation.		
cited_context	CelebV-Text	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.48550/arXiv.2207.12393 (2022)	The CelebV-Text dataset is used to generate text descriptions from facial emotions in videos, demonstrating improvements in linguistic naturalness and complexity through grammar structures and synonym replacement. It is also used to compare with other face video datasets, focusing on the representation and scale of facial attributes, and to highlight the diversity, complexity, and naturalness of texts, emphasizing the quality and characteristics of the textual content.		
cited_context	Chinese Weibo	https://doi.org/10.48550/arXiv.2210.08753 (2022)	https://doi.org/10.18653/V1/2020.ACL-DEMOS.30 (2019)	The Chinese Weibo dataset is used to conduct experiments on personalized text generation, specifically focusing on social media content from a Chinese platform. Researchers employ this dataset to develop and test models that can generate text tailored to individual users, leveraging the unique linguistic and cultural nuances present in the Weibo data. This enables the exploration of how personalized content can be effectively created and adapted for Chinese social media users.		
cited_context | citing_context	Citation Network Dataset (V14)	https://doi.org/10.48550/arXiv.2407.11016 (2024)	https://doi.org/10.1145/1401890.1402008 (2008)	The Citation Network Dataset (V14) is primarily used to generate data samples for personalized text generation, leveraging its extensive collection of 5,259,858 papers and 29 features per paper. Researchers focus on the structure and content of academic citations to enhance the comprehensiveness and relevance of generated text. This dataset enables the creation of more accurate and contextually appropriate personalized text by providing rich, structured data on academic citations.; The Citation Network Dataset (V14) is used to generate data samples for personalized text generation, utilizing 5,259,858 papers and 29 features per paper. This comprehensive dataset enables researchers to create robust models by providing a rich set of attributes for each paper, enhancing the accuracy and relevance of generated text.	Citation Network Dataset (V14)	
citing_context	CMCC	https://doi.org/10.48550/arXiv.2502.08972 (2025)	https://www.semanticscholar.org/paper/2abe6b9ea1b13653b7384e9c8ef14b0d87e20cfc (2004)	The CMCC dataset is used for analyzing emails and essays on controversial topics written by students, focusing on authorship and content analysis. Researchers employ methodologies that involve examining the textual content and stylistic features to understand authorship attributes and thematic elements. This dataset enables detailed studies into how students express opinions and arguments on contentious issues, providing insights into educational and psychological aspects of writing.		
citing_context	CMU DoG	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.18653/v1/D18-1076 (2018)	The CMU DoG dataset is used to train and evaluate document-grounded conversation models, focusing on generating contextually accurate responses based on specified documents, particularly Wikipedia articles about popular movies. This dataset enables researchers to enhance conversational context and accuracy by providing a rich source of grounded information, facilitating the development of more informed and relevant conversational agents.		
citing_context	CMU DoG 23	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.18653/v1/D18-1076 (2018)	The CMU DoG 23 dataset is used to train conversational agents that integrate factual information from documents about popular movies into dialogues. This involves methodologies focused on grounding conversations in specific, factual content to enhance the agents' ability to engage in informed discussions. The dataset's relevance lies in its structured movie-related data, enabling researchers to develop and evaluate conversational systems that can effectively incorporate and discuss factual information.		
cited_context	CODAH	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The CODAH dataset is primarily used for commonsense reasoning in dialogues, focusing on generating and evaluating coherent, contextually appropriate, and culturally aware responses. It is employed in tasks such as detecting and generating commonsense knowledge, evaluating dialogue systems, and predicting plausible continuations of scenarios. This dataset enables researchers to assess and improve the contextual understanding and coherence of conversational agents.		
cited_context | citing_context	Common Crawl	https://doi.org/10.54254/2755-2721/97/20241406 (2024), https://doi.org/10.48550/arXiv.2308.07968 (2023), https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.1109/ICCV.2015.11 (2015), https://doi.org/10.3115/v1/D14-1162 (2014)	The Common Crawl dataset is used to provide a diverse and complex array of web-based textual data, enhancing the richness of training data. It is also employed to train GloVe word embeddings, leveraging co-occurrence statistics to capture semantic and syntactic word relationships. This dataset enables researchers to improve the quality and breadth of natural language processing models by incorporating a wide range of textual content.; The Common Crawl dataset is primarily used as a source for pretrained GloVe embeddings, which focus on word representation in natural language processing. This dataset enables researchers to develop and refine word embeddings, enhancing the accuracy and effectiveness of NLP models by providing a large corpus of web text.	Common Crawl	
citing_context	commonsense conversation dataset	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.24963/ijcai.2018/643 (2018)	The commonsense conversation dataset is used to generate commonsense-aware conversations, specifically focusing on one-turn post-response pairs. Researchers employ this dataset to incorporate associated knowledge graphs, enhancing the contextual relevance and coherence of conversational responses. This approach addresses the challenge of integrating commonsense knowledge into conversational systems, enabling more natural and contextually appropriate interactions.		
cited_context | citing_context	Convai2	https://doi.org/10.1109/ICKECS56523.2022.10059789 (2022), https://doi.org/10.48550/arXiv.2305.11482 (2023)	https://doi.org/10.1007/978-3-030-29135-8_7 (2019), https://doi.org/10.18653/v1/P19-1363 (2018)	The ConvAI2 dataset is used to train Seq2Seq models for generating textual responses, specifically aiming to enhance conversational intelligence and personalization in AI-generated dialogues. This dataset enables researchers to focus on improving the quality and naturalness of machine-generated conversations by providing a rich set of human-human interactions.; The ConvAI2 dataset is primarily used for fine-tuning pre-trained RoBERTa models to enhance dialogue natural language inference, focusing on consistency and coherence in dialogues. It is also utilized to generate personalized dialogues by incorporating rich personal information, enabling character-based conversations with specific personal facts. This dataset supports research in improving the quality and personalization of conversational agents.	ConvAI2	
cited_context	ConvAI2 personalized dialogue corpus	https://doi.org/10.5220/0010812500003116 (2021)	https://doi.org/10.1007/978-3-030-29135-8_7 (2019)	The ConvAI2 personalized dialogue corpus is used to evaluate personalized dialogue systems, specifically focusing on their ability to generate contextually appropriate and user-specific responses. Researchers employ this dataset to assess the performance of dialogue models in maintaining coherent and personalized conversations, leveraging its annotated dialogues to measure contextual relevance and personalization accuracy.		
cited_context	CustomConcept101	https://doi.org/10.48550/arXiv.2402.09368 (2024)	https://doi.org/10.1109/CVPR52729.2023.00192 (2022)	The CustomConcept101 dataset is used to validate the VCD framework by providing a diverse representation of animals and objects. It supports multi-concept customization in text-to-image diffusion models, enabling researchers to test and refine the framework's ability to generate images based on complex textual inputs. This dataset's diversity and multi-concept nature are crucial for enhancing the robustness and versatility of text-to-image models.		
cited_context	D4	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.48550/arXiv.2205.11764 (2022)	Dataset D4 is re-organized for depression-diagnosis-oriented chat, specifically focusing on the emotional status of users in Chinese dialogues. It is used to identify empathic comfort utterances within these dialogues. The dataset enables researchers to analyze and understand the nuances of emotional support in mental health contexts, facilitating the development of more effective chatbot interventions.		
citing_context	DailyDialog	https://www.semanticscholar.org/paper/3708ed2aa909ebde32647257f5ffb3c6548b0e8d (2021)	https://www.semanticscholar.org/paper/3108f96f80d129036f53684344f4058257b37c4b (2017)	The DailyDialog dataset is used to train and evaluate personalized text generation models, particularly for multi-turn empathetic conversations. It enables researchers to develop and assess models that can generate contextually appropriate and emotionally responsive dialogues, enhancing the naturalness and effectiveness of conversational agents.		
citing_context	data from user profiles	https://doi.org/10.54254/2755-2721/107/20241355 (2024)	https://doi.org/10.48550/arXiv.2402.05133 (2024)	The dataset from user profiles is used to enhance personalized model performance by integrating user-specific representations, focusing on improving model accuracy. This involves methodologies that incorporate user data to refine and tailor model outputs, specifically addressing the need for more accurate and personalized results. The dataset's user-specific data is crucial for these enhancements.		
citing_context	Datafiniti Product Database on Grammar and Online Product Reviews	https://doi.org/10.48550/arXiv.2501.02157 (2025)		The Datafiniti Product Database on Grammar and Online Product Reviews is used to analyze grammar and sentiment in online product reviews. Researchers focus on personalized text generation techniques and user-specific language patterns, employing methodologies that examine how language varies across different users. This dataset enables detailed analysis of linguistic features and their application in generating personalized text.		
citing_context	Datafiniti Products dataset	https://doi.org/10.48550/arXiv.2501.02157 (2025)		The Datafiniti Products dataset is primarily used as a source for constructing new datasets, specifically focusing on product information and attributes. This dataset enables researchers to compile detailed product data, which is then utilized for developing and training models in personalized text generation. The dataset's rich attribute information facilitates the creation of more contextually relevant and personalized text outputs.		
cited_context	DeepFashion	https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.1109/CVPR.2016.124 (2016)	The DeepFashion dataset is primarily used to enhance clothes recognition and retrieval through rich annotations. It supports personalized text generation for fashion-related content by providing robust data for training models to recognize and describe clothing items accurately. This dataset enables researchers to develop more effective and contextually relevant fashion recommendation systems and content generation tools.		
cited_context | citing_context	Dialog Nli Dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Dialog NLI dataset is used to train sequence classification and NLI models for evaluating persona consistency and natural language inference in dialogue contexts. Specifically, it helps assess the consistency between user comments and generated sentences, ensuring that generated responses align with the user's persona. This dataset enables researchers to improve the coherence and relevance of generated dialogues by providing annotated data that captures the nuances of natural language interactions.; The Dialog NLI dataset is used to train sequence classification and NLI models for evaluating persona consistency between user comments and generated sentences. It focuses on natural language inference annotations in dialogues, enabling researchers to assess the consistency and coherence of generated text in conversational contexts.	Dialog NLI dataset	
citing_context	Dialogue Natural Language Inference (NLI) dataset	https://doi.org/10.18653/v1/2023.acl-long.544 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Dialogue Natural Language Inference (NLI) dataset is primarily used for training models to understand and generate dialogue, focusing on natural language inference tasks. It serves as a foundational resource, enabling researchers to develop models that can accurately infer relationships between utterances in dialogues. This dataset supports the enhancement of dialogue understanding and generation capabilities, crucial for advancing conversational AI systems.		
citing_context	Dialogue NLI	https://doi.org/10.18653/v1/2023.acl-long.544 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Dialogue NLI dataset is used to construct the PersonaExt dataset, which provides natural language inference examples in dialogue contexts. This supports research in personalized text generation by offering structured data that enhances the understanding and generation of contextually appropriate and personalized responses.		
citing_context	DOC	https://doi.org/10.18653/v1/2024.emnlp-main.737 (2023)	https://doi.org/10.18653/v1/2023.acl-long.190 (2023)	The DOC dataset is used to enhance the coherence and narrative structure of long stories by providing detailed outlines. Researchers employ this dataset to control and improve the flow of narratives, focusing on how detailed outlines can guide the generation of more coherent and structured stories. This approach addresses the challenge of maintaining narrative consistency over longer texts.		
citing_context	Dress Code	https://doi.org/10.48550/arXiv.2504.14011 (2025)	https://doi.org/10.1109/CVPRW56347.2022.00243 (2022)	The 'Dress Code' dataset is used to conduct experiments on virtual try-on, specifically focusing on high-resolution multi-category image pairs. This dataset enhances personalized text generation by providing detailed visual data, enabling researchers to explore and improve the integration of visual and textual information in virtual fashion applications.		
cited_context	DUTIR	https://doi.org/10.1145/3613904.3642899 (2024)	https://doi.org/10.18653/v1/D17-1059 (2017)	The DUTIR dataset is used to extract and tally emotional tokens in Chinese texts, focusing on the nuances of indirect emotional expressions in Eastern cultures. It is specifically applied in the context of personalized text generation, where the dataset helps researchers understand and model subtle emotional cues in text. This enables more culturally sensitive and emotionally nuanced text generation systems.		
citing_context	E2E	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025)	https://doi.org/10.3115/1687878.1687893 (2009)	The E2E dataset is used for generating restaurant reviews, addressing the challenges in data-to-document generation within the restaurant domain. Researchers employ this dataset to develop and evaluate models that can transform structured data into coherent and contextually appropriate text. This enables the exploration of natural language generation techniques, focusing on the accuracy and fluency of generated reviews.		
cited_context	ED	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The ED dataset is used to evaluate AI models' performance in generating empathetic dialogues for text-based mental health support. Research focuses on assessing the models' ability to express empathy, using the dataset to test and refine empathetic dialogue generation techniques. This enables researchers to improve the quality and effectiveness of AI-driven mental health support systems.		
cited_context | citing_context	Electronics Dataset	https://www.semanticscholar.org/paper/fb394896bf1b31183839c766afc62dd251a7b9b7 (2021)	https://doi.org/10.3115/1073083.1073135 (2002)	The Electronics dataset is used to evaluate the performance of text generation models on electronics-related content. Researchers focus on measuring the quality of generated sentences using ROUGE scores, which assess the overlap between machine-generated and reference texts. This dataset enables the assessment of model accuracy and coherence in generating electronics-specific text.; The Electronics dataset is used to evaluate the performance of text generation models on electronics-related content. Researchers focus on measuring the quality of generated sentences using ROUGE scores, which assess the overlap between machine-generated and reference texts. This dataset enables the assessment of model accuracy and coherence in generating technical and product-related text.	Electronics dataset	
cited_context	ELI5	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The ELI5 dataset is used to train and evaluate models on generating explanatory answers, particularly focusing on simplifying complex concepts and ensuring clarity and detail in responses. This dataset, derived from the Explain Like I'm 5 subreddit, is utilized in research to enhance the ability of models to produce clear and understandable explanations, making it valuable for improving the comprehensibility of automated responses in educational and informational contexts.		
cited_context	EMH	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The EMH dataset is used to evaluate the performance of AI models, particularly large language models (LLMs), in generating empathetic and appropriate responses for mental health support. Research focuses on comparing different models (e.g., Vicuna, ChatGPT) and selection methods (top-1 vs. random) to assess the quality, helpfulness, and empathy of text-based mental health support. The dataset enables researchers to systematically analyze and improve the effectiveness of AI in providing emotional and mental health assistance.		
citing_context	EmotionLines	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://www.semanticscholar.org/paper/ba4c923b43360325cba984549aa3c3224863d1f6 (2018)	The EmotionLines dataset is used to collect and analyze emotional dialogues from telescripts and Facebook, with a focus on multi-party conversations. It enhances personalized text generation by providing a rich source of emotional interactions, enabling researchers to develop more nuanced and contextually appropriate dialogue systems.		
cited_context	Empa	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.48550/arXiv.2205.11764 (2022)	The Empa dataset is used to re-organize empathetic dialogues, focusing on the user's emotional status. While specific methodological details are not provided, the dataset enables researchers to address how empathetic responses can be structured in dialogue systems. This supports the development of more emotionally attuned conversational agents.		
citing_context	empathetic dataset	https://www.semanticscholar.org/paper/3708ed2aa909ebde32647257f5ffb3c6548b0e8d (2021)	https://doi.org/10.18653/v1/W17-0906 (2017)	The empathetic dataset is used to evaluate models' performance in generating empathetic responses, with a focus on emotional understanding and personalized interaction in dialogue systems. Researchers employ this dataset to assess how well models can produce contextually appropriate and emotionally sensitive replies, enhancing the quality of human-computer interactions.		
cited_context	EmpatheticDialogues	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/P19-1534 (2018)	The EmpatheticDialogues dataset is used to evaluate empathetic open-domain conversation models, specifically focusing on the test set to assess model performance in generating empathetic responses. This dataset enables researchers to measure how well models can produce emotionally appropriate and contextually relevant dialogues, emphasizing the importance of empathy in conversational AI.		
cited_context	En-Es	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The UN data from WMT (En-De, En-Es, En-Fr) is primarily used to train models for personalized text generation, specifically focusing on multilingual United Nations documents. This dataset enables researchers to develop and refine algorithms that can generate text tailored to individual users, leveraging the rich linguistic diversity and formal structure of UN documents.		
cited_context	En-Fr)	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The UN data from WMT (En-De, En-Es, En-Fr) is primarily used to train models for personalized text generation, specifically focusing on multilingual United Nations documents. This dataset enables researchers to develop and refine algorithms that can generate text tailored to individual users, leveraging the rich linguistic diversity and formal structure of UN documents.		
cited_context	English Reddit	https://doi.org/10.48550/arXiv.2210.08753 (2022)	https://doi.org/10.18653/V1/2020.ACL-DEMOS.30 (2019)	The English Reddit dataset is used to conduct experiments on personalized text generation, specifically focusing on social media content from an English-speaking platform. Researchers employ this dataset to explore and develop models that can generate text tailored to individual users, leveraging the rich and diverse content available on Reddit. This dataset enables the examination of user-specific language patterns and preferences, enhancing the personalization of generated text.		
citing_context	English Wikipedia	https://doi.org/10.54254/2755-2721/97/20241406 (2024)	https://doi.org/10.1109/ICCV.2015.11 (2015)	The English Wikipedia dataset is utilized as a vast repository of factual information and diverse writing styles to enhance the breadth and depth of generated text. It provides a rich source of content that improves the quality and variety of text generation, supporting research in natural language processing and machine learning models.		
cited_context	Europarl	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The Europarl dataset is primarily used to train models for personalized text generation, with a focus on multilingual parliamentary proceedings. It enables researchers to develop and evaluate models that can generate text reflecting the style and content of parliamentary debates in multiple languages. The dataset's extensive multilingual corpus is crucial for training these models effectively.		
cited_context	Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages	https://doi.org/10.18653/v1/P18-1080 (2018)	https://www.semanticscholar.org/paper/42d3c35a2a10a71b8dddb9fde15a3fee245c2bcc (2018)	The dataset of Facebook posts from all 412 current members of the United States Senate and House is used to analyze top-level comments, focusing on public engagement and response patterns. Researchers employ this dataset to study how the public interacts with political figures, examining comment content and engagement metrics. This analysis helps understand the dynamics of online political communication and public sentiment.		
citing_context	Facescape	https://doi.org/10.1145/3664647.3680861 (2024)		The Facescape dataset is used to provide high-resolution 3D facial UV-texture data, which is specifically employed to enhance personalized text generation. This dataset enables researchers to focus on detailed facial textures, improving the realism and personalization of generated text. The high-resolution characteristic of the dataset is crucial for achieving fine-grained detail in facial representations, thereby enhancing the quality of personalized text outputs.		
citing_context	Fairface	https://doi.org/10.48550/arXiv.2309.05793 (2023)	https://www.semanticscholar.org/paper/744fe47157477235032f7bb3777800f9f2f45e52 (2017)	The Fairface dataset is used to fine-tune models for balanced representation of race, gender, and age attributes, enhancing fairness in machine learning applications. It is specifically employed to improve the fairness of personalized text generation models by ensuring balanced and representative data, addressing issues of bias and underrepresentation in these models.		
citing_context	FFHQ-UV	https://doi.org/10.1145/3664647.3680861 (2024)		The FFHQ-UV dataset is used to provide high-resolution 3D facial UV-texture data, specifically for enhancing personalized text generation. It focuses on detailed facial textures, which are crucial for improving the realism and personalization of generated text. This dataset enables researchers to develop more sophisticated models that can incorporate fine-grained facial details, thereby enhancing the overall quality and personalization of text outputs.		
cited_context	Flicker30K	https://doi.org/10.1109/ICASSP48485.2024.10447048 (2024)	https://doi.org/10.1162/tacl_a_00166 (2014)	The Flicker30K dataset is used to enhance the robustness of personalized text generation models by providing a diverse set of images with captions. It serves as a regularization tool, improving model performance through exposure to varied image-caption pairs. This dataset enables researchers to address challenges in generating coherent and contextually appropriate text by leveraging its rich and diverse content.		
cited_context	FoCus	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The FoCus dataset is used in research focused on conversational information-seeking scenarios, specifically analyzing user queries and information retrieval within personalized dialogue systems. It enables researchers to study how users formulate queries and seek information in interactive dialogues, enhancing the understanding of user behavior and improving the effectiveness of personalized dialogue systems.		
cited_context | citing_context	Forumsum	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/N19-1260 (2018)	The ForumSum dataset is utilized in research to generate summaries from forum discussions, focusing on capturing key points in user-generated content. This involves employing natural language processing techniques to distill essential information, enabling researchers to address the challenge of summarizing diverse and unstructured online conversations effectively.; The ForumSum dataset is used to summarize forum discussions, focusing on the effectiveness of multi-level memory networks in capturing conversational nuances. This involves applying these networks to analyze and condense lengthy forum threads, enabling researchers to evaluate how well such models can preserve the context and subtleties of online conversations.	ForumSum	
citing_context	Gender	https://doi.org/10.1109/ICKECS56523.2022.10059789 (2022)	https://doi.org/10.18653/v1/P18-1080 (2018)	The 'Gender' dataset is used for personal style transfer experiments, specifically to alter gender-related language in text. This involves methodologies that manipulate textual attributes to reflect different gender identities, enabling research into the nuances of gender expression in language. The dataset facilitates these experiments by providing a basis for testing and validating style transfer techniques.		
cited_context	GoEmotions	https://doi.org/10.1109/ICDMW65004.2024.00071 (2024)	https://doi.org/10.18653/v1/2020.acl-main.372 (2020)	The GoEmotions dataset is primarily used for fine-grained emotion classification and recognition in text. Researchers employ it to compare model performance, evaluate personalized fine-tuning methods, and study the challenges of capturing subtle emotional nuances. The dataset's wide range of emotion labels enables detailed analysis and enhances the evaluation of models in recognizing nuanced emotional expressions.		
citing_context	Google Web Trillion Word Corpus	https://doi.org/10.18653/v1/W19-8634 (2019)		The Google Web Trillion Word Corpus is used to sort words by frequency, which supports the methodology of grouping words for personalized text generation. This dataset enables researchers to systematically organize and analyze word frequencies, facilitating more nuanced and contextually appropriate text generation models.		
citing_context	Grammar and Online Product dataset	https://doi.org/10.48550/arXiv.2501.02157 (2025)		The Grammar and Online Product dataset is used to study stylistic variation across multiple platforms and domains. Researchers employ this dataset to analyze how platform and domain influence text style, using it to explore the impact of these factors on linguistic and stylistic choices in online content. This dataset enables a comparative analysis of text styles, providing insights into how different contexts shape language use.		
cited_context	HellaSWAG	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The HellaSWAG dataset is primarily used for next situation prediction, extending the SWAG dataset with more complex and diverse scenarios. It assesses models' commonsense reasoning by evaluating their ability to generate plausible continuations of given contexts. This dataset is employed to test and improve the text generation capabilities of models, particularly in understanding and predicting the most likely outcomes in complex scenarios.		
citing_context	Helpsteer	https://www.semanticscholar.org/paper/831b89a50ce08af10b1708cbcd841fb7fee48d7b (2025)	https://doi.org/10.18653/v1/2022.acl-long.229 (2021)	The Helpstead dataset is used to evaluate multi-attribute helpfulness in language models, specifically focusing on steering these models to generate more useful and relevant responses. This involves assessing the effectiveness of language models in producing content that is perceived as helpful across various attributes. The dataset enables researchers to test and improve the utility and relevance of model-generated text, enhancing its practical applicability.		
cited_context | citing_context	Helpsteer2	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The HelpSteer2 dataset is utilized in the development of a personalized reward model, focusing on enhancing user guidance and interaction. This involves employing methodologies that emphasize user feedback and engagement to refine the reward system. The dataset enables researchers to address specific research questions related to improving user experience and interaction in personalized systems.	HelpSteer2	
citing_context	Home	https://doi.org/10.1145/3404835.3462854 (2021)	https://doi.org/10.1609/AAAI.V33I01.33016690 (2019)	The 'Home' dataset is used to evaluate the limitations of the USN model in personalized review summarization. Researchers employ this dataset to identify and analyze areas where the model's performance is suboptimal, focusing on specific challenges in generating accurate and contextually relevant summaries. This dataset enables a detailed assessment of model weaknesses, facilitating targeted improvements in personalized text generation.		
cited_context | citing_context	Huffington Post Articles	https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.48550/arXiv.2209.11429 (2022)	The Huffington Post articles dataset is used to create a categorized dataset for personalized text generation, specifically focusing on classifying news articles into various topics. This involves employing methodologies that categorize and label articles, enabling research into how different topics can be identified and utilized in personalized text generation systems.; The Huffington Post articles dataset is used to create a categorized dataset for personalized text generation, specifically focusing on classifying news articles into distinct topics. This involves employing methodologies that categorize and label articles, enabling research into how personalized content can be generated based on topic-specific data.	Huffington Post articles	
citing_context	IAM On-Line Handwriting Database	https://doi.org/10.1109/SMC53654.2022.9945138 (2022)	https://doi.org/10.1109/ICDAR.2005.132 (2005)	The IAM On-Line Handwriting Database is used to train a conditional recurrent variational autoencoder (C-RVAE) for generating personalized digital ink. This focuses on handwriting recognition and synthesis, enabling researchers to develop models that can accurately recognize and synthesize individual handwriting styles. The dataset's detailed stroke information and large sample size facilitate robust training and testing of these models.		
citing_context	IAMOnDB	https://doi.org/10.1109/SMC53654.2022.9945138 (2022)	https://doi.org/10.1109/ICDAR.2005.132 (2005)	The IAMOnDB dataset is used to train a conditional recurrent variational autoencoder (C-RVAE) for generating personalized digital ink, specifically focusing on handwriting recognition and synthesis. This methodology enables researchers to explore the nuances of individual writing styles, enhancing the accuracy and realism of synthesized handwriting. The dataset's detailed digital ink recordings are crucial for capturing the dynamic and stylistic elements of handwriting, facilitating advancements in personalized text generation and recognition systems.		
cited_context	ImageNet	https://doi.org/10.48550/arXiv.2403.03206 (2024)	https://doi.org/10.1007/s11263-015-0816-y (2014)	"The ImageNet dataset is primarily used for enhancing text-to-image models by converting images into a format that includes captions like ""a photo of a 〈 class name 〉."" This conversion enriches the dataset, making it suitable for personalized text generation tasks. The dataset's extensive class labels and high-quality images enable researchers to train models that generate more accurate and contextually relevant images from textual descriptions."		
cited_context | citing_context	Imdb	https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.18653/v1/2022.naacl-main.252 (2021)	https://doi.org/10.18653/v1/2021.findings-acl.129 (2021)	The IMDB dataset is primarily used for personalized sentiment prediction, focusing on user-specific sentiment patterns in movie reviews. Researchers employ this dataset to explore and predict personalized product ratings and sentiments, utilizing publicly available movie review data. The dataset's rich textual content and user-specific annotations enable detailed analysis and modeling of individual sentiment variations.; The IMDB dataset is primarily used for personalized sentiment prediction and sentiment analysis, focusing on user-specific sentiment patterns in movie reviews. Researchers employ it to explore personalized product rating predictions and conduct comparison and ablation studies, evaluating methods like UserAdapter for performance on movie reviews. This dataset enables detailed analysis of individual user sentiments, enhancing the accuracy of sentiment prediction models.	IMDB	
cited_context	IMDB movie reviews	https://www.semanticscholar.org/paper/e04a80263d252a3d8a382ba37a249b9345620570 (2019)	https://www.semanticscholar.org/paper/1c61f9ef06fe74505775a833ff849185757199e7 (2011)	The IMDB movie reviews dataset is primarily used for training and evaluating sentiment classifiers, focusing on sentiment analysis in movie reviews. Researchers employ this dataset to assess the performance of different methods in controlling sentiment attributes, using an external sentiment classifier to evaluate attribute control. This enables the development and refinement of techniques for managing sentiment in text data.		
cited_context | citing_context	Imdb62	https://doi.org/10.48550/arXiv.2402.04914 (2024)	https://doi.org/10.1162/COLI_a_00173 (2014)	The IMDb62 dataset is used for authorship attribution studies, focusing on 62,000 movie reviews from 62 prolific IMDb users, each contributing 1,000 reviews. Researchers employ this dataset to analyze writing styles and patterns, enabling the development and testing of algorithms that can accurately attribute authorship based on textual features. This dataset facilitates the exploration of linguistic characteristics and the robustness of attribution models in large-scale, diverse review data.; The IMDb62 dataset is used to study authorship attribution by analyzing the writing styles of 62 prolific IMDb reviewers, each of whom has written 1,000 movie reviews. Topic models are employed to capture and compare stylistic elements, enabling researchers to address questions related to author identification and stylistic consistency. The large, consistent review count per author enhances the reliability of the analysis.	IMDb62	
citing_context	Instruct-QA	https://doi.org/10.48550/arXiv.2504.02867 (2025)	https://doi.org/10.18653/v1/P19-1612 (2019)	The Instruct-QA dataset is used to evaluate and train models on three different information-seeking QA tasks, including open-domain QA. It encompasses diverse question types, enabling researchers to assess model performance across various query complexities. This dataset facilitates the development of more robust and versatile QA systems by providing a rich set of questions for training and evaluation.		
cited_context	KIT Motion-Language MoCap dataset	https://doi.org/10.1109/TPAMI.2024.3355414 (2022)	https://www.semanticscholar.org/paper/d92514b811d767369c61c66e71aa66c639f6d158 (2020)	The KIT Motion-Language MoCap dataset is used to synthesize human motions from textual descriptions, emphasizing the diversity and quality of generated motions. Researchers employ the TEMOS model to achieve this, focusing on accurate motion synthesis. This dataset enables the evaluation of motion generation models by providing paired text and motion data, facilitating advancements in natural and realistic motion synthesis from text.		
citing_context	Knowledge-based dataset	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.24963/ijcai.2018/643 (2018)	The Knowledge-based dataset is used to train models for generating commonsense-aware conversations, specifically focusing on one-turn post-response pairs. It incorporates associated knowledge graphs to enhance the contextual relevance and coherence of the generated responses. This dataset enables researchers to develop more sophisticated conversational agents by providing structured knowledge that informs the model's understanding and generation processes.		
citing_context	L A MP-C AP	https://doi.org/10.48550/arXiv.2506.06561 (2025)		The L A MP-C AP dataset is used to evaluate the performance of large language models (LLMs) in generating personalized captions. Specifically, it focuses on assessing the models' ability to tailor captions to individual users. This dataset enables researchers to compare and analyze the effectiveness of different LLMs in personalized text generation tasks, providing insights into user-specific content creation.		
citing_context	L2-ARCTIC dataset	https://doi.org/10.48550/arXiv.2410.13342 (2024)	https://doi.org/10.21437/interspeech.2019-2441 (2019)	The L2-ARCTIC dataset is used for training text-to-speech (TTS) models, particularly focusing on non-native speakers to enhance cross-lingual synthesis. This dataset enables researchers to develop TTS systems that better accommodate and improve the speech quality for individuals speaking a second language, addressing the specific challenges and nuances of non-native pronunciation and intonation.		
cited_context	LAION-400M	https://doi.org/10.1109/CVPRW63382.2024.00100 (2024)	https://doi.org/10.48550/arXiv.2303.09319 (2023)	The LAION-400M dataset is used to train multimodal latent diffusion models that combine text and image inputs. This approach focuses on generating images conditioned on both textual and visual subjects, enabling research in joint subject and text conditional image generation. The dataset's large-scale multimodal content facilitates the development of models capable of producing high-quality, contextually relevant images.		
cited_context | citing_context	Laion-5B	https://doi.org/10.48550/arXiv.2504.20998 (2025), https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.48550/arXiv.2210.08402 (2022), https://doi.org/10.1109/CVPR.2016.124 (2016)	The LAION-5B dataset is used to sample easy-negative examples, enhancing model robustness against irrelevant data. This methodology ensures consistency across various concepts, improving the reliability and performance of models in diverse applications. The dataset's large scale and diverse content enable researchers to address challenges related to data irrelevance and model consistency.; The LAION-5B dataset is leveraged as a large-scale multimodal resource, primarily providing diverse 2D human images. It is used to train and improve personalized text generation models by enhancing the models' ability to generate text that is contextually relevant and personalized. The dataset's extensive size and variety of images enable researchers to develop more robust and versatile text generation systems.	LAION-5B	
citing_context	LAMP	https://doi.org/10.48550/arXiv.2502.06560 (2025)	https://doi.org/10.48550/arXiv.2407.11016 (2024)	The LAMP dataset is used to benchmark personalized text generation tasks, such as tweet generation, movie reviewing, email writing, and social media post writing. It provides actionable performance metrics, enabling researchers to evaluate and compare the effectiveness of different models in generating contextually appropriate and personalized text.		
citing_context	LaMP-4	https://doi.org/10.1007/978-3-031-88714-7_40 (2025)	https://doi.org/10.48550/arXiv.2304.11406 (2023)	The LaMP-4 dataset is used to evaluate personalized text generation, specifically focusing on the integration of user preferences and context within large language models. This dataset enables researchers to assess how well these models can incorporate individual user inputs and contextual information, enhancing the personalization and relevance of generated text.		
citing_context	LaMP-5	https://doi.org/10.1007/978-3-031-88714-7_40 (2025)	https://doi.org/10.48550/arXiv.2304.11406 (2023)	The LaMP-5 dataset is used to evaluate personalized text generation, specifically focusing on the integration of user preferences and context within large language models. This dataset enables researchers to assess how well these models can incorporate individual user inputs and contextual information, enhancing the personalization and relevance of generated text.		
citing_context	LaMP-5U	https://doi.org/10.1007/978-3-031-88714-7_40 (2025)	https://doi.org/10.48550/arXiv.2304.11406 (2023)	The LaMP-5U dataset is used to analyze and compare document lengths across various datasets, specifically examining how document length impacts personalization in large language models. This involves quantitative comparisons and statistical analyses to understand the relationship between document length and the effectiveness of personalization techniques. The dataset's focus on document length provides insights into optimizing personalization in language models.		
citing_context	LaMP-7	https://doi.org/10.1007/978-3-031-88714-7_40 (2025)	https://doi.org/10.48550/arXiv.2304.11406 (2023)	The LaMP-7 dataset is used to evaluate personalized text generation, specifically focusing on the integration of user preferences and context. Researchers employ this dataset to assess how well models can incorporate these elements, enhancing the relevance and personalization of generated text. This evaluation helps in refining algorithms to better align with user-specific needs and contexts.		
cited_context	lexicon from Wilson et al.	https://www.semanticscholar.org/paper/2a215755d7548ffc82079ce734c4ac60b62f6f56 (2017)	https://doi.org/10.3115/1220575.1220619 (2005)	The lexicon from Wilson et al., 2005 is used to provide sentiment labels for words, which enhances the emotional context in personalized text generation models. This dataset enables researchers to incorporate nuanced emotional content, improving the authenticity and relevance of generated text.		
citing_context	LibriTTS	https://www.semanticscholar.org/paper/155a3210bbc715b04a455b2d396f9fbb585540aa (2021)	https://www.semanticscholar.org/paper/3321263fd0b2be6011f20d7b74b8ae801741eb21 (2018)	The LibriTTS dataset is primarily used to train multi-speaker English speech synthesis models, such as StyleSpeech and Meta-StyleSpeech. It provides a rich corpus that enables researchers to develop controllable speech synthesis systems, focusing on enhancing the naturalness and variability of synthesized speech. The dataset's extensive speaker diversity and high-quality audio recordings are crucial for these applications.		
cited_context	LiveChat	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The LiveChat dataset is used to study live streaming interactions, focusing on real-time user engagement and response patterns. It is employed in research to analyze how users engage in real-time conversations, with a specific emphasis on personalized text generation. This dataset enables researchers to explore the dynamics of immediate user responses and interaction patterns, providing insights into the effectiveness of personalized messaging in live streaming environments.		
citing_context	Long-form Language Model Personalization (LongLaMP) benchmark	https://doi.org/10.48550/arXiv.2501.04167 (2025)	https://doi.org/10.48550/arXiv.2407.11016 (2024)	The Long-form Language Model Personalization (LongLaMP) benchmark is used to evaluate personalized long-form text generation models across four diverse tasks. It focuses on assessing the effectiveness of personalization in generating coherent and contextually relevant text. This dataset enables researchers to measure how well models can adapt to individual user characteristics and maintain consistency over longer text sequences.		
citing_context	LongLaMP	https://www.semanticscholar.org/paper/831b89a50ce08af10b1708cbcd841fb7fee48d7b (2025), https://doi.org/10.48550/arXiv.2501.04167 (2025), https://doi.org/10.48550/arXiv.2502.06560 (2025)	https://doi.org/10.48550/arXiv.2407.11016 (2024)	The LongLaMP dataset is used to evaluate personalized long-form text generation models, focusing on the validation set to assess performance, coherence, and consistency in generated texts over extended lengths. It extends the LAMP dataset, enhancing the scope of tasks and providing deeper insights into model capabilities. This dataset enables researchers to rigorously test and improve the quality of personalized text generation.		
cited_context	LSMDC	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2019.00468 (2019)	The LSMDC dataset is primarily used to collect caption sentences for video-and-language research, contributing to a diverse corpus of movie descriptions. It is utilized to train models for generating personalized text, enhancing the dataset's utility in creating rich, contextually relevant captions from various sources.		
cited_context	LSUN-Church	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN-Church dataset is used to test personalized text generation models, specifically focusing on church images. Researchers employ this dataset to evaluate the model's ability to generate text for diverse visual contexts, ensuring the generated text accurately reflects the unique characteristics of church imagery. This application helps in assessing the model's performance in context-specific text generation tasks.		
cited_context	LSUN-Church-256	https://doi.org/10.1109/CVPR52688.2022.00246 (2021)	https://www.semanticscholar.org/paper/4dcdae25a5e33682953f0853ee4cf7ca93be58a9 (2015)	The LSUN-Church-256 dataset is used to pretrain denoising diffusion probabilistic models, specifically focusing on church images. This pretraining helps diversify the visual contexts in personalized text generation tasks. The dataset's large collection of high-resolution church images enables researchers to enhance the visual diversity and quality of generated text, contributing to more nuanced and contextually rich outputs.		
citing_context	massive generic dialogue data	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.1137/1.9781611975321.71 (2018)	The 'massive generic dialogue data' dataset is primarily used to pre-train dialogue models, enabling them to generate general conversational responses. This pre-training step focuses on building a robust foundation of conversational skills before any personalization is applied. The dataset's large scale and generic nature make it suitable for enhancing the model's ability to handle a wide range of conversational contexts, thereby improving its overall performance in subsequent personalized text generation tasks.		
citing_context	Melon Playlist Dataset	https://doi.org/10.48550/arXiv.2301.08145 (2023)	https://www.semanticscholar.org/paper/c6c734e16f66fbfcefac7625cc64599e83292c1e (2020)	The Melon Playlist Dataset is used to evaluate the KLEU RoberTa small model for generating playlist titles. Research focuses on the effectiveness of the model's embeddings in producing diverse and relevant titles, highlighting the dataset's utility in assessing natural language generation techniques for music playlists.		
citing_context	Million Playlist Dataset	https://doi.org/10.48550/arXiv.2301.08145 (2023)	https://www.semanticscholar.org/paper/c6c734e16f66fbfcefac7625cc64599e83292c1e (2020)	The Million Playlist Dataset is used to evaluate models, such as all-MiniLM-L6-v2, for generating playlist titles. Research focuses on assessing the model's ability to produce diverse and contextually appropriate titles. The dataset's large scale and rich metadata enable comprehensive performance evaluations, ensuring that generated titles are both relevant and varied.		
cited_context	MIND	https://doi.org/10.48550/arXiv.2305.06566 (2023)	https://doi.org/10.18653/v1/2020.acl-main.331 (2020)	The MIND dataset is used to analyze news articles for personalized text generation, specifically focusing on extracting and categorizing main ideas and content types such as 'guidance' or 'instructions'. This involves methodologies that parse and interpret textual data to understand and generate personalized content. The dataset's rich textual information and structured content types enable researchers to develop and refine algorithms for generating tailored text based on article analysis.		
cited_context	MM-Vox	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The MM-Vox dataset is used to generate personalized text descriptions of faces by focusing on 36 facial attributes. Researchers employ Probabilistic Context-Free Grammar (PCFG) to explore the relationship between visual and textual data, enabling the creation of detailed and accurate text descriptions that reflect specific facial characteristics. This approach supports studies in personalized text generation, particularly in enhancing the precision and relevance of text outputs based on visual inputs.		
cited_context	Modified MUG	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ACCESS.2020.3017881 (2020)	The Modified MUG dataset is used to generate text descriptions from facial emotions in a closed-world text-video dataset containing 1,039 videos. It focuses on different emotions, enabling researchers to explore the relationship between facial expressions and corresponding textual descriptions. This dataset facilitates the development and evaluation of models that can accurately translate visual emotional cues into text, enhancing applications in affective computing and emotion recognition.		
citing_context	Movie	https://doi.org/10.1145/3404835.3462854 (2021)	https://doi.org/10.1609/AAAI.V33I01.33016690 (2019)	The Movie dataset is used to evaluate the performance of the USN model in personalized review summarization, specifically focusing on user-aware sequence networks. This involves analyzing user-specific patterns in reviews to generate more accurate and personalized summaries. The dataset's user-centric nature enables researchers to test and improve models that can capture individual preferences and writing styles, enhancing the effectiveness of personalized text generation in review contexts.		
cited_context	Movie-DiC	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://www.semanticscholar.org/paper/e2efefbba8bf3e76605db24da0ba15df7b0adc9e (2012)	The Movie-DiC dataset is used to develop and evaluate dialogue generation systems, particularly for movie dialogues. It provides a rich source of conversational data, supporting research in personalized text generation. The dataset enables researchers to create more contextually appropriate and engaging dialogues by leveraging its extensive collection of movie conversations.		
citing_context	MPST	https://doi.org/10.18653/v1/2024.emnlp-main.737 (2023)	https://doi.org/10.18653/v1/2023.acl-long.190 (2023)	The MPST dataset is primarily used for multi-perspective storytelling, enabling researchers to generate narratives from various viewpoints. It provides a structured basis for exploring narrative diversity and perspective-taking in computational models. The dataset's key feature is its multi-perspective content, which supports the development and evaluation of algorithms designed to create coherent and varied stories. This facilitates research into narrative generation and the cognitive processes involved in storytelling.		
citing_context	MS COCO	https://doi.org/10.1109/TPAMI.2018.2824816 (2019)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The MS COCO dataset is primarily used for training and evaluating image captioning models. It provides a large set of images, each with multiple ground truth captions, which helps improve the quality and diversity of generated text. Researchers use it to highlight limitations, such as the limited number of objects in images, and to enhance model performance through diverse annotations.		
cited_context	MSCOCO	https://doi.org/10.1109/ICASSP48485.2024.10447048 (2024)	https://doi.org/10.1162/tacl_a_00166 (2014)	The MSCOCO dataset is primarily used for enhancing the training of models in personalized text generation by providing a large set of annotated images. This dataset enables researchers to apply regularization techniques, improving model performance and robustness in generating personalized text. The extensive annotations and diverse image content are crucial for training models to understand context and generate more accurate and contextually relevant text.		
citing_context	MSR-VTT	https://doi.org/10.48550/arXiv.2502.02885 (2025)	https://doi.org/10.1109/CVPR.2016.571 (2016)	The MSR-VTT dataset is used to train and evaluate models that generate personalized text descriptions of video content. It focuses on the relationship between visual and textual information, enabling researchers to develop and test methods that accurately capture and describe the content of videos in a personalized manner.		
citing_context	myPersonality dataset	https://doi.org/10.1109/ICCCNT61001.2024.10724424 (2024)	https://www.semanticscholar.org/paper/0e00a7e0eed484f0099eb46a0cdcb99df1a42336 (2020)	The myPersonality dataset is used to detect particularity values from utterances, aiding in the construction of a personality identifier. It focuses on the relationship between language use and personality traits, employing methodologies that analyze linguistic patterns to infer individual personality characteristics. This dataset enables researchers to explore how specific language features correlate with different personality types, enhancing understanding of human behavior and communication.		
citing_context	NBDESCRIB	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025)	https://doi.org/10.3115/v1/P15-1142 (2015)	The NBDESCRIB dataset is used to train and evaluate models for generating personalized text descriptions from tabular data. It focuses on compositional semantic parsing and controlled table-to-text generation, enhancing the accuracy and relevance of generated text through scientific reasoning and decomposition of evidence and questions. This dataset enables research in high-fidelity personalized text generation, specifically tailored for table-based reasoning tasks.		
cited_context | citing_context	News Categorization Dataset	https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.48550/arXiv.2209.11429 (2022)	The News Categorization dataset, derived from HuffPost articles, is primarily used to construct and evaluate personalized text generation models. It leverages news articles categorized by topic, enabling researchers to train models that can generate text tailored to specific categories or user preferences. This dataset facilitates the development and assessment of algorithms designed to produce contextually relevant and personalized content.; The News Categorization dataset is primarily used to construct a dataset for personalized text generation, leveraging news articles categorized from the HuffPost website. This dataset enables researchers to develop and test algorithms that can generate personalized text content, focusing on the categorization and contextual relevance of news articles.	News Categorization dataset	
cited_context	news data	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The 'news data' dataset is used to train models on news articles from diverse sources, including those curated by Hermann et al., Barrault et al., Sandhaus, and Grusky et al. This training focuses on leveraging large, varied corpora to enhance model performance in understanding and generating coherent news content. The dataset's broad source base ensures exposure to different writing styles and topics, crucial for robust model training.		
cited_context	news data (Barrault et al.	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Barrault et al., 2019)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.		
cited_context	news data (Grusky et al.	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Grusky et al., 2018)' dataset is used to train models for personalized text generation, specifically focusing on news articles and summaries. This dataset enables researchers to develop and refine algorithms that can generate personalized content, enhancing the relevance and engagement of news articles for individual users.		
cited_context	news data (Hermann et al.	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Hermann et al., 2015)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and their summaries. This dataset enables researchers to develop and refine algorithms that can generate coherent and contextually relevant summaries, enhancing the personalization of news content delivery.		
cited_context	news data (Sandhaus	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The 'news data (Sandhaus, 2008)' dataset is primarily used to train models for personalized text generation, specifically focusing on news articles and summaries. This involves employing machine learning techniques to generate tailored content, enhancing the relevance and engagement of news for individual users. The dataset's extensive collection of news articles and summaries enables researchers to develop and refine algorithms that can produce personalized text outputs.		
cited_context	News dataset	https://doi.org/10.18653/v1/2020.emnlp-main.698 (2020)	https://doi.org/10.18653/v1/N16-1014 (2015)	The News dataset is used to evaluate the diversity of generated text in neural conversation models. Researchers focus on comparing Dist-n scores between model-generated and human-generated text, assessing the model's ability to produce varied and natural responses. This dataset enables the quantitative analysis of text diversity, crucial for improving conversational AI systems.		
cited_context	OMCS corpus	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1007/3-540-36124-3_77 (2002)	The OMCS corpus is used to enhance personalized text generation by retrieving relevant propositions as distant rationales in training data. This methodology incorporates common sense knowledge, improving the model's ability to generate more contextually appropriate and coherent text. The dataset's rich common sense information enables researchers to address the challenge of generating personalized content that aligns with user-specific contexts and preferences.		
cited_context	One-Billion-Word	https://doi.org/10.18653/v1/2021.emnlp-main.681 (2021)		The One-Billion-Word dataset is used to train and evaluate personalized text generation models, with a focus on large-scale language modeling and adaptation techniques. Researchers employ this dataset to develop and refine models that can generate text tailored to individual users, leveraging its extensive size and diverse content to enhance model performance and adaptability.		
citing_context	one-billion-words dataset	https://doi.org/10.1145/3580305.3599535 (2022)	https://doi.org/10.18653/v1/N19-1423 (2019)	The one-billion-words dataset is primarily used to fine-tune the BERT-large model for downstream tasks, which enhances the model's performance in personalized text generation. This dataset provides a large corpus of text that is essential for improving the contextual understanding and generative capabilities of the model.		
citing_context	OP-I-MISTRAL	https://doi.org/10.48550/arXiv.2503.00449 (2025)	https://doi.org/10.48550/arXiv.2402.11683 (2024)	The OP-I-MISTRAL dataset is used to evaluate opinion summaries generated by large language models (LLMs), specifically focusing on aspect coverage and sentiment consistency. Researchers employ this dataset to assess the quality and reliability of LLM-generated summaries, ensuring they accurately reflect the opinions and sentiments present in the source data. This evaluation helps in refining and improving the performance of LLMs in generating coherent and comprehensive opinion summaries.		
cited_context	OpenWebText	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The OpenWebText dataset is used to train models on web-scraped text, enhancing the diversity of training data. It is specifically employed for personalized text generation, providing a rich and varied set of web-based textual content. This dataset enables researchers to develop models that can generate more diverse and contextually relevant text by leveraging its extensive and varied content.		
cited_context	Partiprompts set	https://doi.org/10.48550/arXiv.2403.03206 (2024)	https://doi.org/10.48550/arXiv.2206.10789 (2022)	The Partiprompts set is used to conduct human preference studies, specifically evaluating captions generated by models. In these studies, approximately 128 captions are assessed with around three voters per prompt, comparing different outputs. This dataset enables researchers to gather qualitative feedback on the effectiveness and quality of generated text, facilitating the refinement of text generation models through direct user evaluation.		
cited_context	Pchatbot	https://doi.org/10.48550/arXiv.2503.02614 (2025)	https://doi.org/10.1609/aaai.v36i10.21326 (2021)	The Pchatbot dataset, compiled from Weibo and judicial forums, is used to explore diverse conversational contexts and legal information in chatbot applications. Researchers employ this dataset to analyze and enhance chatbot interactions by integrating real-world conversational data and legal content, focusing on improving the contextual relevance and accuracy of chatbot responses in various scenarios.		
cited_context	PChatbotW	https://doi.org/10.1145/3404835.3462828 (2021)	https://doi.org/10.1145/3404835.3463239 (2020)	The PChatbotW dataset is used for training personalized chatbots, specifically to enhance conversational AI systems. Sourced from Weibo and covering a one-year period, this large-scale dataset is employed to improve the performance and personalization of chatbot interactions. The dataset's extensive temporal coverage and scale enable researchers to develop more sophisticated and contextually aware conversational models.		
cited_context | citing_context	Pens	https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.48550/arXiv.2503.02614 (2025), https://doi.org/10.48550/arXiv.2304.11406 (2023)	https://doi.org/10.18653/v1/2021.acl-long.7 (2021)	The PENS dataset is used to construct personalized headline generation systems, leveraging realistic user interaction data from Microsoft News. It focuses on enhancing personalization and improving user engagement through tailored news headlines. The dataset enables researchers to develop and evaluate methods that generate more relevant and engaging headlines for individual users.; The PENS dataset is used to develop and evaluate personalized news headline generation models, focusing on integrating user preferences and interests. It employs user click history data from Microsoft News to enhance personalization and improve user engagement through tailored headlines. The dataset serves as a benchmark for constructing systems that generate more realistic and engaging personalized news headlines.	PENS	
citing_context	Per-DOC	https://doi.org/10.18653/v1/2024.emnlp-main.737 (2023)	https://doi.org/10.18653/v1/2023.acl-long.190 (2023)	The Per-DOC dataset is used in research to enhance the coherence and quality of generated long stories through detailed outline control. It is employed in pairwise comparisons in personalized settings to evaluate narrative consistency and structure, addressing the challenge of maintaining coherence in long-form narratives. This dataset facilitates the development and assessment of methods for generating structured and coherent stories.		
citing_context	Per-MPST	https://doi.org/10.18653/v1/2024.emnlp-main.737 (2023)	https://doi.org/10.18653/v1/2023.acl-long.190 (2023)	The Per-MPST dataset is used for evaluating and training models in personalized text generation. It supports pointwise evaluation in individual settings, focusing on specific metrics to assess performance. Additionally, it is utilized for reproducing and studying personalized multi-perspective storytelling, aiming to generate coherent narratives from multiple viewpoints. The dataset also enhances the training of personalized instruction data, improving model coherence and contextual relevance in responses.		
citing_context	person-chat dataset	https://doi.org/10.1109/AINIT59027.2023.10212566 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The 'person-chat dataset' is used to label Natural Language Inference (NLI) tags within dialogue contexts, enhancing personalized text generation. This involves employing NLI techniques to improve the coherence and relevance of generated text in conversational settings. The dataset's focus on dialogue provides rich contextual data, enabling researchers to develop more nuanced and context-aware text generation models.		
citing_context	persona attribute triplet extraction dataset	https://doi.org/10.18653/v1/2023.acl-long.544 (2023)	https://doi.org/10.18653/v1/P19-1363 (2018)	The persona attribute triplet extraction dataset is used to enhance personalized text generation by extracting persona attributes from dialogues. Researchers employ triplet annotations to identify and structure these attributes, which are then utilized to improve the personalization and coherence of generated text. This dataset specifically supports methodologies focused on annotating and utilizing persona information to enrich dialogue systems.		
citing_context	PERSONA CHAT	https://doi.org/10.1145/3439816 (2021), https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/P18-1205 (2018)	The PERSONA CHAT dataset is used to train and evaluate personalized dialogue agents, focusing on incorporating user profile information into conversations to enhance personalization and engagement. This dataset enables researchers to develop models that can effectively integrate personal details into conversational responses, improving the naturalness and relevance of interactions.		
cited_context | citing_context	Persona Dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/P19-1363 (2018)	The Persona dataset is primarily used to create NLI annotations for training models to assess persona consistency in dialog systems. It serves as the foundation for the Dialog NLI dataset, which includes annotations between persona descriptions and dialogue utterances. This enhances the development of models for personalized text generation by ensuring that dialog responses are consistent with the persona.; The Persona dataset is used to create the Dialog NLI dataset, which contains persona descriptions and dialogue utterances annotated with Natural Language Inference (NLI) labels. This enables researchers to study personalized text generation and persona consistency through NLI annotations, focusing on how personas are represented and maintained in dialogues.	Persona dataset	
citing_context	persona-based empathetic conversations	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/2020.emnlp-main.531 (2020)	The 'persona-based empathetic conversations' dataset is used to train and evaluate a BERT-based response selection model, CoBERT. This model focuses on learning higher-level interactive matching through multi-hop co-attention, specifically in the context of empathetic conversations. The dataset enables researchers to assess the effectiveness of CoBERT in generating contextually appropriate and empathetic responses, enhancing the quality of conversational interactions.		
cited_context | citing_context	Persona-Chat	https://doi.org/10.48550/arXiv.2310.18342 (2023), https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.1109/ICAC3N56670.2022.10074067 (2022) (+1), https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.18653/v1/N19-1170 (2019)	https://doi.org/10.48550/arXiv.2305.11482 (2023), https://doi.org/10.18653/v1/P18-1205 (2018)	The PERSONA-CHAT dataset is used to train and fine-tune dialogue systems for generating personalized responses in conversations. It enhances personalized dialogue generation by incorporating persona information, improving conversational coherence and engagement. Researchers use it to construct multi-turn chitchat interactions and to generate conversation topics by randomly selecting personas, which are then fed as input to models during training. This dataset specifically supports the development of more natural and contextually relevant dialogues.; The PERSONA-CHAT dataset is used to train and evaluate dialogue systems that generate personalized responses, focusing on incorporating persona information into conversations. It supports the development of dialogue agents for chitchat interactions, enhancing engagement by optimizing control over repetition, specificity, and question-asking across multiple turns. This dataset enables researchers to improve the naturalness and coherence of conversational agents in both human-human and human-bot settings.	PERSONA-CHAT	
citing_context	PersonaExt	https://doi.org/10.18653/v1/2023.acl-long.544 (2023)	https://doi.org/10.18653/v1/D18-1514 (2018)	The PersonaExt dataset is used to evaluate the performance of frameworks in personalized text generation tasks. It is specifically employed to measure improvements and compare against strong baselines, enabling researchers to assess the effectiveness of their models in generating personalized text.		
cited_context	personal knowledge base	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://doi.org/10.1109/35021BIGCOMP.2015.7072837 (2015)	The 'personal knowledge base' dataset is used to enhance the personalization of dialogue interactions in open-domain conversation systems. It employs example-based approaches to improve responses ranking, ensuring that the generated dialogues are more contextually relevant and personalized. This dataset enables researchers to focus on improving the quality and naturalness of conversational agents by leveraging user-specific data.		
citing_context	Personal Preference Eval	https://www.semanticscholar.org/paper/831b89a50ce08af10b1708cbcd841fb7fee48d7b (2025)	https://doi.org/10.18653/v1/2022.acl-long.229 (2021)	The 'Personal Preference Eval' dataset is used to evaluate how well language models can align with individual human preferences. It assesses the models' ability to reflect specific user preferences, focusing on the alignment between generated text and personal preferences. This dataset enables researchers to measure and improve the personalization capabilities of language models through systematic evaluation.		
cited_context | citing_context	Personaldialog	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://www.semanticscholar.org/paper/a3ce3004a0eade48a3ae652dbf5c04a60c2416aa (2019)	The PersonalDialog dataset is used to develop and enhance personalized dialogue systems, focusing on maintaining coherent and engaging conversations over multiple turns. It is employed to generate personalized dialogues with diversified traits, particularly by incorporating varied conversational elements from Chinese social media Weibo. This dataset supports research aimed at improving conversational diversity and personalization in chatbot interactions.; The PersonalDialog dataset is used to develop and enhance personalized dialogue systems, focusing on maintaining coherent and engaging conversations over multiple turns. It is employed to generate personalized dialogues with diversified traits, specifically to improve conversational diversity and personalization in chatbot interactions. Derived from Weibo, the dataset supports research aimed at incorporating varied traits into conversational responses, thereby enriching user engagement and interaction quality.	PersonalDialog	
citing_context	Personality Captioning dataset	https://doi.org/10.18653/v1/2020.acl-main.673 (2020)	https://doi.org/10.1109/CVPR.2019.01280 (2018)	The Personality Captioning dataset is used to collect image captions that reflect 215 different personality traits, aiming to enhance engagement by generating personalized and engaging captions. This dataset supports research in creating more interactive and tailored image captioning systems, focusing on the impact of personality traits on user engagement and content relevance.		
citing_context	personalized CWI dataset	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The personalized CWI dataset is used to refine substitution candidate rankings in personalized text generation by excluding non-complex target words and removing complex substitution candidates tailored to individual users. This approach enhances the relevance and appropriateness of text substitutions, addressing the specific needs and preferences of each user. The dataset's focus on personalization and complexity filtering enables more effective and user-specific text generation research.		
citing_context	personalized dialogue data	https://doi.org/10.1145/3439816 (2021)	https://www.semanticscholar.org/paper/bfe6d67ed1c9119f91774e62fe0f4f328830526e (2017)	The 'personalized dialogue data' dataset is used to train response-generation models, specifically focusing on speaker-role adaptation in neural conversation models. This involves employing multi-task learning to enhance the model's ability to generate contextually appropriate responses that reflect different speaker roles. The dataset enables researchers to develop more nuanced and role-specific conversational agents by providing diverse dialogue examples.		
citing_context	Personalized product description dataset	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.1145/3292500.3330725 (2019)	The 'Personalized product description dataset' is used to generate personalized product descriptions in e-commerce settings. Researchers incorporate knowledge and user category attributes to enhance user experience. The dataset enables the development of models that tailor product descriptions to individual users, improving relevance and engagement. This approach focuses on enhancing the personalization of e-commerce interactions through data-driven text generation.		
citing_context	Personalized VideoIC dataset	https://doi.org/10.1145/3589334.3645711 (2024)		The Personalized VideoIC dataset is used to develop methods for generating personalized video content automatically, without the need for manually labeled personality traits. It focuses on video-to-text generation, enabling researchers to explore automatic personalization techniques in multimedia content creation. This dataset supports the development and evaluation of algorithms that can infer and apply personalization directly from video data, enhancing the relevance and engagement of generated content.		
cited_context	PersonalSum	https://doi.org/10.48550/arXiv.2502.14289 (2025)	https://doi.org/10.1037/0033-295X.84.3.231 (1977)	The PersonalSum dataset is used to develop and evaluate personalized summarization techniques for large language models. It focuses on enhancing summarization quality through user-subjective guidance, enabling researchers to assess how well these models can generate summaries that align with individual user preferences.		
citing_context	Pinterest image dataset	https://doi.org/10.1109/TMM.2024.3399075 (2024)		The Pinterest image dataset is used to construct subsets for personalized text generation, specifically focusing on image-based content. Researchers employ this dataset to enhance user engagement and personalization by leveraging the visual elements. The dataset's image-centric nature is crucial for developing algorithms that generate personalized text descriptions, improving the relevance and appeal of content for users.		
citing_context	Political slant	https://doi.org/10.1109/ICKECS56523.2022.10059789 (2022)	https://doi.org/10.18653/v1/P18-1080 (2018)	The 'Political slant' dataset is used for political style transfer experiments, specifically to modify the political bias in text. Researchers employ this dataset to develop and test algorithms that can alter the ideological tone of written content, enabling studies on the impact of political framing and bias in communication.		
citing_context	PPDB	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The PPDB dataset is used in research to evaluate substitution candidates, enabling a clear comparison between different methods, particularly focusing on simplicity-based approaches. Its extensive substitution options facilitate this evaluation, making it a valuable resource for refining and assessing text simplification techniques.		
citing_context	PPDB Set	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The PPDB Set is used in natural language processing research for deriving personalized rankings of substitution candidates and evaluating paraphrase quality. It focuses on simplification and semantic equivalence, enhancing contextually appropriate paraphrasing by filtering out complex and irrelevant substitutions. This dataset enables researchers to improve the accuracy and relevance of paraphrases in text generation tasks.		
cited_context	PRISM	https://doi.org/10.48550/arXiv.2502.14289 (2025)	https://doi.org/10.48550/arXiv.2404.16019 (2024)	The PRISM dataset is used to collect preference annotations from conversations involving over a thousand users, focusing on the subjective and multicultural alignment of large language models. Despite limitations in annotation quantity per user, the dataset enables researchers to evaluate and improve the cultural and personal relevance of language models through user feedback.		
citing_context	product descriptions and user history	https://doi.org/10.48550/arXiv.2310.11593 (2023)	https://doi.org/10.18653/v1/D19-1319 (2019)	The 'product descriptions and user history' dataset is used to generate personalized reviews by integrating domain-specific features into the text generation process. This approach enhances personalization, leveraging user history and product details to create more relevant and tailored review content. The dataset's focus on domain-specific features is crucial for improving the authenticity and utility of the generated reviews.		
cited_context	Project Gutenberg	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The Project Gutenberg dataset is used to train models on a large collection of public domain books. This enables researchers to develop and test algorithms that can process and understand extensive textual content, focusing on natural language processing tasks. The dataset's vast corpus of literary works supports the training of robust language models, enhancing their ability to generate and analyze text.		
cited_context	PsyQA	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2021.findings-acl.130 (2021)	The PsyQA dataset is used to generate and evaluate long counseling texts for mental health support, focusing on the quality, appropriateness, and effectiveness of AI-generated responses. It is employed to compare different selection methods (e.g., top-1 vs. random) and to assess the performance of large language models like ChatGLM and BELLE. The dataset enables researchers to measure helpfulness, empathy, and win rates in generating supportive texts.		
cited_context | citing_context	Pushshift Reddit Dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.5281/ZENODO.3608135 (2020)	The Pushshift Reddit Dataset is used to provide raw data for preprocessing user attributes, with a focus on ensuring the reproducibility of models and scripts. This dataset enables researchers to develop and validate methods for handling and analyzing large-scale social media data, enhancing the transparency and replicability of their research.; The Pushshift Reddit Dataset is used to provide raw data for preprocessing user attributes, focusing on the reproducibility of models and scripts. It enables researchers to ensure that their methodologies and analyses can be replicated, enhancing transparency and reliability in their studies.	Pushshift Reddit Dataset	
cited_context	R EDDIT	https://doi.org/10.18653/v1/D18-1298 (2018)	https://www.semanticscholar.org/paper/be3a65ef15f79ebb8296e6a0e8d1a9cb5c0f3638 (2015)	The R EDDIT dataset is used to train and evaluate dialog systems, specifically focusing on enhancing the quality of generated responses. Researchers employ a large-scale comment dataset to assess system performance, ensuring that the dialogues are coherent and contextually appropriate. This dataset enables the development of more effective and natural conversational agents by providing a rich source of human interaction data.		
citing_context	RateBeer	https://doi.org/10.1145/3580305.3599535 (2022), https://doi.org/10.1145/3696410.3714583 (2024)	https://doi.org/10.1145/2488388.2488466 (2013)	The RateBeer dataset is used for personalized text generation research, specifically focusing on user reviews of beers and breweries. It is employed to evaluate model performance in generating nuanced and specific content, as well as assessing aspect coverage, phrase diversity, and distinct-2 metrics in personalized beer recommendations. This dataset enables researchers to test and refine models for generating high-quality, user-specific textual content.		
citing_context	ratebeer.com user reviews	https://www.semanticscholar.org/paper/28a70871ef06acda945b95ac6e4a7a5fbe58528d (2014)	https://doi.org/10.1145/1341531.1341560 (2008)	The 'ratebeer.com user reviews' dataset is used to analyze beer reviews, focusing on taste and experience descriptions. This analysis employs natural language processing techniques to generate personalized recommendations. The dataset's rich textual content, detailing user experiences and preferences, enables researchers to develop models that can suggest beers tailored to individual tastes.		
cited_context	Reddit	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.48550/arXiv.2308.07968 (2023), https://doi.org/10.1145/3404835.3462828 (2021) (+1)	https://doi.org/10.18653/v1/D16-1127 (2016)	The Reddit dataset is used to study and enhance conversational patterns in dialogue systems, focusing on natural and engaging interactions. It is employed to pre-train word embeddings, improving semantic and syntactic understanding in English language tasks. The dataset, drawn from 45 subreddits, provides diverse user-generated content, enabling the training and evaluation of personalized dialogue agents that incorporate user attributes and utterances.		
citing_context	Reddit dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021), https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/D16-1127 (2016)	The Reddit dataset is utilized to study casual and diverse conversation patterns, enhancing models' ability to engage in natural and varied dialogues. It is also used to train and evaluate personalized dialogue agents, focusing on user attributes and utterances to generate contextually relevant responses. This dataset enables researchers to improve the conversational skills of AI systems by providing a rich source of real-world, user-generated content.		
citing_context	REDDIT TaoDescribe	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/D18-1298 (2018)	The REDDIT TaoDescribe dataset is used to extract personalized characteristics from users' posts, which enhances the ability to generate personalized dialogues and product descriptions. This involves analyzing user-generated content to capture individual traits and preferences, enabling more tailored and contextually relevant text generation.		
cited_context | citing_context	Reddit Tifu-Long	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/N19-1260 (2018)	The Reddit TIFU-long dataset is used for narrative summarization of long-form Reddit posts from the 'TIFU' subreddit. Researchers apply this dataset to develop and evaluate summarization models, focusing on capturing the essence of personal stories and incidents. The dataset's long-form nature and narrative structure enable the testing of summarization techniques on complex, real-world text.; The Reddit TIFU-long dataset is used to generate summaries from long Reddit posts, focusing on maintaining coherence and relevance in longer texts. Researchers employ this dataset to evaluate summarization techniques, specifically addressing the challenge of preserving context and meaning over extended content. This dataset enables the development and testing of algorithms designed to handle and condense large volumes of textual data effectively.	Reddit TIFU-long	
citing_context	REDDIT19	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/D18-1298 (2018)	The REDDIT19 dataset is used to construct a profile-based dialogue dataset by extracting personalized characteristics from users' social posts. This dataset trains personalized dialogue agents, focusing on building more authentic and contextually relevant conversational models. The methodology involves analyzing user-generated content to capture individual traits, enhancing the agents' ability to simulate natural and personalized interactions.		
cited_context	response) pairs	https://doi.org/10.18653/v1/2023.findings-emnlp.806 (2023)	https://doi.org/10.18653/v1/2020.emnlp-main.425 (2020)	The '10k (post, response) pairs' dataset is used to train and evaluate models focused on expressing empathy in text-based mental health support. It emphasizes emotional reactions, interpretations, and explorations. The dataset enables researchers to develop and assess models that can effectively convey empathy, enhancing the quality of automated mental health support systems.		
citing_context	review-generation datasets in the restaurant domain	https://doi.org/10.48550/arXiv.2408.09865 (2024)	https://doi.org/10.1145/3340531.3411992 (2020)	The review-generation datasets in the restaurant domain are used to enhance the quality of aspect terms and label associated aspect categories, which improves the generation of neural template explanations for recommendation systems. This methodology focuses on refining the textual content to provide more accurate and contextually relevant recommendations. The dataset's inclusion of high-quality aspect terms and labeled categories enables researchers to develop more sophisticated and nuanced recommendation algorithms.		
cited_context | citing_context	Rewards-In-Context	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The 'Rewards-in-Context' dataset is used to align foundation models with dynamic user preferences, focusing on enhancing multi-objective alignment in personalized reward systems. This involves methodologies that integrate dynamic preference data to improve model adaptability and responsiveness. The dataset enables researchers to address the challenge of maintaining alignment as user preferences evolve, making it particularly useful for developing more adaptive and responsive personalized systems.; The 'Rewards-in-Context' dataset is used in research to align foundation models with multi-objective goals and dynamic preferences. It enables the adjustment of model outputs to better reflect user-specific or context-specific requirements, enhancing the adaptability and alignment of AI systems. This dataset supports methodologies focused on improving the flexibility and responsiveness of foundation models to varying objectives and preferences.	Rewards-in-Context	
citing_context	ROBOCUP	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025)	https://doi.org/10.3115/1687878.1687893 (2009)	The ROBOCUP dataset is used to generate sports commentaries, focusing on grounded language acquisition in a sports domain. Researchers apply the dataset to test and develop models that can produce contextually relevant and accurate commentaries, enhancing the understanding of how language is learned and used in specific environments.		
citing_context	ROCStories	https://www.semanticscholar.org/paper/3708ed2aa909ebde32647257f5ffb3c6548b0e8d (2021)	https://doi.org/10.1007/978-3-319-10602-1_48 (2014)	The ROCStories dataset is utilized to evaluate narrative coherence and logical flow in short story generation. Researchers employ this dataset to assess models' abilities to produce compelling and consistent narratives, focusing on the logical progression and coherence of generated stories. This dataset enables the testing of models' narrative skills, ensuring they can create stories that are both engaging and logically structured.		
citing_context	Rotowire	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025)	https://doi.org/10.3115/1687878.1687893 (2009)	The Rotowire dataset is utilized in research for generating sports news articles, focusing on the challenge of data-to-document generation. It enables researchers to develop and test algorithms that convert structured sports data into coherent and contextually accurate news articles. This dataset is particularly relevant for evaluating the performance of natural language generation models in creating content that mimics human-written sports journalism.		
cited_context | citing_context	Saferlhf	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The SafeRLHF dataset is used to ensure safety in reinforcement learning models through human feedback, focusing on the development of a robust personalized reward model. It supports methodologies that integrate human preferences to enhance the safety and reliability of RL systems, addressing research questions related to aligning AI behavior with human values and ensuring ethical outcomes.; The SafeRLHF dataset is used to ensure safety in personalized reward models by incorporating human feedback into reinforcement learning. It focuses on enhancing the safety of generated outputs, addressing research questions related to aligning AI-generated content with human values and ethical standards. This dataset enables researchers to train models that are more reliable and trustworthy in personalized text generation tasks.	SafeRLHF	
cited_context	SHHQ	https://doi.org/10.1109/3DV62453.2024.00152 (2023)	https://doi.org/10.1109/CVPR.2016.124 (2016)	The SHHQ dataset is utilized for developing personalized text generation models by providing high-quality 2D human images. These images support the models in generating text that requires detailed visual data, enhancing the accuracy and relevance of the generated content. The dataset's high-resolution images are crucial for training models to understand and describe complex visual scenes effectively.		
citing_context	Simple PPDB	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The Simple PPDB dataset is used as a reference for paraphrase simplification in research, not for simplicity ranking due to its non-overlapping word pairs. It provides a benchmark for evaluating simplification techniques, enabling researchers to compare and refine methods for generating simpler paraphrases.		
citing_context	Simple Wikipedia	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The Simple Wikipedia dataset is used alongside the standard Wikipedia to train models that rank substitution candidates by simplicity. This involves focusing on simpler language and concepts, enabling research into methods for improving text simplification and accessibility. The dataset's characteristic use of straightforward language supports the development of algorithms that can effectively identify and rank simpler alternatives to complex terms and phrases.		
citing_context	small-scale personalized dialogue data	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.1137/1.9781611975321.71 (2018)	The 'small-scale personalized dialogue data' dataset is used to fine-tune pre-trained models, specifically to incorporate user-specific information. This enables the generation of more personalized responses in dialogue systems. The dataset's focus on user-specific details enhances the model's ability to produce contextually relevant and individualized interactions.		
cited_context	SNLI 2	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2017.83 (2017)	The SNLI 2 dataset is used to enrich corpora with logical and semantic relationships through natural language inference, specifically to enhance personalized text generation. This enrichment allows researchers to incorporate deeper contextual understanding into text generation models, improving the coherence and relevance of generated text.		
cited_context	SogouCS&CA corpus	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f (2013)	The SogouCS&CA corpus is used to train word embeddings for Chinese text analysis, particularly focusing on the 2008 version. This dataset enables the generation of distributed representations of words and phrases, facilitating research in natural language processing and enhancing the understanding of semantic relationships in Chinese text.		
cited_context	SogouCS&CA corpus (2008 version)	https://doi.org/10.1007/s11280-018-0598-6 (2017)	https://doi.org/10.1145/1367497.1367560 (2008)	The SogouCS&CA corpus (2008 version) is primarily used for training word2vec models to construct word vectors for Chinese text analysis. This dataset enables researchers to perform natural language processing tasks by providing a large, structured collection of Chinese text data, facilitating the development and evaluation of algorithms that require robust word embeddings.		
citing_context	Sports	https://doi.org/10.1145/3626772.3657821 (2024)		The 'Sports' dataset is used to evaluate personalized text generation models, specifically focusing on sports-related content and user preferences. This involves assessing how well these models can generate text that aligns with individual user interests and contexts within the sports domain. The dataset enables researchers to test and refine algorithms that enhance personalization in text generation, ensuring the content is relevant and engaging for users.		
citing_context	SQuAD	https://doi.org/10.48550/arXiv.2206.04187 (2022)	https://doi.org/10.18653/v1/P17-1123 (2017)	The SQuAD dataset is primarily used to train and evaluate neural Seq2Seq models for question generation, specifically focusing on reading comprehension tasks. It enables researchers to assess the model's ability to generate questions from given passages, emphasizing the accuracy and relevance of the generated questions. This dataset facilitates the development and testing of models that can understand and process textual information effectively.		
citing_context	SQUAD 2	https://doi.org/10.1109/ICAC3N56670.2022.10074067 (2022)	https://www.semanticscholar.org/paper/7a064df1aeada7e69e5173f7d4c8606f4470365b (2019)	The SQUAD 2 dataset is primarily used to fine-tune models for question answering tasks, enhancing their contextual understanding. It is employed in research focused on improving the performance of models in generating accurate and contextually relevant answers. This dataset enables researchers to address specific challenges in natural language processing, particularly in the domain of question answering, by providing a large set of context-question-answer triples.		
citing_context	standard Wikipedia	https://doi.org/10.18653/v1/W19-8634 (2019)	https://doi.org/10.18653/v1/P16-2024 (2016)	The standard Wikipedia dataset is used to train models for ranking substitution candidates by simplicity, leveraging its rich, diverse corpus. This enables researchers to compare original and simplified versions, focusing on enhancing text simplification techniques. The dataset's extensive content and variability support the development and evaluation of algorithms aimed at improving text accessibility.		
citing_context	Stanford Politeness Corpus (SPC)	https://doi.org/10.48550/arXiv.2310.18342 (2023)	https://doi.org/10.1162/tacl_a_00027 (2018)	The Stanford Politeness Corpus (SPC) is used to study politeness in dialogue, focusing on linguistic markers of politeness across various contexts. Researchers highlight issues with noise and low-resource stylization within the dataset, which informs methodological approaches to analyzing politeness. This dataset enables research into how politeness is expressed and perceived in different dialogic situations, contributing to a deeper understanding of linguistic behavior.		
cited_context | citing_context	Stellar	https://doi.org/10.48550/arXiv.2312.06116 (2023)	https://doi.org/10.1109/cvpr42600.2020.00559 (2019)	The Stellar dataset is used in research for generating personalized text based on multimodal prompts featuring imaginary human-centric depictions. It employs publicly available images of celebrities for training and evaluation, enabling researchers to explore the integration of visual and textual data in creating more contextually rich and personalized text outputs.; The Stellar dataset is used in research for generating imaginary human-centric depictions through multimodal prompts. It employs publicly available images of celebrities for training and evaluation. This dataset enables researchers to explore the integration of visual and textual data in creating imaginative content, focusing on the development of models that can generate coherent and contextually relevant text based on visual inputs.	Stellar	
citing_context	story data	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/D19-1615 (2019)	The 'story data' dataset is used for intermediate fine-tuning of pre-trained GPT-2 models to adapt them to the domain of stories. This process enhances the model's ability to generate coherent narratives. The dataset enables researchers to improve the contextual understanding and narrative coherence of generated texts, focusing on the specific characteristics of storytelling.		
cited_context	SWAG	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.18653/v1/P18-2016 (2017)	The SWAG dataset is primarily used for next situation prediction and commonsense reasoning tasks, focusing on generating plausible continuations of everyday scenarios. Researchers employ it to evaluate models' ability to predict likely outcomes, emphasizing situational understanding. This dataset enables the assessment of models' reasoning capabilities in practical, real-world contexts.		
citing_context	synthetic polite conversational data	https://doi.org/10.48550/arXiv.2310.18342 (2023)	https://doi.org/10.1162/tacl_a_00027 (2018)	The synthetic polite conversational data dataset is used to generate and evaluate polite conversational responses, specifically to improve the politeness in automated systems. Researchers focus on the effectiveness of synthetic data, noting its utility despite lower quality compared to ChatGPT-generated data. This dataset enables the assessment of politeness in generated conversations, contributing to the development of more courteous AI interactions.		
citing_context	Taobao Advertising	https://doi.org/10.1109/ICASSP49357.2023.10096932 (2023)	https://doi.org/10.18653/v1/D16-1128 (2016)	The Taobao Advertising dataset is used for generating text from structured data in Chinese, specifically focusing on advertising content. It employs methodologies that convert tabular information into natural language descriptions to enhance user engagement and personalize advertising messages. This dataset enables researchers to explore how structured advertising data can be effectively transformed into compelling and relevant text, addressing research questions related to user engagement and content personalization.		
citing_context	TaoDescribe	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.1145/3292500.3330725 (2019)	The TaoDescribe dataset is used for generating personalized and knowledge-based product descriptions in e-commerce. It integrates user preferences and product knowledge to enhance the relevance and appeal of product descriptions. This dataset enables researchers to develop and test algorithms that incorporate user-specific data and detailed product information, improving the user experience in online shopping environments.		
citing_context	target story generation dataset	https://www.semanticscholar.org/paper/e324d92c005ccdec0ce04dfb9941dd99ded21920 (2019)	https://doi.org/10.18653/v1/D19-1615 (2019)	The 'target story generation dataset' is used for final fine-tuning with multi-task learning to enhance model performance on specific story generation tasks. It focuses on targeted common sense grounding, improving the model's ability to generate stories that are contextually coherent and grounded in realistic scenarios. This dataset enables researchers to address challenges in generating narratives that require nuanced understanding and application of common sense knowledge.		
citing_context	task-oriented dialogue system dataset	https://doi.org/10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00176 (2019)	https://www.semanticscholar.org/paper/a51158f795e260e07c8dc540c07a2749add411cd (2017)	The task-oriented dialogue system dataset is primarily used to support research on personalized text generation. It contains conversations that include users' personalized information, enabling researchers to develop and evaluate models that can generate more contextually relevant and user-specific responses. This dataset facilitates the exploration of how incorporating user-specific data can enhance the effectiveness and naturalness of dialogue systems.		
citing_context	TCFC dataset	https://doi.org/10.48550/arXiv.2310.18342 (2023)	https://doi.org/10.1162/tacl_a_00027 (2018)	The TCFC dataset is primarily used to analyze and study formal language style, focusing on stylistic elements in formal communication. Researchers employ this dataset to address limitations in data quality and resource availability, particularly in text style transfer applications. The dataset helps in examining noise and low-resource stylization issues, enabling more nuanced research into formal language styles.		
cited_context	TED gender-annotated data	https://doi.org/10.18653/V1/E17-1101 (2016)	https://doi.org/10.18653/v1/D15-1130 (2015)	The TED gender-annotated data is used to train and evaluate personality-aware machine translation models, specifically focusing on gender annotations to enhance personalization in generated text. This dataset enables researchers to explore how gender information can be effectively integrated into translation models to produce more contextually appropriate and personalized translations.		
citing_context	The Pile	https://doi.org/10.48550/arXiv.2402.04914 (2024)	https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e (2020)	The Pile is used for pretraining language models, leveraging its 825GB of English text from 22 diverse sources, including academic writing, internet, prose, dialogue, and miscellaneous content. This dataset enables researchers to develop robust language models by providing a wide range of textual data, enhancing model performance across various linguistic contexts and applications.		
cited_context	the Pile dataset	https://doi.org/10.48550/arXiv.2402.04914 (2024)	https://www.semanticscholar.org/paper/db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e (2020)	The Pile dataset, a 825GB English corpus from 22 diverse sources including academic writing, internet, prose, dialogue, and miscellaneous, is primarily used for pretraining models. This extensive and varied content enables researchers to develop robust language models capable of handling a wide range of textual data, enhancing their performance in various natural language processing tasks.		
citing_context	Toys	https://doi.org/10.1145/3626772.3657821 (2024)		The 'Toys' dataset is used to evaluate personalized text generation models, specifically focusing on toy-related content and user preferences. Researchers employ this dataset to assess how well these models can generate text that aligns with individual user interests and preferences in the context of toys. This evaluation helps in refining algorithms to better personalize text outputs for users.		
citing_context	train-clean-100 subset of LibriTTS	https://doi.org/10.48550/arXiv.2410.13342 (2024)	https://doi.org/10.21437/interspeech.2019-2441 (2019)	The train-clean-100 subset of LibriTTS is primarily used for training text-to-speech (TTS) models, leveraging its clean audio data. This dataset enables researchers to develop TTS systems that produce high-quality, natural-sounding speech. The focus on clean audio enhances model performance, making it suitable for applications requiring clear and intelligible speech synthesis.		
cited_context	TripAdvisor (hotels)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The TripAdvisor (hotels) dataset is used to generate neural template explanations for hotel recommendations, leveraging user reviews and accommodation features. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich textual content and feature details enable researchers to create more transparent and user-friendly explanations for hotel recommendations.		
cited_context	TripAdvisor 4	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The TripAdvisor 4 dataset is used in research to analyze hotel reviews, focusing on user preferences and personalized recommendations. Studies employ this dataset to understand how user-specific data can enhance recommendation systems, leveraging the rich textual content and user ratings to tailor suggestions more effectively. This dataset enables researchers to explore methodologies for improving the personalization of travel recommendations.		
cited_context	TripAdvisor dataset	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.3115/1073083.1073135 (2002)	The TripAdvisor dataset is used to evaluate the performance of methods in improving BLEU scores, focusing on the automatic evaluation of machine translation. It provides a benchmark for assessing translation quality, enabling researchers to compare different machine translation techniques and measure their effectiveness in generating accurate translations.		
citing_context	Twitter data	https://doi.org/10.58729/1941-6679.1460 (2021)	https://doi.org/10.2307/2087772 (1982)	The Twitter data dataset is used to gather personal information for personalized text generation, specifically focusing on user attitudes and behaviors expressed in tweets. This involves analyzing tweet content to understand individual user perspectives and actions, which is then utilized to tailor text generation models to reflect these personal attributes.		
cited_context | citing_context	Twitter Dataset	https://doi.org/10.18653/V1/2021.NAACL-MAIN.157 (2021)	https://doi.org/10.18653/v1/D16-1127 (2016)	The Twitter dataset is used to analyze short-form, informal communication, enhancing the generation of concise and contextually appropriate responses. Researchers employ this dataset to improve natural language processing techniques, focusing on the unique characteristics of tweets such as brevity and colloquial language. This enables more effective and realistic text generation in conversational AI systems.; The Twitter dataset is utilized to analyze short-form, informal conversations, enhancing models' capabilities to handle real-world social media interactions. Researchers employ this dataset to improve the understanding and processing of natural, unstructured language, focusing on the nuances of social media communication. This enables more effective modeling of user interactions and conversational dynamics in online platforms.	Twitter dataset	
citing_context	Ubuntu Dialogue Corpus	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.18653/v1/w15-4640 (2015)	The Ubuntu Dialogue Corpus is used to research and develop unstructured multi-turn dialogue systems, particularly focusing on context-based interactions. It leverages almost one million multi-turn conversations extracted from Ubuntu chat logs to train context-sensitive technical dialogue systems. This dataset enables researchers to enhance the contextual understanding and responsiveness of dialogue models in technical support scenarios.		
citing_context	Ultra-Chat	https://www.semanticscholar.org/paper/831b89a50ce08af10b1708cbcd841fb7fee48d7b (2025)	https://doi.org/10.18653/v1/2022.acl-long.229 (2021)	The Ultra-Chat dataset is used to enhance chat language models by scaling high-quality dialogues, specifically focusing on personalized text generation and improving conversational quality. This dataset enables researchers to develop more natural and contextually relevant responses in chat systems, leveraging its large volume of high-quality dialogue data.		
cited_context | citing_context	Ultrafeedback	https://www.semanticscholar.org/paper/95124cb03a6e5de7a623db32b987531d7830629e (2024)	https://doi.org/10.48550/arXiv.2402.10207 (2024)	The Ultrafeedback dataset is used to develop a personalized reward model, specifically focusing on incorporating user feedback to dynamically adjust preferences. This approach enables researchers to refine and personalize text generation models by continuously aligning them with user preferences, enhancing the relevance and satisfaction of generated content.	Ultrafeedback	
cited_context	UN data	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/W17-2623 (2016)	The UN data dataset is used to train models on multilingual United Nations documents, focusing on enhancing the models' understanding of formal and diplomatic language. This involves employing machine learning techniques to process and analyze the linguistic nuances present in these documents. The dataset's multilingual nature and formal content are crucial for improving the model's performance in handling complex, official texts.		
cited_context	UN data from WMT (En-De	https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.18653/v1/N18-1065 (2018)	The UN data from WMT (En-De, En-Es, En-Fr) is primarily used to train models for personalized text generation, specifically focusing on multilingual United Nations documents. This dataset enables researchers to develop and refine algorithms that can generate text tailored to individual users, leveraging the rich linguistic diversity and formal structure of UN documents.		
cited_context	Unhealthy Conversations	https://doi.org/10.1109/ICDMW65004.2024.00071 (2024)	https://doi.org/10.18653/v1/2020.acl-main.372 (2020)	The 'Unhealthy Conversations' dataset is used to assess and evaluate the performance of personalized fine-tuning methods in identifying and handling attributes of unhealthy conversations. Research focuses on measuring performance gains from personalization, specifically in the context of unhealthy interactions. This dataset enables researchers to test and improve models that can better recognize and manage problematic conversational elements.		
cited_context	VATEX	https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312 (2019)	https://doi.org/10.1109/ICCV.2019.00468 (2019)	The VATEX dataset is used to collect multilingual video captions, enhancing the diversity and quality of training data for video-and-language research. It focuses on high-quality annotations across multiple languages, supporting the development of models that can generate personalized text and improve cross-lingual understanding in video captioning tasks.		
cited_context	VGGFace2	https://doi.org/10.1109/ICCV51070.2023.00202 (2023)	https://doi.org/10.1109/FG.2018.00020 (2017)	The VGGFace2 dataset is primarily used to train DreamBooth models for personalized text generation, focusing on generating high-quality, correct-identity images for 50 identities. This enhances recognition accuracy across various poses and ages, leveraging the dataset's extensive and diverse facial image collection.		
cited_context	VoxCeleb	https://doi.org/10.1109/CVPR52729.2023.01422 (2023)	https://doi.org/10.1109/ICCV.2015.425 (2014)	The VoxCeleb dataset is primarily used for speaker identification and facial attribute labeling in research. It serves as a large-scale source of face videos, enabling the development and evaluation of models that can accurately identify speakers and label facial attributes. This dataset facilitates the creation of robust multimodal systems by providing diverse and extensive video data.		
citing_context	WEATHERGOV	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025)	https://doi.org/10.3115/1687878.1687893 (2009)	The WEATHERGOV dataset is used for generating weather forecasts, with a focus on establishing semantic correspondences using minimal supervision. This approach leverages the dataset's weather-related content to enhance forecast accuracy and coherence, addressing the challenge of producing reliable and contextually appropriate weather predictions with reduced training data.		
citing_context	Weibo	https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.18653/v1/D18-1298 (2018)	The Weibo dataset is used to train and evaluate personalized dialogue agents, focusing on integrating user attributes and utterances to generate contextually relevant responses. This involves methodologies that emphasize the contextual understanding and personalization of dialogues, enabling research into more natural and user-specific conversational interactions.		
cited_context	Weibo dataset	https://doi.org/10.1145/3404835.3462828 (2021), https://doi.org/10.48550/arXiv.2308.07968 (2023)	https://doi.org/10.1145/3404835.3463239 (2020)	The Weibo dataset is used as a subset of PChatbotW to train and evaluate personalized chatbots, focusing on social media interactions and conversations from Weibo over a one-year period. It enhances contextual understanding and personalization by incorporating user attributes and utterances, improving conversational relevance in chatbot responses.		
cited_context | citing_context	Wikibio	https://www.semanticscholar.org/paper/09f90a7fe63eaff68292ad463838c4ba13ea104c (2025), https://doi.org/10.1109/ICASSP49357.2023.10096932 (2023), https://doi.org/10.18653/v1/D17-1239 (2017)	https://doi.org/10.18653/v1/D16-1128 (2016)	The Wikibio dataset is primarily used as a benchmark for table-to-text generation, specifically for generating biographical texts from structured data. It is employed to convert tabular information from Wikipedia into coherent and informative narratives, enhancing the personalization and accuracy of biographical information. This dataset facilitates research in natural language generation, focusing on the biography domain.; The WIKIBIO dataset is used to generate biographical text from structured data, emphasizing the scalability of neural text generation models. Researchers focus on increasing the number of tokens and record types to enhance model performance. This dataset enables the exploration of methods to efficiently convert structured information into coherent biographical narratives, addressing challenges in data scaling and text coherence.	WIKIBIO, Wikibio	
citing_context	Wikidata	https://doi.org/10.1145/3439816 (2021)	https://doi.org/10.18653/v1/D19-1299 (2019)	Wikidata is used as an external knowledge base to link extracted entities from data fields, enhancing neural data-to-text generation models with background information. This linkage provides context and enriches the generated text, improving the accuracy and relevance of the output. The dataset's extensive and structured information supports the integration of real-world knowledge into text generation processes.		
cited_context | citing_context	Wikipedia	https://doi.org/10.1145/3580305.3599535 (2022), https://www.semanticscholar.org/paper/75acc731bdd2b626edc74672a30da3bc51010ae8 (2019)	https://doi.org/10.1145/2488388.2488466 (2013), https://doi.org/10.18653/v1/W17-2623 (2016)	The Wikipedia dataset is used as a pre-training corpus to ensure fair comparisons with baseline models, focusing on general language understanding and generation tasks. It provides a large, diverse text corpus that supports initial model training, enabling researchers to establish robust foundational models before fine-tuning on more specific datasets. This approach facilitates consistent performance evaluations across different studies.; The Wikipedia dataset is used to train models on multilingual text, encompassing English, German, Spanish, and French versions. This approach leverages the extensive and diverse content of Wikipedia to develop models capable of handling multiple languages. The dataset's multilingual nature enables research focused on improving cross-lingual understanding and translation capabilities.	Wikipedia	
cited_context	Wikipedia Tables	https://doi.org/10.18653/v1/D17-1239 (2017)	https://doi.org/10.18653/v1/D16-1128 (2016)	The Wikipedia Tables dataset is used to generate short biographies by conditioning language models on structured data. Researchers extract structured information from the tables to form coherent narratives, focusing on creating personalized text that accurately represents the subjects of the biographies. This approach leverages the dataset's structured format to enhance the coherence and relevance of the generated text.		
citing_context	Wizard of Wikipedia	https://doi.org/10.1145/3439816 (2021)	https://www.semanticscholar.org/paper/227458886343b86bd15adf58c769be326b4b058a (2018)	The Wizard of Wikipedia dataset is used to train conversational agents that integrate Wikipedia knowledge, enhancing dialogue with factual information to improve engagement and coherence. Research focuses on grounding conversations with retrieved knowledge, using the dataset to develop knowledge-powered conversational systems. This approach aims to make dialogues more informative and engaging by leveraging structured Wikipedia data.		
cited_context	WMT15	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT15 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset supports research aimed at improving the accuracy and naturalness of machine-translated text, specifically in the German language.		
cited_context	WMT15-WMT17	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT15-WMT17 dataset is used to collect multilingual data with English as the target language, primarily for evaluating machine translation systems. It focuses on assessing these systems' ability to generate personalized text, employing methodologies that involve comparing translations against human-generated references to measure accuracy and personalization.		
cited_context	WMT16	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT16 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset enables researchers to address specific challenges in personalized translation, improving the overall performance and naturalness of generated text.		
cited_context	WMT17	https://doi.org/10.18653/v1/2021.emnlp-main.701 (2021)	https://doi.org/10.18653/v1/W17-4717 (2017)	The WMT17 dataset is used to train and evaluate personalized text generation models, particularly focusing on German data. It enhances translation quality and fluency by providing a robust corpus for model training and evaluation. This dataset enables researchers to address specific challenges in personalized translation, improving the overall performance and naturalness of generated text.		
cited_context | citing_context	Yelp	https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.1145/3580305.3599535 (2022), https://doi.org/10.1145/3626772.3657821 (2024) (+3), https://doi.org/10.48550/arXiv.2304.11406 (2023), https://doi.org/10.18653/v1/2021.emnlp-main.681 (2021), https://doi.org/10.18653/v1/2023.acl-long.4 (2023) (+1)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021), https://doi.org/10.18653/v1/2021.findings-acl.129 (2021)	The Yelp dataset is primarily used for sentiment style transfer and personalized sentiment prediction in user reviews. Researchers employ Transformer-based models to change the sentiment of reviews while preserving content, and to predict user-specific sentiment patterns. The dataset also supports the evaluation of fairness, utility, and aspect coverage in generated text, as well as the modeling of user behavior evolution in personalized text generation.; The Yelp dataset is primarily used for personalized text generation and sentiment prediction research, focusing on user-specific styles, preferences, and sentiment patterns in restaurant reviews. It enables the evaluation of methods like UserAdapter for sentiment analysis and machine translation, assessing BLEU scores and personalized product rating predictions. The dataset's rich user reviews facilitate these tasks by providing diverse and detailed textual data.	Yelp	
cited_context	Yelp (restaurant)	https://doi.org/10.18653/v1/2021.acl-long.383 (2021)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp (restaurant) dataset is used to evaluate explainable recommendation systems, focusing on user reviews and ratings for restaurants. Researchers employ this dataset to analyze and enhance the explainability of recommendations, leveraging the rich textual and numerical data to understand user preferences and improve system transparency.		
cited_context	Yelp (restaurants)	https://doi.org/10.18653/v1/2023.acl-long.4 (2023)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp (restaurants) dataset is used to generate neural template explanations for restaurant recommendations, leveraging user reviews and dining experiences. This involves employing neural network models to analyze and synthesize review data, enhancing the interpretability of recommendation systems. The dataset's rich textual content and user feedback enable researchers to create more transparent and user-friendly recommendation explanations.		
cited_context	Yelp 3	https://doi.org/10.48550/arXiv.2209.12613 (2022)	https://doi.org/10.1145/2872427.2883037 (2016)	The Yelp 3 dataset is used to analyze restaurant reviews, focusing on sentiment and personalization in user feedback. Researchers employ methodologies that examine the emotional tone and individualized aspects of reviews. This dataset enables studies to address research questions related to customer satisfaction and personalized experiences, leveraging the rich textual data and user-specific attributes.		
citing_context	Yelp19	https://doi.org/10.48550/arXiv.2408.09865 (2024)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp19 dataset is primarily used for generating neural template explanations in recommendation systems, particularly in the restaurant domain. It is employed to train and evaluate models like MAPLE, focusing on improving the interpretability and effectiveness of recommendations. The dataset also supports sentiment analysis to understand user opinions and preferences, and is used in case studies to enhance personalized text generation and user recommendations.		
citing_context	Yelp23	https://doi.org/10.48550/arXiv.2408.09865 (2024)	https://doi.org/10.1145/3340531.3411992 (2020)	The Yelp23 dataset is primarily used for benchmarking and training models in personalized text generation, particularly in the restaurant domain. It focuses on generating neural template explanations for recommendation systems, evaluating model efficiency and effectiveness. Additionally, the dataset is utilized for sentiment analysis to understand user opinions and preferences from customer reviews.		
citing_context	Yelp6	https://doi.org/10.1145/3580305.3599535 (2022)	https://doi.org/10.1145/2488388.2488466 (2013)	The Yelp6 dataset is used for evaluating models in personalized text generation, specifically focusing on restaurant reviews. Researchers employ this dataset to assess the model's ability to generate contextually relevant and personalized content, emphasizing the accuracy and relevance of the generated text in a restaurant review context.		
