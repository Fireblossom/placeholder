{
  "results": [
    {
      "context_text": "• DeepFM (Guo et al., 2017), a widely used method that combines factorization machines and deep neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DeepFM, which is a method combining factorization machines and deep neural networks, not a dataset. No datasets are mentioned.",
      "processing_time": 31.527265787124634,
      "citing_paper_id": "196186167",
      "cited_paper_id": 970388
    },
    {
      "context_text": "• DeepFM (Guo et al., 2017), a widely used method that combines factorization machines and deep neural networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DeepFM, which is a method combining factorization machines and deep neural networks, not a dataset. No datasets are mentioned.",
      "processing_time": 31.527265787124634,
      "citing_paper_id": "196186167",
      "cited_paper_id": 3352400
    },
    {
      "context_text": "Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 18.79384970664978,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1118412
    },
    {
      "context_text": "Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 18.79384970664978,
      "citing_paper_id": "196186167",
      "cited_paper_id": 14673162
    },
    {
      "context_text": "Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 18.79384970664978,
      "citing_paper_id": "196186167",
      "cited_paper_id": 18089543
    },
    {
      "context_text": "Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 18.79384970664978,
      "citing_paper_id": "196186167",
      "cited_paper_id": 51607280
    },
    {
      "context_text": "Many conventional news recommendation methods rely on manual feature engineering to build news and user representations (Phelan et al., 2009; Liu et al., 2010; Li et al., 2010; Son et al., 2013; Li et al., 2014; Bansal et al., 2015; Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 18.79384970664978,
      "citing_paper_id": "196186167",
      "cited_paper_id": 207178795
    },
    {
      "context_text": "In our approach, the news encoder is a combination of CNN network and an attention network (denoted as CNN+Att).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method combining CNN and attention networks. No verifiable resources are identified.",
      "processing_time": 29.91018009185791,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "In our approach, the news encoder is a combination of CNN network and an attention network (denoted as CNN+Att).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method combining CNN and attention networks. No verifiable resources are identified.",
      "processing_time": 29.91018009185791,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Wang et al. (2018) proposed to learn representations of news from their titles via a knowledge-aware CNN network, and learn representations of users from the representations of their browsed news articles weighted by their similarities with the candidate news.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for learning representations of news and users but does not reference any particular dataset.",
      "processing_time": 32.345930337905884,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "Wang et al. (2018) proposed to learn representations of news from their titles via a knowledge-aware CNN network, and learn representations of users from the representations of their browsed news articles weighted by their similarities with the candidate news.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for learning representations of news and users but does not reference any particular dataset.",
      "processing_time": 32.345930337905884,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "• CNN (Kim, 2014), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CNN' which is a method, not a dataset. No specific dataset is named or described in the citation.",
      "processing_time": 17.913670539855957,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "• CNN (Kim, 2014), using CNN with max pooling to learn news representations from the titles of browsed news by keeping the most salient features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'CNN' which is a method, not a dataset. No specific dataset is named or described in the citation.",
      "processing_time": 17.913670539855957,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "• DKN (Wang et al., 2018), a deep news recommendation model which contains CNN and candidate-aware attention on the news browsing histories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DKN, a deep news recommendation model, but does not refer to a specific dataset. The mention is about a method/model, not a dataset.",
      "processing_time": 22.205440282821655,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "• DKN (Wang et al., 2018), a deep news recommendation model which contains CNN and candidate-aware attention on the news browsing histories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DKN, a deep news recommendation model, but does not refer to a specific dataset. The mention is about a method/model, not a dataset.",
      "processing_time": 22.205440282821655,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention (LSTM+Att), to validate the effectiveness of our approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 15.99033784866333,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention (LSTM+Att), to validate the effectiveness of our approach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 15.99033784866333,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Wang et al. (2018) proposed to learn the representations of news using knowledge-aware convolutional neural network (CNN), and learn the representations of users from their browsed news based on the similarities between the candidate news and the browsed news.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning representations of news and users. No verifiable resources are identified.",
      "processing_time": 21.004392862319946,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "Wang et al. (2018) proposed to learn the representations of news using knowledge-aware convolutional neural network (CNN), and learn the representations of users from their browsed news based on the similarities between the candidate news and the browsed news.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning representations of news and users. No verifiable resources are identified.",
      "processing_time": 21.004392862319946,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "The second layer in title encoder is a convolutional neural network (CNN) (LeCun et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a CNN but does not refer to any specific dataset. The citation is about the method (CNN) rather than a dataset.",
      "processing_time": 29.905333280563354,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "The second layer in title encoder is a convolutional neural network (CNN) (LeCun et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a CNN but does not refer to any specific dataset. The citation is about the method (CNN) rather than a dataset.",
      "processing_time": 29.905333280563354,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "First, the news recommendation methods (e.g. CNN , DKN and LSTUR ) which use neural networks to learn news and user representations can signiﬁcantly outperform the methods using manual feature engineering (e.g. LibFM , DeepFM , Wide & Deep , and DSSM ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 16.651769876480103,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "First, the news recommendation methods (e.g. CNN , DKN and LSTUR ) which use neural networks to learn news and user representations can signiﬁcantly outperform the methods using manual feature engineering (e.g. LibFM , DeepFM , Wide & Deep , and DSSM ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 16.651769876480103,
      "citing_paper_id": "196186167",
      "cited_paper_id": 3352400
    },
    {
      "context_text": "First, the news recommendation methods (e.g. CNN , DKN and LSTUR ) which use neural networks to learn news and user representations can signiﬁcantly outperform the methods using manual feature engineering (e.g. LibFM , DeepFM , Wide & Deep , and DSSM ).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 16.651769876480103,
      "citing_paper_id": "196186167",
      "cited_paper_id": 8384258
    },
    {
      "context_text": "Thus, we apply a CNN network to learn contextual word representations by capturing the local context information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CNN) for learning contextual word representations. No verifiable resources are identified.",
      "processing_time": 17.51335334777832,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "Thus, we apply a CNN network to learn contextual word representations by capturing the local context information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CNN) for learning contextual word representations. No verifiable resources are identified.",
      "processing_time": 17.51335334777832,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Second, our LSTUR approach outperforms all baseline methods compared here, including deep learning models such as CNN , GRU and DKN .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 27.536200523376465,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "The number of ﬁlters in CNN network is 300, and the window size of the ﬁlters in CNN network is set to 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters for a CNN network. There are no verifiable resources or datasets mentioned.",
      "processing_time": 30.21381402015686,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the use of attention mechanisms with CNN and LSTM in encoders.",
      "processing_time": 16.980902910232544,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the use of attention mechanisms with CNN and LSTM in encoders.",
      "processing_time": 16.980902910232544,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "6, using attention mechanism in both encoders based on CNN and LSTM can achieve better performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the use of attention mechanisms with CNN and LSTM in encoders.",
      "processing_time": 16.980902910232544,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "In addition, encoders using CNN outperform those using LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between CNN and LSTM encoders. No verifiable resources are identified.",
      "processing_time": 32.0470666885376,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "In addition, encoders using CNN outperform those using LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between CNN and LSTM encoders. No verifiable resources are identified.",
      "processing_time": 32.0470666885376,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "In addition, encoders using CNN outperform those using LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between CNN and LSTM encoders. No verifiable resources are identified.",
      "processing_time": 32.0470666885376,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "We explore the effectiveness of GRU in encoding news by replacing it with several other encoders, including: 1) Average: using the average of all the news representations in recent browsing history; 2) Attention: the summation of news representations weighted by their attention weights; 3) LSTM (Hochreiter and Schmidhuber, 1997), re-placing GRU with LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only different encoding methods. The cited paper 'Long Short-Term Memory' is a method, not a dataset.",
      "processing_time": 20.367929697036743,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "…it with several other encoders, including: 1) Average: using the average of all the news representations in recent browsing history; 2) Attention: the summation of news representations weighted by their attention weights; 3) LSTM (Hochreiter and Schmidhuber, 1997), re-placing GRU with LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to describe different encoders, not datasets.",
      "processing_time": 20.364991664886475,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "5, the sequence-based encoders (e.g., GRU, LSTM) outperform the Average and Attention based encoders.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 30.194217920303345,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "In addition, GRU achieves better performance than LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between GRU and LSTM models.",
      "processing_time": 29.342045783996582,
      "citing_paper_id": "196186167",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "• Wide & Deep (Cheng et al., 2016), another deep learning based recommendation method that combines a wide channel and a deep channel.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Wide & Deep' as a recommendation method, not a dataset. No specific dataset is referenced.",
      "processing_time": 18.966396808624268,
      "citing_paper_id": "196186167",
      "cited_paper_id": 3352400
    },
    {
      "context_text": "Motivated by (Huang et al., 2013) and (Zhai et al., 2016), we propose to use the negative sampling technique for model training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique (negative sampling) for model training.",
      "processing_time": 15.688730955123901,
      "citing_paper_id": "196186167",
      "cited_paper_id": 8384258
    },
    {
      "context_text": "• DSSM (Huang et al., 2013), deep structured semantic model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DSSM, which is a method/model, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 32.88057780265808,
      "citing_paper_id": "196186167",
      "cited_paper_id": 8384258
    },
    {
      "context_text": "We compare it with several variants, i.e., CNN, LSTM, and LSTM with attention (LSTM+Att), to validate the effectiveness of our\napproach.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 16.410564184188843,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "First, the news recommendation methods (e.g. CNN, DKN and LSTUR) which use neural networks to learn news and user representations can significantly outperform the methods using manual feature engineering (e.g. LibFM, DeepFM, Wide & Deep, and DSSM).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing neural network-based methods with those using manual feature engineering.",
      "processing_time": 29.621421813964844,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "The number of filters in CNN network is 300, and the window size of the filters in CNN network is set to 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only parameters for a CNN model.",
      "processing_time": 15.135987043380737,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Second, our LSTUR approach outperforms all baseline methods compared here, including deep learning models such as CNN, GRU and DKN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model comparisons. No verifiable resources are identified.",
      "processing_time": 15.470951557159424,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "C and b are the parameters of the convolutional filters in CNN, and M is the window size.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only parameters of a CNN model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 18.29008150100708,
      "citing_paper_id": "196186167",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Some user interests may last for a long time and are consistent for the same user (Li et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of user interests. No verifiable resources are identified.",
      "processing_time": 29.07532787322998,
      "citing_paper_id": "196186167",
      "cited_paper_id": 14673162
    },
    {
      "context_text": "Li et al. (2014) used a Latent Dirichlet Allocation (LDA) (Blei et al., 2003) model to generate topic distribution features as the news representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method (LDA) used for generating topic distribution features.",
      "processing_time": 19.702438592910767,
      "citing_paper_id": "196186167",
      "cited_paper_id": 14673162
    },
    {
      "context_text": "In recent years, several deep learning meth-ods were proposed for personalized news recommendation (Wang et al., 2018; Okura et al., 2017; Zheng et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning methods for personalized news recommendation.",
      "processing_time": 16.5869619846344,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "Motivated by (Okura et al., 2017), we use the simple dot production to compute the news click probability score.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing news click probability scores.",
      "processing_time": 16.60564088821411,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "Learning accurate user representations is critical for news recommendation (Okura et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of user representations for news recommendation.",
      "processing_time": 28.908790349960327,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "For example, Okura et al. (2017) proposed to learn representations of news from news bodies using denoising autoen-coder, and learn representations of users from the representations of their browsed news using a GRU network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the approach to learn representations of news and users.",
      "processing_time": 20.33318042755127,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "For example, Okura et al. (2017) proposed to learn representations of news using denoising autoen-coder and learn representations of users from their browsed news using GRU network (Cho et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the approach to learn representations of news and users.",
      "processing_time": 20.035035371780396,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "Existing news recommendation methods usually learn a single representation for each user (Okura et al., 2017; Lian et al., 2018; Wu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for news recommendation. No verifiable resources are identified.",
      "processing_time": 19.33794593811035,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "Existing news recommendation methods usually learn a single representation for each user (Okura et al., 2017; Lian et al., 2018; Wu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for news recommendation. No verifiable resources are identified.",
      "processing_time": 19.33794593811035,
      "citing_paper_id": "196186167",
      "cited_paper_id": 51607280
    },
    {
      "context_text": "Existing news recommendation methods usually learn a single representation for each user (Okura et al., 2017; Lian et al., 2018; Wu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for news recommendation. No verifiable resources are identified.",
      "processing_time": 19.33794593811035,
      "citing_paper_id": "196186167",
      "cited_paper_id": 196181724
    },
    {
      "context_text": "• GRU (Okura et al., 2017), learning news representations by a denoising autoencoder and user representations by a GRU network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GRU networks and denoising autoencoders for learning news and user representations.",
      "processing_time": 34.23592472076416,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "We propose to learn the short-term representations of users from their recent browsing history to capture their temporal interests, and use gated recurrent networks (GRU) (Cho et al., 2014) network to capture the sequential news reading patterns (Okura et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It focuses on methodological approaches using GRU networks and embedding-based recommendations.",
      "processing_time": 20.610204696655273,
      "citing_paper_id": "196186167",
      "cited_paper_id": 19919625
    },
    {
      "context_text": "However, massive news are generated everyday, making it impossible for users to read through all news (Lian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about the volume of news generated daily.",
      "processing_time": 18.651522397994995,
      "citing_paper_id": "196186167",
      "cited_paper_id": 51607280
    },
    {
      "context_text": "Wu et al. (2019) proposed to learn news and user representations with personalized word-and news-level attention networks, which exploits the embedding of user ID to generate the query vector for the attentions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for neural news recommendation using personalized attention.",
      "processing_time": 17.835192918777466,
      "citing_paper_id": "196186167",
      "cited_paper_id": 196181724
    },
    {
      "context_text": "Online news platforms such as MSN News 1 and Google News 2 which aggregate news from various sources and distribute them to users have gained huge popularity and attracted hundreds of millions of users (Das et al., 2007; Wang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only platforms and their popularity. No clear identifiers for datasets are present.",
      "processing_time": 30.754799842834473,
      "citing_paper_id": "196186167",
      "cited_paper_id": 207163129
    },
    {
      "context_text": "huge popularity and attracted hundreds of millions of users (Das et al., 2007; Wang et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general user statistics. No dataset names are present in the citation span.",
      "processing_time": 19.65535068511963,
      "citing_paper_id": "196186167",
      "cited_paper_id": 207163129
    },
    {
      "context_text": ", 2017; Lian et al., 2018; Wu et al., 2019). For example, Okura et al. (2017) proposed to learn representations of news using denoising autoencoder and learn representations of users from their browsed news using GRU network (Cho et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about learning representations of news and users, which is relevant to personalized text generation but does not specify any datasets.",
      "processing_time": 32.20933246612549,
      "citing_paper_id": "196186167",
      "cited_paper_id": 207178795
    },
    {
      "context_text": "Goodman and colleagues [11] drew inspiration from speech recognition for stylus keyboard tapping and applied a character n-gram model to correct stylus taps 1 , achieving a reduction in character-level error rate from 2.40% to 1.39%.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for correcting stylus taps using a character n-gram model.",
      "processing_time": 28.17413592338562,
      "citing_paper_id": "12900424",
      "cited_paper_id": 773012
    },
    {
      "context_text": "model whose parameters depend on hand posture (finger, thumb, or two thumbs), the portion of the keyboard (left, right, top, or bottom), and the individual user [10, 1, 3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and parameters. No verifiable resources are identified.",
      "processing_time": 30.73863196372986,
      "citing_paper_id": "12900424",
      "cited_paper_id": 821106
    },
    {
      "context_text": "[10]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets or resources. It only ends with a citation marker.",
      "processing_time": 28.376314401626587,
      "citing_paper_id": "12900424",
      "cited_paper_id": 821106
    },
    {
      "context_text": "Innovations such as word-gesture typing, or shape writing [36, 37], also leverage language regularities in the form of a lexicon that could further be adapted to the individual user by extracting a vocabulary from the user’s past documents [22], or by moving passive words into an in-use active…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and innovations in text input systems, but does not reference any named datasets.",
      "processing_time": 33.41023325920105,
      "citing_paper_id": "12900424",
      "cited_paper_id": 1697605
    },
    {
      "context_text": "A model-intrinsic measure like per-word per-plexity, though commonly used to compare LMs, can correlate poorly with downstream metrics, as is the case with word error rate in speech recognition [6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses evaluation metrics and their correlation with downstream metrics.",
      "processing_time": 15.048883199691772,
      "citing_paper_id": "12900424",
      "cited_paper_id": 1995619
    },
    {
      "context_text": "In ASR it was only when a reference recognizer was applied that language modeling studies could move from intrinsic measures such as perplexity to extrinsic measures such as WER [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods.",
      "processing_time": 14.520436525344849,
      "citing_paper_id": "12900424",
      "cited_paper_id": 1995619
    },
    {
      "context_text": "In practice, however, even signiﬁcant improvements in perplexity do not necessarily correlate with commensurate improvements in extrinsic objectives, particularly in speech recognition research, where WER is the dominant evaluation metric [6, 12].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and their correlation with performance improvements.",
      "processing_time": 15.22057318687439,
      "citing_paper_id": "12900424",
      "cited_paper_id": 1995619
    },
    {
      "context_text": "Innovations such as word-gesture typing, or shape writing [36, 37], also leverage language regularities in the form of a lexicon that could further be adapted to the individual user by extracting a vocabulary from the user’s past documents [22], or by moving passive words into an in-use active vocabulary [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses methods and innovations in text input systems, but does not reference any dataset by name.",
      "processing_time": 18.883766889572144,
      "citing_paper_id": "12900424",
      "cited_paper_id": 3970190
    },
    {
      "context_text": "The results of [8] indicate that using an exponentially-decaying cache results in improvements in perplexity, but as shown later, our extrinsic evaluation of text entry appears to indicate that these gains are negligible in our application.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (exponentially-decaying cache) and its impact on perplexity and text entry performance.",
      "processing_time": 18.14945960044861,
      "citing_paper_id": "12900424",
      "cited_paper_id": 5735444
    },
    {
      "context_text": "The decay rate α describes how quickly word weights decay in the cache, and β is a normalizing constant: We used a grid search on held-out data to ﬁnd that the optimal decay rate α is 0.0003, which is a slow rate of decay but consistent with the values determined in [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses parameters and methods but does not reference a named dataset.",
      "processing_time": 17.351903915405273,
      "citing_paper_id": "12900424",
      "cited_paper_id": 5735444
    },
    {
      "context_text": "Equation 6 (adapted from [8]) illustrates the basic method, where P cache is the conditional probability of word w i given the cache w 1 through w i − 1 , and I is a binary function such that I ( A ) = 1 if A is true, and 0 otherwise.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for language model adaptation. No verifiable resources are identified.",
      "processing_time": 17.10126519203186,
      "citing_paper_id": "12900424",
      "cited_paper_id": 5735444
    },
    {
      "context_text": "The exponentially-decaying cache was derived from [8], which ﬁrst described the technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a technique. The context and title do not provide information about a reusable dataset.",
      "processing_time": 18.1817467212677,
      "citing_paper_id": "12900424",
      "cited_paper_id": 5735444
    },
    {
      "context_text": "Cache-based LMs are further explored in [8], which describes a method in which the relative weights of words in the cache are made to decay exponentially, with “older” words having lower weights.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting language models using an exponentially decaying cache.",
      "processing_time": 17.337907314300537,
      "citing_paper_id": "12900424",
      "cited_paper_id": 5735444
    },
    {
      "context_text": "Third, while more complex models of touch have been explored in the literature (Holz et al. [13], Azenkot & Zhai [1], and others highlight additional phenomena involved with ﬁn-ger touches, in particular spatial offsets that depend on ﬁnger position, visual cues, and hand posture), it is quite hard…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and phenomena related to touch. There are no verifiable resources or datasets mentioned.",
      "processing_time": 19.946234226226807,
      "citing_paper_id": "12900424",
      "cited_paper_id": 8312298
    },
    {
      "context_text": "Keystroke savings (see Equation 4) is the de-facto evaluation standard for text entry efﬁciency measurement [25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric for evaluating text entry efficiency.",
      "processing_time": 16.502930641174316,
      "citing_paper_id": "12900424",
      "cited_paper_id": 10596609
    },
    {
      "context_text": "Klarlund and Riley [17] applied N-gram language modeling to a cluster keyboard (i.e. a keyboard with reduced number of keys), and showed large improvements over uni-gram frequency models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (N-gram language modeling) applied to a cluster keyboard, which is not a dataset.",
      "processing_time": 21.851205825805664,
      "citing_paper_id": "12900424",
      "cited_paper_id": 17379458
    },
    {
      "context_text": "An excellent review of various algorithms can be found in [2] and [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers for algorithm reviews.",
      "processing_time": 17.094019651412964,
      "citing_paper_id": "12900424",
      "cited_paper_id": 31694302
    },
    {
      "context_text": "The Enron Corpus [18] is a large set of emails that were collected by the Federal Regulatory Commission when the Enron Corporation was under investigation in late 2001.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Enron Corpus"
      ],
      "dataset_descriptions": {
        "Enron Corpus": "Used to study email communication patterns and content, specifically analyzing a large set of emails collected during the Enron investigation for potential misuse and fraud."
      },
      "confidence_score": 1.0,
      "reasoning": "The Enron Corpus is explicitly mentioned and described as a large set of emails used for investigation purposes, which fits the criteria for a dataset.",
      "processing_time": 25.282734632492065,
      "citing_paper_id": "12900424",
      "cited_paper_id": 44854032
    },
    {
      "context_text": "have the same meaning as the corresponding input sentences. Following existing literature, we use the BLEU metric5 (Papineni et al. 2002) and the ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-L (Lin 2004)). The core contribution of our evaluation framework is in the linguistic-motivation used to quantify the stylistic alignment of a generated piece of text with the target style we wish to achieve. Whi",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU and ROUGE). These are excluded as per the instructions.",
      "processing_time": 20.190064668655396,
      "citing_paper_id": "202719307",
      "cited_paper_id": 964287
    },
    {
      "context_text": "As it can be inferred from the results presented in Table 2, StyleLM performs better than the supervised approach in terms of BLEU, ROUGE-3, ROUGE-L, and lexical stylistic alignment.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and a model (StyleLM).",
      "processing_time": 26.31309151649475,
      "citing_paper_id": "202719307",
      "cited_paper_id": 964287
    },
    {
      "context_text": "This is further substantiated by the higher values for ROUGE-2, ROUGE-3 and ROUGE-L scores.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (ROUGE scores).",
      "processing_time": 27.03203511238098,
      "citing_paper_id": "202719307",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Following existing literature, we use the BLEU metric5 (Papineni et al. 2002) and the ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-L (Lin 2004)).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (BLEU and ROUGE) but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 28.916978120803833,
      "citing_paper_id": "202719307",
      "cited_paper_id": 964287
    },
    {
      "context_text": "…we use the following three corpora to obtain our source sentences : (a) texts from books written by Mark Twain, (b) Opinosis Review dataset (Ganesan, Zhai, and Han 2010), (c) a Wikipedia article on Artificial Intelligence (https://en.wikipedia.org/wiki/Artificial intelligence) which does…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Opinosis Review dataset"
      ],
      "dataset_descriptions": {
        "Opinosis Review dataset": "Used to obtain source sentences for personalized text generation, focusing on highly redundant opinions to generate abstractive summaries."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three sources of text, including a specific dataset (Opinosis Review dataset) and a Wikipedia article. The Mark Twain books are not a specific, verifiable dataset.",
      "processing_time": 38.14358067512512,
      "citing_paper_id": "202719307",
      "cited_paper_id": 988010
    },
    {
      "context_text": "For inference during test-time, we use the following three corpora to obtain our source sentences : (a) texts from books written by Mark Twain, (b) Opinosis Review dataset (Ganesan, Zhai, and Han 2010), (c) a Wikipedia article on Artificial Intelligence (https://en.wikipedia.org/wiki/Artificial intelligence) which does not appear in the original mix of the Wikipedia training corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "texts from books written by Mark Twain",
        "Opinosis Review dataset"
      ],
      "dataset_descriptions": {
        "texts from books written by Mark Twain": "Used to obtain source sentences for inference, focusing on the style and content of Mark Twain's writing.",
        "Opinosis Review dataset": "Used to obtain source sentences for inference, focusing on summarizing highly redundant opinions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three sources of text used for inference during test-time. Two of these are specific datasets: texts from books by Mark Twain and the Opinosis Review dataset. The third is a Wikipedia article, which is not considered a dataset.",
      "processing_time": 48.25588536262512,
      "citing_paper_id": "202719307",
      "cited_paper_id": 988010
    },
    {
      "context_text": "All the fine-tuned StyleLM models are tested on a test set that spans different domains – (a) Opinosis (Ganesan, Zhai, and Han 2010) which contains sentences extracted from user reviews on a variety of topics from Tripadvisor (hotels), Edmunds.com (cars) and Amazon.com (various electronics), (b)…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Opinosis"
      ],
      "dataset_descriptions": {
        "Opinosis": "Used to test fine-tuned StyleLM models across various domains, including user reviews from Tripadvisor, Edmunds.com, and Amazon.com, focusing on summarization of highly redundant opinions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Opinosis' as a specific dataset used for testing fine-tuned StyleLM models. It is a multi-word proper noun and fits the criteria for a dataset.",
      "processing_time": 39.84692025184631,
      "citing_paper_id": "202719307",
      "cited_paper_id": 988010
    },
    {
      "context_text": "All the fine-tuned StyleLM models are tested on a test set that spans different domains – (a) Opinosis (Ganesan, Zhai, and Han 2010) which contains sentences extracted from user reviews on a variety of topics from Tripadvisor (hotels), Edmunds.com (cars) and Amazon.com (various electronics), (b) text from Mark Twain’s books, and (c) a Wikipedia page on Artificial Intelligence7.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Opinosis"
      ],
      "dataset_descriptions": {
        "Opinosis": "Used to test fine-tuned StyleLM models, specifically evaluating performance on sentences extracted from user reviews on hotels, cars, and various electronics from Tripadvisor, Edmunds.com, and Amazon.com."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Opinosis' as a specific dataset used for testing fine-tuned StyleLM models. It is described as containing sentences from user reviews across multiple domains.",
      "processing_time": 40.07959794998169,
      "citing_paper_id": "202719307",
      "cited_paper_id": 988010
    },
    {
      "context_text": "Texts from these sources span a diverse range of topics and writing styles – while Mark\nTwain’s writings are literary, Opinosis reviews are everyday, the Wikipedia article on AI presents an interesting scenario where many of the words in the source text are not present in target author’s corpus, given the different timelines.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Opinosis reviews"
      ],
      "dataset_descriptions": {
        "Opinosis reviews": "Used to study everyday writing styles in abstractive summarization, focusing on highly redundant opinions and their representation in target texts."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Opinosis reviews' which is a specific dataset used for summarization. No other datasets are explicitly named.",
      "processing_time": 37.26974678039551,
      "citing_paper_id": "202719307",
      "cited_paper_id": 988010
    },
    {
      "context_text": "Implementation Details During pre-training with MLM, we use the Transformer encoder (Vaswani et al. 2017) (12-layer) with GELU activations (Hendrycks and Gimpel 2017), 512 hidden units, 16 heads, a dropout rate of 0.1 and learned positional embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on implementation details of pre-training with a Transformer encoder and GELU activations.",
      "processing_time": 32.37145471572876,
      "citing_paper_id": "202719307",
      "cited_paper_id": 2359786
    },
    {
      "context_text": "2017) (12-layer) with GELU activations (Hendrycks and Gimpel 2017), 512 hidden units, 16 heads, a dropout rate of 0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model configurations and activation functions. No verifiable resources are identified.",
      "processing_time": 29.658191680908203,
      "citing_paper_id": "202719307",
      "cited_paper_id": 2359786
    },
    {
      "context_text": "Tasks like genre classification (Kessler, Numberg, and Schütze 1997), author profiling (Garera and Yarowsky 2009), sentiment analysis (Wilson, Wiebe, and Hoffmann 2005), social relationship classification (Peterson, Hohensee, and Xia 2011) have been of active interest to the community.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and areas of interest. No dataset names are present in the text.",
      "processing_time": 30.933565855026245,
      "citing_paper_id": "202719307",
      "cited_paper_id": 3063772
    },
    {
      "context_text": "Tasks like genre classification (Kessler, Numberg, and Schütze 1997), author profiling (Garera and Yarowsky 2009), sentiment analysis (Wilson, Wiebe, and Hoffmann 2005), social relationship classification (Peterson, Hohensee, and Xia 2011) have been of active interest to the community.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and areas of interest. No dataset names are present in the text.",
      "processing_time": 30.933565855026245,
      "citing_paper_id": "202719307",
      "cited_paper_id": 11113728
    },
    {
      "context_text": "We train our models with the Adam optimizer (Kingma and Ba 2014), and a learning rate of 10−4 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the Adam optimizer which is a method, not a dataset.",
      "processing_time": 29.65112042427063,
      "citing_paper_id": "202719307",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "There has been a growing interest in studying style in natural language and solving tasks related to it (Hu et al. 2017; Shen et al. 2017; Subramanian et al. 2018; Fu et al. 2018; Vadapalli et al. 2018; Niu and Bansal 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a growing interest in studying style in natural language and related tasks. No verifiable resources are named.",
      "processing_time": 32.67540240287781,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "There has been a growing interest in studying style in natural language and solving tasks related to it (Hu et al. 2017; Shen et al. 2017; Subramanian et al. 2018; Fu et al. 2018; Vadapalli et al. 2018; Niu and Bansal 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a growing interest in studying style in natural language and related tasks. No verifiable resources are named.",
      "processing_time": 32.67540240287781,
      "citing_paper_id": "202719307",
      "cited_paper_id": 53036695
    },
    {
      "context_text": "There has been a growing interest in studying style in natural language and solving tasks related to it (Hu et al. 2017; Shen et al. 2017; Subramanian et al. 2018; Fu et al. 2018; Vadapalli et al. 2018; Niu and Bansal 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a growing interest in studying style in natural language and related tasks. No verifiable resources are named.",
      "processing_time": 32.67540240287781,
      "citing_paper_id": "202719307",
      "cited_paper_id": 53295789
    },
    {
      "context_text": "The approaches themselves can range from completely supervised, which is contingent on the availability of parallel data (Ficler and Goldberg 2017), to unsupervised (Li et al. 2018; Shen et al. 2017; Jain et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about the range of approaches from supervised to unsupervised, but no dataset names are provided.",
      "processing_time": 33.7428765296936,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "A majority of the work here is focused around generating text with different levels of sentiment (Shen et al. 2017; Ficler and Goldberg 2017) and formality (Jain et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing text generation with sentiment and formality. No clear, verifiable datasets are identified.",
      "processing_time": 33.02580285072327,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "A majority of the work here is focused around generating text with different levels of sentiment (Shen et al. 2017; Ficler and Goldberg 2017) and formality (Jain et al. 2019)\n∗A part of this work was done when the author was an intern at Adobe Research.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No verifiable resources are identified.",
      "processing_time": 30.930790662765503,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "Stylized Text Generation: In recent times, several explorations that aim to generate stylized text define a psycholinguistic aspect, like, formality or sentiment (Jain et al. 2019; Shen et al. 2017; Ficler and Goldberg 2017) and transfer text along this dimension.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to stylized text generation.",
      "processing_time": 30.90960383415222,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "For quantifying these lexical elements, we use a list of seed words for each of the eight categories above, viz. subjective, objective, concrete, abstract, literary, colloquial, formal and informal (Brooke and Hirst 2013).",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a list of seed words for lexical categories but does not specify a dataset name. The reference is to a method or resource for categorizing lexical elements.",
      "processing_time": 33.89138698577881,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7708706
    },
    {
      "context_text": "Following Brooke and Hirst (2013), we compute normalized pointwise mutual information index (PMI) to obtain a raw style score for each dimension, by leveraging co-occurrences of words in the large corpus.",
      "catation_intent": "method",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large corpus' but does not provide a specific name or identifier. It is a generic reference and thus does not meet the criteria for inclusion.",
      "processing_time": 33.526777029037476,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7708706
    },
    {
      "context_text": "Inspired from Brooke and Hirst (2013), we consider four different spectrums to take lexicalstyle into account: (i) subjective-objective, (ii) concreteabstract, (iii) literary-colloquial, and (iv) formal-informal.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only lexical style dimensions. No clear identifiers for datasets are present.",
      "processing_time": 31.208049058914185,
      "citing_paper_id": "202719307",
      "cited_paper_id": 7708706
    },
    {
      "context_text": "Jhamtani et al. (2017) and Tikhonov and Yamshchikov (2018) overcome this by using the content preservation metrics as a proxy of transfer strength, leveraging the availability of the ground-truth stylized text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of content preservation metrics as a proxy for transfer strength. No verifiable dataset names are provided.",
      "processing_time": 33.42516303062439,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "Jhamtani et al. (2017) and Tikhonov and Yamshchikov (2018) overcome this by using the content preservation metrics as a proxy of transfer strength, leveraging the availability of the ground-truth stylized text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of content preservation metrics as a proxy for transfer strength. No verifiable dataset names are provided.",
      "processing_time": 33.42516303062439,
      "citing_paper_id": "202719307",
      "cited_paper_id": 49879813
    },
    {
      "context_text": "While the work by Jhamtani et al. (2017) tries to generate “Shakespearized” text from Modern English and is in a similar vein, it relies on the availability of parallel data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'parallel data' but does not specify a dataset name. The title confirms the use of parallel data but does not name a specific dataset.",
      "processing_time": 32.91290473937988,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "However, since Jhamtani et al.’s model requires a parallel corpus, we compare this baseline only for generating Shakespearized text (using their data from (Jhamtani et al. 2017)).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'their data' from Jhamtani et al. 2017, which is a parallel corpus used for generating Shakespearized text. However, the name 'their data' is too generic and does not meet the criteria for a specific, verifiable dataset.",
      "processing_time": 37.56839084625244,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "The performance, as quantified by rest of the metrics, is comparable to that of (Jhamtani et al. 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics.",
      "processing_time": 29.86016082763672,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "There are some works that adapt an input text to the writing style of a specific author (Jhamtani et al. 2017; Tikhonov and Yamshchikov 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to adapting text to an author's writing style.",
      "processing_time": 31.421032190322876,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "There are some works that adapt an input text to the writing style of a specific author (Jhamtani et al. 2017; Tikhonov and Yamshchikov 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to adapting text to an author's writing style.",
      "processing_time": 31.421032190322876,
      "citing_paper_id": "202719307",
      "cited_paper_id": 49879813
    },
    {
      "context_text": "While Jhamtani et al. (2017) aim to generate Shakespearized version of modern English language using parallel data, Tikhonov and Yamshchikov (2018) use the multilingual setup to generate author-stylized poetry using paired instances of Russian and English poetry.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'parallel data' and 'paired instances of Russian and English poetry', which are likely datasets used in the respective studies. However, specific dataset names are not provided.",
      "processing_time": 34.89823031425476,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "While Jhamtani et al. (2017) aim to generate Shakespearized version of modern English language using parallel data, Tikhonov and Yamshchikov (2018) use the multilingual setup to generate author-stylized poetry using paired instances of Russian and English poetry.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'parallel data' and 'paired instances of Russian and English poetry', which are likely datasets used in the respective studies. However, specific dataset names are not provided.",
      "processing_time": 34.89823031425476,
      "citing_paper_id": "202719307",
      "cited_paper_id": 49879813
    },
    {
      "context_text": "Comparison with Supervised Approach While StyleLM performs better than the other unsupervised stylized generation models, as shown in Table 1, it is critical to determine its performance with respect to the supervised approach proposed by Jhamtani et al. (2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between an unsupervised model (StyleLM) and a supervised approach. No verifiable resources are identified.",
      "processing_time": 34.56603503227234,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "However, there has been lack of explorations that aim to generate text across author styles – wherein the notion of style is not a specific psycholinguistic aspect but an amalgam of the author’s linguistic choices expressed in their writing (Jhamtani et al. 2017; Tikhonov and Yamshchikov 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a discussion about the lack of exploration in generating text across author styles.",
      "processing_time": 33.15504574775696,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "However, there has been lack of explorations that aim to generate text across author styles – wherein the notion of style is not a specific psycholinguistic aspect but an amalgam of the author’s linguistic choices expressed in their writing (Jhamtani et al. 2017; Tikhonov and Yamshchikov 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a discussion about the lack of exploration in generating text across author styles.",
      "processing_time": 33.15504574775696,
      "citing_paper_id": "202719307",
      "cited_paper_id": 49879813
    },
    {
      "context_text": "Similar to Table 2, we evaluate the performance of all the models against the approach of (Jhamtani et al. 2017) which relies on parallel data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'parallel data' but does not specify a named dataset. The term 'parallel data' is too generic and lacks a specific identifier.",
      "processing_time": 34.26765322685242,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "Supervised Stylized Rewriting: Jhamtani et al. (2017) propose an LSTM-based encoder-decoder architecture for generating a “Shakespearized” text originally written in modern English, by leveraging parallel data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'parallel data' which is likely a dataset used for training the model, but it does not specify a named dataset. The term 'parallel data' is too generic and lacks a specific identifier.",
      "processing_time": 36.097999572753906,
      "citing_paper_id": "202719307",
      "cited_paper_id": 9737200
    },
    {
      "context_text": "Linguists have identified style, especially in English language, to be expressed at three levels – surface, lexical and syntactic (Strunk 2007; DiMarco and Hirst 1988; Crystal and Davy 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general linguistic concepts and levels of style expression.",
      "processing_time": 32.861573934555054,
      "citing_paper_id": "202719307",
      "cited_paper_id": 16291649
    },
    {
      "context_text": "and also a combination of these attributes (Subramanian et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper. No clear identifiers for datasets are present.",
      "processing_time": 34.255980014801025,
      "citing_paper_id": "202719307",
      "cited_paper_id": 53295789
    },
    {
      "context_text": "Under review.\nand also a combination of these attributes (Subramanian et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a combination of attributes for text style transfer.",
      "processing_time": 35.72716021537781,
      "citing_paper_id": "202719307",
      "cited_paper_id": 53295789
    },
    {
      "context_text": "Vanilla GPT-2 based generation: Radford et al. (2019) show that language models present considerable promise as unsupervised multi-task learners.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capabilities of language models.",
      "processing_time": 36.31993579864502,
      "citing_paper_id": "202719307",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "The recently proposed language model GPT-2 (Radford et al. 2019) is pre-trained on a large and diverse dataset (WebText) and is shown to perform well across several domains and datasets including natural language generation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WebText"
      ],
      "dataset_descriptions": {
        "WebText": "Used for pre-training GPT-2, a large and diverse dataset that contributes to the model's performance across various domains, including natural language generation."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions WebText as a dataset used for pre-training GPT-2, which is relevant to personalized text generation. However, the context does not provide specific details on how WebText is used in the research.",
      "processing_time": 49.69261384010315,
      "citing_paper_id": "202719307",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "The choice of using a Transformer-based architecture is based on their recent success in language modeling (Vaswani et al. 2017; Devlin et al. 2019; Radford et al. 2018; 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and architectures. The context focuses on the success of Transformer-based architectures in language modeling.",
      "processing_time": 38.965919494628906,
      "citing_paper_id": "202719307",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "We use their vanilla GPT-2 pre-trained Transformer decoder (Radford et al. 2019) as our first baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using GPT-2 as a baseline, which is a model, not a dataset. No specific datasets are mentioned.",
      "processing_time": 39.87557053565979,
      "citing_paper_id": "202719307",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Parameter optimization is done using Adagrad (Duchi et al., 2011) with a mini-batch size of 100 and a learning rate α = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Adagrad) for parameter optimization.",
      "processing_time": 39.11086368560791,
      "citing_paper_id": "94285",
      "cited_paper_id": 538820
    },
    {
      "context_text": "Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various tasks and applications of neural network models. No verifiable resources are identified.",
      "processing_time": 40.633784532547,
      "citing_paper_id": "94285",
      "cited_paper_id": 2141094
    },
    {
      "context_text": "Gao et al. (2014a) successfully use an embedding model to reﬁne the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for refining phrase-translation probabilities using an embedding model.",
      "processing_time": 40.01490759849548,
      "citing_paper_id": "94285",
      "cited_paper_id": 2141094
    },
    {
      "context_text": "…have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks and applications. There are no clear identifiers for datasets.",
      "processing_time": 41.22933316230774,
      "citing_paper_id": "94285",
      "cited_paper_id": 2141094
    },
    {
      "context_text": "We take this as evidence that CMM exact matches and DCGM semantic matches interact positively, a ﬁnding that comports with Gao et al. (2014a), who show that semantic relationships mined through phrase embed-dings correlate positively with classic co-occurrence-based estimations.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological finding. No dataset names are present in the citation context.",
      "processing_time": 41.4730761051178,
      "citing_paper_id": "94285",
      "cited_paper_id": 2141094
    },
    {
      "context_text": "The probability distribution over the next word given the previous history is obtained by applying the softmax activation function:\nP (st = w|s1, . . . , st−1) = exp(otw)∑V v=1 exp(otv) .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a mathematical formula for the softmax activation function. There are no verifiable resources or datasets mentioned.",
      "processing_time": 42.4641432762146,
      "citing_paper_id": "94285",
      "cited_paper_id": 2617020
    },
    {
      "context_text": "The log-linear weights are estimated by running one iteration of MERT.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MERT) for estimating log-linear weights.",
      "processing_time": 40.830806493759155,
      "citing_paper_id": "94285",
      "cited_paper_id": 5474833
    },
    {
      "context_text": "Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MERT, which is a method for estimating log-linear weights in statistical machine translation. No specific dataset is mentioned.",
      "processing_time": 41.97937607765198,
      "citing_paper_id": "94285",
      "cited_paper_id": 5474833
    },
    {
      "context_text": ", 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works in the fields of Machine Translation and Language Modeling.",
      "processing_time": 41.975651025772095,
      "citing_paper_id": "94285",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only features and models. The language model is trained on Twitter responses, but 'Twitter responses' is too generic and lacks a specific identifier.",
      "processing_time": 45.37276220321655,
      "citing_paper_id": "94285",
      "cited_paper_id": 9685476
    },
    {
      "context_text": "Our context-sensitive models consistently outperform both context-independent and context-sensitive baselines by up to 11% relative improvement in BLEU in the MT setting and 24% in the IR setting, albeit using a minimal number of features.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance improvements in machine translation and information retrieval settings.",
      "processing_time": 40.99277305603027,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using mined multi-references, BLEU rankings align well with human judgments.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (BLEU) and a method (mining multi-references).",
      "processing_time": 42.883623123168945,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "DCGM-{I-II}+CMM systems each totaling 10 features show increases of up to 0.48 BLEU points over MT+CMM and up to 0.88 BLEU over the model based on Ritter et al. (2011).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and evaluation metrics. The context focuses on comparing system performance using BLEU scores.",
      "processing_time": 43.21539306640625,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores\n4For the human score, we compute corpus-level BLEU with a sampling scheme that randomly leaves out one reference - the human sentence to score - for each reference set.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for computing a human score using BLEU. No verifiable datasets are referenced.",
      "processing_time": 43.22570538520813,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "These results are consistent with the automated BLEU rankings and confirm that our best performing DCGM models outperform both raw baseline and the context-sensitive baseline using CMM features.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the performance of models using BLEU scores. BLEU is a metric, not a dataset.",
      "processing_time": 43.82822942733765,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "METEOR improvements similarly align with BLEU improvements both for MT and IR lists.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU, METEOR).",
      "processing_time": 41.94882154464722,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics (BLEU, METEOR) but does not refer to any specific datasets. The cited paper titles confirm that these are evaluation methods, not datasets.",
      "processing_time": 45.897873878479004,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "CMM MT+CMM, totaling 17 features (9 from MT + 8 CMM), improves 0.38 BLEU points, a 9.5% relative improvement, over the baseline MT model.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BLEU) for evaluating machine translation models.",
      "processing_time": 42.416542291641235,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We have proposed a novel multi-reference extraction technique allowing for robust automated evaluation using standard SMT metrics such as BLEU and METEOR.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics and methods. The cited paper title 'Bleu: a Method for Automatic Evaluation of Machine Translation' confirms that BLEU is a method, not a dataset.",
      "processing_time": 47.76057744026184,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "IR+CMM, with 10 features (IR + word penalty + 8 CMM), benefits even more, attaining 1.8 BLEU points and 1.5 METEOR points over the IR base-\nRANDOM system so as to make BLEU scores comparable.\nline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (BLEU, METEOR) and a method (IR+CMM).",
      "processing_time": 44.832977056503296,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (BLEU scores).",
      "processing_time": 41.928542137145996,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Third, the neural network models contribute measurably to improvement: RLMT and DCGM models outperform baselines, and DCGM models provide more consistent gains than RLMT.\nMT vs. IR BLEU and METEOR scores indicate that the phrase-based MT decoder outperforms a purely IR approach, despite the fact that IR proposes fluent human generated responses.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and evaluation metrics. The context focuses on comparing model performance and evaluation metrics.",
      "processing_time": 44.31244468688965,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "The results of automatic evaluation using BLEU and METEOR are presented in Table 3, where some broad patterns emerge.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BLEU and METEOR, which are evaluation metrics, not datasets. No specific datasets are mentioned in the context.",
      "processing_time": 44.5777313709259,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (BLEU, METEOR) but does not refer to any specific datasets. The context is about evaluating systems, not using datasets.",
      "processing_time": 46.119446992874146,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Table 5 provides examples of responses generated on the tuning corpus by the MT-based DCGM-II+CMM system, our best system in terms of both BLEU and human evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'tuning corpus' but does not provide a specific name or identifier for the corpus. It is a generic reference and thus does not meet the criteria for inclusion.",
      "processing_time": 46.991053104400635,
      "citing_paper_id": "94285",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "For example, Mikolov and Zweig (2012) and Auli et al. (2013) use a pre-trained topic model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions a pre-trained topic model but does not specify a dataset name. The context does not provide enough information to identify a specific, verifiable dataset.",
      "processing_time": 46.111857414245605,
      "citing_paper_id": "94285",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "…activation function: The RLM is trained to minimize the negative log-likelihood of the training sentence s : The recurrence is unrolled backwards in time us-ing the back-propagation through time (BPTT) al-gorithm (Rumelhart et al., 1988), and gradients are accumulated over multiple time-steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BPTT algorithm) and a model (RLM).",
      "processing_time": 44.8207950592041,
      "citing_paper_id": "94285",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "We found that this helps learning the embedding matrix as it reduces the vanishing gradient effect partially due to stacking of squashing non-linearities (Pascanu et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in training recurrent neural networks.",
      "processing_time": 43.16945695877075,
      "citing_paper_id": "94285",
      "cited_paper_id": 14650762
    },
    {
      "context_text": "Modelling such long-range dependencies with an RLM is difficult and is still considered an open problem (Pascanu et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological challenge in training recurrent neural networks.",
      "processing_time": 43.165724754333496,
      "citing_paper_id": "94285",
      "cited_paper_id": 14650762
    },
    {
      "context_text": "In order to speed up training, we use the Noise-Contrastive Estimation (NCE) loss, which avoids repeated summations over V by approximating the probability of the target word (Gutmann and Hyvärinen, 2010).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Noise-Contrastive Estimation) used to speed up training.",
      "processing_time": 44.80945825576782,
      "citing_paper_id": "94285",
      "cited_paper_id": 15816723
    },
    {
      "context_text": "…Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of neural networks. No dataset names are present in the text.",
      "processing_time": 45.07325863838196,
      "citing_paper_id": "94285",
      "cited_paper_id": 221275765
    },
    {
      "context_text": "Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and strategies for obtaining order-sensitive representations.",
      "processing_time": 43.1541588306427,
      "citing_paper_id": "94285",
      "cited_paper_id": 221275765
    },
    {
      "context_text": "(Bousmalis et al. 2016) used adversarial networks to learned shared representations between two domains which don’t contain the individual features of each domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (adversarial networks) and a concept (shared representations).",
      "processing_time": 45.051544427871704,
      "citing_paper_id": "6484065",
      "cited_paper_id": 2127515
    },
    {
      "context_text": "NIST (Doddington et al. 2000) and Meteor (Banerjee and Lavie 2005) are also used widely in Natural Language Processing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NIST and Meteor, but these are not datasets. NIST refers to a speaker recognition evaluation, and Meteor is a metric for machine translation evaluation. Neither fits the criteria for a dataset.",
      "processing_time": 47.9096405506134,
      "citing_paper_id": "6484065",
      "cited_paper_id": 7164502
    },
    {
      "context_text": "NIST (Doddington et al. 2000) and Meteor (Banerjee and Lavie 2005) are also used widely in Natural Language Processing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions NIST and Meteor, but these are not datasets. NIST refers to a speaker recognition evaluation, and Meteor is a metric for machine translation evaluation. Neither fits the criteria for a dataset.",
      "processing_time": 47.9096405506134,
      "citing_paper_id": "6484065",
      "cited_paper_id": 16090741
    },
    {
      "context_text": "2000) and Meteor (Banerjee and Lavie 2005) are also used widely in Natural Language Processing.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions METEOR, which is a metric, not a dataset. No datasets are mentioned in the citation span.",
      "processing_time": 44.77407717704773,
      "citing_paper_id": "6484065",
      "cited_paper_id": 7164502
    },
    {
      "context_text": "The ﬁrst model implements a multi-decoder seq2seq proposed by Sutskever, Vinyals, and Le (2014), where the encoder is used to capture the content c of the input X , and the multi-decoder contains n ( n ≥ 2) decoders to generate outputs in different styles.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture. The citation is about a method, not a dataset.",
      "processing_time": 45.03937220573425,
      "citing_paper_id": "6484065",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Sequence to sequence (seq2seq) neural network models (Sutskever, Vinyals, and Le 2014) have demonstrated great success in many generation tasks, such as machine translation, dialog system and image caption, with the requirement of a large amount of parallel data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the requirement for a large amount of parallel data. No specific dataset names are provided.",
      "processing_time": 45.490187644958496,
      "citing_paper_id": "6484065",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Li et al. (2017) proposes to treat style transfer as a domain adaptation problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to style transfer.",
      "processing_time": 42.118035316467285,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12634703
    },
    {
      "context_text": "…is an important problem in many subﬁelds of artiﬁcial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reﬂects the ability of intelligence systems to generate novel contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of AI and NLP. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 47.13051629066467,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12634703
    },
    {
      "context_text": "…is an important problem in many subﬁelds of artiﬁcial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reﬂects the ability of intelligence systems to generate novel contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of AI and NLP. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 47.13051629066467,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12924204
    },
    {
      "context_text": "…is an important problem in many subﬁelds of artiﬁcial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reﬂects the ability of intelligence systems to generate novel contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of AI and NLP. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 47.13051629066467,
      "citing_paper_id": "6484065",
      "cited_paper_id": 206593710
    },
    {
      "context_text": "In computer vision, Li et al. (2017) proposes an expression to distinguish style and content of a picture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for distinguishing style and content in images.",
      "processing_time": 43.976282596588135,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12634703
    },
    {
      "context_text": "Style transfer is an important problem in many subfields of artificial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reflects the ability of intelligence systems to generate novel contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to style transfer in AI and NLP.",
      "processing_time": 45.264758348464966,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12634703
    },
    {
      "context_text": "Style transfer is an important problem in many subfields of artificial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reflects the ability of intelligence systems to generate novel contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods related to style transfer in AI and NLP.",
      "processing_time": 45.264758348464966,
      "citing_paper_id": "6484065",
      "cited_paper_id": 12924204
    },
    {
      "context_text": "For tasks like poetry generation (Yan et al. 2013; Yan 2016; Ghazvininejad et al. 2016), style transfer can be applied to generate poetry in different styles.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and applications of style transfer in poetry generation.",
      "processing_time": 43.96760439872742,
      "citing_paper_id": "6484065",
      "cited_paper_id": 14079825
    },
    {
      "context_text": "For tasks like poetry generation (Yan et al. 2013; Yan 2016; Ghazvininejad et al. 2016), style transfer can be applied to generate poetry in different styles.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and applications of style transfer in poetry generation.",
      "processing_time": 43.96760439872742,
      "citing_paper_id": "6484065",
      "cited_paper_id": 16853592
    },
    {
      "context_text": "…transfer is an important problem in many subﬁelds of artiﬁcial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reﬂects the ability of intelligence systems to generate…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of AI and NLP. No verifiable resources are identified.",
      "processing_time": 45.45038151741028,
      "citing_paper_id": "6484065",
      "cited_paper_id": 206593710
    },
    {
      "context_text": "Style transfer is an important problem in many subﬁelds of artiﬁcial intelligence (AI), such as natural language processing (NLP) and computer vision (Gatys, Ecker, and Bethge 2016; Gatys et al. 2016; Zhu et al. 2017; Li et al. 2017), as it reﬂects the ability of intelligence systems to generate…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on style transfer techniques in AI, particularly in NLP and computer vision.",
      "processing_time": 46.91523289680481,
      "citing_paper_id": "6484065",
      "cited_paper_id": 206593710
    },
    {
      "context_text": "Afterwards, commonsense knowledge was used in natural language inference (R. Bowman et al. 2015; Zhang et al. 2017) and language generation (Zhou et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of commonsense knowledge in natural language inference and generation. No verifiable resources are identified.",
      "processing_time": 46.49589467048645,
      "citing_paper_id": "52136077",
      "cited_paper_id": 1461182
    },
    {
      "context_text": "Afterwards, commonsense knowledge was used in natural language inference (R. Bowman et al. 2015; Zhang et al. 2017) and language generation (Zhou et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of commonsense knowledge in natural language inference and generation. No verifiable resources are identified.",
      "processing_time": 46.49589467048645,
      "citing_paper_id": "52136077",
      "cited_paper_id": 14604520
    },
    {
      "context_text": "The parameters are set as follows: GloVe.6B (Pennington, Socher, and Manning 2014) is used as word vectors, and the vocabulary size is set to 10,000 and the word vector di-mension to 200.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GloVe.6B as word vectors, but it is a model or method, not a dataset. No other datasets are mentioned.",
      "processing_time": 46.757556200027466,
      "citing_paper_id": "52136077",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "We compared our models with the following state-of-the-art baselines: Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism (Luong, Pham, and Manning 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Seq2Seq).",
      "processing_time": 44.21163773536682,
      "citing_paper_id": "52136077",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "Since our focus in this paper is on using knowledge to benefit story ending generation, instead of devising new methods for representing knowledge, we adopt two existing methods: 1) graph attention (Veličković et al. 2018; Zhou et al. 2018), and 2) contextual attention (Mihaylov and Frank 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for representing knowledge. The context focuses on the use of graph attention and contextual attention methods.",
      "processing_time": 46.4976909160614,
      "citing_paper_id": "52136077",
      "cited_paper_id": 3292002
    },
    {
      "context_text": "And commonsense knowledge has also been shown useful to choose a correct story ending from two candidate endings (Lin, Sun, and Han 2017; Li et al. 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the utility of commonsense knowledge in choosing correct story endings.",
      "processing_time": 45.23223876953125,
      "citing_paper_id": "52136077",
      "cited_paper_id": 5286967
    },
    {
      "context_text": "Feature-based (Chaturvedi, Peng, and Dan 2017; Lin, Sun, and Han 2017) or neural (Mostafazadeh et al. 2016b; Wang, Liu, and Zhao 2017) classiﬁcation models are proposed to measure the coherence between a candidate ending and a story context from various aspects such as event, sentiment, and topic.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses classification models for measuring coherence between story endings and contexts, but does not mention any specific datasets.",
      "processing_time": 45.438676834106445,
      "citing_paper_id": "52136077",
      "cited_paper_id": 5286967
    },
    {
      "context_text": "Feature-based (Chaturvedi, Peng, and Dan 2017; Lin, Sun, and Han 2017) or neural (Mostafazadeh et al. 2016b; Wang, Liu, and Zhao 2017) classiﬁcation models are proposed to measure the coherence between a candidate ending and a story context from various aspects such as event, sentiment, and topic.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses classification models for measuring coherence between story endings and contexts, but does not mention any specific datasets.",
      "processing_time": 45.438676834106445,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "Feature-based (Chaturvedi, Peng, and Dan 2017; Lin, Sun, and Han 2017) or neural (Mostafazadeh et al. 2016b; Wang, Liu, and Zhao 2017) classiﬁcation models are proposed to measure the coherence between a candidate ending and a story context from various aspects such as event, sentiment, and topic.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses classification models for measuring coherence between story endings and contexts, but does not mention any specific datasets.",
      "processing_time": 45.438676834106445,
      "citing_paper_id": "52136077",
      "cited_paper_id": 27249900
    },
    {
      "context_text": "Story generation is an important but challenging task because it requires to deal with logic and implicit knowledge (Li et al. 2013; Soo, Lee, and Chen 2016; Ji et al. 2017; Jain et al. 2017; Martin et al. 2018; Clark, Ji, and Smith 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No verifiable resources are identified.",
      "processing_time": 45.775243520736694,
      "citing_paper_id": "52136077",
      "cited_paper_id": 5564363
    },
    {
      "context_text": "Story generation is an important but challenging task because it requires to deal with logic and implicit knowledge (Li et al. 2013; Soo, Lee, and Chen 2016; Ji et al. 2017; Jain et al. 2017; Martin et al. 2018; Clark, Ji, and Smith 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No verifiable resources are identified.",
      "processing_time": 45.775243520736694,
      "citing_paper_id": "52136077",
      "cited_paper_id": 19127777
    },
    {
      "context_text": "Story generation is an important but challenging task because it requires to deal with logic and implicit knowledge (Li et al. 2013; Soo, Lee, and Chen 2016; Ji et al. 2017; Jain et al. 2017; Martin et al. 2018; Clark, Ji, and Smith 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. No verifiable resources are identified.",
      "processing_time": 45.775243520736694,
      "citing_paper_id": "52136077",
      "cited_paper_id": null
    },
    {
      "context_text": "To capture the deep meaning of key entities and events, Ji et al. (2017) and Clark, Ji, and Smith (2018) explicitly modeled the entities mentioned in story with dynamic representation, and Martin et al. (2018) decomposed the problem into planning successive events and generating sentences from some…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers are about modeling entities and events in stories, which does not provide a clear dataset name.",
      "processing_time": 48.0326292514801,
      "citing_paper_id": "52136077",
      "cited_paper_id": 5564363
    },
    {
      "context_text": "To capture the deep meaning of key entities and events, Ji et al. (2017) and Clark, Ji, and Smith (2018) explicitly modeled the entities mentioned in story with dynamic representation, and Martin et al. (2018) decomposed the problem into planning successive events and generating sentences from some…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches. The cited papers are about modeling entities and events in stories, which does not provide a clear dataset name.",
      "processing_time": 48.0326292514801,
      "citing_paper_id": "52136077",
      "cited_paper_id": null
    },
    {
      "context_text": "Comparing to textual entailment or reading comprehension (Dagan, Glickman, and Magnini 2006; Hermann et al. 2015) story ending generation requires more to deal with the logic and causality information that may span multiple sentences in a story context.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to textual entailment and reading comprehension tasks. No verifiable resources are identified.",
      "processing_time": 46.4786593914032,
      "citing_paper_id": "52136077",
      "cited_paper_id": 8587959
    },
    {
      "context_text": "Figure 1 shows an example of a typical story in the ROCStories corpus (Mostafazadeh et al. 2016b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ROCStories corpus"
      ],
      "dataset_descriptions": {
        "ROCStories corpus": "Used to provide examples of typical stories for personalized text generation, focusing on narrative structure and coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ROCStories corpus, which is a specific dataset used for story generation and comprehension tasks.",
      "processing_time": 50.19369888305664,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "For each word, we retrieved a set of triples from ConceptNet and stored those whose head entity and tail entity are noun or verb, meanwhile both occurring in SCT.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a process of retrieving triples from ConceptNet, which is a knowledge base, not a dataset.",
      "processing_time": 47.81510901451111,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "The corpus we used in this paper was ﬁrst designed for Story Cloze Test (SCT) (Mostafazadeh et al. 2016a), which requires to select a correct ending from two candidates given a story context.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Story Cloze Test (SCT)"
      ],
      "dataset_descriptions": {
        "Story Cloze Test (SCT)": "Used to evaluate story endings by selecting the correct conclusion from two candidates, focusing on narrative coherence and logical flow."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a corpus designed for the Story Cloze Test (SCT), which is a specific dataset used for evaluating story endings.",
      "processing_time": 53.45870876312256,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "We evaluated our model on the ROCStories corpus (Mostafazadeh et al. 2016a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ROCStories corpus"
      ],
      "dataset_descriptions": {
        "ROCStories corpus": "Used to evaluate the model's ability to predict story continuations, focusing on narrative coherence and logical flow in short stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ROCStories corpus, which is a specific dataset used for evaluating story comprehension and continuation models.",
      "processing_time": 50.789769411087036,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "Figure 1 shows an example of a typical story in the ROC-Stories corpus (Mostafazadeh et al. 2016b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ROC-Stories corpus"
      ],
      "dataset_descriptions": {
        "ROC-Stories corpus": "Used to provide examples of typical stories for story completion tasks, focusing on narrative structure and coherence in short stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ROC-Stories corpus, which is a specific dataset used for story completion tasks. The corpus is clearly identified and used in the research context.",
      "processing_time": 53.22416806221008,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "Feature-based (Chaturvedi, Peng, and Dan 2017; Lin, Sun, and Han 2017) or neural (Mostafazadeh et al. 2016b; Wang, Liu, and Zhao 2017) classification models are proposed to measure the coherence between a candidate ending and a story context from various aspects such as event, sentiment, and topic.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 46.454402446746826,
      "citing_paper_id": "52136077",
      "cited_paper_id": 15886159
    },
    {
      "context_text": "…deep meaning of key entities and events, Ji et al. (2017) and Clark, Ji, and Smith (2018) explicitly modeled the entities mentioned in story with dynamic representation, and Martin et al. (2018) decomposed the problem into planning successive events and generating sentences from some given events.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for story generation. The context focuses on modeling entities and events in stories using dynamic representations and planning.",
      "processing_time": 48.29167199134827,
      "citing_paper_id": "52136077",
      "cited_paper_id": 19127777
    },
    {
      "context_text": "; Zhou et al. 2018), and 2) contextual attention (Mihaylov and Frank 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 46.24754333496094,
      "citing_paper_id": "52136077",
      "cited_paper_id": 29151507
    },
    {
      "context_text": "Mihaylov and Frank (2018) incorporated external commonsense knowledge into a neural cloze-style reading comprehension model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions incorporating external commonsense knowledge but does not specify a dataset. The focus is on the method rather than a specific dataset.",
      "processing_time": 47.25856351852417,
      "citing_paper_id": "52136077",
      "cited_paper_id": 29151507
    },
    {
      "context_text": "However, story ending generation (Li, Ding, and Liu 2018; Zhao et al. 2018; Peng et al. 2018) is more challenging in that the task requires to modeling context clues and implicit knowledge to produce reasonable endings.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenge of story ending generation. No verifiable resources are identified.",
      "processing_time": 46.69044899940491,
      "citing_paper_id": "52136077",
      "cited_paper_id": 52002178
    },
    {
      "context_text": "NRT [24] leverages gated recurrent units (GRU) [6] to generate tips.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gated recurrent units) and a task (generating tips).",
      "processing_time": 47.571423053741455,
      "citing_paper_id": "235792544",
      "cited_paper_id": 304614
    },
    {
      "context_text": "NRT [24] leverages gated recurrent units (GRU) [6] to generate tips.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gated recurrent units) and a task (generating tips).",
      "processing_time": 47.571423053741455,
      "citing_paper_id": "235792544",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "LexRank [10] is a widely used unsupervised stochastic graph-based method for text summarization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LexRank as a method for text summarization, not as a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 47.97370171546936,
      "citing_paper_id": "235792544",
      "cited_paper_id": 506350
    },
    {
      "context_text": "BeerAdvocate: We have used the BeerAdvocate dataset [32] [30], which consists of beer reviews.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BeerAdvocate"
      ],
      "dataset_descriptions": {
        "BeerAdvocate": "Used to learn attitudes and attributes from multi-aspect reviews, focusing on sentiment and descriptive aspects of beer reviews."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BeerAdvocate dataset, which is a specific, named dataset consisting of beer reviews. It is used for learning attitudes and attributes from multi-aspect reviews.",
      "processing_time": 54.318849086761475,
      "citing_paper_id": "235792544",
      "cited_paper_id": 882396
    },
    {
      "context_text": "Dataset Data Statistics Mean Squared Error Reviews Users Items Density RRCA (Ours) RR(Ours) DeepCoNN [46] NARRE [4] MPCN [40] NeuMF [14]",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names, only model names and evaluation metrics. The context is about comparing performance metrics across different models.",
      "processing_time": 48.382084369659424,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Like DeepCoNN, NARRE also uses TextCNN to discover latent features for each review to predict ratings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to methods and models.",
      "processing_time": 47.24628186225891,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Like DeepCoNN, NARRE also uses TextCNN to discover latent features for each review to predict ratings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to methods and models.",
      "processing_time": 47.24628186225891,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Multi-Pointer Co-Attention Networks (MPCN) [40] works on the same principle as NARRE that not every review is equally important.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MPCN) and a reference to another method (NARRE).",
      "processing_time": 48.584298610687256,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "MPCN proposes a pointer-based review-by-review learning scheme to infer review importance, unlike NARRE’s attention weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing MPCN and NARRE, which are methods.",
      "processing_time": 48.790809869766235,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "In recent years, deep learning models have become successful in the domain of personalized recommendations [4, 5, 14, 40, 46].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep learning models in personalized recommendations.",
      "processing_time": 48.102723360061646,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "We compare ReXPlug’s RRCA with four baseline models: NeuMF, which uses only ratings, and DeepCoNN, MPCN and NARRE, which use reviews as features to predict ratings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different models for rating prediction.",
      "processing_time": 48.89954972267151,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "We consider the mean squared error (MSE) as an evaluation metric for rating prediction, as has been used by many of ReXPlug’s predecessors [4, 5, 40, 46].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an evaluation metric (MSE) used in previous works. No verifiable resources are identified.",
      "processing_time": 49.81329393386841,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Further modifications to DeepCoNN were in the form of TransNets [2], and attentive networks like MPCN [40] and NARRE [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 50.11403489112854,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Further modifications to DeepCoNN were in the form of TransNets [2], and attentive networks like MPCN [40] and NARRE [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 50.11403489112854,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9932413
    },
    {
      "context_text": "Though MPCN, NARRE and DAML provide some extra information about reviews by weighing their importance, the working remains a black box.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the limitations of certain approaches in providing explainable recommendations.",
      "processing_time": 50.72425365447998,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Though MPCN, NARRE and DAML provide some extra information about reviews by weighing their importance, the working remains a black box.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on the limitations of certain approaches in providing explainable recommendations.",
      "processing_time": 50.72425365447998,
      "citing_paper_id": "235792544",
      "cited_paper_id": 196204237
    },
    {
      "context_text": "Like MPCN and NARRE, DAML does not make an independence assumption of reviews.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 50.410943031311035,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "Like MPCN and NARRE, DAML does not make an independence assumption of reviews.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 50.410943031311035,
      "citing_paper_id": "235792544",
      "cited_paper_id": 196204237
    },
    {
      "context_text": "Neural Attentive Rating Regression (NARRE) [4] improves over this assumption via an attention mechanism which learns a distribution over the individual reviews in the review document.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NARRE) and its improvement over previous assumptions using an attention mechanism.",
      "processing_time": 50.7165961265564,
      "citing_paper_id": "235792544",
      "cited_paper_id": 4901952
    },
    {
      "context_text": "The co-attention mechanism is the same as in MPCN [40] and their language model consists of GRUs to generate explanations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (co-attention mechanism) and a model (language model with GRUs).",
      "processing_time": 51.24705266952515,
      "citing_paper_id": "235792544",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The gradient perturbation by PPLM affects the speed of review generation, making it slower than the LSTM/GRU-based models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between PPLM and LSTM/GRU-based models in terms of review generation speed.",
      "processing_time": 51.41714644432068,
      "citing_paper_id": "235792544",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "HFT employs a traditional MF setup, with an additional regularizer modelling the review text corpus likelihood using Latent Dirichlet Allocation (LDA) [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'review text corpus' but does not specify a named dataset. LDA is a method, not a dataset.",
      "processing_time": 50.87385630607605,
      "citing_paper_id": "235792544",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Sachdeva and McAuley [38] show that a comparatively older method, Hidden Factors and Topics (HFT) [29], outperforms the feature-extraction based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HFT) and a comparison to other methods. No verifiable resources are identified.",
      "processing_time": 51.87846660614014,
      "citing_paper_id": "235792544",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Sachdeva and McAuley [38] show that a comparatively older method, Hidden Factors and Topics (HFT) [29], outperforms the feature-extraction based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HFT) and a comparison to other methods. No verifiable resources are identified.",
      "processing_time": 51.87846660614014,
      "citing_paper_id": "235792544",
      "cited_paper_id": 218870205
    },
    {
      "context_text": "An important observation is that the explanations generated by ReXPlug are very diverse, as is reflected by the Distinct-1 and Distinct-2 scores.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (Distinct-1 and Distinct-2 scores).",
      "processing_time": 51.06847357749939,
      "citing_paper_id": "235792544",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Distinct-1 and Distinct-2 scores measure unigram and bigram diversity, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (Distinct-1 and Distinct-2 scores). These metrics are excluded as per the instructions.",
      "processing_time": 52.096038818359375,
      "citing_paper_id": "235792544",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "We also consider the Distinct-1 and Distinct-2 [22] scores to gauge the diversity of explanations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Distinct-1 and Distinct-2 scores but does not refer to them as datasets. These are metrics, not datasets, and thus should be excluded.",
      "processing_time": 52.58279371261597,
      "citing_paper_id": "235792544",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "TextCNN discovers latent features from the input review documents, followed by a neural network conditioned on these latent features to predict the rating.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method (TextCNN) and its application to review documents, which is too generic.",
      "processing_time": 52.568177461624146,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "To extract the latent features from the input user-item review documents, it uses TextCNN [20], an influential CNN-based architecture.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions TextCNN, which is a method, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 52.08440446853638,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "In addition to using the user 𝑢 and item 𝑖’s review documents for extracting latent features, TransNets also uses the current review for regularization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the use of review documents and the current review, which are not specific datasets.",
      "processing_time": 52.87887120246887,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9932413
    },
    {
      "context_text": "TransNets [2] unsurprisingly show that much of the predictive value of review text comes from reviews of the target user for the target item.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TransNets) and a general finding about review text. No verifiable resources are identified.",
      "processing_time": 53.115291118621826,
      "citing_paper_id": "235792544",
      "cited_paper_id": 9932413
    },
    {
      "context_text": "Following [33], we use the BLEU-3 and BLEU-4 granularities.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLEU-3 and BLEU-4, which are evaluation metrics, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 53.11154794692993,
      "citing_paper_id": "235792544",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Following [33], we use the BLEU-3 and BLEU-4 granularities.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLEU-3 and BLEU-4, which are evaluation metrics, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 53.11154794692993,
      "citing_paper_id": "235792544",
      "cited_paper_id": 202621357
    },
    {
      "context_text": "We quantitatively analyze the generated reviews by (i) automatic evaluation metrics like BLEU and Distinct scores; (ii) computing Pearson Correlation Coefficient between the explanations and the ground truth reviews by ReXPlug; and (iii) comparing the sentiments of the generated reviews with ground truth reviews.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods. The context focuses on the evaluation of generated reviews using various methods.",
      "processing_time": 52.55929780006409,
      "citing_paper_id": "235792544",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "In terms of the BLEU scores, both CAML and ReXPlug give competitive results.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (CAML and ReXPlug) and a metric (BLEU).",
      "processing_time": 52.55482888221741,
      "citing_paper_id": "235792544",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "To compare the ground truth text and the generated text, we use a commonly used measure called BLEU [34] score.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLEU score but does not refer to a specific dataset. BLEU is a metric, not a dataset, and thus should not be included.",
      "processing_time": 53.324020862579346,
      "citing_paper_id": "235792544",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "With the advent of transformers [41], transfer learning approaches are the key in achieving such personalized explanations to recommendations, as pre-trained language models like BERT [9] and GPT-2 [36] can be fine-tuned to datasets of our choice.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained language models which are excluded according to the rules.",
      "processing_time": 51.83071732521057,
      "citing_paper_id": "235792544",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Most explainable recommendation models provide some pre-defined explanations like sentence templates [39, 42, 45] or association rules [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches for providing explanations in recommendation systems.",
      "processing_time": 51.834218978881836,
      "citing_paper_id": "235792544",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "Most explainable recommendation models provide some pre-defined explanations like sentence templates [39, 42, 45] or association rules [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches for providing explanations in recommendation systems.",
      "processing_time": 51.834218978881836,
      "citing_paper_id": "235792544",
      "cited_paper_id": 69392767
    },
    {
      "context_text": "Most explainable recommendation models provide some pre-defined explanations like sentence templates [39, 42, 45] or association rules [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches for providing explanations in recommendation systems.",
      "processing_time": 51.834218978881836,
      "citing_paper_id": "235792544",
      "cited_paper_id": 174799119
    },
    {
      "context_text": "For opinionated text data, [42] introduce a companion learning task of user preference modelling for a recommendation, in parallel with a factorizationbased recommendation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'opinionated text data' but does not specify a named dataset. The cited paper title suggests a focus on multi-task learning in opinionated text data, but no specific dataset is named.",
      "processing_time": 55.51604437828064,
      "citing_paper_id": "235792544",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "Hence, text generation with respect to a controlling attribute (sentiment) is needed in this case [16, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for text generation with attribute controls.",
      "processing_time": 51.66255021095276,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53217784
    },
    {
      "context_text": "Training the discriminator is a relatively computationally inexpensive task against the other controlled text generation models [16, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other models or methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.069066762924194,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53217784
    },
    {
      "context_text": "During the training time, with the help of Universal Sentence Encoder [3], we first get a fixed-sized vector for the review associated with user 𝑢 and item 𝑖 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Universal Sentence Encoder but does not refer to it as a dataset. It is used as a method to generate fixed-sized vectors for reviews.",
      "processing_time": 53.71165657043457,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53245704
    },
    {
      "context_text": "During the training time, with the help of Universal Sentence Encoder [3], we first get a fixed-sized vector for the review associated with user u and item i .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the Universal Sentence Encoder but does not refer to it as a dataset. It is used as a method to generate fixed-sized vectors for reviews.",
      "processing_time": 53.89560389518738,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53245704
    },
    {
      "context_text": "We have a list of encoded reviews written by user 𝑢, given by 𝐷𝑢 , encoded as earlier with the Universal Sentence Encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'encoded reviews' and 'Universal Sentence Encoder', but does not specify a named dataset. The Universal Sentence Encoder is a model, not a dataset.",
      "processing_time": 54.183557987213135,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53245704
    },
    {
      "context_text": "To represent the review text as a fixed-dimensional vector, we leverage the Deep Averaging Network [17] based Universal Sentence Encoder [3].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The Universal Sentence Encoder and Deep Averaging Network are mentioned as tools used to process text, not as datasets.",
      "processing_time": 55.08535671234131,
      "citing_paper_id": "235792544",
      "cited_paper_id": 53245704
    },
    {
      "context_text": "To represent the review text as a fixed-dimensional vector, we leverage the Deep Averaging Network [17] based Universal Sentence Encoder [3].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The Universal Sentence Encoder and Deep Averaging Network are mentioned as tools used to process text, not as datasets.",
      "processing_time": 55.08535671234131,
      "citing_paper_id": "235792544",
      "cited_paper_id": 216848261
    },
    {
      "context_text": "Using Microsoft Concept Graph, DEAML (Deep Explicit Attentive Multi-View Learning) [11] generates template-based explainable recommendations through attentive multi-view learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DEAML) and a tool (Microsoft Concept Graph).",
      "processing_time": 53.44994759559631,
      "citing_paper_id": "235792544",
      "cited_paper_id": 69392767
    },
    {
      "context_text": "FacT [39] builds regression trees on users and items respectively with user-generated reviews and associates a latent profile to each node on the trees to represent users and items.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'user-generated reviews' but does not specify a dataset name. The term 'user-generated reviews' is too generic and lacks a specific identifier.",
      "processing_time": 54.8134069442749,
      "citing_paper_id": "235792544",
      "cited_paper_id": 174799119
    },
    {
      "context_text": "Instead, we consider this problem an ordinal classification problem and train an XLNet classifier on each dataset’s validation set (as the validation set is oblivious to the generated explanations; the train and test sets are not) [43].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets by name, only referring to 'each dataset’s validation set' without providing a clear identifier.",
      "processing_time": 54.04092597961426,
      "citing_paper_id": "235792544",
      "cited_paper_id": 195069387
    },
    {
      "context_text": "Dual Attention Mutual Learning (DAML) [25] DAML utilizes local and mutual attention of CNNs to jointly learn the features of reviews to enhance the interpretability of the proposed DAMLmodel.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DAML) and its application. The title confirms that DAML is a method, not a dataset.",
      "processing_time": 55.25821352005005,
      "citing_paper_id": "235792544",
      "cited_paper_id": 196204237
    },
    {
      "context_text": "We also compute the Pearson Correlation Coefficient between the ground truth review from the test set of each dataset and the reviews generated by encoding the two texts with a RoBERTamodel [26] trained explicitly for the task of semantic similarity, as given by the STS benchmark [37].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'test set of each dataset' but does not specify the names of the datasets. It also mentions the STS benchmark, which is a benchmark rather than a dataset.",
      "processing_time": 56.071263790130615,
      "citing_paper_id": "235792544",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "…146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It lists various applications of text generation but does not provide details on datasets used.",
      "processing_time": 54.361993074417114,
      "citing_paper_id": "257427629",
      "cited_paper_id": 961020
    },
    {
      "context_text": "…or “task completion speed” [14] in web search and information retrieval, where increased ranking accuracy in search results [54], via implementations of personalised algorithms like PageRank [178], improves the efﬁciency and reduces the cognitive burden of trawling through irrelevant information.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PageRank) and its application in improving search result accuracy. No verifiable datasets are referenced.",
      "processing_time": 54.79390549659729,
      "citing_paper_id": "257427629",
      "cited_paper_id": 1508503
    },
    {
      "context_text": "Floridi [62]’s notion of “informational identity” is particularly relevant, where the ﬂow of digital traces in information and communication technologies impact how a user self-identiﬁes, as well as how others and algorithms understand them.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to personal identity and digital traces.",
      "processing_time": 52.79967999458313,
      "citing_paper_id": "257427629",
      "cited_paper_id": 1581509
    },
    {
      "context_text": "Thus, inferential proﬁling, if used in personalised LLMs, could be an attack on individual autonomy to deﬁne their identity [62].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concern about personalized LLMs and individual autonomy.",
      "processing_time": 53.24411725997925,
      "citing_paper_id": "257427629",
      "cited_paper_id": 1581509
    },
    {
      "context_text": "User inputs to personalised LLMs and ratings of their outputs may contribute a large amount of personal, sensitive and intimate detail to an individual’s information identity [62], in turn heightening the risk of proﬁling, or security breaches and hacks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the risks associated with personal data in the context of personalized LLMs but does not reference any named datasets.",
      "processing_time": 56.06728911399841,
      "citing_paper_id": "257427629",
      "cited_paper_id": 1581509
    },
    {
      "context_text": "Potential solutions already explored include batching of users into like-minded groups [18] or recognising when a new user is similar to a known customisation case and then applying transfer learning [157].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches such as transfer learning. No verifiable resources are identified.",
      "processing_time": 53.83424496650696,
      "citing_paper_id": "257427629",
      "cited_paper_id": 2963092
    },
    {
      "context_text": "These systems cover a wide range of tasks including dialogue [127, 157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.52761697769165,
      "citing_paper_id": "257427629",
      "cited_paper_id": 2963092
    },
    {
      "context_text": "These systems cover a wide range of tasks including dialogue [127, 157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.52761697769165,
      "citing_paper_id": "257427629",
      "cited_paper_id": 202120896
    },
    {
      "context_text": "These systems cover a wide range of tasks including dialogue [127, 157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.52761697769165,
      "citing_paper_id": "257427629",
      "cited_paper_id": 225076493
    },
    {
      "context_text": "These systems cover a wide range of tasks including dialogue [127, 157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245],…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.52761697769165,
      "citing_paper_id": "257427629",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "The risk of anthropomorphism in AI systems is widely discussed – with concerns that humans may too readily befriend or empathise with anthropomorphisedagents [197, 189], leading to privacy risks in encouraging the sharing of intimate information [32, 242].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concerns about anthropomorphism in AI systems.",
      "processing_time": 53.01192307472229,
      "citing_paper_id": "257427629",
      "cited_paper_id": 3355359
    },
    {
      "context_text": "This “prompt efﬁciency” is analogous to “query efﬁciency” or “task completion speed” [14] in web search and information retrieval, where increased ranking accuracy in search results [54], via implementations of personalised algorithms like PageRank [178], improves the efﬁciency and reduces the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concepts related to web search and information retrieval. No verifiable resources are identified.",
      "processing_time": 53.99748182296753,
      "citing_paper_id": "257427629",
      "cited_paper_id": 7770673
    },
    {
      "context_text": "…133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of personalized text generation. No verifiable resources are identified.",
      "processing_time": 53.827373027801514,
      "citing_paper_id": "257427629",
      "cited_paper_id": 11469641
    },
    {
      "context_text": "Personalisation can centre the end-user in the designation of model behaviours, allowing them to exert more control over their interactions [33], and become a “perceived locus of casuality” [176, p.5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the concept of personalisation in user interactions.",
      "processing_time": 54.33318471908569,
      "citing_paper_id": "257427629",
      "cited_paper_id": 14000299
    },
    {
      "context_text": "Personalisation can centre the end-user in the designation of model behaviours, allowing them to exert more control over their interactions [33], and become a “perceived locus of casuality” [176, p.5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the concept of personalisation in user interactions.",
      "processing_time": 54.33318471908569,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "However, depending on how power is distributed between the algorithm and the user, personalised technologies have the potential to improve on self-determination and autonomy, by promoting a sense of origin and thus transforming the technology to ‘my technology’ [176, p.1].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the potential of personalized technologies in improving self-determination and autonomy.",
      "processing_time": 54.75388836860657,
      "citing_paper_id": "257427629",
      "cited_paper_id": 14000299
    },
    {
      "context_text": "…245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as education [118, 163, 241], medicine [3, 15, 225, 235] and news consumption [58, 10, 61, 190].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PENS"
      ],
      "dataset_descriptions": {
        "PENS": "Used to generate personalized news headlines, focusing on adapting content to individual user preferences and interests."
      },
      "confidence_score": 0.8,
      "reasoning": "The context does not mention any specific datasets, only general applications and domains. However, the cited paper 'PENS: A Dataset and Generic Framework for Personalized News Headline Generation' suggests the existence of a dataset called PENS.",
      "processing_time": 63.922816038131714,
      "citing_paper_id": "257427629",
      "cited_paper_id": 16153576
    },
    {
      "context_text": "…245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as education [118, 163, 241], medicine [3, 15, 225, 235] and news consumption [58, 10, 61, 190].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PENS"
      ],
      "dataset_descriptions": {
        "PENS": "Used to generate personalized news headlines, focusing on adapting content to individual user preferences and interests."
      },
      "confidence_score": 0.8,
      "reasoning": "The context does not mention any specific datasets, only general applications and domains. However, the cited paper 'PENS: A Dataset and Generic Framework for Personalized News Headline Generation' suggests the existence of a dataset called PENS.",
      "processing_time": 63.922816038131714,
      "citing_paper_id": "257427629",
      "cited_paper_id": 236460075
    },
    {
      "context_text": "…to “query efﬁciency” or “task completion speed” [14] in web search and information retrieval, where increased ranking accuracy in search results [54], via implementations of personalised algorithms like PageRank [178], improves the efﬁciency and reduces the cognitive burden of trawling through…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. The focus is on the effectiveness of personalized web search algorithms.",
      "processing_time": 54.51175260543823,
      "citing_paper_id": "257427629",
      "cited_paper_id": 16908335
    },
    {
      "context_text": "…pedagogies may be particularly helpful to even the playing ﬁeld in paid tutoring services across socioeconomic class [117]; and some have suggested the lower cost and wider reach of personalised healthcare assistants may improve health disparities by meeting challenges with healthcare demand [139].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and suggestions about personalized healthcare assistants and pedagogies. No verifiable resources are identified.",
      "processing_time": 55.411479473114014,
      "citing_paper_id": "257427629",
      "cited_paper_id": 17416431
    },
    {
      "context_text": "…[147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108];…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various NLP tasks and related papers. There are no clear identifiers for datasets.",
      "processing_time": 54.72474670410156,
      "citing_paper_id": "257427629",
      "cited_paper_id": 18927627
    },
    {
      "context_text": "…157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of personalized text generation. No clear identifiers for datasets are present.",
      "processing_time": 54.4996874332428,
      "citing_paper_id": "257427629",
      "cited_paper_id": 19247366
    },
    {
      "context_text": "For example, targeted advertising has been applied to nudge users towards certain political views or brand preferences [211, 34, 213], and is particularly damaging if users are unaware of the inﬂuence [164].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about targeted advertising and its influence. No verifiable resources are identified.",
      "processing_time": 54.98511004447937,
      "citing_paper_id": "257427629",
      "cited_paper_id": 21711924
    },
    {
      "context_text": "For example, targeted advertising has been applied to nudge users towards certain political views or brand preferences [211, 34, 213], and is particularly damaging if users are unaware of the inﬂuence [164].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts about targeted advertising and its influence. No verifiable resources are identified.",
      "processing_time": 54.98511004447937,
      "citing_paper_id": "257427629",
      "cited_paper_id": 86477172
    },
    {
      "context_text": "These risks can somewhat be mitigated by (i) technological design decisions which prioritise retaining a degree of debate [98] and consensus building [18]; and (ii) policy design decisions which restrict the bounds of personalisation, excluding for example extremist or particularly harmful views.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses general strategies to mitigate risks in AI personalisation.",
      "processing_time": 54.71259593963623,
      "citing_paper_id": "257427629",
      "cited_paper_id": 22050710
    },
    {
      "context_text": "Prior works have applied learning from user feedback to adapt semantic relatedness [170] or query intent [165].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts of learning from user feedback. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.008629322052,
      "citing_paper_id": "257427629",
      "cited_paper_id": 22120944
    },
    {
      "context_text": "Prior works have applied learning from user feedback to adapt semantic relatedness [170] or query intent [165].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts of learning from user feedback. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.008629322052,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "The risk is particularly severe if personalised LLMs operate with sensitive information, such as in healthcare [81, 12], or seek to persuade their users [213] and encourage information disclosure [122].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with personalized LLMs in sensitive domains. No verifiable resources are named.",
      "processing_time": 55.63360404968262,
      "citing_paper_id": "257427629",
      "cited_paper_id": 30083400
    },
    {
      "context_text": "The risk is particularly severe if personalised LLMs operate with sensitive information, such as in healthcare [81, 12], or seek to persuade their users [213] and encourage information disclosure [122].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with personalized LLMs in sensitive domains. No verifiable resources are named.",
      "processing_time": 55.63360404968262,
      "citing_paper_id": "257427629",
      "cited_paper_id": 86477172
    },
    {
      "context_text": "The risk is particularly severe if personalised LLMs operate with sensitive information, such as in healthcare [81, 12], or seek to persuade their users [213] and encourage information disclosure [122].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with personalized LLMs in sensitive domains. No verifiable resources are named.",
      "processing_time": 55.63360404968262,
      "citing_paper_id": "257427629",
      "cited_paper_id": 255892294
    },
    {
      "context_text": "These narrow information spaces could be impacting the functioning of democracy [186], with Allcott and Gentzkow [6] reporting that ideologically segregated social media networks were an important driver of political preference in the 2016 US Election.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about social media networks and their impact on political preferences.",
      "processing_time": 53.600242137908936,
      "citing_paper_id": "257427629",
      "cited_paper_id": 32730475
    },
    {
      "context_text": "Finally, despite some degree of personalisation, homogenisation can occur at the technology level – where a user behaves more like the technology defaults, a form of “algorithmic confounding” [38].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept of 'algorithmic confounding' in recommendation systems.",
      "processing_time": 54.28588533401489,
      "citing_paper_id": "257427629",
      "cited_paper_id": 39558129
    },
    {
      "context_text": "For example, model behaviours and interactions could be inclusive of users with disabilities [49], neurodivergentlearning pathways [22], or visual impairments (if paired with personalised speech recognition [24]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses the importance of inclusivity in personalized systems but does not reference any particular dataset.",
      "processing_time": 55.992557764053345,
      "citing_paper_id": "257427629",
      "cited_paper_id": 43653551
    },
    {
      "context_text": "For example, model behaviours and interactions could be inclusive of users with disabilities [49], neurodivergentlearning pathways [22], or visual impairments (if paired with personalised speech recognition [24]).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses the importance of inclusivity in personalized systems but does not reference any particular dataset.",
      "processing_time": 55.992557764053345,
      "citing_paper_id": "257427629",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "…194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as education [118,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and applications. No verifiable resources are identified.",
      "processing_time": 54.45658469200134,
      "citing_paper_id": "257427629",
      "cited_paper_id": 53081334
    },
    {
      "context_text": "Emphatic alignment may be particularly important if LLMs are used for mental health provision or emotional support, in cases where more conventional social or professional services are in short supply or outside an individual’s budget [97].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application of LLMs in mental health provision. No verifiable resources are identified.",
      "processing_time": 55.6042959690094,
      "citing_paper_id": "257427629",
      "cited_paper_id": 53719693
    },
    {
      "context_text": "The integration of personalised LLMs will likely mostly affect minimum wage jobs [140], routine jobs [55] and may impact the demand for crowdwork [7] by redistributing the responsibility for providing feedback data.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general impacts on job types. No verifiable resources are identified.",
      "processing_time": 54.26955270767212,
      "citing_paper_id": "257427629",
      "cited_paper_id": 54195153
    },
    {
      "context_text": "The integration of personalised LLMs will likely mostly affect minimum wage jobs [140], routine jobs [55] and may impact the demand for crowdwork [7] by redistributing the responsibility for providing feedback data.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general impacts on job types. No verifiable resources are identified.",
      "processing_time": 54.26955270767212,
      "citing_paper_id": "257427629",
      "cited_paper_id": 233904629
    },
    {
      "context_text": "co/ (6)For example, there is BERT [53], ClinicalBERT [93], BioBERT [126], LegalBERT [243], HateBERT [37] and BERTweet [167].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several BERT variants but does not indicate the use of any specific datasets. These are models, not datasets.",
      "processing_time": 54.69099450111389,
      "citing_paper_id": "257427629",
      "cited_paper_id": 59291975
    },
    {
      "context_text": "co/ (6)For example, there is BERT [53], ClinicalBERT [93], BioBERT [126], LegalBERT [243], HateBERT [37] and BERTweet [167].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several BERT variants but does not indicate the use of any specific datasets. These are models, not datasets.",
      "processing_time": 54.69099450111389,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "Furthermore, Gibson [73]’s theory of affordances, often applied to study the impact of technological systems including AI chatbots [210, 101], argues that the interactions between an agent and their environment condition the possibilities and constraints for action.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theories and applications of affordances in chatbots. No verifiable resources are identified.",
      "processing_time": 55.35579872131348,
      "citing_paper_id": "257427629",
      "cited_paper_id": 69960436
    },
    {
      "context_text": "Furthermore, Gibson [73]’s theory of affordances, often applied to study the impact of technological systems including AI chatbots [210, 101], argues that the interactions between an agent and their environment condition the possibilities and constraints for action.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theories and applications of affordances in chatbots. No verifiable resources are identified.",
      "processing_time": 55.35579872131348,
      "citing_paper_id": "257427629",
      "cited_paper_id": 202325752
    },
    {
      "context_text": "Furthermore, Gibson [73]’s theory of affordances, often applied to study the impact of technological systems including AI chatbots [210, 101], argues that the interactions between an agent and their environment condition the possibilities and constraints for action.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theories and applications of affordances in chatbots. No verifiable resources are identified.",
      "processing_time": 55.35579872131348,
      "citing_paper_id": "257427629",
      "cited_paper_id": 270480238
    },
    {
      "context_text": "On one hand, if personalised LLMs do bring a range of individual beneﬁts, then those excluded will be left behind, which is particularly worrisome for entrenching education or health disparities [99, 160].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only concerns about digital disparities. No verifiable resources are identified.",
      "processing_time": 54.247243881225586,
      "citing_paper_id": "257427629",
      "cited_paper_id": 75778936
    },
    {
      "context_text": "While labour displacement is a general concern of AI systems [63], personalised LLMs may exacerbate the automation of tasks in an individual’s workﬂow simply by bringing higher utility.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concern about AI systems and their impact on labor. No verifiable resources are identified.",
      "processing_time": 55.7610239982605,
      "citing_paper_id": "257427629",
      "cited_paper_id": 85514938
    },
    {
      "context_text": "The contribution of search engines to the reinforcement of societal biases is well-documented [83, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general contributions of search engines to societal biases. No verifiable resources are identified.",
      "processing_time": 55.12756633758545,
      "citing_paper_id": "257427629",
      "cited_paper_id": 109815188
    },
    {
      "context_text": "The contribution of search engines to the reinforcement of societal biases is well-documented [83, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general contributions of search engines to societal biases. No verifiable resources are identified.",
      "processing_time": 55.12756633758545,
      "citing_paper_id": "257427629",
      "cited_paper_id": 213647029
    },
    {
      "context_text": "…[156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and applications. No verifiable resources are identified.",
      "processing_time": 54.670392751693726,
      "citing_paper_id": "257427629",
      "cited_paper_id": 145048418
    },
    {
      "context_text": "The risk of selective exposure to information has been widely documented in respect to social media platforms – where feedback loops prioritise opinion-congruent information [105], in turn leading users to over-estimate the popularity of their viewpoint [123].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about selective exposure and social media feedback loops.",
      "processing_time": 54.03516483306885,
      "citing_paper_id": "257427629",
      "cited_paper_id": 149270695
    },
    {
      "context_text": "Some argue that digital disparities are already made deeper by AI and Big Data [144], person-alised media [47], or search engines [201].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and areas of concern. There are no verifiable resources or specific datasets named.",
      "processing_time": 55.757230281829834,
      "citing_paper_id": "257427629",
      "cited_paper_id": 150003029
    },
    {
      "context_text": "This is a problem also faced by online trust and safety regulation – where for example, early iterations of the Online Safety Bill [181] included separate treatment of illegal content versus legal but harmful content, as well as tiered restrictions for children and minors versus adults.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses regulatory aspects of online safety, which is not directly related to personalized text generation.",
      "processing_time": 55.7409348487854,
      "citing_paper_id": "257427629",
      "cited_paper_id": 152622905
    },
    {
      "context_text": "Some speciﬁc challenges remain: Compliance with Existing Regulation Any technology permitting the personalisation of LLMs would need to comply with existing regulations and laws such as the GDPR [180], the Online Safety Bill [181] and the various European AI standards [45, 46].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only regulatory documents. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.323623180389404,
      "citing_paper_id": "257427629",
      "cited_paper_id": 152622905
    },
    {
      "context_text": "By exacerbating epistemic harms through conﬁrmation biases, personalised LLMs risk contributing to a “post-truth” society [85], where each individual occupies their own information bubble.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concern about personalized LLMs and their impact on society.",
      "processing_time": 54.91486406326294,
      "citing_paper_id": "257427629",
      "cited_paper_id": 153212254
    },
    {
      "context_text": "Polarisation also increases susceptibility to misinformation where increasingly fragmented communities overestimate trust in the factuality of ‘in-group’ information [57], leading to a regime of “post-truth” politics [85].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about polarization and misinformation. No verifiable resources are identified.",
      "processing_time": 54.910693883895874,
      "citing_paper_id": "257427629",
      "cited_paper_id": 153212254
    },
    {
      "context_text": "In this so-called “attention economy” [95, 234], technologies compete in a ‘race to the bottom’ to capture user attention, are optimised for utility and engagingness, and thus risk being highly addictive [27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the concept of the 'attention economy' and the risks associated with technologies optimized for user engagement.",
      "processing_time": 57.55598449707031,
      "citing_paper_id": "257427629",
      "cited_paper_id": 158524745
    },
    {
      "context_text": "Volunteer labour to shape the internet landscape has analogies in consumers writing product reviews [196] and social media users ﬂagging content [74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It refers to general concepts of volunteer labor, product reviews, and content flagging, which do not qualify as specific datasets.",
      "processing_time": 58.25816869735718,
      "citing_paper_id": "257427629",
      "cited_paper_id": 167605068
    },
    {
      "context_text": "Volunteer labour to shape the internet landscape has analogies in consumers writing product reviews [196] and social media users ﬂagging content [74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It refers to general concepts of volunteer labor, product reviews, and content flagging, which do not qualify as specific datasets.",
      "processing_time": 58.25816869735718,
      "citing_paper_id": "257427629",
      "cited_paper_id": 213571185
    },
    {
      "context_text": "The beneﬁts of more user control in content moderation technologies have also been noted [207].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a general concept of user control in content moderation technologies.",
      "processing_time": 55.728700160980225,
      "citing_paper_id": "257427629",
      "cited_paper_id": 197878790
    },
    {
      "context_text": "For example, inclusive pedagogies may be particularly helpful to even the playing ﬁeld in paid tutoring services across socioeconomic class [117]; and some have suggested the lower cost and wider reach of personalised healthcare assistants may improve health disparities by meeting challenges with…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and suggestions about the impact of personalized healthcare assistants.",
      "processing_time": 54.39707541465759,
      "citing_paper_id": "257427629",
      "cited_paper_id": 198051676
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 202120896
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 208910447
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 222291028
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245123993
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 250562745
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252693337
    },
    {
      "context_text": "Some authors claim a degree of universality in morals or values [135, 131, 104, 248, 64, 94, 136, 192]; others target preferences on attributes such as quality, usefulness or helpfulness of an LLM’s output which arguably have limited standardisation across individuals [209, 247, 156, 147, 100, 52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general discussions about universality in morals, values, and preferences. No verifiable resources are identified.",
      "processing_time": 57.035017251968384,
      "citing_paper_id": "257427629",
      "cited_paper_id": 253306024
    },
    {
      "context_text": "The danger of polarisation in health and vaccine information [230] was made clear by the COVID-19 pandemic [158].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about the spread of misinformation.",
      "processing_time": 53.99121809005737,
      "citing_paper_id": "257427629",
      "cited_paper_id": 203580422
    },
    {
      "context_text": "Second, what is being aligned – there are subtle differences between aligning models to instructions, intentions, revealed or ideal preferences, and values [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses alignment concepts in AI.",
      "processing_time": 53.497170209884644,
      "citing_paper_id": "257427629",
      "cited_paper_id": 210920551
    },
    {
      "context_text": "The question of how “aligned” LLMs are is unresolved due to normative obstacles in deﬁning what alignment means, what the target of alignment is (for example, values, preferences or intent) and who we are aligning to [128, 65, 66, 111].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses conceptual issues around AI alignment.",
      "processing_time": 53.67131495475769,
      "citing_paper_id": "257427629",
      "cited_paper_id": 210920551
    },
    {
      "context_text": "…functional alignment i.e., seeking improvement in general model capabilities or instruction-following and avoiding gaming of misspeciﬁed objectives [65, 111, 128, 177] – versus worldview or social value alignment i.e., embedding some general notion of “shared” human values and morals [86, 8, 104].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only concepts related to AI alignment. No verifiable resources are identified.",
      "processing_time": 55.279486656188965,
      "citing_paper_id": "257427629",
      "cited_paper_id": 210920551
    },
    {
      "context_text": "…functional alignment i.e., seeking improvement in general model capabilities or instruction-following and avoiding gaming of misspeciﬁed objectives [65, 111, 128, 177] – versus worldview or social value alignment i.e., embedding some general notion of “shared” human values and morals [86, 8, 104].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only concepts related to AI alignment. No verifiable resources are identified.",
      "processing_time": 55.279486656188965,
      "citing_paper_id": "257427629",
      "cited_paper_id": 220968818
    },
    {
      "context_text": "…functional alignment i.e., seeking improvement in general model capabilities or instruction-following and avoiding gaming of misspeciﬁed objectives [65, 111, 128, 177] – versus worldview or social value alignment i.e., embedding some general notion of “shared” human values and morals [86, 8, 104].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only concepts related to AI alignment. No verifiable resources are identified.",
      "processing_time": 55.279486656188965,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "…functional alignment i.e., seeking improvement in general model capabilities or instruction-following and avoiding gaming of misspeciﬁed objectives [65, 111, 128, 177] – versus worldview or social value alignment i.e., embedding some general notion of “shared” human values and morals [86, 8, 104].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only concepts related to AI alignment. No verifiable resources are identified.",
      "processing_time": 55.279486656188965,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248512563
    },
    {
      "context_text": "…functional alignment i.e., seeking improvement in general model capabilities or instruction-following and avoiding gaming of misspeciﬁed objectives [65, 111, 128, 177] – versus worldview or social value alignment i.e., embedding some general notion of “shared” human values and morals [86, 8, 104].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only concepts related to AI alignment. No verifiable resources are identified.",
      "processing_time": 55.279486656188965,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252693337
    },
    {
      "context_text": "Personalised LLMs can represent the values held by wider swaths of society and avoid the “value-monism” of current alignment techniques [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of personalized LLMs and value representation. No verifiable resources are identified.",
      "processing_time": 56.72663426399231,
      "citing_paper_id": "257427629",
      "cited_paper_id": 210920551
    },
    {
      "context_text": "Autonomy may seem a counter-intuitive beneﬁt of personalised systems, given the wide literature on the loss of autonomy from algorithmic nudges, tailored advertising or recommender systems [154].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general literature review on the ethical challenges of recommender systems.",
      "processing_time": 55.04517960548401,
      "citing_paper_id": "257427629",
      "cited_paper_id": 211525674
    },
    {
      "context_text": "Other risks have analogies in personalised content moderation [75] or recommender systems [154, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to personalized content moderation and recommender systems. No verifiable resources are identified.",
      "processing_time": 56.73846507072449,
      "citing_paper_id": "257427629",
      "cited_paper_id": 211525674
    },
    {
      "context_text": "Other risks have analogies in personalised content moderation [75] or recommender systems [154, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to personalized content moderation and recommender systems. No verifiable resources are identified.",
      "processing_time": 56.73846507072449,
      "citing_paper_id": "257427629",
      "cited_paper_id": 237495104
    },
    {
      "context_text": "Other risks have analogies in personalised content moderation [75] or recommender systems [154, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to personalized content moderation and recommender systems. No verifiable resources are identified.",
      "processing_time": 56.73846507072449,
      "citing_paper_id": "257427629",
      "cited_paper_id": 251704630
    },
    {
      "context_text": "Personalised LLMs could be weaponised in the commodiﬁcation of attention, similarly to how social media feeds seek to optimise the time that users spend on the platform to maximise advertising revenue [74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concern about the use of personalised LLMs in social media. The cited paper title also does not indicate a dataset.",
      "processing_time": 58.214505195617676,
      "citing_paper_id": "257427629",
      "cited_paper_id": 213571185
    },
    {
      "context_text": "In the early internet, many contributions were voluntary – consider Wikipedia edits [116] or community-based moderation of the blogosphere [74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to general concepts of voluntary contributions and content moderation.",
      "processing_time": 55.26006197929382,
      "citing_paper_id": "257427629",
      "cited_paper_id": 213571185
    },
    {
      "context_text": "…assumption that many of the public-facing impacts of AI systems in the coming years will be driven by development and product decisions of Big Tech, in the same way that the impact of social media has been shaped by the overall design choices and content moderation decisions of platforms [74].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about the impact of social media platforms. No verifiable resources are identified.",
      "processing_time": 56.12722992897034,
      "citing_paper_id": "257427629",
      "cited_paper_id": 213571185
    },
    {
      "context_text": "Nguyen et al. [169] examine the robustness of reinforcement learning methods under more realistic properties of human feedback such as high variance, skew and restricted granularity, proposing an approach where performance does not degrade under noisy preference data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the robustness of reinforcement learning methods under noisy human feedback, but does not mention any specific datasets.",
      "processing_time": 54.34949851036072,
      "citing_paper_id": "257427629",
      "cited_paper_id": 215824512
    },
    {
      "context_text": "In discussing a limitation of the ETHICS dataset, Hendrycks et al. [86] note that we “must engage more stakeholders and successfully implement more diverse and individualized values” (p.9).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ETHICS"
      ],
      "dataset_descriptions": {
        "ETHICS": "Used to highlight the limitation of lacking diverse and individualized values, emphasizing the need for broader stakeholder engagement in AI ethics."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions the ETHICS dataset in the context of its limitations, specifically the need for more diverse and individualized values.",
      "processing_time": 62.8661789894104,
      "citing_paper_id": "257427629",
      "cited_paper_id": 220968818
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 220968818
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "To align the language modelling objective with human preferences, many recent works have ﬁne-tuned LLMs by reinforcing human rewards or feedback [177, 247, 16, 13, 209, 165, 168]; deﬁned rules for LLMs to learn from [17, 77]; and analysed how they make moral or ethical decisions [104, 86, 248, 103].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for fine-tuning language models using human feedback. No verifiable resources are identified.",
      "processing_time": 57.499735593795776,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252693337
    },
    {
      "context_text": "Evidence from previous RLHF studies demonstrate that human raters generally perceive ﬁne-tuned models as better at following instructions [177], more capable of high-quality outputs [209, 247] or generally more “helpful” [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general findings from previous studies on reinforcement learning from human feedback (RLHF).",
      "processing_time": 56.69016218185425,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Evidence from previous RLHF studies demonstrate that human raters generally perceive ﬁne-tuned models as better at following instructions [177], more capable of high-quality outputs [209, 247] or generally more “helpful” [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general findings from previous studies on reinforcement learning from human feedback (RLHF).",
      "processing_time": 56.69016218185425,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Evidence from previous RLHF studies demonstrate that human raters generally perceive ﬁne-tuned models as better at following instructions [177], more capable of high-quality outputs [209, 247] or generally more “helpful” [16].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general findings from previous studies on reinforcement learning from human feedback (RLHF).",
      "processing_time": 56.69016218185425,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "First, without sufﬁcient safeguards, personalised LLMs could be used to reproduce harmful, illegal or antisocial language at scale [209].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concern about personalized LLMs. No verifiable resources are identified.",
      "processing_time": 55.854020833969116,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "A number of RLHF papers test performance over a range of data requirements [185, 209, 247].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a range of data requirements. No clear, verifiable datasets are identified.",
      "processing_time": 55.84979033470154,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "A number of RLHF papers test performance over a range of data requirements [185, 209, 247].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a range of data requirements. No clear, verifiable datasets are identified.",
      "processing_time": 55.84979033470154,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "…sample biases are exacerbated by a lack of dataset or labour force documentation: while some papers can be commended for reporting full demographics and acknowledging the speciﬁcity of their crowdworkers [e.g., 77, 209, 177, 16], others provide little or no details [e.g., 18, 165, 247, 236, 142].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the issue of lack of documentation in some papers regarding dataset or labor force specifics.",
      "processing_time": 56.966676473617554,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "…sample biases are exacerbated by a lack of dataset or labour force documentation: while some papers can be commended for reporting full demographics and acknowledging the speciﬁcity of their crowdworkers [e.g., 77, 209, 177, 16], others provide little or no details [e.g., 18, 165, 247, 236, 142].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the issue of lack of documentation in some papers regarding dataset or labor force specifics.",
      "processing_time": 56.966676473617554,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "…sample biases are exacerbated by a lack of dataset or labour force documentation: while some papers can be commended for reporting full demographics and acknowledging the speciﬁcity of their crowdworkers [e.g., 77, 209, 177, 16], others provide little or no details [e.g., 18, 165, 247, 236, 142].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the issue of lack of documentation in some papers regarding dataset or labor force specifics.",
      "processing_time": 56.966676473617554,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "…sample biases are exacerbated by a lack of dataset or labour force documentation: while some papers can be commended for reporting full demographics and acknowledging the speciﬁcity of their crowdworkers [e.g., 77, 209, 177, 16], others provide little or no details [e.g., 18, 165, 247, 236, 142].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses the issue of lack of documentation in some papers regarding dataset or labor force specifics.",
      "processing_time": 56.966676473617554,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "Many LLMs trained on human feedback rely on such crowdworking platforms like MTurk [e.g. 136, 100], Upwork [177, 165, 209], SurgeAI [17] or Proliﬁc [124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowdworking platforms but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 55.82488751411438,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Many LLMs trained on human feedback rely on such crowdworking platforms like MTurk [e.g. 136, 100], Upwork [177, 165, 209], SurgeAI [17] or Proliﬁc [124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowdworking platforms but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 55.82488751411438,
      "citing_paper_id": "257427629",
      "cited_paper_id": 222291028
    },
    {
      "context_text": "Many LLMs trained on human feedback rely on such crowdworking platforms like MTurk [e.g. 136, 100], Upwork [177, 165, 209], SurgeAI [17] or Proliﬁc [124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowdworking platforms but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 55.82488751411438,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "Many LLMs trained on human feedback rely on such crowdworking platforms like MTurk [e.g. 136, 100], Upwork [177, 165, 209], SurgeAI [17] or Proliﬁc [124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowdworking platforms but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 55.82488751411438,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Many LLMs trained on human feedback rely on such crowdworking platforms like MTurk [e.g. 136, 100], Upwork [177, 165, 209], SurgeAI [17] or Proliﬁc [124].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowdworking platforms but does not specify any datasets. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 55.82488751411438,
      "citing_paper_id": "257427629",
      "cited_paper_id": 256194393
    },
    {
      "context_text": "Personalised LLMs avoid technology providers and/or crowdworkers deciding which values are prioritised or what factors deﬁne a “good” output [209].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of personalized LLMs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 57.47618770599365,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Even without personalisation, Stiennon et al. [209]’s RLHF model required 320 GPU days to train (p.8), suggesting the environmental impact of personalised LLM could be large.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the computational requirements and potential environmental impact of training a model.",
      "processing_time": 55.452473878860474,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "The technical apparatus for effective feedback learning exists : A growing body of work applies preference reward modelling to effectively condition LLM behaviours [e.g 177, 16, 77, 247, 209, 13, 165, 142, 236, 216].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'human feedback' which is too generic. No dataset names are present in the context.",
      "processing_time": 58.16339659690857,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "The technical apparatus for effective feedback learning exists : A growing body of work applies preference reward modelling to effectively condition LLM behaviours [e.g 177, 16, 77, 247, 209, 13, 165, 142, 236, 216].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'human feedback' which is too generic. No dataset names are present in the context.",
      "processing_time": 58.16339659690857,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "The technical apparatus for effective feedback learning exists : A growing body of work applies preference reward modelling to effectively condition LLM behaviours [e.g 177, 16, 77, 247, 209, 13, 165, 142, 236, 216].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'human feedback' which is too generic. No dataset names are present in the context.",
      "processing_time": 58.16339659690857,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "The technical apparatus for effective feedback learning exists : A growing body of work applies preference reward modelling to effectively condition LLM behaviours [e.g 177, 16, 77, 247, 209, 13, 165, 142, 236, 216].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'human feedback' which is too generic. No dataset names are present in the context.",
      "processing_time": 58.16339659690857,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "The technical apparatus for effective feedback learning exists : A growing body of work applies preference reward modelling to effectively condition LLM behaviours [e.g 177, 16, 77, 247, 209, 13, 165, 142, 236, 216].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to 'human feedback' which is too generic. No dataset names are present in the context.",
      "processing_time": 58.16339659690857,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "In personalised dialogue systems speciﬁcally, alignment in conversational styles and word usage is an important driver of engaging-ness in human-human interactions and has been argued as an determinant of user satisfaction in human-agent conversations [68, 227].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about personalized dialogue systems and user satisfaction.",
      "processing_time": 54.991392374038696,
      "citing_paper_id": "257427629",
      "cited_paper_id": 221703773
    },
    {
      "context_text": "For example, there are extensive works documenting LLMs on fairness and bias [2, 115, 143, 162, 166, 191, 205, 222]; truthfulness, uncertainty, or hallucination [130, 106, 102]; robustness [208, 112]; privacy [35]; and toxicity [72, 171].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CrowS-Pairs",
        "HONEST"
      ],
      "dataset_descriptions": {
        "CrowS-Pairs": "Used to measure social biases in masked language models, focusing on stereotype perpetuation and fairness in NLP systems.",
        "HONEST": "Used to measure hurtful sentence completion in language models, specifically evaluating the generation of harmful content."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of research. However, the cited paper titles suggest the presence of specific datasets.",
      "processing_time": 68.60454607009888,
      "citing_paper_id": "257427629",
      "cited_paper_id": 222090785
    },
    {
      "context_text": "For example, there are extensive works documenting LLMs on fairness and bias [2, 115, 143, 162, 166, 191, 205, 222]; truthfulness, uncertainty, or hallucination [130, 106, 102]; robustness [208, 112]; privacy [35]; and toxicity [72, 171].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CrowS-Pairs",
        "HONEST"
      ],
      "dataset_descriptions": {
        "CrowS-Pairs": "Used to measure social biases in masked language models, focusing on stereotype perpetuation and fairness in NLP systems.",
        "HONEST": "Used to measure hurtful sentence completion in language models, specifically evaluating the generation of harmful content."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of research. However, the cited paper titles suggest the presence of specific datasets.",
      "processing_time": 68.60454607009888,
      "citing_paper_id": "257427629",
      "cited_paper_id": 235097294
    },
    {
      "context_text": "For example, there are extensive works documenting LLMs on fairness and bias [2, 115, 143, 162, 166, 191, 205, 222]; truthfulness, uncertainty, or hallucination [130, 106, 102]; robustness [208, 112]; privacy [35]; and toxicity [72, 171].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CrowS-Pairs",
        "HONEST"
      ],
      "dataset_descriptions": {
        "CrowS-Pairs": "Used to measure social biases in masked language models, focusing on stereotype perpetuation and fairness in NLP systems.",
        "HONEST": "Used to measure hurtful sentence completion in language models, specifically evaluating the generation of harmful content."
      },
      "confidence_score": 0.85,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of research. However, the cited paper titles suggest the presence of specific datasets.",
      "processing_time": 68.60454607009888,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249062690
    },
    {
      "context_text": "A wide variety of types of feedback data have been experimented with, including binary comparisons [67, 247, 100, 17, 13], ranked preferences [13, 142], demonstrations of optimal behaviours [209, 177, 165] or revisions [84, 135, 224].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to types of feedback data used in experiments, which are not specific datasets.",
      "processing_time": 57.67135143280029,
      "citing_paper_id": "257427629",
      "cited_paper_id": 222291028
    },
    {
      "context_text": "A wide variety of types of feedback data have been experimented with, including binary comparisons [67, 247, 100, 17, 13], ranked preferences [13, 142], demonstrations of optimal behaviours [209, 177, 165] or revisions [84, 135, 224].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to types of feedback data used in experiments, which are not specific datasets.",
      "processing_time": 57.67135143280029,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "A wide variety of types of feedback data have been experimented with, including binary comparisons [67, 247, 100, 17, 13], ranked preferences [13, 142], demonstrations of optimal behaviours [209, 177, 165] or revisions [84, 135, 224].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to types of feedback data used in experiments, which are not specific datasets.",
      "processing_time": 57.67135143280029,
      "citing_paper_id": "257427629",
      "cited_paper_id": 253306024
    },
    {
      "context_text": "Similarly, the reinforcement of extremist or anti-social beliefs has been demonstrated in ‘incel’ communities, where members become increasingly embedded via repeated interactions with like-minded individuals [172, 195]; and in white power communities, where “certain beliefs become sacred and unquestionable” [p.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the reinforcement of beliefs in 'incel' and white power communities, but does not reference any named datasets.",
      "processing_time": 58.81002402305603,
      "citing_paper_id": "257427629",
      "cited_paper_id": 224879685
    },
    {
      "context_text": "Similarly, the reinforcement of extremist or anti-social beliefs has been demonstrated in ‘incel’ communities, where members become increasingly embedded via repeated interactions with like-minded individuals [172, 195]; and in white power communities, where “certain beliefs become sacred and unquestionable” [p.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the reinforcement of beliefs in 'incel' and white power communities, but does not reference any named datasets.",
      "processing_time": 58.81002402305603,
      "citing_paper_id": "257427629",
      "cited_paper_id": 263496625
    },
    {
      "context_text": "Increasing personalisation of information consumption online has been attributed with creating echo chambers [44, 249] and ﬁlter bubbles [179].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts related to echo chambers and filter bubbles.",
      "processing_time": 54.96553325653076,
      "citing_paper_id": "257427629",
      "cited_paper_id": 232038574
    },
    {
      "context_text": "This is an example of dishonest anthropomorphism, where artiﬁcial systems give false or misleading signals of being human [79].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, model, or method. It refers to a concept discussed in another paper, which is not a reusable resource.",
      "processing_time": 57.07996702194214,
      "citing_paper_id": "257427629",
      "cited_paper_id": 235358232
    },
    {
      "context_text": "It is a common concern with digital technologies such as the internet of things [223] or targeted advertising [221, 212].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concerns about digital technologies. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 57.084885120391846,
      "citing_paper_id": "257427629",
      "cited_paper_id": 236519511
    },
    {
      "context_text": "This “missing ratings” problem is a known challenge in recommender systems, where users only provide feedback to seen items [200], in turn introducing biases [26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in recommender systems. No verifiable resources are identified.",
      "processing_time": 56.65107178688049,
      "citing_paper_id": "257427629",
      "cited_paper_id": 237495104
    },
    {
      "context_text": "Furthermore, alignment is a technical challenge which is not solved by scaling parameter counts [135, 130, 177, 136, 89, 185].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the technical challenge of alignment, which is not addressed by increasing model size.",
      "processing_time": 58.127397775650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 237532606
    },
    {
      "context_text": "Furthermore, alignment is a technical challenge which is not solved by scaling parameter counts [135, 130, 177, 136, 89, 185].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the technical challenge of alignment, which is not addressed by increasing model size.",
      "processing_time": 58.127397775650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Furthermore, alignment is a technical challenge which is not solved by scaling parameter counts [135, 130, 177, 136, 89, 185].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the technical challenge of alignment, which is not addressed by increasing model size.",
      "processing_time": 58.127397775650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 250562745
    },
    {
      "context_text": "Furthermore, alignment is a technical challenge which is not solved by scaling parameter counts [135, 130, 177, 136, 89, 185].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the technical challenge of alignment, which is not addressed by increasing model size.",
      "processing_time": 58.127397775650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 253306024
    },
    {
      "context_text": "Furthermore, alignment is a technical challenge which is not solved by scaling parameter counts [135, 130, 177, 136, 89, 185].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the technical challenge of alignment, which is not addressed by increasing model size.",
      "processing_time": 58.127397775650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "General concerns over the risk of essentialism and simpliﬁcations of ﬂuid identity via digital technologies have been voiced [23, 204].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references general concerns about digital technologies and identity.",
      "processing_time": 56.64135026931763,
      "citing_paper_id": "257427629",
      "cited_paper_id": 237691749
    },
    {
      "context_text": "Convergence on the mental and emotional level is an important feature of human-human interactions [56], and a number of previous works seek to improve emotional alignment in agent-human interactions via ‘artiﬁcial emphathy’ [138, 246].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of 'artificial empathy' in improving emotional alignment in agent-human interactions.",
      "processing_time": 57.06039881706238,
      "citing_paper_id": "257427629",
      "cited_paper_id": 238354260
    },
    {
      "context_text": "…generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various NLP tasks and applications. No verifiable resources are named.",
      "processing_time": 56.0351836681366,
      "citing_paper_id": "257427629",
      "cited_paper_id": 241583806
    },
    {
      "context_text": "Recent attempts to “align” LLMs with human preferences commonly apply a form of reward learning, such as reinforcement learning from human feedback (RLHF) [e.g. 177, 165, 16, 13, 247].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.232903480529785,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "Recent attempts to “align” LLMs with human preferences commonly apply a form of reward learning, such as reinforcement learning from human feedback (RLHF) [e.g. 177, 165, 16, 13, 247].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.232903480529785,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "Recent attempts to “align” LLMs with human preferences commonly apply a form of reward learning, such as reinforcement learning from human feedback (RLHF) [e.g. 177, 165, 16, 13, 247].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.232903480529785,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "Widely-known models include OpenAI’s instruct-GPT3 [177], Anthropic’s HHH assistants [13, 17, 17], Google’s LaMDA [216] and MetaAI’s LLaMA [152].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models but does not refer to any specific datasets. The context is focused on describing models rather than datasets.",
      "processing_time": 57.05685901641846,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "However, many works have demonstrated little to no alignment tax [77, 13, 16, 136].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to previous works demonstrating little to no alignment tax, which is a concept rather than a reusable resource.",
      "processing_time": 59.52700710296631,
      "citing_paper_id": "257427629",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "However, many works have demonstrated little to no alignment tax [77, 13, 16, 136].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to previous works demonstrating little to no alignment tax, which is a concept rather than a reusable resource.",
      "processing_time": 59.52700710296631,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "However, many works have demonstrated little to no alignment tax [77, 13, 16, 136].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to previous works demonstrating little to no alignment tax, which is a concept rather than a reusable resource.",
      "processing_time": 59.52700710296631,
      "citing_paper_id": "257427629",
      "cited_paper_id": 250562745
    },
    {
      "context_text": "The risk of ‘value proﬁling’ is evidenced by Qiu et al. [192]’s recent work which uses an LLM to create a numeric speaker proﬁle – where for example, the authors say that a speaker “saying ‘I miss my mum’ implies that the speaker values benevolence” (p.7) while the speaker “saying ’forcing my…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ValueNet"
      ],
      "dataset_descriptions": {
        "ValueNet": "Used to create a numeric speaker profile based on implied values in statements, focusing on the relationship between language and human values in dialogue systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ValueNet', which is a dataset for human value-driven dialogue systems, and it is used to create a numeric speaker profile based on the values implied by their statements.",
      "processing_time": 64.99486041069031,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245123993
    },
    {
      "context_text": "Despite this lacking standardisation, many approaches enforce a ‘prescriptive paradigm’ in data annotation [198] by explicitly deﬁning in detailed guidelines what counts as a “good” model output.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about data annotation paradigms.",
      "processing_time": 55.56947922706604,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245130931
    },
    {
      "context_text": "Individualised cultural personalisation may aid utility in some tasks: for example, Nakano et al. [165] demonstrate that their system, when asked “what does a wedding look like?”",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of individualised cultural personalisation. No clear, verifiable resource is identified.",
      "processing_time": 57.85231852531433,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "Nakano et al. [165] demonstrate that their system (WebGPT) predominately accepts implicit assumptions in a user input, reﬂecting the same stance in its answers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a system (WebGPT) and its behavior. No verifiable resources are identified.",
      "processing_time": 57.61981987953186,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "As Nakano et al. [165] argue, long-form question answering with LLM systems, may “become one of the main ways people learn about the world” (p.1).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about long-form question answering with LLM systems.",
      "processing_time": 56.6022469997406,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "For example, Nakano et al. [165] report the top 5 contractors account for 50% of their data, and for Bai et al. [16] roughly 20 workers contribute 80%.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general statistics about data distribution. No verifiable resources are identified.",
      "processing_time": 56.59805631637573,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "For example, Nakano et al. [165] report the top 5 contractors account for 50% of their data, and for Bai et al. [16] roughly 20 workers contribute 80%.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general statistics about data distribution. No verifiable resources are identified.",
      "processing_time": 56.59805631637573,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "The impact of digital assistants in improving work productivity has been demonstrated [148], where AI can augment and complement human capabilities by automating routine or repetitive tasks [125].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the impact of digital assistants on productivity.",
      "processing_time": 55.10745072364807,
      "citing_paper_id": "257427629",
      "cited_paper_id": 245956159
    },
    {
      "context_text": "If personalised LLMs are primarily provided by private companies, then their customers become the agenda setters and stand to beneﬁt the most from any improvements in the technology [177].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about personalized LLMs and their providers. No verifiable resources are identified.",
      "processing_time": 57.832722663879395,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "As Ouyang et al. [177] note “it is impossible that one can train a system that is aligned to everyone’s preferences at once” (p.18).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about training systems to align with user preferences.",
      "processing_time": 55.99717092514038,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "The capabilities of large language models (LLMs) to complete tasks and follow natural language instructions has substantially improved in recent years [177].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general capabilities of large language models.",
      "processing_time": 55.11371946334839,
      "citing_paper_id": "257427629",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Often a KL-divergence penalty is included during training to prevent the ﬁne-tuned model deviating too far from the pre-trained representations [247, 136, 177, 165, 16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological approach involving KL-divergence penalty during training. No verifiable resources are identified.",
      "processing_time": 58.304906129837036,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "Often a KL-divergence penalty is included during training to prevent the ﬁne-tuned model deviating too far from the pre-trained representations [247, 136, 177, 165, 16].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a methodological approach involving KL-divergence penalty during training. No verifiable resources are identified.",
      "processing_time": 58.304906129837036,
      "citing_paper_id": "257427629",
      "cited_paper_id": 250562745
    },
    {
      "context_text": "Alignment Tax A concern with RLHF techniques is model overﬁtting [16].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concern about model overfitting in RLHF techniques.",
      "processing_time": 56.574697494506836,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "…democratises how values or preferences are embedded into an LLM, so it could be seen as moving towards more participatory AI, where stakeholders from more diverse backgrounds than those currently employed in the RLHF process can inform use-cases, intents and design of the technology [250, 120].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of participatory design in AI systems.",
      "processing_time": 55.98098301887512,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248420038
    },
    {
      "context_text": "Building persuasive agents have been explicitly targeted [218, 228, 121] and is indirectly mentioned by Bakker et al. [18] who note the potential mis-use of their RLHF-trained system for presenting arguments in a manipulative or coercive manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research on building persuasive agents and the potential misuse of RLHF-trained systems.",
      "processing_time": 57.57978940010071,
      "citing_paper_id": "257427629",
      "cited_paper_id": 248780483
    },
    {
      "context_text": "Building persuasive agents have been explicitly targeted [218, 228, 121] and is indirectly mentioned by Bakker et al. [18] who note the potential mis-use of their RLHF-trained system for presenting arguments in a manipulative or coercive manner.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research on building persuasive agents and the potential misuse of RLHF-trained systems.",
      "processing_time": 57.57978940010071,
      "citing_paper_id": "257427629",
      "cited_paper_id": 253481040
    },
    {
      "context_text": "In Weidinger et al. [232], polarisation risks exacerbate misinformation harms.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses risks associated with language models.",
      "processing_time": 55.075307846069336,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "Privacy in Weidinger et al. [232]’s taxonomy comes under information hazards, from inferring or leaking private and sensitive information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a taxonomy of risks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.16194677352905,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "In Weidinger et al. [232], it aligns with disparate access due to hardware, software or skill constraints.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses risks related to language models.",
      "processing_time": 55.08557319641113,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "It is a similar categorisation to Weidinger et al. [232]’s malicious use, which includes personalised disinformation campaigns; reducing the cost of disinformation campaigns; and facilitating fraud and impersonation scams.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a categorization of risks posed by language models. No verifiable resources are identified.",
      "processing_time": 57.56458020210266,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "The risks of Anthropomorphism align with Weidinger et al. [232]’s human-computer interaction harms, where anthropomorphism leads to over-reliance or unsafe use, and creates avenues for exploiting user trust to obtain private information.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses risks and harms associated with anthropomorphism in human-computer interaction.",
      "processing_time": 57.149683237075806,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "Some risks are inherited from LLMs [232, 30, 24] and AI systems [203] more generally.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with LLMs and AI systems. No verifiable resources are identified.",
      "processing_time": 58.2692711353302,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "Some risks are inherited from LLMs [232, 30, 24] and AI systems [203] more generally.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with LLMs and AI systems. No verifiable resources are identified.",
      "processing_time": 58.2692711353302,
      "citing_paper_id": "257427629",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "Some risks are inherited from LLMs [232, 30, 24] and AI systems [203] more generally.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general risks associated with LLMs and AI systems. No verifiable resources are identified.",
      "processing_time": 58.2692711353302,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "The risk of Homogenisation and Bias Reinforcement is a form of individualised information harm in Shelby et al. [203]’s and Weidinger et al. [232]’s taxonomies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to taxonomies of risks posed by language models.",
      "processing_time": 56.53905892372131,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "In Weidinger et al. [232]’s taxonomy, it is the inverse of some discrimination and exclusion harms, particularly by narrowing performance differentials in predicting user intent across a wider userbase; and by redeﬁning exclusionary norms in the values currently prioritised in LLMs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses concepts related to risks and harms in language models.",
      "processing_time": 56.535600900650024,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "It is also the inverse of Weidinger et al. [232]’s access harms.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a concept discussed in another paper.",
      "processing_time": 57.316946506500244,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "Second, personalised LLMs could be used for manipulation via targeted and personalised disinformation campaigns or fraud [232], intimately drawing on the vulnerabilities and values of the user.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general risk associated with personalized LLMs. No verifiable resources are identified.",
      "processing_time": 57.75718140602112,
      "citing_paper_id": "257427629",
      "cited_paper_id": 249872629
    },
    {
      "context_text": "Training complexity could be further reduced by implementing batched or ofﬂine training [209, 168, 136], instead of online training [247].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training methods. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 57.536513566970825,
      "citing_paper_id": "257427629",
      "cited_paper_id": 250562745
    },
    {
      "context_text": "Deciding the limits of personalisation is inherently a normative decision, which involves making subjective and contentious choices about what should be permitted [66, 110].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses normative decisions in personalisation, which is a conceptual issue rather than a technical one.",
      "processing_time": 59.30052185058594,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252070540
    },
    {
      "context_text": "Some recent work suggests LLMs require no additional training to ‘role-play’ as different individuals, adopting their worldview [9], mirroring their play in economic games [91, 5] or predicting their voting preferences [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capabilities of LLMs in role-playing and simulating human behavior.",
      "processing_time": 57.995749950408936,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252280474
    },
    {
      "context_text": "Some recent work suggests LLMs require no additional training to ‘role-play’ as different individuals, adopting their worldview [9], mirroring their play in economic games [91, 5] or predicting their voting preferences [11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to the capabilities of LLMs in role-playing and simulating human behavior.",
      "processing_time": 57.995749950408936,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "As Birhane et al. [28] argue, active participation is a key component for successful participatory AI.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of participatory AI. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.977070569992065,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252355515
    },
    {
      "context_text": "The risk of co-optation is particularly concerning if minoritised communities are shouldered with the burden of effort to adapt the system to their needs, where participation counter-productively reinforces uneven power dynamics [28].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the risks and challenges associated with participatory AI, particularly the burden on minoritised communities.",
      "processing_time": 59.6023211479187,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252355515
    },
    {
      "context_text": "While this process is participatory, it risks being extractive – a form of volunteer labour on the part of end-users for the beneﬁt or proﬁt of technology providers [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the participatory nature of AI and potential issues, but no verifiable resources are identified.",
      "processing_time": 59.408042192459106,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252355515
    },
    {
      "context_text": "…70, 245], sentiment analysis [80, 155, 226], domain classiﬁcation [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as education [118, 163, 241], medicine [3, 15, 225, 235] and news consumption [58, 10, 61,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It lists various applications and domains but does not provide names of datasets.",
      "processing_time": 57.980133056640625,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252624646
    },
    {
      "context_text": "These systems cover a wide range of tasks including dialogue [127, 157, 36, 39, 41, 109, 133, 146, 149, 206, 238, 244], recipe or diet generation [147, 87, 159], summarisation [215, 240], machine translation [156, 153, 194, 237], QA [137, 193], search and information retrieval [4, 40, 59, 70, 245], sentiment analysis [80, 155, 226], domain classification [129, 114, 113], entity resolution [132], and aggression or abuse detection [107, 108]; and are applied to a number of societal domains such as education [118, 163, 241], medicine [3, 15, 225, 235] and news consumption [58, 10, 61, 190].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a range of tasks and applications. No verifiable resources are identified.",
      "processing_time": 57.496302366256714,
      "citing_paper_id": "257427629",
      "cited_paper_id": 252819096
    },
    {
      "context_text": "Similarly, Perez et al. [185] ﬁnd that as models scale with RLHF, they become sycophants – simply mirroring the user’s prior opinions and telling them what they want to hear.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about model behavior.",
      "processing_time": 55.26730990409851,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "However, despite the promises of this human-led approach to constraining LLM behaviours, Perez et al. [185] ﬁnd evidence of an inverse scaling law – whereby more RLHF training degrades pre-trained representations, resulting in a model that has more polarised views on issues such as gun rights or…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about the effects of RLHF training on pre-trained models.",
      "processing_time": 57.27307367324829,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "Smaller yet more personalised models may be a preferred pathway because (i) model scale may not contribute signiﬁcantly to performance [209], and (ii) increased scale may actually harm performance, for example leading to increased sycophancy or goal preservation [185].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model scale and performance issues.",
      "processing_time": 55.4385871887207,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "This problem may be exacerbated by RLHF, for example in entrenching one set of political, cultural or religious standpoints [185, 165].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential issue with Reinforcement Learning from Human Feedback (RLHF). No verifiable resources are identified.",
      "processing_time": 58.8731906414032,
      "citing_paper_id": "257427629",
      "cited_paper_id": 254854519
    },
    {
      "context_text": "[122] find that personalisation positively influenced consumer intentions to disclose personal information to a digital assistant.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about personalization influencing consumer behavior.",
      "processing_time": 56.47662544250488,
      "citing_paper_id": "257427629",
      "cited_paper_id": 255892294
    },
    {
      "context_text": "Lai et al. [124] speciﬁcally focus on making AI explanations more selective to better align systems with how humans create and consume information, ﬁnding that their method improved user satisfaction.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for improving AI explanations. The focus is on the findings and the research work itself.",
      "processing_time": 58.61581802368164,
      "citing_paper_id": "257427629",
      "cited_paper_id": 256194393
    },
    {
      "context_text": "General concerns over the environmental costs to train ever larger models with cloud compute and data centres is discussed by Bender et al. [24].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concerns about the environmental impact of training large models.",
      "processing_time": 55.61399984359741,
      "citing_paper_id": "257427629",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "…LLMs trained under the speciﬁcations of large technology providers and ﬁne-tuned based on feedback from a small set of crowdworkers, there is a clear need to improve the inclusion and accessibility of LLMs to serve marginalised populations whose voices are currently deprioritised [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concern about the training and fine-tuning of large language models.",
      "processing_time": 57.704683780670166,
      "citing_paper_id": "257427629",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "Homogenisation with personalised LLMs can occur at a number of levels, with analogies from how recommender systems homogenise taste [174, 88].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to recommender systems. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 58.60438513755798,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "Instead they use pre-trained foundation models, which have been created by large AI labs, and then domain-adapt, fine-tune, prompt-tune or teach-in-context to create models for specific tasks [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of pre-trained foundation models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.357656955718994,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "Concerns have also been raised over an over-reliance on social media for information and communication [1]; as well as more general concerns that humans become over-reliant on ML technologies [92], blindly trusting their outputs even if incorrect [182].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It discusses general concerns about social media and ML technologies.",
      "processing_time": 57.692479610443115,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "It has been suggested that ChatGPT already burns “millions of dollars a day” in inference costs [119] and likely has a large carbon footprint.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the cost and environmental impact of running ChatGPT.",
      "processing_time": 58.592201948165894,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "It has been suggested that ChatGPT already burns “millions of dollars a day” in inference costs [119] and likely has a large carbon footprint.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the cost and environmental impact of running ChatGPT.",
      "processing_time": 58.592201948165894,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "We wish to avoid focusing too much on recent model releases from industry labs (OpenAI’s ChatGPT [173], Anthropic’s Claude [78], Microsoft’s New Bing [151], or Google’s Bard [187]).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and platforms. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.35883355140686,
      "citing_paper_id": "257427629",
      "cited_paper_id": null
    },
    {
      "context_text": "It is computed as the average LPIPS [30] cosine similarity between samples generated with the same prompt.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (LPIPS) which is not a dataset. The context is about evaluating generated samples using a perceptual metric.",
      "processing_time": 60.034533739089966,
      "citing_paper_id": "259262648",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "The condition vector is often obtained from Transformer-based [13] models like BERT [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.337523460388184,
      "citing_paper_id": "259262648",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The condition vector c is often obtained from Transformer [16] models like BERT [17] applied to the input text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 57.026514768600464,
      "citing_paper_id": "259262648",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "VAE Latent representations for images E ( x ) 4.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (VAE latent representations). The title 'AUTO-ENCODING VARIATIONAL BAYES' confirms this is about a method, not a dataset.",
      "processing_time": 61.305333852767944,
      "citing_paper_id": "259262648",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "5, which uses VAE [15] as an encoder-decoder model for latent representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (VAE).",
      "processing_time": 55.57104468345642,
      "citing_paper_id": "259262648",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "Second, this criterion can be viewed as directly maximizing the CLIP score, which is known to produce adversarial examples for CLIP instead of actually improving the image quality [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (CLIP) and a potential issue with adversarial examples. No verifiable dataset is referenced.",
      "processing_time": 59.51540803909302,
      "citing_paper_id": "259262648",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "1 Introduction Large text-to-image synthesis models have recently attracted the attention of the research community due to their ability to generate high-quality and diverse images that correspond to the user’s prompt in natural language [1, 2, 3, 4, 5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to text-to-image synthesis models. No verifiable resources are identified.",
      "processing_time": 58.5609335899353,
      "citing_paper_id": "259262648",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "1 Introduction Large text-to-image synthesis models have recently attracted the attention of the research community due to their ability to generate high-quality and diverse images that correspond to the user’s prompt in natural language [1, 2, 3, 4, 5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to text-to-image synthesis models. No verifiable resources are identified.",
      "processing_time": 58.5609335899353,
      "citing_paper_id": "259262648",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "The most natural and well-studied source of guidance for generative models is the natural language [13, 14, 15] because of its convenience for the user, ease of collecting training data, and significant improvements in text representations over the past several years [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to natural language and improvements in text representations. No verifiable resources are identified.",
      "processing_time": 58.99607515335083,
      "citing_paper_id": "259262648",
      "cited_paper_id": 249145348
    },
    {
      "context_text": "The most natural and well-studied source of guidance for generative models is the natural language [13, 14, 15] because of its convenience for the user, ease of collecting training data, and significant improvements in text representations over the past several years [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to natural language and improvements in text representations. No verifiable resources are identified.",
      "processing_time": 58.99607515335083,
      "citing_paper_id": "259262648",
      "cited_paper_id": 256416326
    },
    {
      "context_text": "While Textual Inversion only learns the embedding of the target token, DreamBooth [7] does the same with a fully unfrozen U-Net component, and Custom Diffusion [8] updates the projection matrices of cross-attention layers that correspond to keys and values.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.32355189323425,
      "citing_paper_id": "259262648",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "While Textual Inversion only learns the embedding of the target token, DreamBooth [7] does the same with a fully unfrozen U-Net component, and Custom Diffusion [8] updates the projection matrices of cross-attention layers that correspond to keys and values.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.32355189323425,
      "citing_paper_id": "259262648",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Three prominent methods for text-to-image model personalization are Textual Inversion [6], DreamBooth [7], and Custom Diffusion [8]: the first one trains a single token embedding for the target dataset while keeping most of the weights unchanged, the second one finetunes the full model (possibly with parameter-efficient techniques like LoRA [9]), and the third finetunes only the token embedding and cross-attention weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses methods for text-to-image model personalization but does not mention any specific datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.31740880012512,
      "citing_paper_id": "259262648",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Three prominent methods for text-to-image model personalization are Textual Inversion [6], DreamBooth [7], and Custom Diffusion [8]: the first one trains a single token embedding for the target dataset while keeping most of the weights unchanged, the second one finetunes the full model (possibly with parameter-efficient techniques like LoRA [9]), and the third finetunes only the token embedding and cross-attention weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses methods for text-to-image model personalization but does not mention any specific datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 59.31740880012512,
      "citing_paper_id": "259262648",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "For this, we perform inversion of all concepts released by [6, 7, 8] (resulting in 48 concepts): an example of such an experiment for one concept is available in Figure 2.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the inversion of concepts from cited works. No clear, verifiable dataset names are provided.",
      "processing_time": 58.793620109558105,
      "citing_paper_id": "259262648",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "For this, we perform inversion of all concepts released by [6, 7, 8] (resulting in 48 concepts): an example of such an experiment for one concept is available in Figure 2.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the inversion of concepts from cited works. No clear, verifiable dataset names are provided.",
      "processing_time": 58.793620109558105,
      "citing_paper_id": "259262648",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "For example, Prompt-to-Prompt [25] solves the task by injecting the new prompt into cross-attention, and MagicMix [26] replaces the guidance vector after several steps during inference.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on describing techniques used in personalized text generation.",
      "processing_time": 58.311439752578735,
      "citing_paper_id": "259262648",
      "cited_paper_id": 253224280
    },
    {
      "context_text": "For example, in P + [21], the concept is inverted into a sequence of per-layer tokens, and SVDiff [22] focuses on learning the singular values of the SVD decomposition of weight matrices.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.306663513183594,
      "citing_paper_id": "259262648",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, in P + [21], the concept is inverted into a sequence of per-layer tokens, and SVDiff [22] focuses on learning the singular values of the SVD decomposition of weight matrices.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 58.306663513183594,
      "citing_paper_id": "259262648",
      "cited_paper_id": null
    },
    {
      "context_text": "odel[2,22,35].Theattention-basedSeq2Seqmodel has demonstrated to be effective in a number of tasks of text generation, including neural machine translation [2, 35, 40], abstractive text summarization [4, 27, 30], dialogue generation [3, 33, 38], etc. Generally speaking, the Seq2Seq model has become one of the most common frameworks for text generation. While most of the Seq2Seq models are based on RNN, recen",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 58.51620936393738,
      "citing_paper_id": "88524997",
      "cited_paper_id": 133195
    },
    {
      "context_text": "researchers have proposed frameworks based on CNN [8] and attention mechanism [37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of CNN and attention mechanisms in frameworks.",
      "processing_time": 58.979487657547,
      "citing_paper_id": "88524997",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "eq2Seqmodel has demonstrated to be effective in a number of tasks of text generation, including neural machine translation [2, 35, 40], abstractive text summarization [4, 27, 30], dialogue generation [3, 33, 38], etc. Generally speaking, the Seq2Seq model has become one of the most common frameworks for text generation. While most of the Seq2Seq models are based on RNN, recently researchers have proposed fra",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various applications of the Seq2Seq model. No verifiable resources are identified.",
      "processing_time": 58.50861358642578,
      "citing_paper_id": "88524997",
      "cited_paper_id": 7562717
    },
    {
      "context_text": "networks have been widely used for automatic text generation. In this section, we briefly review related literature. Textgeneration A basic model for text generation is the attentionbasedSeq2Seqmodel[2,22,35].Theattention-basedSeq2Seqmodel has demonstrated to be effective in a number of tasks of text generation, including neural machine translation [2, 35, 40], abstractive text summarization [4, 27, 30],",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It focuses on describing the attention-based Seq2Seq model and its applications in text generation tasks.",
      "processing_time": 59.74647307395935,
      "citing_paper_id": "88524997",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "t users or external knowledge into consideration. Owing to the success of neural networks in natural language processing, we propose to apply neural methods and sequence-tosequence (Seq2Seq) learning [35] to product description generation. The Seq2Seq model has achieved tremendous success in natural language generation, including neural machine translation [2, 35, 40] and abstractive summarization [27",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural methods and sequence-to-sequence learning. No verifiable resources are identified.",
      "processing_time": 58.5163300037384,
      "citing_paper_id": "88524997",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "…to be effective in a number of tasks of text generation, including neural machine translation [2, 34, 39], abstractive text summarization [4, 26, 29], dialogue generation [3, 32, 37], etc. Generally speaking, the Seq2Seq model has become one of the most common frameworks for text generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general tasks and applications of Seq2Seq models. No verifiable resources are identified.",
      "processing_time": 58.7498996257782,
      "citing_paper_id": "88524997",
      "cited_paper_id": 8928715
    },
    {
      "context_text": "The Seq2Seq model has achieved tremendous success in natural language generation, including neural machine translation [2, 34, 39] and abstractive summarization [26, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the success of Seq2Seq models in certain tasks. No verifiable resources are identified.",
      "processing_time": 58.94882297515869,
      "citing_paper_id": "88524997",
      "cited_paper_id": 8928715
    },
    {
      "context_text": "Then we train a CNN-based [14] multi-class classifier on Z which takes the text as input and predicts the user category it belongs to.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions training a CNN-based classifier on 'Z', but does not specify what 'Z' is. There are no clear identifiers for a dataset, and the title does not help disambiguate.",
      "processing_time": 61.3907585144043,
      "citing_paper_id": "88524997",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "Then we train a CNN-based [13] multi-class classifier on Z which takes the text as input and predicts the user category it belongs to.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a generic reference 'Z'. There are no multi-word proper nouns, acronyms, or hyphenated names with digits that could be considered as datasets.",
      "processing_time": 61.55996823310852,
      "citing_paper_id": "88524997",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "• Attr-D (Dedicated) Before moving on to conditioned models, we train a dedicated model on each subset of the data following Ficler and Goldberg [7].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'subset of the data' but does not specify a named dataset. The term 'Attr-D' appears to be a model or method rather than a dataset.",
      "processing_time": 60.66834378242493,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "Related literature can be also found in [1, 7, 11, 12, 18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references other literature.",
      "processing_time": 57.814260959625244,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "Following Ficler and Goldberg [7], Sennrich et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to authors and their work. There is no indication of a reusable resource being used.",
      "processing_time": 59.44045448303223,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "Attribute FusionModel In a conditioned encoder-decoder framework similar to [7, 36], we add an additional conditioning context, which is attribute a in our case.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is about adding an additional conditioning context in a neural language generation model.",
      "processing_time": 60.16230630874634,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "Text generation A basic model for text generation is the attentionbased Seq2Seqmodel [2, 22, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (attention-based Seq2Seq model).",
      "processing_time": 58.06992220878601,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The Seq2Seq model has achieved tremendous success in natural language generation, including neural machine translation [2, 35, 40] and abstractive summarization [27, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of Seq2Seq models. No verifiable resources are identified.",
      "processing_time": 58.927510023117065,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The attention-based Seq2Seqmodel has demonstrated to be effective in a number of tasks of text generation, including neural machine translation [2, 35, 40], abstractive text summarization [4, 27, 30], dialogue generation [3, 33, 38], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of the attention-based Seq2Seq model. No verifiable resources are identified.",
      "processing_time": 59.7096426486969,
      "citing_paper_id": "88524997",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "There are several ideas to condition the model on a context [18, 24, 31].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to conditioning models on context. No verifiable resources are identified.",
      "processing_time": 58.71750831604004,
      "citing_paper_id": "88524997",
      "cited_paper_id": 19247366
    },
    {
      "context_text": "• Attr-T (start of Target sequence) Another technique to attribute fusion is to use an attribute-specific start token for the target sequence [24], instead of a shared token.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique for personalized neural machine translation.",
      "processing_time": 57.549925088882446,
      "citing_paper_id": "88524997",
      "cited_paper_id": 19247366
    },
    {
      "context_text": "nditions of semantic information and sentiment with a language model, and Tang et al. [36] conditioned their generation on discrete and continuous information. Related literature can be also found in [1, 7, 11, 12, 18]. To the best of our knowledge, our research takes the first attempt to use neural methods and sequence-to-sequence learning for product description generation by considering personalization and infor",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to related literature and methods. No verifiable resources are identified.",
      "processing_time": 40.210585832595825,
      "citing_paper_id": "88524997",
      "cited_paper_id": 34405847
    },
    {
      "context_text": "ng the studies, we implement our methods upon the Transformer framework. Productdescriptiongeneration As for product description generation, previous studies focused on statistical frameworks such as [39], which incorporates statistical methods with the template for the generation of product descriptions. Gerani et al. [9] also generates summarization of product reviews by applying a templatebased NLG",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of a statistical framework and template-based methods for product description generation, but no dataset names are provided.",
      "processing_time": 61.02392053604126,
      "citing_paper_id": "88524997",
      "cited_paper_id": 36574384
    },
    {
      "context_text": "on and user preferences. Recently, there has been increasing attention on automatic product description generation. Most existing methods are based on templates and traditional statistical frameworks [17, 39]. Although applicable, these methods have some inherent drawbacks that they are limited in several respects. They place a restriction on the phrasal expression and discourse structure, and they cannot",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 58.686891317367554,
      "citing_paper_id": "88524997",
      "cited_paper_id": 36574384
    },
    {
      "context_text": ", generation of images conditioned on text prompt, have been extensively studied in recent years (Qiao et al., 2019b; Li et al., 2019a; Ding et al., 2021; Hinz et al., 2020; Tao et al., 2020; Li et al., 2019b; Qiao et al., 2019a; Ramesh et al., 2021; Zhang et al., 2018; Crowson et al., 2022; Gafni et al., 2022; Chang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on text-to-image synthesis. No verifiable resources are identified.",
      "processing_time": 59.382139444351196,
      "citing_paper_id": "258078921",
      "cited_paper_id": 3614800
    },
    {
      "context_text": ", generation of images conditioned on text prompt, have been extensively studied in recent years (Qiao et al., 2019b; Li et al., 2019a; Ding et al., 2021; Hinz et al., 2020; Tao et al., 2020; Li et al., 2019b; Qiao et al., 2019a; Ramesh et al., 2021; Zhang et al., 2018; Crowson et al., 2022; Gafni et al., 2022; Chang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on text-to-image synthesis. No verifiable resources are identified.",
      "processing_time": 59.382139444351196,
      "citing_paper_id": "258078921",
      "cited_paper_id": 248239727
    },
    {
      "context_text": ", 2012), reinforcement learning (Salimans et al., 2017; Hu et al., 2017), objective detection (Zhang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets.",
      "processing_time": 58.691736459732056,
      "citing_paper_id": "258078921",
      "cited_paper_id": 11410889
    },
    {
      "context_text": "also termed as black-box, zeroth-order or derivative-free optimization (Conn et al., 2009; Kolda et al., 2003; Rios & Sahinidis, 2013; Sahu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only optimization methods and techniques. There are no verifiable resources that meet the criteria.",
      "processing_time": 59.38989567756653,
      "citing_paper_id": "258078921",
      "cited_paper_id": 12575623
    },
    {
      "context_text": "In recent years, personalization has become a prominent factor in various fields within machine learning area [48, 61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend towards personalization in machine learning.",
      "processing_time": 58.20511603355408,
      "citing_paper_id": "258078921",
      "cited_paper_id": 14890747
    },
    {
      "context_text": "Finally, we propose two subspace decomposition strategies for projection 𝑊 𝑝 , including PCA and prior normalization, in Sec.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses subspace decomposition strategies, which are part of the methodology.",
      "processing_time": 60.29120492935181,
      "citing_paper_id": "258078921",
      "cited_paper_id": 15038271
    },
    {
      "context_text": "Their approach searches for the optimal embedding that can represent the concept with gradient backward and calibrated learning rate scheme (Ruder, 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing embeddings.",
      "processing_time": 57.05649709701538,
      "citing_paper_id": "258078921",
      "cited_paper_id": 17485266
    },
    {
      "context_text": "Representative GFO algorithms include evolutionary algorithms (Hansen & Ostermeier, 2001), Bayesian optimization (Frazier, 2018), etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 59.659494161605835,
      "citing_paper_id": "258078921",
      "cited_paper_id": 49656213
    },
    {
      "context_text": "Here, we resort to the gradient-free optimization (GFO), also termed as black-box, zeroth-order or derivative-free optimization [8, 24, 41, 46].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization methods. No verifiable resources are identified.",
      "processing_time": 58.4188277721405,
      "citing_paper_id": "258078921",
      "cited_paper_id": 58957330
    },
    {
      "context_text": "Here, we resort to the gradient-free optimization (GFO), also termed as black-box, zeroth-order or derivative-free optimization [8, 24, 41, 46].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization methods. No verifiable resources are identified.",
      "processing_time": 58.4188277721405,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "In contrast, we try to optimize the inversion embedding without gradient, enabling the generation of novel images of the subject while preserving computation efficiency and safety.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the optimization of inversion embedding for image generation.",
      "processing_time": 59.865681648254395,
      "citing_paper_id": "258078921",
      "cited_paper_id": 135464234
    },
    {
      "context_text": "In contrast, we try to optimize the inversion embedding without gradient, enabling the generation of novel images of the subject while preserving computation efficiency and safety.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the optimization of inversion embedding for image generation.",
      "processing_time": 59.865681648254395,
      "citing_paper_id": "258078921",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "In contrast, we try to optimize the inversion embedding without gradient, enabling the generation of novel images of the subject while preserving computation efficiency and safety.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the optimization of inversion embedding for image generation.",
      "processing_time": 59.865681648254395,
      "citing_paper_id": "258078921",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "In contrast, we try to optimize the inversion embedding without gradient, enabling the generation of novel images of the subject while preserving computation efficiency and safety.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the optimization of inversion embedding for image generation.",
      "processing_time": 59.865681648254395,
      "citing_paper_id": "258078921",
      "cited_paper_id": 252815928
    },
    {
      "context_text": "Diffusion models inversion, which aims to find a noise map and a conditioning vector that corresponds to a generated image, is a challenging task and provides a promising solution to image generation controls [30, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenge and potential of diffusion models inversion in image generation.",
      "processing_time": 59.18052005767822,
      "citing_paper_id": "258078921",
      "cited_paper_id": 202577442
    },
    {
      "context_text": "Diffusion models inversion, which aims to find a noise map and a conditioning vector that corresponds to a generated image, is a challenging task and provides a promising solution to image generation controls [30, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenge and potential of diffusion models inversion in image generation.",
      "processing_time": 59.18052005767822,
      "citing_paper_id": "258078921",
      "cited_paper_id": 235212350
    },
    {
      "context_text": "Similarly, previous works in high-dimensional GFO [25, 33, 57] try to utilize a normal distribution to set each entry in subspace projections.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using a normal distribution for subspace projections.",
      "processing_time": 17.21406364440918,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "GFO algorithms repeat the above sampling-and-updating procedure so as to iteratively enhance the quality of solutions [40].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or algorithm. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 60.0907506942749,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Similarly, previous works in high-dimensional GFO (Wang et al., 2016; Qian et al., 2016; Letham et al., 2020) try to utilize a normal distribution to set each entry in subspace projections.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about using normal distributions in subspace projections for high-dimensional Bayesian optimization.",
      "processing_time": 60.0873019695282,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Representative GFO algorithms include evolutionary algorithms [20], Bayesian optimization [15], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 59.61989974975586,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Gradient-free optimization (GFO) realizes optimization only via the function values on the sampled solutions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes a general concept of gradient-free optimization.",
      "processing_time": 59.32585787773132,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Overall, the contributions of this paper are four fold: • We introduce a new scenario of textual inversion in gradient-free framework, which to our best knowledge is the first trial of GFO methods on personalized text-to-image generation tasks; • This paper offers a solution with an improved evolution strategy in the searching scenario to accomplish the common text-to-image personalization task; • To accelerate the convergence of iterative process, we provide the general condition initialization for pseudo-token embedding and decomposed subspace for effective incremental optimization; • Empirical results show that gradient-free textual inversion can successfully deal with real-world applications, achieving comparable performance with gradient-based counterparts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodological contributions and empirical results. No verifiable resources are identified.",
      "processing_time": 59.33731818199158,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "In general, GFO involves a kind of optimization algorithms that do not require gradients, but only rely on function values or fitness values of iteratively sampled solutions [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a class of optimization algorithms. There are no verifiable resources or datasets mentioned.",
      "processing_time": 60.060462474823,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "The results of metrics of our method, standard textual inversion, and GFO in original space are 88.2%, 81.3%, and 57.6%, where we believe that the high-dimentional searching space leads to the inferior performance of GFO.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics of different methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.550150632858276,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "To improve the convergence and stability of optimization, we introduce to ( i ) initialize the pseudo-token embedding with general condition, i.e. , non-parametric cross-attention of pre-trained word embedding and personalized visual features; ( ii ) decompose the original searching space of GFO into a smaller subspace using Principal Components Analysis (PCA) or prior normalization and solve the transferred problem with some derivative-free optimizer in the subspace for incremental elements.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 59.83456063270569,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Most GFO algorithms share a common structure of sampling and updating to enhance the quality of solutions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes a general characteristic of GFO algorithms.",
      "processing_time": 59.83060693740845,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Due to their ability to address complex optimization tasks, GFO algorithms have achieved many impressive applications in automatic machine learning [51], reinforcement learning [23, 47], objective detection [64], few-shot learning [5, 53, 60], and black-box attack [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various application areas of GFO algorithms. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 60.93734812736511,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "However, GFO algorithms are known to suffer from a slow convergence in high-dimensional search space, due to the massive searching directions for continuous text embedding.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the limitations of GFO algorithms in high-dimensional search spaces.",
      "processing_time": 58.814167499542236,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "By generating Wp from random norm (Letham et al., 2020), PCA, and prior norm, we simulate the distribution of the projected incremental WpQ and compare it with the distribution of text encoder word embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and models, but no dataset names are provided.",
      "processing_time": 60.227927923202515,
      "citing_paper_id": "258078921",
      "cited_paper_id": 211003707
    },
    {
      "context_text": "Text-to-image synthesis, i.e. , generation of images conditioned on text prompt, have been extensively studied in recent years [6, 10, 13, 16, 22, 26, 27, 34, 35, 39, 54, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers studying text-to-image synthesis. No verifiable resources are identified.",
      "processing_time": 60.52713918685913,
      "citing_paper_id": "258078921",
      "cited_paper_id": 221112715
    },
    {
      "context_text": "(Dhariwal & Nichol, 2021) show that deterministic DDIM sampling (Song et al., 2020)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 60.52393460273743,
      "citing_paper_id": "258078921",
      "cited_paper_id": 222140788
    },
    {
      "context_text": ", “a photo of v” for objective and “a style of v” for style with every token v in vocabulary V , we use the pretrained CLIP model c(·) (Radford et al., 2021) to extract their features and calculate the cosine similarity s(·) between every pair of visual and text embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using a pretrained CLIP model to extract features and calculate cosine similarity, but does not mention any specific datasets.",
      "processing_time": 60.03804159164429,
      "citing_paper_id": "258078921",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Large-scale text-to-image models, enabling high-quality and diverse synthesis of images based on a text prompt written in natural language, have achieved remarkable progress and become an exciting direction [31, 38, 42, 45, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing progress in text-to-image models.",
      "processing_time": 59.58171629905701,
      "citing_paper_id": "258078921",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Their approach searches for the optimal embedding that can represent the concept with gradient backward and calibrated learning rate scheme [43].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about the technique used for finding optimal embeddings.",
      "processing_time": 60.49915814399719,
      "citing_paper_id": "258078921",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "We retain the original hyper-parameter choices of the stable diffusion model from diffusers [55] and the CLIP model from hug-gingface [58].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their hyper-parameters. No verifiable datasets are referenced.",
      "processing_time": 60.02666759490967,
      "citing_paper_id": "258078921",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "We retain the original hyper-parameter choices of the stable diffusion model from diffusers [55] and the CLIP model from hug-gingface [58].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their hyper-parameters. No verifiable datasets are referenced.",
      "processing_time": 60.02666759490967,
      "citing_paper_id": "258078921",
      "cited_paper_id": null
    },
    {
      "context_text": "In this regard, (Liu et al., 2021) propose a modification to classifier guidance that allows for the guidance of diffusion models using images and text, allowing for semantic variations of an image, although the identity of the subject often varies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modifying classifier guidance in diffusion models.",
      "processing_time": 59.284696102142334,
      "citing_paper_id": "258078921",
      "cited_paper_id": 245117331
    },
    {
      "context_text": ", 2022), or the latent space of image encoding (Rombach et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or models. The context is about using latent spaces in image synthesis, which does not indicate the use of a specific dataset.",
      "processing_time": 62.181676149368286,
      "citing_paper_id": "258078921",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "We use the stable diffusion model (Rombach et al., 2022) as the base model in our experiments, though our method is not constrained to any specific text-to-image models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (stable diffusion model) but does not refer to any specific dataset. The citation is about using a model, not a dataset.",
      "processing_time": 61.39338397979736,
      "citing_paper_id": "258078921",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Note that (i) as the CLIP model and diffusion-based text-to-image model (Rombach et al., 2022) use the same pre-trained text encoder, the initialized embedding e0 is constructed under the same distribution of latent space E and (ii) the resulting embedding can be viewed as an visual-adaptive integration of existing vocabulary and thus contains sufficient visual semantic and explainable representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the technical aspects of the models and their integration.",
      "processing_time": 60.77337408065796,
      "citing_paper_id": "258078921",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Large-scale text-to-image models, enabling high-quality and diverse synthesis of images based on a text prompt written in natural language, have achieved remarkable progress and become an exciting direction (Nichol et al., 2021; Saharia et al., 2022; Ramesh et al., 2022; Rombach et al., 2022; Yu et al., 2022b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and their capabilities. No verifiable resources are identified.",
      "processing_time": 60.47668433189392,
      "citing_paper_id": "258078921",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "[12] show that deterministic DDIM sampling [52] can be inverted to extract a latent noise map that will produce a given real image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 60.893425941467285,
      "citing_paper_id": "258078921",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "Recent work of prompt-to-prompt [21] allows for local and global editing without any input mask requirements, but meets hard up when given image set and personalizes to new situations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for text-to-image generation. The context focuses on the capabilities and limitations of the method.",
      "processing_time": 61.56185245513916,
      "citing_paper_id": "258078921",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "One of the main advantages of these models is the strong semantic prior learned from scalable collections of image-caption pairs, leading to their broad application in artistic creation, e.g. , as sources of inspiration, and even in the designing of new physical products.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'scalable collections of image-caption pairs' without naming any particular dataset.",
      "processing_time": 61.5489776134491,
      "citing_paper_id": "258078921",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "While it requires hundreds of images to learn a semantic prior only for the facial domain, this paper aims to reconstruct the identity of different types or styles of subjects in new contexts with only several casual images and can be transferred with prompts (Gal et al., 2022; Ruiz et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the capabilities of the method described in the cited papers.",
      "processing_time": 61.545023918151855,
      "citing_paper_id": "258078921",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "While the generation capabilities of text-to-image models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts (Ruiz et al., 2022), i.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a capability gap in text-to-image models. The context focuses on the limitations of existing models rather than the use of a particular dataset.",
      "processing_time": 62.57636642456055,
      "citing_paper_id": "258078921",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "In this regard, [28] propose a modification to classifier guidance that allows for the guidance of diffusion models using images and text, allowing for semantic variations of an image, although the identity of the subject often varies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modifying classifier guidance in diffusion models.",
      "processing_time": 60.46764898300171,
      "citing_paper_id": "258078921",
      "cited_paper_id": 254095924
    },
    {
      "context_text": "While it requires hundreds of images to learn a semantic prior only for the facial domain, this paper aims to reconstruct the identity of different types or styles of subjects in new contexts with only several casual images and can be transferred with prompts [17, 44, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for personalized text-to-image generation. No verifiable resources are identified.",
      "processing_time": 61.724706172943115,
      "citing_paper_id": "258078921",
      "cited_paper_id": 258041269
    },
    {
      "context_text": ", 2022) and the CLIP model from huggingface (Wolf et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 61.02307081222534,
      "citing_paper_id": "258078921",
      "cited_paper_id": 279181686
    },
    {
      "context_text": "(Nitzan et al., 2022; Melnik et al., 2022) finetune a face synthesis GAN on a specific identity to build a personalized prior.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (face synthesis GAN) and a general approach (finetuning on a specific identity).",
      "processing_time": 62.137505769729614,
      "citing_paper_id": "258078921",
      "cited_paper_id": null
    },
    {
      "context_text": "…personalized dialogue generation (examples provided in the introduction), CVAEs have been applied to conditional dialogue generation tasks such as\nemotional dialogue generation (Liu et al., 2021; Asghar et al., 2020; Zhou and Wang, 2018) as well as topical dialogue generation (Wang et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions emotional dialogue generation and topical dialogue generation but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.28566384315491,
      "citing_paper_id": "244478160",
      "cited_paper_id": 3033303
    },
    {
      "context_text": "…personalized dialogue generation (examples provided in the introduction), CVAEs have been applied to conditional dialogue generation tasks such as\nemotional dialogue generation (Liu et al., 2021; Asghar et al., 2020; Zhou and Wang, 2018) as well as topical dialogue generation (Wang et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions emotional dialogue generation and topical dialogue generation but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 62.28566384315491,
      "citing_paper_id": "244478160",
      "cited_paper_id": 212633799
    },
    {
      "context_text": "Also, the Bag-of-Words (BoW) loss (Zhao et al., 2017) is incorporated into the loss function to address the vanishing latent variable problem.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BoW loss) used in the loss function.",
      "processing_time": 61.52815771102905,
      "citing_paper_id": "244478160",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "This follows previous work which reported an increase in response diversity due to the stochasticity introduced by the latent variable (Zhao et al., 2017; Song et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous work on neural dialog models.",
      "processing_time": 61.33364152908325,
      "citing_paper_id": "244478160",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "Similar to the CVAE-based dialogue model proposed by Zhao et al. (2017), which involves only generating and sampling from the latent distribution over responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CVAE-based dialogue model).",
      "processing_time": 61.82065534591675,
      "citing_paper_id": "244478160",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "…the Conditional Variational Auto Encoder (CVAE) (Sohn et al., 2015) have been applied to the task of open-domain dialogue generation, where the potential dialogue responses are modelled as a latent Gaussian distribution (Li et al., 2020; Shen et al., 2018; Zhao et al., 2017; Serban et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the application of CVAE to dialogue generation.",
      "processing_time": 63.03864884376526,
      "citing_paper_id": "244478160",
      "cited_paper_id": 14688760
    },
    {
      "context_text": ", 2015) have been applied to the task of open-domain dialogue generation, where the potential dialogue responses are modelled as a latent Gaussian distribution (Li et al., 2020; Shen et al., 2018; Zhao et al., 2017; Serban et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the application of certain techniques to open-domain dialogue generation.",
      "processing_time": 63.53575897216797,
      "citing_paper_id": "244478160",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "Another approach involves incorporating personality or persona related metadata into the decoding process (Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving personality or persona metadata.",
      "processing_time": 62.41897177696228,
      "citing_paper_id": "244478160",
      "cited_paper_id": 51608471
    },
    {
      "context_text": "…typically involves conditioning dialogue responses on the dialogue context in addition to textual persona descriptions (Lee et al., 2021; Na et al., 2021; Liu et al., 2020; Majumder et al., 2020; Madotto et al., 2019; Wolf et al., 2019; Zheng et al., 2020; Chan et al., 2019; Song et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for personalized text generation.",
      "processing_time": 62.415648460388184,
      "citing_paper_id": "244478160",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "…typically involves conditioning dialogue responses on the dialogue context in addition to textual persona descriptions (Lee et al., 2021; Na et al., 2021; Liu et al., 2020; Majumder et al., 2020; Madotto et al., 2019; Wolf et al., 2019; Zheng et al., 2020; Chan et al., 2019; Song et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches for personalized text generation.",
      "processing_time": 62.415648460388184,
      "citing_paper_id": "244478160",
      "cited_paper_id": 202788651
    },
    {
      "context_text": "A popular approach typically involves conditioning dialogue responses on the dialogue context in addition to textual persona descriptions (Lee et al., 2021; Na et al., 2021; Liu et al., 2020; Majumder et al., 2020; Madotto et al., 2019; Wolf et al., 2019; Zheng et al., 2020; Chan et al., 2019; Song et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and methods. No verifiable resources are identified.",
      "processing_time": 62.863086223602295,
      "citing_paper_id": "244478160",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "Similar to TransferTransfo (Wolf et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called TransferTransfo. No verifiable resources are identified.",
      "processing_time": 26.481670379638672,
      "citing_paper_id": "244478160",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "We evaluate our approach on the ConvAI2 personalized dialogue corpus (Dinan et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConvAI2 personalized dialogue corpus"
      ],
      "dataset_descriptions": {
        "ConvAI2 personalized dialogue corpus": "Used to evaluate personalized dialogue systems, focusing on generating contextually appropriate responses that reflect user preferences and personalities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'ConvAI2 personalized dialogue corpus', which is used for evaluating the approach in personalized dialogue systems.",
      "processing_time": 69.65740633010864,
      "citing_paper_id": "244478160",
      "cited_paper_id": 59553505
    },
    {
      "context_text": "Latent variable models are a category of models that involve inferring latent random variables from a group of observable variables (Verbeke and Molenberghs, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a category of models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 63.341052532196045,
      "citing_paper_id": "244478160",
      "cited_paper_id": 126165565
    },
    {
      "context_text": "In addition to the CVAE, the basic Variational Auto Encoder (VAE) and the Wasserstein Auto Encoder (WAE) have also been applied to personalized dialogue generation in a similar manner (Chan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VAE and WAE but does not specify any datasets. The focus is on methods/models rather than datasets.",
      "processing_time": 63.337666273117065,
      "citing_paper_id": "244478160",
      "cited_paper_id": 202788651
    },
    {
      "context_text": "On the other hand, the Persona-Aware Variational Response Generator (PAGenerator) (Wu et al., 2020) relies on user embeddings trained concurrently with the CVAE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PAGenerator) and a model (CVAE).",
      "processing_time": 63.625423431396484,
      "citing_paper_id": "244478160",
      "cited_paper_id": 207881153
    },
    {
      "context_text": "Some approaches also involve implicitly learning personality user embeddings (Zheng et al., 2019a; Wu et al., 2020; Al-Rfou et al., 2016; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or models for learning personality user embeddings.",
      "processing_time": 62.83843541145325,
      "citing_paper_id": "244478160",
      "cited_paper_id": 207881153
    },
    {
      "context_text": "emotional dialogue generation (Liu et al., 2021; Asghar et al., 2020; Zhou and Wang, 2018) as well as topical dialogue generation (Wang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. No clear identifiers for datasets are present.",
      "processing_time": 63.33393144607544,
      "citing_paper_id": "244478160",
      "cited_paper_id": 212633799
    },
    {
      "context_text": "…control over biases, which models may actively use as part of their heuristics in contrastive and downstream tasks (Bender et al., 2021; Davidson et al., 2019; Mitchell et al., 2019; Ferrer et al., 2020; Koolen and van Cranenburgh, 2017; Xia et al., 2020; Zhou et al., 2021; Hovy and Spruit, 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies discussing biases in NLP models. No verifiable resources are named.",
      "processing_time": 63.97284150123596,
      "citing_paper_id": "256631001",
      "cited_paper_id": 1083991
    },
    {
      "context_text": "…control over biases, which models may actively use as part of their heuristics in contrastive and downstream tasks (Bender et al., 2021; Davidson et al., 2019; Mitchell et al., 2019; Ferrer et al., 2020; Koolen and van Cranenburgh, 2017; Xia et al., 2020; Zhou et al., 2021; Hovy and Spruit, 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies discussing biases in NLP models. No verifiable resources are named.",
      "processing_time": 63.97284150123596,
      "citing_paper_id": "256631001",
      "cited_paper_id": 11405869
    },
    {
      "context_text": "…control over biases, which models may actively use as part of their heuristics in contrastive and downstream tasks (Bender et al., 2021; Davidson et al., 2019; Mitchell et al., 2019; Ferrer et al., 2020; Koolen and van Cranenburgh, 2017; Xia et al., 2020; Zhou et al., 2021; Hovy and Spruit, 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies discussing biases in NLP models. No verifiable resources are named.",
      "processing_time": 63.97284150123596,
      "citing_paper_id": "256631001",
      "cited_paper_id": 221005761
    },
    {
      "context_text": "…(Christian et al., 2021; Ireland and Mehl, 2014; Park et al., 2015; Schwartz et al., 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 64.44788146018982,
      "citing_paper_id": "256631001",
      "cited_paper_id": 3687643
    },
    {
      "context_text": ", 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on mood disorders and conditions such as schizophrenia. No verifiable resources are identified.",
      "processing_time": 64.29617810249329,
      "citing_paper_id": "256631001",
      "cited_paper_id": 3687643
    },
    {
      "context_text": ", 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on mood disorders and conditions such as schizophrenia. No verifiable resources are identified.",
      "processing_time": 64.29617810249329,
      "citing_paper_id": "256631001",
      "cited_paper_id": 18498622
    },
    {
      "context_text": "…2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola et al., 2022) or ASD (Boorse et al., 2019; Rouhizadeh et al., 2014; Song et al., 2021), and political affiliation…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conditions and affiliations. No verifiable resources are identified.",
      "processing_time": 63.60319375991821,
      "citing_paper_id": "256631001",
      "cited_paper_id": 5600230
    },
    {
      "context_text": "For optimization, we use Adam with weight decay (Kingma and Ba, 2017), initialized us-ing the following parameters: initial learning rate: 2e-5; 10k warm-up steps; weight decay rate: 0.01; β 1 =.9, β 2 =.999, ϵ =1e-6.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Adam optimization method and its parameters.",
      "processing_time": 62.824357986450195,
      "citing_paper_id": "256631001",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Conversely, conditioning text representations on author attributes (e.g., gender or personality) has been shown to enhance performance in several NLP tasks, ranging from sentiment analysis to stance detection (Bamman and Smith, 2015; Flek, 2020; Hovy, 2018, 2015; Lynn et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions conditioning text representations on author attributes but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.29297947883606,
      "citing_paper_id": "256631001",
      "cited_paper_id": 14021168
    },
    {
      "context_text": "Conversely, conditioning text representations on author attributes (e.g., gender or personality) has been shown to enhance performance in several NLP tasks, ranging from sentiment analysis to stance detection (Bamman and Smith, 2015; Flek, 2020; Hovy, 2018, 2015; Lynn et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions conditioning text representations on author attributes but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.29297947883606,
      "citing_paper_id": "256631001",
      "cited_paper_id": 198905491
    },
    {
      "context_text": "Conversely, conditioning text representations on author attributes (e.g., gender or personality) has been shown to enhance performance in several NLP tasks, ranging from sentiment analysis to stance detection (Bamman and Smith, 2015; Flek, 2020; Hovy, 2018, 2015; Lynn et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions conditioning text representations on author attributes but does not specify any datasets. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 64.29297947883606,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "…et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola et al., 2022) or ASD (Boorse et al., 2019; Rouhizadeh et al., 2014; Song et al., 2021), and political affiliation (Tatman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conditions and affiliations. No verifiable resources are identified.",
      "processing_time": 63.83346676826477,
      "citing_paper_id": "256631001",
      "cited_paper_id": 14922761
    },
    {
      "context_text": "…et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola et al., 2022) or ASD (Boorse et al., 2019; Rouhizadeh et al., 2014; Song et al., 2021), and political affiliation (Tatman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conditions and affiliations. No verifiable resources are identified.",
      "processing_time": 63.83346676826477,
      "citing_paper_id": "256631001",
      "cited_paper_id": 226305409
    },
    {
      "context_text": "…et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola et al., 2022) or ASD (Boorse et al., 2019; Rouhizadeh et al., 2014; Song et al., 2021), and political affiliation (Tatman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only conditions and affiliations. No verifiable resources are identified.",
      "processing_time": 63.83346676826477,
      "citing_paper_id": "256631001",
      "cited_paper_id": null
    },
    {
      "context_text": "…Ireland and Mehl, 2014; Park et al., 2015; Schwartz et al., 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 18.6458740234375,
      "citing_paper_id": "256631001",
      "cited_paper_id": 18498622
    },
    {
      "context_text": "…al., 2012; Liesen-feld et al., 2021), personality traits (Christian et al., 2021; Ireland and Mehl, 2014; Park et al., 2015; Schwartz et al., 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. No clear, verifiable resources are identified.",
      "processing_time": 64.05930280685425,
      "citing_paper_id": "256631001",
      "cited_paper_id": 33952526
    },
    {
      "context_text": "…between patterns of language use and demographics (Bamman et al., 2012; Liesen-feld et al., 2021), personality traits (Christian et al., 2021; Ireland and Mehl, 2014; Park et al., 2015; Schwartz et al., 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019;…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that explore various aspects of language use in relation to demographics, personality traits, and mood disorders.",
      "processing_time": 65.34114217758179,
      "citing_paper_id": "256631001",
      "cited_paper_id": 60960233
    },
    {
      "context_text": "…or no control over biases, which models may actively use as part of their heuristics in contrastive and downstream tasks (Bender et al., 2021; Davidson et al., 2019; Mitchell et al., 2019; Ferrer et al., 2020; Koolen and van Cranenburgh, 2017; Xia et al., 2020; Zhou et al., 2021; Hovy and…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing biases in models and datasets. No clear, verifiable dataset names are provided.",
      "processing_time": 65.02267551422119,
      "citing_paper_id": "256631001",
      "cited_paper_id": 168170119
    },
    {
      "context_text": "Building on insights from transfer learning (Ruder et al., 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al., 2020) on a variant of triplet loss (Schroff et al., 2015)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on fine-tuning a pretrained DistilBERT architecture using a variant of triplet loss.",
      "processing_time": 65.48496651649475,
      "citing_paper_id": "256631001",
      "cited_paper_id": 186206211
    },
    {
      "context_text": "Building on insights from transfer learning (Ruder et al., 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al., 2020) on a variant of triplet loss (Schroff et al., 2015)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on fine-tuning a pretrained DistilBERT architecture using a variant of triplet loss.",
      "processing_time": 65.48496651649475,
      "citing_paper_id": "256631001",
      "cited_paper_id": 232046315
    },
    {
      "context_text": "Building on insights from transfer learning (Ruder et al., 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al., 2020) on a variant of triplet loss (Schroff et al., 2015)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on fine-tuning a pretrained DistilBERT architecture using a variant of triplet loss.",
      "processing_time": 65.48496651649475,
      "citing_paper_id": "256631001",
      "cited_paper_id": 233296292
    },
    {
      "context_text": "performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to performance in language modeling and generation. No verifiable resources are identified.",
      "processing_time": 64.64246249198914,
      "citing_paper_id": "256631001",
      "cited_paper_id": 198179747
    },
    {
      "context_text": "performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to performance in language modeling and generation. No verifiable resources are identified.",
      "processing_time": 64.64246249198914,
      "citing_paper_id": "256631001",
      "cited_paper_id": 226306982
    },
    {
      "context_text": "performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to performance in language modeling and generation. No verifiable resources are identified.",
      "processing_time": 64.64246249198914,
      "citing_paper_id": "256631001",
      "cited_paper_id": 248693617
    },
    {
      "context_text": "Conditioning on author attributes and states can also improve performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to improving language modeling and generation through conditioning on author attributes and states.",
      "processing_time": 64.51915836334229,
      "citing_paper_id": "256631001",
      "cited_paper_id": 198179747
    },
    {
      "context_text": "Conditioning on author attributes and states can also improve performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to improving language modeling and generation through conditioning on author attributes and states.",
      "processing_time": 64.51915836334229,
      "citing_paper_id": "256631001",
      "cited_paper_id": 226306982
    },
    {
      "context_text": "Conditioning on author attributes and states can also improve performance in language modeling and generation (Harrison et al., 2019; Oba et al., 2019; Oraby et al., 2018; Soni et al., 2022; Welch et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to improving language modeling and generation through conditioning on author attributes and states.",
      "processing_time": 64.51915836334229,
      "citing_paper_id": "256631001",
      "cited_paper_id": 248693617
    },
    {
      "context_text": "Given the large potential for positive societal impact, we believe that the NLP community should promote interdisciplinary efforts aimed at collecting and safely sharing such resources (Chekroud et al., 2021; Dukart et al., 2021; Ewbank et al., 2020; Low et al., 2020).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general call for collecting and sharing resources. No specific resource names are provided.",
      "processing_time": 65.00137448310852,
      "citing_paper_id": "256631001",
      "cited_paper_id": 201277680
    },
    {
      "context_text": "Given the large potential for positive societal impact, we believe that the NLP community should promote interdisciplinary efforts aimed at collecting and safely sharing such resources (Chekroud et al., 2021; Dukart et al., 2021; Ewbank et al., 2020; Low et al., 2020).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general call for collecting and sharing resources. No specific resource names are provided.",
      "processing_time": 65.00137448310852,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "We fine-tune a pretrained DistilBERT model on triplet loss, a contrastive learning function first introduced in the context of face encoding (Schroff et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a method (triplet loss) but does not refer to any specific dataset. The cited paper is about face recognition, which is not directly related to personalized text generation.",
      "processing_time": 66.81069540977478,
      "citing_paper_id": "256631001",
      "cited_paper_id": 206592766
    },
    {
      "context_text": ", 2020) on a variant of triplet loss (Schroff et al., 2015) using Reddit submissions",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Reddit submissions' but does not specify a named dataset. It is a generic reference to data from Reddit, which does not meet the criteria for a specific, verifiable dataset.",
      "processing_time": 67.08197903633118,
      "citing_paper_id": "256631001",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "…et al., 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al., 2020) on a variant of triplet loss (Schroff et al., 2015) using Reddit submissions from more than 1.7m users.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit submissions"
      ],
      "dataset_descriptions": {
        "Reddit submissions": "Used to fine-tune a pretrained DistilBERT model on a variant of triplet loss, focusing on personalized text generation by leveraging user-generated content from over 1.7 million users."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Reddit submissions' but does not specify a named dataset. It describes a large-scale user-generated content source, which is domain-qualified but lacks a specific, identifiable name.",
      "processing_time": 75.9222481250763,
      "citing_paper_id": "256631001",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "Future iterations will require the implementation of thorough bias testing and, potentially, the introduction of optimization constraints at training that help counter their emergence (Shah et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the need for bias testing and potential optimization constraints.",
      "processing_time": 64.23729228973389,
      "citing_paper_id": "256631001",
      "cited_paper_id": 209461005
    },
    {
      "context_text": "…mood disorders (Eichstaedt et al., 2018; Tackman et al., 2019; Schwartz et al., 2014), conditions such as schizophrenia (Elvevåg et al., 2011; de Boer et al., 2020; Li et al., 2021; Mitchell et al., 2015; Parola et al., 2022) or ASD (Boorse et al., 2019; Rouhizadeh et al., 2014; Song et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on mood disorders, schizophrenia, and ASD. No verifiable resources are identified.",
      "processing_time": 66.00823974609375,
      "citing_paper_id": "256631001",
      "cited_paper_id": 211100350
    },
    {
      "context_text": "…assessment (see Chekroud et al. 2021; Zhang et al. 2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation (Flek, 2020; Hovy, 2018; Kanwal et al., 2021; Leung et al., 2020; Ma et al., 2011; Oba et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear, verifiable resources are identified.",
      "processing_time": 65.15318655967712,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "…assessment (see Chekroud et al. 2021; Zhang et al. 2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation (Flek, 2020; Hovy, 2018; Kanwal et al., 2021; Leung et al., 2020; Ma et al., 2011; Oba et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear, verifiable resources are identified.",
      "processing_time": 65.15318655967712,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220347196
    },
    {
      "context_text": "…assessment (see Chekroud et al. 2021; Zhang et al. 2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation (Flek, 2020; Hovy, 2018; Kanwal et al., 2021; Leung et al., 2020; Ma et al., 2011; Oba et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts and methods. No clear, verifiable resources are identified.",
      "processing_time": 65.15318655967712,
      "citing_paper_id": "256631001",
      "cited_paper_id": 232071812
    },
    {
      "context_text": "We transfer this approach to text to train a language-based author encoder – that is, a model that learns to produce compact representation of an individual based on her linguistic behavior, with downstream applications in language-based prediction of individual attributes and contextualized and personalized NLP.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general approach to training a language-based author encoder. No verifiable resources are named.",
      "processing_time": 29.691235542297363,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "However, most research in these domains has focused on developing predictive methods to infer individual attributes from text, or on investigating how conditioning text representations on such attributes can improve performance in downstream NLP tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research trends and methods. No verifiable resources are identified.",
      "processing_time": 64.97044253349304,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "Language-based user encodings learned through self-supervised methods could also have a significant impact on contextualized and personalized NLP (Flek, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to self-supervised methods and their impact on personalized NLP.",
      "processing_time": 65.29002475738525,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "We highlighted important applications in clinical and contextualized and personalized NLP.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications in personalized NLP. No clear identifiers or names of datasets are provided.",
      "processing_time": 65.78711605072021,
      "citing_paper_id": "256631001",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "Previous research has uncovered systematic associations between patterns of language use and demographics (Bamman et al., 2012; Liesenfeld et al., 2021), personality traits (Christian et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research findings about language use and demographics. No clear, verifiable resource names are present.",
      "processing_time": 66.16953110694885,
      "citing_paper_id": "256631001",
      "cited_paper_id": 232068881
    },
    {
      "context_text": "2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation (Flek, 2020; Hovy, 2018; Kanwal et al., 2021; Leung et al., 2020; Ma et al., 2011; Oba et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. There are no clear identifiers for datasets, and the context is focused on methodologies and research works.",
      "processing_time": 67.04465055465698,
      "citing_paper_id": "256631001",
      "cited_paper_id": 232071812
    },
    {
      "context_text": ", 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.61744785308838,
      "citing_paper_id": "256631001",
      "cited_paper_id": 233296292
    },
    {
      "context_text": ", 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 65.61744785308838,
      "citing_paper_id": "256631001",
      "cited_paper_id": 251299631
    },
    {
      "context_text": "…systematic associations between patterns of language use and demographics (Bamman et al., 2012; Liesen-feld et al., 2021), personality traits (Christian et al., 2021; Ireland and Mehl, 2014; Park et al., 2015; Schwartz et al., 2013; Yarkoni, 2010), mood disorders (Eichstaedt et al., 2018;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that explore various aspects of language use and personal attributes. No clear, verifiable datasets are named.",
      "processing_time": 66.76376748085022,
      "citing_paper_id": "256631001",
      "cited_paper_id": 234754396
    },
    {
      "context_text": "and psychological assessment (see Chekroud et al. 2021; Zhang et al. 2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation (Flek, 2020; Hovy, 2018; Kanwal et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No clear identifiers for datasets are present.",
      "processing_time": 65.12011218070984,
      "citing_paper_id": "256631001",
      "cited_paper_id": 248004235
    },
    {
      "context_text": "…and related behaviors - with potentially impactful applications in language-based clinical and psychological assessment (see Chekroud et al. 2021; Zhang et al. 2022) - or for author-based conditioning in the context of contextualized text classification, language modeling and language generation…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of NLP in mental illness detection and author-based conditioning. No clear, verifiable datasets are identified.",
      "processing_time": 67.0320954322815,
      "citing_paper_id": "256631001",
      "cited_paper_id": 248004235
    },
    {
      "context_text": "…on insights from transfer learning (Ruder et al., 2019) and contrastive representation learning (Gao et al., 2021; Rethmeier and Augenstein, 2021; Xie et al., 2022), we fine-tune a pretrained DistilBERT architecture (Sanh et al., 2020) on a variant of triplet loss (Schroff et al., 2015) using…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of transfer learning and contrastive representation learning techniques.",
      "processing_time": 66.31363916397095,
      "citing_paper_id": "256631001",
      "cited_paper_id": 251299631
    },
    {
      "context_text": "…approach enforces little or no control over biases, which models may actively use as part of their heuristics in contrastive and downstream tasks (Bender et al., 2021; Davidson et al., 2019; Mitchell et al., 2019; Ferrer et al., 2020; Koolen and van Cranenburgh, 2017; Xia et al., 2020; Zhou et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing biases in language models.",
      "processing_time": 64.81171035766602,
      "citing_paper_id": "256631001",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "For both psychological and clinical applications, complementing traditional methods with naturalistic text-based techniques could not only yield general performance advantages, but also help increase scalability and generalizability (Panch et al., 2020; Parola et al., 2022; Rybner et al., 2022), and reduce subjective biases (Park et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to methods and potential benefits of using naturalistic text-based techniques.",
      "processing_time": 66.13567113876343,
      "citing_paper_id": "256631001",
      "cited_paper_id": null
    },
    {
      "context_text": "…complementing traditional methods with naturalistic text-based techniques could not only yield general performance advantages, but also help increase scalability and generalizability (Panch et al., 2020; Parola et al., 2022; Rybner et al., 2022), and reduce subjective biases (Park et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and potential benefits of using naturalistic text-based techniques.",
      "processing_time": 65.75381803512573,
      "citing_paper_id": "256631001",
      "cited_paper_id": null
    },
    {
      "context_text": "It has been demonstrated that personality is vital for building a human-like dialogue system [15, 39] which can exhibit a consistent persona.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of personality in dialogue systems. No verifiable resources are identified.",
      "processing_time": 66.12885427474976,
      "citing_paper_id": "59316441",
      "cited_paper_id": 2284353
    },
    {
      "context_text": "It has been demonstrated that personality is vital for building a human-like dialogue system [15, 39] which can exhibit a consistent persona.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the importance of personality in dialogue systems. No verifiable resources are identified.",
      "processing_time": 66.12885427474976,
      "citing_paper_id": "59316441",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "Some personalized dialogues systems were built upon the basis of “ Big Five ”, such as Personage [24, 25] and the work of Gill et al. [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to personality traits and dialogue systems. No verifiable resources are identified.",
      "processing_time": 15.14760971069336,
      "citing_paper_id": "59316441",
      "cited_paper_id": 2817528
    },
    {
      "context_text": "Some personalized dialogues systems were built upon the basis of “ Big Five ”, such as Personage [24, 25] and the work of Gill et al. [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to personality traits and dialogue systems. No verifiable resources are identified.",
      "processing_time": 15.14760971069336,
      "citing_paper_id": "59316441",
      "cited_paper_id": 20595956
    },
    {
      "context_text": "Some personalized dialogues systems were built upon the basis of “Big Five”, such as Personage [24, 25] and the work of Gill et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to works that build personalized dialogue systems using the 'Big Five' personality traits.",
      "processing_time": 49.29145956039429,
      "citing_paper_id": "59316441",
      "cited_paper_id": 2817528
    },
    {
      "context_text": "embedding vector in neural models to capture the arXiv:1901.09672v1 [cs.CL] 28 Jan 2019 Y. Zheng et al. implicit speakingstyleofanindividualspeaker[20,23,31,46,48],or the style of a group of speakers [45]. Other approaches also attempt to endow dialogue models with personae which are described by natural language sentences [26, 47]. Recent studies on personalized neural conversational models can be br",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general approaches and methods for capturing speaking styles and personalizing dialogue models.",
      "processing_time": 15.138120651245117,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3005526
    },
    {
      "context_text": "afromeachspeaker. When there are no such data available, it is unlikely to train a reliable model. A possible attempt to deal with this issue is to train personalized models with the gender attribute [45]. This approach helps to alleviate the data sparsity issue since the dialogue data within a group of same gendered speakers can be shared. Notethatpersonalitytraitsintheseembedding-basedapproaches are",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a general approach to dealing with data sparsity in training personalized models using gender attributes.",
      "processing_time": 18.876708269119263,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3005526
    },
    {
      "context_text": "f each model took about a week on a Titan X GPU machine. 5.3 Baselines We chose several baselines: •A Seq2Seq model, which does not use any persona features. •ThreeGroupLinguisticBiasAware(GLBA)models[45],whichrespectively incorporate three individual personality traits, namely Gender, Age, and Location. We implemented several variants of our proposed model with different combinations of trait fusion",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the baselines used in the experiment, including a Seq2Seq model and GLBA models.",
      "processing_time": 20.32449722290039,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3005526
    },
    {
      "context_text": "Initial efforts collect dialogues from movie scripts [8, 44], with the annotations of Character Styles.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'dialogues from movie scripts' but does not specify a named dataset. The term 'Character Styles' is not a specific dataset name.",
      "processing_time": 19.604794025421143,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3101865
    },
    {
      "context_text": "Some early studies focused on modeling characters in movie dialogues [3, 8], in which the presented “Character Style” usually depends on the scenes and plots of each movie.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general studies on modeling characters in movie dialogues. No clear, verifiable resource names are provided.",
      "processing_time": 30.18080997467041,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3101865
    },
    {
      "context_text": "Initial efforts focus on modeling characters in movie [3, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not provide specific dataset names or details about the datasets used. It only mentions modeling characters in movies, which is too generic.",
      "processing_time": 14.105025053024292,
      "citing_paper_id": "59316441",
      "cited_paper_id": 3101865
    },
    {
      "context_text": "Building human-like conversational systems has been a long-standing goal in artificial intelligence, where one of the major challenges is to present a consistent personality, so that the system can gain the user’s confidence and trust [39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general challenge in building conversational systems.",
      "processing_time": 25.819815635681152,
      "citing_paper_id": "59316441",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "Personality settings such as age, gender, level of knowledge, and personal interests can be implicitly or explicitly expressed during the conversations [39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of personality settings in conversations.",
      "processing_time": 14.374506711959839,
      "citing_paper_id": "59316441",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "Specifically, the encoder and decoder are 2-layer GRUs with 512 hidden units for each layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model architecture details. No verifiable resources are identified.",
      "processing_time": 28.792381286621094,
      "citing_paper_id": "59316441",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "In this study, the encoder we use is a two-layer bi-directional RNN with gated recurrent units (GRU) [6], and the decoder is also a two-layer GRU.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the architecture of the encoder and decoder used in the study.",
      "processing_time": 16.031273365020752,
      "citing_paper_id": "59316441",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Further development of personalized dialogue generation models is inspired by the successful application of social media data [34, 35] and the sequence to sequence learning framework [36, 37, 40, 41, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to social media data and sequence-to-sequence learning frameworks. No verifiable resources are identified.",
      "processing_time": 17.548157930374146,
      "citing_paper_id": "59316441",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "Further development of personalized dialogue generation models is inspired by the successful application of social media data [34, 35] and the sequence to sequence learning framework [36, 37, 40, 41, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to social media data and sequence-to-sequence learning frameworks. No verifiable resources are identified.",
      "processing_time": 17.548157930374146,
      "citing_paper_id": "59316441",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In this paper, we employ the sequence to sequence learning framework [41, 43] and devise a trait fusion module to capture the persona of each speaker in the response generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 28.007314205169678,
      "citing_paper_id": "59316441",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "The backbone of our model is the sequence to sequence (Seq2Seq) learning framework [41, 43], which is commonly used in language generation tasks such as machine translation and dialogue generation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Seq2Seq learning framework) used in language generation tasks.",
      "processing_time": 27.49917769432068,
      "citing_paper_id": "59316441",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In addition, our dataset can also facilitate the modeling of dialectal variations [9] as well as syntactic and pragmatic variations with respected to Age, Gender, Location, or a mixture of these traits.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'our dataset' which is a generic placeholder.",
      "processing_time": 17.908926725387573,
      "citing_paper_id": "59316441",
      "cited_paper_id": 17677868
    },
    {
      "context_text": "Traditional models are proposed to build personalized dialogue systems by modeling the “Big Five” [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to 'Big Five' personality traits, which is a theoretical framework rather than a dataset.",
      "processing_time": 18.472713232040405,
      "citing_paper_id": "59316441",
      "cited_paper_id": 20595956
    },
    {
      "context_text": "Moreover, the dialogue data with “ Big Five ” annotation are extremely complex and expensive to collect.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'dialogue data with “Big Five” annotation' but does not provide a specific name or identifier for the dataset. It is described as complex and expensive to collect, but there is no clear, verifiable resource name.",
      "processing_time": 19.75120997428894,
      "citing_paper_id": "59316441",
      "cited_paper_id": 20595956
    },
    {
      "context_text": "Therefore, “ Big Five ” is not suitable for building large-scale personalized dialogue systems, particularly with data-driven neural models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a critique of the 'Big Five' personality traits for building personalized dialogue systems.",
      "processing_time": 17.097193956375122,
      "citing_paper_id": "59316441",
      "cited_paper_id": 20595956
    },
    {
      "context_text": "Traditional models are proposed to build personalized dialogue systems by modeling the “ Big Five ” [14].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to the 'Big Five' personality traits, which is a theoretical framework rather than a dataset.",
      "processing_time": 19.318825244903564,
      "citing_paper_id": "59316441",
      "cited_paper_id": 20595956
    },
    {
      "context_text": "[18] is constructed using limited templates and thus not suitable for dialogue generation tasks.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to templates which are not considered a dataset.",
      "processing_time": 14.547582626342773,
      "citing_paper_id": "59316441",
      "cited_paper_id": 29473470
    },
    {
      "context_text": "Other approaches also attempt to endow dialogue models with personae which are described by natural language sentences [26, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only approaches to endowing dialogue models with personae.",
      "processing_time": 16.465074062347412,
      "citing_paper_id": "59316441",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "However, PersonalDialog provides a very different settings because aWeibo post usually do not specify a particular audience, which provides us a chance to validate the findings of prior sociolinguistics studies on new units of analysis [42].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific, verifiable dataset. 'PersonalDialog' and 'aWeibo' are mentioned but not as datasets, and there is no clear indication of their nature or usage.",
      "processing_time": 32.40419888496399,
      "citing_paper_id": "59316441",
      "cited_paper_id": 60609045
    },
    {
      "context_text": "PersonalDialog presentsalarge amount of informal dialogue contents generated in computer mediated communications [16].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'a large amount of informal dialogue contents' but does not specify a named dataset. The cited paper title does not help in identifying a specific dataset.",
      "processing_time": 31.880047082901,
      "citing_paper_id": "59316441",
      "cited_paper_id": 65292628
    },
    {
      "context_text": "Our first method is inspired by templatebased relation extraction methods (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which are based on the observation that words in the vicinity of the subject x and object y in a large corpus often describe the relation r.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large corpus' but does not specify a named dataset. It refers to a general concept rather than a specific, verifiable resource.",
      "processing_time": 17.48629856109619,
      "citing_paper_id": "208513249",
      "cited_paper_id": 226541
    },
    {
      "context_text": "Our first method is inspired by templatebased relation extraction methods (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which are based on the observation that words in the vicinity of the subject x and object y in a large corpus often describe the relation r.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large corpus' but does not specify a named dataset. It refers to a general concept rather than a specific, verifiable resource.",
      "processing_time": 17.48629856109619,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7579604
    },
    {
      "context_text": "Our first method is inspired by template-based relation extraction methods (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), which are based on the observation that words in the vicinity of the subject x and object y in a large corpus often describe the relation r .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large corpus' but does not specify a named dataset. It refers to a general concept rather than a specific, verifiable resource.",
      "processing_time": 27.2176673412323,
      "citing_paper_id": "208513249",
      "cited_paper_id": 226541
    },
    {
      "context_text": "Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based meth-ods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 16.424174308776855,
      "citing_paper_id": "208513249",
      "cited_paper_id": 226541
    },
    {
      "context_text": "Specifically, as shown in Figure 1, these are mining-based methods inspired by previous relation extraction methods (Ravichandran and Hovy, 2002), and paraphrasing-based methods that take a seed prompt (either manually created or automatically mined), and paraphrase it into several other semantically similar expressions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing the methodologies used for relation extraction and paraphrasing.",
      "processing_time": 16.421725034713745,
      "citing_paper_id": "208513249",
      "cited_paper_id": 226541
    },
    {
      "context_text": "While many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple method of using back-translation (Sennrich et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for paraphrasing. There are no verifiable resources or datasets mentioned.",
      "processing_time": 16.38748288154602,
      "citing_paper_id": "208513249",
      "cited_paper_id": 1753223
    },
    {
      "context_text": "While many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt intoB candidates in another language, each of which is…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for paraphrasing. No verifiable resources are identified.",
      "processing_time": 27.453842878341675,
      "citing_paper_id": "208513249",
      "cited_paper_id": 1753223
    },
    {
      "context_text": "While many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple method of using back-translation (Sennrich et al., 2016; Mallinson et al., 2017) to first translate the initial prompt intoB candidates in another language, each of which is…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for paraphrasing. No verifiable resources are identified.",
      "processing_time": 27.453842878341675,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7647654
    },
    {
      "context_text": "Orthogonally, some previous works integrate external knowledge bases so that the language generation process is explicitly conditioned on symbolic knowledge (Ahn et al., 2016; Yang et al., 2017; Logan et al., 2019; Hayashi et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to integrating external knowledge bases in language generation.",
      "processing_time": 15.57197642326355,
      "citing_paper_id": "208513249",
      "cited_paper_id": 1899153
    },
    {
      "context_text": ", 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model investigations. No verifiable resources are identified.",
      "processing_time": 15.569517612457275,
      "citing_paper_id": "208513249",
      "cited_paper_id": 5545615
    },
    {
      "context_text": "…by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 18.44009232521057,
      "citing_paper_id": "208513249",
      "cited_paper_id": 5545615
    },
    {
      "context_text": "…by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 18.44009232521057,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7100502
    },
    {
      "context_text": "…by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 18.44009232521057,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "…by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 18.44009232521057,
      "citing_paper_id": "208513249",
      "cited_paper_id": 13017314
    },
    {
      "context_text": "…by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b; Smith et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide clear dataset names.",
      "processing_time": 18.44009232521057,
      "citing_paper_id": "208513249",
      "cited_paper_id": 14091946
    },
    {
      "context_text": "When optimizing ensemble parameters, we use Adam (Kingma and Ba, 2015) with default parameters and batch size of 32.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Adam, which is a method for stochastic optimization, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 15.318928718566895,
      "citing_paper_id": "208513249",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "7 When optimizing ensemble parameters, we use Adam (Kingma and Ba, 2015) with default parameters and batch size of 32.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the optimization method Adam.",
      "processing_time": 47.95123267173767,
      "citing_paper_id": "208513249",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "ternal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide additional context to identify datasets.",
      "processing_time": 28.91182255744934,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7100502
    },
    {
      "context_text": "ternal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide additional context to identify datasets.",
      "processing_time": 28.91182255744934,
      "citing_paper_id": "208513249",
      "cited_paper_id": 14091946
    },
    {
      "context_text": "Much work has focused on understanding the internal representations in neural NLP models (Belinkov and Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and research works.",
      "processing_time": 13.419707298278809,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7197724
    },
    {
      "context_text": "…Glass, 2019), either by using extrinsic probing tasks to examine whether certain linguistic properties can be predicted from those representations (Shi et al., 2016; Linzen et al., 2016; Belinkov et al., 2017), or by ablations to the models to investigate how behavior varies (Li et al., 2016b;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper title does not help in identifying a dataset.",
      "processing_time": 28.905776977539062,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7197724
    },
    {
      "context_text": "Inspired by the maximum mutual information objective used in Li et al. (2016a), we add the backward log probability logPLM(x|y, tr,i) of each\n9In theory, this algorithm can be applied to both masked LMs like BERT and traditional left-to-right LMs, since the masked probability can be computed using…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or objective function. There are no clear identifiers for datasets in the text.",
      "processing_time": 16.793179512023926,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Although many methods could be used for paraphrasing (Romano et al., 2006; Bhagat and Ravichandran, 2008), we follow the simple",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for paraphrasing. There are no clear identifiers for datasets in the given context.",
      "processing_time": 19.267915725708008,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7647654
    },
    {
      "context_text": "Prompts\nwhere the hidden vectors learned through a language modeling objective are then used in downstream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for language modeling and downstream language understanding systems.",
      "processing_time": 17.462827682495117,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7890036
    },
    {
      "context_text": "Prompts\nwhere the hidden vectors learned through a language modeling objective are then used in downstream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods used for language modeling and downstream language understanding systems.",
      "processing_time": 17.462827682495117,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "where the hidden vectors learned through a language modeling objective are then used in downstream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and models. The context focuses on the use of hidden vectors learned through a language modeling objective.",
      "processing_time": 30.358985424041748,
      "citing_paper_id": "208513249",
      "cited_paper_id": 7890036
    },
    {
      "context_text": "This is conceptually similar to query expansion techniques used in information retrieval that reformulate a given query to improve retrieval performance (Carpineto and Romano, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general technique in information retrieval. No verifiable resources are identified.",
      "processing_time": 27.14966058731079,
      "citing_paper_id": "208513249",
      "cited_paper_id": 10393627
    },
    {
      "context_text": "This is conceptually similar to query expansion techniques used in information retrieval that reformulate a given query to improve retrieval performance (Carpineto and Romano, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general technique in information retrieval. No verifiable resources are identified.",
      "processing_time": 27.14966058731079,
      "citing_paper_id": "208513249",
      "cited_paper_id": 56657817
    },
    {
      "context_text": "This is conceptually similar to query expansion techniques used in information retrieval that refor-mulate a given query to improve retrieval performance (Carpineto and Romano, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique used in information retrieval.",
      "processing_time": 15.56402039527893,
      "citing_paper_id": "208513249",
      "cited_paper_id": 10393627
    },
    {
      "context_text": "phrasing, we follow the simple method of using back-translation (Prabhumoye et al., 2018) to first translate the initial prompt into B candidates in another language, each of which is then back-translated into B candidates in the original language.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions back-translation but does not specify any dataset. The method is described, but no dataset name is provided.",
      "processing_time": 17.019506216049194,
      "citing_paper_id": "208513249",
      "cited_paper_id": 13959787
    },
    {
      "context_text": "Recent years have seen the primary role of language models (LMs) transition from generating or evaluating the fluency of natural text (Mikolov and Zweig, 2012; Merity et al., 2018; Melis et al., 2018; Gamon et al., 2005) to being a powerful tool for text understanding.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the transition in the role of language models. No verifiable resources are identified.",
      "processing_time": 15.28250503540039,
      "citing_paper_id": "208513249",
      "cited_paper_id": 33513311
    },
    {
      "context_text": "In previous work (McCann et al., 2018; Radford et al., 2019; Petroni et al., 2019), tr has been a single manually defined prompt based on the intuition of the experimenter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving manually defined prompts.",
      "processing_time": 26.3218834400177,
      "citing_paper_id": "208513249",
      "cited_paper_id": 49393754
    },
    {
      "context_text": "In previous work (McCann et al., 2018; Radford et al., 2019; Petroni et al., 2019), t r has been a single manually defined prompt based on the intuition of the experimenter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving manually defined prompts.",
      "processing_time": 13.55654525756836,
      "citing_paper_id": "208513249",
      "cited_paper_id": 49393754
    },
    {
      "context_text": "Interestingly, it is also becoming apparent that LMs1 themselves can be used as a tool for text understanding by formulating queries in natural language and either generating textual answers directly (McCann et al., 2018; Radford et al., 2019), or assessing multiple choices and picking the most likely one (Zweig and Burges, 2011; Rajani et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of language models for text understanding and question answering.",
      "processing_time": 30.930869102478027,
      "citing_paper_id": "208513249",
      "cited_paper_id": 49393754
    },
    {
      "context_text": "…1 themselves can be used as a tool for text understanding by formulating queries in natural language and either generating textual answers directly (McCann et al., 2018; Radford et al., 2019), or assessing multiple choices and picking the most likely one (Zweig and Burges, 2011; Rajani et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for text understanding and generation.",
      "processing_time": 26.995910167694092,
      "citing_paper_id": "208513249",
      "cited_paper_id": 49393754
    },
    {
      "context_text": "down-stream language understanding systems (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018; Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss language understanding systems. No verifiable datasets are named.",
      "processing_time": 15.912735939025879,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "1Some models we use in this paper, e.g. BERT (Devlin et al., 2019), are bi-directional, and do not directly define probability distribution over text, which is the underlying definition of an LM. Nonetheless, we call them LMs for simplicity.\nar X\niv :1\n91 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their properties. There are no verifiable resources or datasets mentioned.",
      "processing_time": 37.81535530090332,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": ", BERT (Devlin et al., 2019), are bi-directional, and do not directly define probability distribution over text, which is the underlying definition of an LM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT).",
      "processing_time": 26.985544204711914,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "1% on BERT-base (Devlin et al., 2019), with similar gains being obtained with BERT-large as well.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (BERT-base and BERT-large). The citation is used to reference model performance, not a dataset.",
      "processing_time": 28.671162605285645,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We first demonstrate that improved prompts significantly improve accuracy on this task, with the one-best prompt extracted by our method raising accuracy from 31.1% to 34.1% on BERT-base (Devlin et al., 2019), with similar gains being obtained with BERT-large as well.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. It is used as a model for evaluating the effectiveness of improved prompts.",
      "processing_time": 25.8418550491333,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Models As for the models to probe, in our main experiments we use the standard BERT-base and BERT-large models (Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT models but does not refer to any specific dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 18.71829056739807,
      "citing_paper_id": "208513249",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For contextualized representations in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a,b; Jawahar et al., 2019; Goldberg, 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a suite of NLP tasks. The context focuses on the analysis of syntactic and semantic properties using contextualized representations.",
      "processing_time": 16.984103441238403,
      "citing_paper_id": "208513249",
      "cited_paper_id": 58007068
    },
    {
      "context_text": "…a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a; Tenney et al., 2019b; Jawahar et al., 2019; Goldberg, 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing NLP tasks and linguistic knowledge. No verifiable resources are identified.",
      "processing_time": 16.30345392227173,
      "citing_paper_id": "208513249",
      "cited_paper_id": 58007068
    },
    {
      "context_text": "We use the round-trip English-German neural machine translation models pre-trained on WMT’19 (Ng et al., 2019) for back-translation, as English-German is one of the most highly resourced language pairs.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using pre-trained models on WMT'19, which is a benchmark rather than a dataset. No specific dataset is named or used directly.",
      "processing_time": 17.412503242492676,
      "citing_paper_id": "208513249",
      "cited_paper_id": 196621535
    },
    {
      "context_text": "…themselves can be used as a tool for text understanding by formulating queries in natural language and either generating textual answers directly (McCann et al., 2018; Radford et al., 2019), or assessing multiple choices and picking the most likely one (Zweig and Burges, 2011; Rajani et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for text understanding and generation.",
      "processing_time": 27.871358394622803,
      "citing_paper_id": "208513249",
      "cited_paper_id": 201698258
    },
    {
      "context_text": "…in particular, a broad suite of NLP tasks are used to analyze both syntactic and semantic properties, providing evidence that contextualized representations encode linguistic knowledge in different layers (Hewitt and Manning, 2019; Tenney et al., 2019a,b; Jawahar et al., 2019; Goldberg, 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to various NLP tasks and studies. No verifiable resources are identified.",
      "processing_time": 28.194324493408203,
      "citing_paper_id": "208513249",
      "cited_paper_id": 202538740
    },
    {
      "context_text": "In open information extraction systems (Banko et al., 2007), manually defined patterns are often leveraged to filter out noisy relational phrases.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for information extraction.",
      "processing_time": 14.981253862380981,
      "citing_paper_id": "208513249",
      "cited_paper_id": 207169186
    },
    {
      "context_text": ", 2019), answer common sense queries (Trinh and Le, 2018; Sap et al., 2019), or extract factual knowledge about relations between entities (Petroni et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research activities. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 17.004945039749146,
      "citing_paper_id": "208513249",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, LMs have been used to answer factoid questions (Radford et al., 2019), answer common sense queries (Trinh and Le, 2018; Sap et al., 2019), or extract factual knowledge about relations between entities (Petroni et al., 2019; Baldini Soares et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 19.20039963722229,
      "citing_paper_id": "208513249",
      "cited_paper_id": null
    },
    {
      "context_text": "Then, we encode each embedded caption vector Capi, i ∈ {1, 2, 3, 4, 5} using the Dense Caption Encoder, which is an LSTM network (Hochreiter and Schmidhuber 1997) shown below wherew t,i denotes a word vector in Capi at time t.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM network) and a general process (encoding caption vectors).",
      "processing_time": 29.152937650680542,
      "citing_paper_id": "232306930",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Then, we encode each embedded caption vector Capi, i ∈ {1, 2, 3, 4, 5} using the Dense Caption Encoder, which is an LSTM network (Hochreiter and Schmidhuber 1997) shown below wherewdpt,i denotes a word vector in Capi at time t.\nhdpt,i, c dp t,i = LSTM(w dp t,i, (h dp t−1,i, c dp t−1,i)) (1)\nWe…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM) and a general process for encoding caption vectors. No verifiable datasets are referenced.",
      "processing_time": 19.829025983810425,
      "citing_paper_id": "232306930",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "multi-style generative caption models along with various image encoding strategies (He et al. 2016; Xie et al. 2017) using several state-of-the-art image captioning models (Xu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 28.421374797821045,
      "citing_paper_id": "232306930",
      "cited_paper_id": 8485068
    },
    {
      "context_text": "multi-style generative caption models along with various image encoding strategies (He et al. 2016; Xie et al. 2017) using several state-of-the-art image captioning models (Xu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 28.421374797821045,
      "citing_paper_id": "232306930",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "C V\n] 2\n0 M\nar 2\n02 1\nmulti-style generative caption models along with various image encoding strategies (He et al. 2016; Xie et al. 2017) using several state-of-the-art image captioning models (Xu et al. 2015; Anderson et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 17.38627290725708,
      "citing_paper_id": "232306930",
      "cited_paper_id": 8485068
    },
    {
      "context_text": "C V\n] 2\n0 M\nar 2\n02 1\nmulti-style generative caption models along with various image encoding strategies (He et al. 2016; Xie et al. 2017) using several state-of-the-art image captioning models (Xu et al. 2015; Anderson et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 17.38627290725708,
      "citing_paper_id": "232306930",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "We compare the performance of our 3M model against their model using using BLEU (Papineni et al. 2002), ROUGE-L (Lin 2004), CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015), and SPICE (Anderson et al.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (BLEU, ROUGE-L, CIDEr, SPICE) but does not reference any specific datasets. The context is focused on comparing model performance using these metrics.",
      "processing_time": 18.942378759384155,
      "citing_paper_id": "232306930",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "As Table 3 shows, our 3M model significantly outperforms two other multi-style models, MsCap (Guo et al. 2019) and MemCap (Zhao, Wu, and Zhang 2020) on BLEU, CIDEr, Meteor, and ppl on the FlickrStyle10K dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlickrStyle10K"
      ],
      "dataset_descriptions": {
        "FlickrStyle10K": "Used to evaluate multi-style image captioning models, focusing on performance metrics such as BLEU, CIDEr, Meteor, and perplexity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the FlickrStyle10K dataset, which is used to evaluate the performance of different models on various metrics. The dataset is clearly identified and used for evaluation.",
      "processing_time": 36.45930290222168,
      "citing_paper_id": "232306930",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "As Table 3 shows, our 3M model significantly outperforms two other multi-style models, MsCap (Guo et al. 2019) and MemCap (Zhao, Wu, and Zhang 2020) on BLEU, CIDEr, Meteor, and ppl on the FlickrStyle10K dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlickrStyle10K"
      ],
      "dataset_descriptions": {
        "FlickrStyle10K": "Used to evaluate multi-style image captioning models, focusing on performance metrics such as BLEU, CIDEr, Meteor, and perplexity."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the FlickrStyle10K dataset, which is used to evaluate the performance of different models on various metrics. The dataset is clearly identified and used for evaluation.",
      "processing_time": 36.45930290222168,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "We report BLEU, Meteor (Banerjee and Lavie 2005), CIDEr, the style classification accuracy (cls) and the average perplexity (ppl) for comparison and results are showed in Table 3.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods.",
      "processing_time": 12.974377632141113,
      "citing_paper_id": "232306930",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We compare the performance of our 3M model against their model using us-\ning BLEU (Papineni et al. 2002), ROUGE-L (Lin 2004), CIDEr (Vedantam, Lawrence Zitnick, and Parikh 2015), and SPICE (Anderson et al. 2016).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (BLEU, ROUGE-L, CIDEr, SPICE) but does not reference any specific datasets. These metrics are excluded as per the instructions.",
      "processing_time": 18.06700825691223,
      "citing_paper_id": "232306930",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "BLEU, ROUGE-L, CIDEr, and SPICE are reported in Table 2 for evaluating the relevance between image and generations. we also report the number of unique words used across all generated captions per model in Table 2 to show the expressiveness of each generative model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The context focuses on reporting scores and expressiveness of generative models.",
      "processing_time": 27.054279804229736,
      "citing_paper_id": "232306930",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "With FlickrStyle10K, researchers have built singlestyle captioners (Gan et al. 2017; Chen et al. 2018) where they make use of both factual captions and stylized captions for training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlickrStyle10K"
      ],
      "dataset_descriptions": {
        "FlickrStyle10K": "Used to train single-style captioners, leveraging both factual and stylized captions to generate personalized text descriptions of images."
      },
      "confidence_score": 1.0,
      "reasoning": "FlickrStyle10K is mentioned as a dataset used for training single-style captioners, which aligns with the topic of personalized text generation.",
      "processing_time": 22.95340323448181,
      "citing_paper_id": "232306930",
      "cited_paper_id": 49667950
    },
    {
      "context_text": "We compare against the model introduced previously by Shuster et al. (Shuster et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model. The context is about comparing against a previously introduced model.",
      "processing_time": 26.443891048431396,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "In total, we train 30 epochs when using the PERSONALITY-CAPTIONS dataset (Shuster et al. 2019) with a batch size of 128 and evaluate the model every 3000 iterations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONALITY-CAPTIONS"
      ],
      "dataset_descriptions": {
        "PERSONALITY-CAPTIONS": "Used to train and evaluate a model for generating personalized captions, focusing on engaging image captioning with personality traits."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the PERSONALITY-CAPTIONS dataset for training and evaluating a model, which is directly relevant to personalized text generation.",
      "processing_time": 22.947906970977783,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "2017) while captions in PERSONALITY-CAPTIONS are created to be engaging and have a conversational style (Shuster et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONALITY-CAPTIONS"
      ],
      "dataset_descriptions": {
        "PERSONALITY-CAPTIONS": "Used to generate engaging and conversational image captions, focusing on creating a more interactive and personalized user experience in image captioning systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PERSONALITY-CAPTIONS', which is a specific dataset used for generating engaging and conversational image captions.",
      "processing_time": 35.42755913734436,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "We also listed original result of (Shuster et al. 2019) which is under supervised and reinforcement learning for reference.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper for comparison. The title 'Engaging Image Captioning via Personality' suggests the paper might involve datasets, but no specific names are provided in the citation context.",
      "processing_time": 20.617671728134155,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Note: Results of (Shuster et al. 2019) under supervised learning are from re-training due to performance on supervised method not reported in (Shuster et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to re-training a model. No verifiable resources are identified.",
      "processing_time": 27.59617590904236,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Shuster et al. built a multi-style module by converting each personality to a multi-dimensional vector.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for converting personalities into vectors.",
      "processing_time": 13.450433731079102,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Since we use a subset of the original PERSONALITYCAPTIONS dataset, we retrain the method outlined by Shuster et al. using similar settings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONALITYCAPTIONS"
      ],
      "dataset_descriptions": {
        "PERSONALITYCAPTIONS": "Used to train a method for generating engaging image captions with personality, focusing on improving the quality and relevance of captions through retraining with a subset of the original dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of a subset of the PERSONALITYCAPTIONS dataset, which is a specific, verifiable dataset used for training a method for engaging image captioning.",
      "processing_time": 38.54932999610901,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "In their work, Shuster et al. built an image caption retrieval model and also explored the\nar X\niv :2\n10 3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and a research direction. No verifiable resources are identified.",
      "processing_time": 13.88955807685852,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "In general, stylish captioning systems are divided into two categories based on how they are trained: single style (Gan et al. 2017; Zhang et al. 2018) and multi-style (Shuster et al. 2019; Guo et al. 2019; Zhao, Wu, and Zhang 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of captioning systems. No verifiable resources are identified.",
      "processing_time": 14.174290418624878,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "In general, stylish captioning systems are divided into two categories based on how they are trained: single style (Gan et al. 2017; Zhang et al. 2018) and multi-style (Shuster et al. 2019; Guo et al. 2019; Zhao, Wu, and Zhang 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of captioning systems. No verifiable resources are identified.",
      "processing_time": 14.174290418624878,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "We extend the best performing supervised model presented in Shuster et al.’s work to build a multi-style model which incorporates multi-modality image features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to extending a model from another work.",
      "processing_time": 13.449457168579102,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "The ground truth captions in PERSONALITY-CAPTIONS (Shuster et al. 2019) are created to be engaging and have a human-like style.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONALITY-CAPTIONS"
      ],
      "dataset_descriptions": {
        "PERSONALITY-CAPTIONS": "Used to generate engaging and human-like style captions, focusing on improving the quality and personality of image captions through a dataset of ground truth captions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, PERSONALITY-CAPTIONS, which is used to create engaging and human-like style captions.",
      "processing_time": 23.252866983413696,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Captions in FlickrStyle10K are created to have either a Humorous or Romantic linguistic style (Gan et al. 2017) while captions in PERSONALITY-CAPTIONS are created to be engaging and have a conversational style (Shuster et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlickrStyle10K",
        "PERSONALITY-CAPTIONS"
      ],
      "dataset_descriptions": {
        "FlickrStyle10K": "Used to generate captions with Humorous or Romantic styles, focusing on linguistic variation in image captioning.",
        "PERSONALITY-CAPTIONS": "Used to generate engaging and conversational captions, focusing on creating more interactive and human-like image descriptions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FlickrStyle10K and PERSONALITY-CAPTIONS, which are used to create captions with distinct linguistic styles.",
      "processing_time": 29.708376169204712,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Shuster et al. released the PERSONALITY-CAPTIONS containing 215 personalities in 2019 for building engaging caption generations models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONALITY-CAPTIONS"
      ],
      "dataset_descriptions": {
        "PERSONALITY-CAPTIONS": "Used to build engaging caption generation models, specifically focusing on generating captions with distinct personalities. The dataset contains 215 personalities, enhancing the diversity and engagement of generated captions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, PERSONALITY-CAPTIONS, which is used for building engaging caption generation models. The dataset is clearly identified and its purpose is described.",
      "processing_time": 25.441352128982544,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "2019) under supervised learning are from re-training due to performance on supervised method not reported in (Shuster et al. 2019) and some data of original dataset not available.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide a specific, verifiable dataset name. It mentions 'original dataset' but does not specify it.",
      "processing_time": 15.153978109359741,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Specifically, we extract mean-pooled image features and spatial features from the ResNeXt network (Shuster et al. 2019) and 5 dense captions from each image with a dense caption network (Johnson, Karpathy, and Fei-Fei 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on extracting features and captions using neural networks.",
      "processing_time": 16.249148845672607,
      "citing_paper_id": "232306930",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "They applied a supervised learning model plus reinforcement learning training strategy (Rennie et al. 2017) using CIDEr score (Vedantam, Lawrence Zitnick, and Parikh 2015) as reward.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and a metric. The context focuses on the training strategy and reward mechanism.",
      "processing_time": 16.653030157089233,
      "citing_paper_id": "232306930",
      "cited_paper_id": 206594923
    },
    {
      "context_text": "Since only 7000 images are publicly available, we evaluate using a similar process to the one outlined in (Guo et al. 2019; Zhao, Wu, and Zhang 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a reference to a similar evaluation process. No clear, verifiable datasets are identified.",
      "processing_time": 17.32191824913025,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "Following (Zhao, Wu, and Zhang 2020), on FlickrStyle10K,\nwe trained a logistic regression classifier for style classification and a pretrained language model using SRILM toolkit (Stolcke 2002) to measure perplexity.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlickrStyle10K"
      ],
      "dataset_descriptions": {
        "FlickrStyle10K": "Used to train a logistic regression classifier for style classification, focusing on measuring perplexity with a pretrained language model."
      },
      "confidence_score": 1.0,
      "reasoning": "FlickrStyle10K is mentioned as a dataset used for training a logistic regression classifier for style classification. The context clearly indicates its use in the research.",
      "processing_time": 23.890926837921143,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "Later researchers explored training multi-style networks (Guo et al. 2019; Zhao, Wu, and Zhang 2020) that can generate multiple types of stylish outputs using a single model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the development of multi-style networks for generating multiple types of stylish outputs.",
      "processing_time": 19.12020516395569,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "• MemCap (2020), a multi-style model trained with paired factual sentences and unpaired stylized captions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MemCap (2020)', which is a model, not a dataset. No specific dataset is mentioned.",
      "processing_time": 18.00707483291626,
      "citing_paper_id": "232306930",
      "cited_paper_id": 213205959
    },
    {
      "context_text": "We experiment with two popular backbone architectures when designing neural speakers trained on ArtEmis: the Show-Attend-Tell (SAT) approach [62], which combines an image encoder with a word/image attentive LSTM; and the recent line of work of top-down, bottom-up meshed-memory transformers ( M 2 )…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ArtEmis"
      ],
      "dataset_descriptions": {
        "ArtEmis": "Used to train neural speakers with two backbone architectures: Show-Attend-Tell and meshed-memory transformers, focusing on generating personalized text based on images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ArtEmis' as a dataset used for training neural speakers. No other datasets are mentioned that meet the criteria.",
      "processing_time": 23.76262879371643,
      "citing_paper_id": "231639297",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "This is unlike most natural images that are commonly labeled through purely objective content-based labeling mechanisms based on the objects or actions they include [14, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to general labeling mechanisms for natural images, which are not considered specific datasets.",
      "processing_time": 18.610361576080322,
      "citing_paper_id": "231639297",
      "cited_paper_id": 1710722
    },
    {
      "context_text": "The seminal works of [58, 29] opened this path by capitalizing on advancements done in deep recurrent networks (LSTMs [27]), along with other classic ideas like training with Teacher Forcing [60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers are also about models and methods, not datasets.",
      "processing_time": 18.86589765548706,
      "citing_paper_id": "231639297",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "The seminal works of [58, 29] opened this path by capitalizing on advancements done in deep recurrent networks (LSTMs [27]), along with other classic ideas like training with Teacher Forcing [60].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers are also about models and methods, not datasets.",
      "processing_time": 18.86589765548706,
      "citing_paper_id": "231639297",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "Our approach is supported by signiﬁcant research that argues for the central role of language in capturing and even helping to form emotions [37, 6], including the Theory of Constructed Emotions [6, 7, 5, 8] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical frameworks and research supporting the role of language in emotion. No verifiable resources are identified.",
      "processing_time": 19.523601055145264,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2032354
    },
    {
      "context_text": "Our approach is supported by signiﬁcant research that argues for the central role of language in capturing and even helping to form emotions [37, 6], including the Theory of Constructed Emotions [6, 7, 5, 8] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical frameworks and research supporting the role of language in emotion. No verifiable resources are identified.",
      "processing_time": 19.523601055145264,
      "citing_paper_id": "231639297",
      "cited_paper_id": 4843496
    },
    {
      "context_text": "Our approach is supported by signiﬁcant research that argues for the central role of language in capturing and even helping to form emotions [37, 6], including the Theory of Constructed Emotions [6, 7, 5, 8] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical frameworks and research supporting the role of language in emotion. No verifiable resources are identified.",
      "processing_time": 19.523601055145264,
      "citing_paper_id": "231639297",
      "cited_paper_id": 7750265
    },
    {
      "context_text": "Our approach is supported by signiﬁcant research that argues for the central role of language in capturing and even helping to form emotions [37, 6], including the Theory of Constructed Emotions [6, 7, 5, 8] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical frameworks and research supporting the role of language in emotion. No verifiable resources are identified.",
      "processing_time": 19.523601055145264,
      "citing_paper_id": "231639297",
      "cited_paper_id": 11317082
    },
    {
      "context_text": "This is unlike most natural images that are commonly labeled through purely objective content-based labeling mechanisms capturing the objects or actions they include [14, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to 'natural images' in a general sense without naming any particular dataset.",
      "processing_time": 25.73312258720398,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "For instance COCO-captions [14] concern descriptions of common objects in natural images, the data of Monroe et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to generate captions for common objects in natural images, focusing on descriptive text generation and evaluation methodologies."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO-captions' which is a specific dataset used for generating captions for images. The dataset is clearly identified and relevant to the topic of personalized text generation.",
      "processing_time": 35.8165397644043,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "We stress that this new dataset was collected following the AMT protocol used to build COCO-captions, i.e., asking only for objective (not affective) descriptions of the main objects, colors etc. present in an artwork.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Referenced for its data collection protocol, which guided the creation of a new dataset focusing on objective descriptions of artworks."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'COCO-captions' as a reference for the data collection protocol, but does not indicate that the COCO-captions dataset itself is used in the research.",
      "processing_time": 36.614558696746826,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "5% of ArtEmis to the neutral sentiment, while for COCO-captions it assigns 77 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to evaluate caption generation models, focusing on the distribution of sentiment in generated captions compared to neutral sentiment in ArtEmis."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'ArtEmis' and 'COCO-captions', both of which appear to be datasets. However, 'ArtEmis' is not a well-known dataset and lacks clear provenance. 'COCO-captions' is a known dataset, and the cited paper title confirms it.",
      "processing_time": 42.425331115722656,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "First, we observe that on metrics that measure the linguistic similarity to the held-out utterances (BLEU, METEOR, etc.) the speakers fare noticeably worse as compared to how the same architectures fare (modulo secondary-order details) when trained and tested with objective datasets like COCO-captions; e.g., BLEU-1 with SOTA [15] is 82.0.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to train and test models on linguistic similarity metrics, comparing performance against held-out utterances in personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO-captions' as a dataset used for training and testing models, which is relevant to personalized text generation.",
      "processing_time": 35.67779064178467,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "Histograms comparing ArtEmis to COCO-captions [14] along the axes of (a) Concreteness, (b) Subjectivity, and (c) Sentiment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to compare ArtEmis along dimensions of concreteness, subjectivity, and sentiment, providing a benchmark for evaluating generated captions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO-captions' which is a specific dataset used for comparison with ArtEmis along dimensions of concreteness, subjectivity, and sentiment.",
      "processing_time": 37.248074531555176,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "For instance COCO-captions [14] concern descriptions of common objects in natural images, the data of Monroe et al. [43] include discriminative references for 2D monochromatic colors, Achlioptas et al. [1, 2] collects discriminative utterances for 3D objects, etc.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions",
        "data of Monroe et al.",
        "Achlioptas et al."
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to generate descriptions of common objects in natural images, focusing on caption accuracy and diversity in the context of image understanding.",
        "data of Monroe et al.": "Used to create discriminative references for 2D monochromatic colors, focusing on color differentiation and linguistic expression in visual contexts.",
        "Achlioptas et al.": "Used to collect discriminative utterances for 3D objects, focusing on the linguistic representation of 3D shapes and their attributes."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions specific datasets with clear identifiers and their usage in generating descriptions or discriminative references for various types of visual data.",
      "processing_time": 51.51511192321777,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "There is correspondingly also a large volume on deep-net based captioning approaches [38, 40, 56, 66, 43, 65, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep-net based captioning approaches. No verifiable resources are identified.",
      "processing_time": 28.530255794525146,
      "citing_paper_id": "231639297",
      "cited_paper_id": 2808607
    },
    {
      "context_text": "There is correspondingly also a large volume on deep-net based captioning approaches [38, 40, 56, 66, 43, 65, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep-net based captioning approaches. No verifiable resources are identified.",
      "processing_time": 28.530255794525146,
      "citing_paper_id": "231639297",
      "cited_paper_id": 8745888
    },
    {
      "context_text": "There is correspondingly also a large volume on deep-net based captioning approaches [38, 40, 56, 66, 43, 65, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep-net based captioning approaches. No verifiable resources are identified.",
      "processing_time": 28.530255794525146,
      "citing_paper_id": "231639297",
      "cited_paper_id": 269183775
    },
    {
      "context_text": "There is correspondingly also a large volume on deep-net based captioning approaches [39, 41, 57, 67, 44, 66, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers on deep-net based captioning approaches.",
      "processing_time": 27.521103620529175,
      "citing_paper_id": "231639297",
      "cited_paper_id": 4406645
    },
    {
      "context_text": "To measure the extent to which our generations are linguistically similar to held-out ground-truth human captions, we use various popular machine-based metrics: e.g., BLEU 1-4 [46], ROUGE-L [35], METEOR [19].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several metrics (BLEU, ROUGE-L, METEOR) but does not refer to any specific datasets. These metrics are used to evaluate the quality of generated text against human captions.",
      "processing_time": 20.27038049697876,
      "citing_paper_id": "231639297",
      "cited_paper_id": 5923323
    },
    {
      "context_text": "There is a lot of work and corresponding captioning datasets [64, 31, 54, 34, 40, 47] that focus on different aspects of human cognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 30.17314600944519,
      "citing_paper_id": "231639297",
      "cited_paper_id": 6308361
    },
    {
      "context_text": "There is a lot of work and corresponding captioning datasets [64, 31, 54, 34, 40, 47] that focus on different aspects of human cognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 30.17314600944519,
      "citing_paper_id": "231639297",
      "cited_paper_id": 8745888
    },
    {
      "context_text": "There is a lot of work and corresponding captioning datasets [64, 31, 54, 34, 40, 47] that focus on different aspects of human cognition.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation span mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 30.17314600944519,
      "citing_paper_id": "231639297",
      "cited_paper_id": 51876975
    },
    {
      "context_text": "Last, the work of [59] introduces a large scale dataset of artistic imagery with multiple attribute annotations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BAM! The Behance Artistic Media Dataset"
      ],
      "dataset_descriptions": {
        "BAM! The Behance Artistic Media Dataset": "Used to introduce a large-scale dataset of artistic imagery with multiple attribute annotations, aimed at recognition beyond photography, enhancing personalized text generation through diverse visual inputs."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a 'large scale dataset of artistic imagery with multiple attribute annotations', which fits the criteria for a dataset. The title 'BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography' confirms it is a dataset.",
      "processing_time": 41.88428616523743,
      "citing_paper_id": "231639297",
      "cited_paper_id": 8140521
    },
    {
      "context_text": "For the ANP baseline, we use the Karpathy splits [29] to train the same (SAT) backbone network we used elsewhere.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using 'Karpathy splits' but does not specify a dataset name. It refers to a method or split strategy rather than a specific dataset.",
      "processing_time": 30.440253973007202,
      "citing_paper_id": "231639297",
      "cited_paper_id": 8517067
    },
    {
      "context_text": "We highlight that CIDEr-D [57] which requires a generation to be semantically close to all human-annotations of an artwork, is not a metric well-suited for ArtEmis, due to the large diversity and inherent subjectivity of our dataset.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'our dataset' without providing a specific name, and focuses on the unsuitability of the CIDEr-D metric rather than describing a dataset. No specific, verifiable dataset is identified.",
      "processing_time": 32.79067349433899,
      "citing_paper_id": "231639297",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "We highlight that CIDEr-D [58] which requires a generation to be semantically close to all human-annotations of an artwork, is not a well suited metric for ArtEmis, due to the large diversity and inherent subjectivity of our dataset (see more on this on Supp.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'our dataset' without providing a specific name, and focuses on the limitations of the CIDEr-D metric rather than describing a dataset. No specific, verifiable dataset is mentioned.",
      "processing_time": 32.66673159599304,
      "citing_paper_id": "231639297",
      "cited_paper_id": 9026666
    },
    {
      "context_text": ", they do not involve direct human judgement, which is regarded as the golden standard for quality assessment [17, 32] of synthetic captions.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of automatic metrics for image captioning. No verifiable resources are identified.",
      "processing_time": 29.892773151397705,
      "citing_paper_id": "231639297",
      "cited_paper_id": 10281325
    },
    {
      "context_text": "Even a single person can have a range of emotional reactions to a given stimulus [41, 50, 10, 51] and, as shown in Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific datasets, only a general reference to emotional reactions. The cited paper title suggests a potential dataset but it is not explicitly mentioned in the context.",
      "processing_time": 31.814626455307007,
      "citing_paper_id": "231639297",
      "cited_paper_id": 10747323
    },
    {
      "context_text": "We note that while awe can be associated with a negative state, following previous works ([41, 48]), we treat awe as a positive emotion in our analyses.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to treating awe as a positive emotion. No verifiable resources are identified.",
      "processing_time": 30.73649287223816,
      "citing_paper_id": "231639297",
      "cited_paper_id": 10747323
    },
    {
      "context_text": "We use a rule-based sentiment analyzer (VADER [28]) to demonstrate this point.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions VADER but does not refer to it as a dataset. It is described as a rule-based sentiment analyzer, which is a method or tool, not a dataset.",
      "processing_time": 32.15794920921326,
      "citing_paper_id": "231639297",
      "cited_paper_id": 12233345
    },
    {
      "context_text": "Even a single person can have a range of emotional reactions to a given stimulus [42, 51, 10, 52] and, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to emotional reactions. No clear, verifiable resources are identified.",
      "processing_time": 30.151220560073853,
      "citing_paper_id": "231639297",
      "cited_paper_id": 15289543
    },
    {
      "context_text": "There is debate within the neuroscience community on whether human emotions are innate, generated by patterns of neural activity, or learned [53, 4, 8, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a debate within the neuroscience community about the nature of human emotions.",
      "processing_time": 31.35440158843994,
      "citing_paper_id": "231639297",
      "cited_paper_id": 32814094
    },
    {
      "context_text": "To address this problem we ﬁne-tune an ImageNet-based [18] pretrained ResNet-32 encoder [26] by minimizing the KL-divergence between its output and the empirical user distributions of ArtEmis.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ArtEmis"
      ],
      "dataset_descriptions": {
        "ArtEmis": "Used to fine-tune a pretrained ResNet-32 encoder by minimizing the KL-divergence between the encoder's output and the empirical user distributions, focusing on personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'ArtEmis' as a source of empirical user distributions, which suggests it is a dataset. However, 'ImageNet' is mentioned only in the context of pretraining the ResNet-32 encoder, so it is excluded.",
      "processing_time": 43.439496755599976,
      "citing_paper_id": "231639297",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "[1, 2] collects discriminative utterances for 3D objects, etc.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide enough information to identify a specific, verifiable dataset. The mention of 'discriminative utterances for 3D objects' is too generic and lacks a clear identifier.",
      "processing_time": 33.22831153869629,
      "citing_paper_id": "231639297",
      "cited_paper_id": 147704191
    },
    {
      "context_text": "The four negative emotions are considered universal and basic (as proposed by Ekman in [22]) and have been shown to capture well the discrete emotions of the International Affective Picture System [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "International Affective Picture System"
      ],
      "dataset_descriptions": {
        "International Affective Picture System": "Used to capture discrete emotions, specifically the four negative emotions considered universal and basic, to inform the development of personalized text generation systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the International Affective Picture System (IAPS) as a resource used to capture discrete emotions, which is relevant to the study of personalized text generation.",
      "processing_time": 37.35495209693909,
      "citing_paper_id": "231639297",
      "cited_paper_id": 150728988
    },
    {
      "context_text": "Our approach is supported by significant research that argues for the central role of language in capturing and even helping to form emotions [37, 6], including the Theory of Constructed Emotions [6, 7, 5, 8] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical works and concepts. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 31.357131719589233,
      "citing_paper_id": "231639297",
      "cited_paper_id": 189277712
    },
    {
      "context_text": "Our approach is informed by significant research that argues for the central role of language in capturing and even helping to form emotions [36], including the Theory of Constructed Emotions [6, 7] by Lisa Feldman Barrett.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and concepts. No verifiable resources are identified.",
      "processing_time": 29.851848125457764,
      "citing_paper_id": "231639297",
      "cited_paper_id": 189277712
    },
    {
      "context_text": "[5] relies on separately computed object-bounding-box detections (computed using Faster R-CNN [25]).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Faster R-CNN) used for object detection.",
      "processing_time": 30.407771110534668,
      "citing_paper_id": "231639297",
      "cited_paper_id": 206770307
    },
    {
      "context_text": "We experiment with two popular backbone architectures when designing neural speakers trained on ArtEmis: the Show-Attend-Tell (SAT) approach [62], which combines an image encoder with a word/image attentive LSTM; and the recent line of work of top-down, bottom-up meshed-memory transformers ( M 2 ) [15], which replaces the recurrent units with transformer units and capitalizes on separately computed object-bounding-box detections (computed using Faster R-CNN [25]).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ArtEmis"
      ],
      "dataset_descriptions": {
        "ArtEmis": "Used to train neural speakers with two different architectures, focusing on generating personalized text based on images and attention mechanisms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ArtEmis' as a dataset used for training neural speakers. No other datasets are mentioned that meet the criteria.",
      "processing_time": 35.91967701911926,
      "citing_paper_id": "231639297",
      "cited_paper_id": 206770307
    },
    {
      "context_text": "…combines an image encoder with a word/image attentive LSTM; and the recent line of work of top-down, bottom-up meshed-memory transformers ( M 2 ) [15], which replaces the recurrent units with transformer units and capitalizes on separately computed object-bounding-box detections (computed using…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the architecture and components of the models used.",
      "processing_time": 31.966009855270386,
      "citing_paper_id": "231639297",
      "cited_paper_id": 219635470
    },
    {
      "context_text": "…the linguistic similarity to the held-out utterances (BLEU, METEOR, etc.) the speakers fare noticeably worse as compared to how the same architectures fare (modulo secondary-order details) when trained and tested with objective datasets like COCO-captions; e.g., BLEU-1 with SOTA [15] is 82.0.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO-captions"
      ],
      "dataset_descriptions": {
        "COCO-captions": "Used to train and test image captioning models, specifically comparing performance metrics like BLEU-1 against state-of-the-art models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO-captions' as a dataset used for training and testing image captioning models. It is a well-known dataset in the field of computer vision and natural language processing.",
      "processing_time": 40.96756315231323,
      "citing_paper_id": "231639297",
      "cited_paper_id": 219635470
    },
    {
      "context_text": "In what follows we analyze the key characteristics of ArtEmis, while pointing the interested reader to the Supplemental Material [3] for further details.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'ArtEmis' but does not provide enough information to determine if it is a dataset, method, or other resource. No specific usage or methodology is described.",
      "processing_time": 33.3815815448761,
      "citing_paper_id": "231639297",
      "cited_paper_id": null
    },
    {
      "context_text": "For automatic evaluation, we report RougeLSum [20] as used for the original ASQA data, as well as the score from each fine-grained reward model (Rφ1 , Rφ2 , and Rφ3 ).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RougeLSum, which is a metric, and fine-grained reward models, which are not datasets. No specific datasets are mentioned.",
      "processing_time": 32.929389238357544,
      "citing_paper_id": "259064099",
      "cited_paper_id": 964287
    },
    {
      "context_text": "For automatic evaluation, we report RougeLSum [20] as used for the original ASQA data, as well as the score from each fine-grained reward model ( R ϕ 1 , R ϕ 2 , and R ϕ 3 ).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ASQA data"
      ],
      "dataset_descriptions": {
        "ASQA data": "Used for automatic evaluation, specifically reporting RougeLSum scores and fine-grained reward model scores in the context of summarization."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'ASQA data' which appears to be a specific dataset used for evaluation. However, there is no clear indication of how it is used beyond reporting RougeLSum scores.",
      "processing_time": 42.87063407897949,
      "citing_paper_id": "259064099",
      "cited_paper_id": 964287
    },
    {
      "context_text": "ChatGPT achieves a RougeLSum score of 40.92 on the test set, which is much lower than our models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (RougeLSum score) and a comparison to other models.",
      "processing_time": 35.72871446609497,
      "citing_paper_id": "259064099",
      "cited_paper_id": 964287
    },
    {
      "context_text": "This is motivated by early work [19] that shows the better reliability of pairwise comparison than error classification when assessing a full generation sequence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to evaluating dialogue systems.",
      "processing_time": 35.157177686691284,
      "citing_paper_id": "259064099",
      "cited_paper_id": 202538657
    },
    {
      "context_text": "Motivated by [19], R ϕ 3 predicts a single scalar reward and is trained with a pairwise comparison loss [29]: where R ϕ 3 ( x, y ) is the scalar output of the reward model for input x and output y ; ¯ y p and ¯ y l are sampled from the same input x , and ¯ y p has less missed information compared…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training a reward model. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 36.74084806442261,
      "citing_paper_id": "259064099",
      "cited_paper_id": 202538657
    },
    {
      "context_text": "Methods for using a reward model to guide LM generation towards desired behaviors at inference time [21, 7] can complement our work that aims to improve the LM during training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for guiding language model generation. No verifiable resources are identified.",
      "processing_time": 37.01459574699402,
      "citing_paper_id": "259064099",
      "cited_paper_id": 208617790
    },
    {
      "context_text": "Methods for using a reward model to guide LM generation towards desired behaviors at inference time [21, 7] can complement our work that aims to improve the LM during training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for guiding language model generation. No verifiable resources are identified.",
      "processing_time": 37.01459574699402,
      "citing_paper_id": "259064099",
      "cited_paper_id": 235313967
    },
    {
      "context_text": "QA-FEEDBACK is based on ASQA [39], a dataset that focuses on answering ambiguous factoid questions [26] in an open-domain setting.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ASQA"
      ],
      "dataset_descriptions": {
        "ASQA": "Used to address the challenge of answering ambiguous factoid questions in an open-domain setting, focusing on disambiguation and accurate response generation."
      },
      "confidence_score": 1.0,
      "reasoning": "ASQA is identified as a dataset in the context, which is used for answering ambiguous factoid questions in an open-domain setting.",
      "processing_time": 46.04182839393616,
      "citing_paper_id": "259064099",
      "cited_paper_id": 216056269
    },
    {
      "context_text": "RLHF), and the state-of-the-art controlled generation approaches GeDi [17] and DEXPERTS [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles confirm that they are about methods, not datasets.",
      "processing_time": 38.83739924430847,
      "citing_paper_id": "259064099",
      "cited_paper_id": 221655075
    },
    {
      "context_text": "RLHF), and the state-of-the-art controlled generation approaches GeDi [17] and DEXPERTS [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles confirm that they are about methods, not datasets.",
      "processing_time": 38.83739924430847,
      "citing_paper_id": "259064099",
      "cited_paper_id": 235313967
    },
    {
      "context_text": "We follow previous work [17, 21] to report the toxicity score calculated on each full generation sequence from the PERPLEXITY API, as well as other commonly used metrics for REALTOXICITYPROMPTS, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy for fluency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERPLEXITY API",
        "REALTOXICITYPROMPTS"
      ],
      "dataset_descriptions": {
        "PERPLEXITY API": "Used to calculate toxicity scores on full generation sequences, providing a measure of the model's output quality and potential harmfulness.",
        "REALTOXICITYPROMPTS": "Used to evaluate n-gram diversity and GPT-2 XL perplexity as proxies for fluency and diversity in generated text, focusing on the quality and safety of the outputs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the use of the PERPLEXITY API and REALTOXICITYPROMPTS, which are specific resources used for evaluating the toxicity and quality of generated text.",
      "processing_time": 59.56799674034119,
      "citing_paper_id": "259064099",
      "cited_paper_id": 221655075
    },
    {
      "context_text": "We follow previous work [17, 21] to report the toxicity score calculated on each full generation sequence from the PERPLEXITY API, as well as other commonly used metrics for REALTOXICITYPROMPTS, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy for fluency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERPLEXITY API",
        "REALTOXICITYPROMPTS"
      ],
      "dataset_descriptions": {
        "PERPLEXITY API": "Used to calculate toxicity scores on full generation sequences, providing a measure of the model's output quality and potential harmfulness.",
        "REALTOXICITYPROMPTS": "Used to evaluate n-gram diversity and GPT-2 XL perplexity as proxies for fluency and diversity in generated text, focusing on the quality and safety of the outputs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the use of the PERPLEXITY API and REALTOXICITYPROMPTS, which are specific resources used for evaluating the toxicity and quality of generated text.",
      "processing_time": 59.56799674034119,
      "citing_paper_id": "259064099",
      "cited_paper_id": 235313967
    },
    {
      "context_text": "We follow previous work [17, 21] and use GPT-2 large model as the initial policy model Pθinit .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of the GPT-2 large model as an initial policy model. No verifiable datasets are referenced.",
      "processing_time": 40.45779371261597,
      "citing_paper_id": "259064099",
      "cited_paper_id": 221655075
    },
    {
      "context_text": "We follow previous work [17, 21] and use GPT-2 large model as the initial policy model Pθinit .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of the GPT-2 large model as an initial policy model. No verifiable datasets are referenced.",
      "processing_time": 40.45779371261597,
      "citing_paper_id": "259064099",
      "cited_paper_id": 235313967
    },
    {
      "context_text": "RLHF ), and the state-of-the-art controlled generation approaches GeDi [17] and D EXPERTS [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The titles of the cited papers do not introduce any datasets either.",
      "processing_time": 39.93066930770874,
      "citing_paper_id": "259064099",
      "cited_paper_id": 221655075
    },
    {
      "context_text": "We conduct experiments on two language generation tasks—detoxification [12] (§3) and long-form question answering (QA) [39] (§4).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets by name. It only refers to language generation tasks, which are not datasets themselves.",
      "processing_time": 39.67562460899353,
      "citing_paper_id": "259064099",
      "cited_paper_id": 221878771
    },
    {
      "context_text": "Additionally, to ensure the fluency of generated outputs, we follow [41] to add an approximate KL divergence penalty to each token a t with a weight β , that is not backpropagated through during training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adding a KL divergence penalty during training.",
      "processing_time": 38.60955834388733,
      "citing_paper_id": "259064099",
      "cited_paper_id": 237593001
    },
    {
      "context_text": "[23, 6] collect and store NL human feedback in a feedback memory for the model to retrieve and then perform the end task conditioning on the retrieved feedback.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NL human feedback"
      ],
      "dataset_descriptions": {
        "NL human feedback": "Used to collect and store natural language human feedback in a feedback memory, which the model retrieves to condition its performance on end tasks, enhancing model adaptability and user interaction."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'NL human feedback' which is domain-qualified data but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 50.074973821640015,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246016194
    },
    {
      "context_text": "[23, 6] collect and store NL human feedback in a feedback memory for the model to retrieve and then perform the end task conditioning on the retrieved feedback.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NL human feedback"
      ],
      "dataset_descriptions": {
        "NL human feedback": "Used to collect and store natural language human feedback in a feedback memory, which the model retrieves to condition its performance on end tasks, enhancing model adaptability and user interaction."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'NL human feedback' which is domain-qualified data but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 50.074973821640015,
      "citing_paper_id": "259064099",
      "cited_paper_id": 248405661
    },
    {
      "context_text": "This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization [40], instruction following [29], question answering [24, 27] and reducing harmfulness [3, 2, 22, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and improvements in model performance. No verifiable resources are identified.",
      "processing_time": 40.44536757469177,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization [40], instruction following [29], question answering [24, 27] and reducing harmfulness [3, 2, 22, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and improvements in model performance. No verifiable resources are identified.",
      "processing_time": 40.44536757469177,
      "citing_paper_id": "259064099",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization [40], instruction following [29], question answering [24, 27] and reducing harmfulness [3, 2, 22, 10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and improvements in model performance. No verifiable resources are identified.",
      "processing_time": 40.44536757469177,
      "citing_paper_id": "259064099",
      "cited_paper_id": 249152301
    },
    {
      "context_text": "Motivated by [19], Rφ3 predicts a single scalar reward and is trained with a pairwise comparison loss [29]:",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training language models.",
      "processing_time": 38.39516735076904,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "State-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29, 28, 9], with a combination of supervised learning and reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 39.915576457977295,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "State-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29, 28, 9], with a combination of supervised learning and reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 39.915576457977295,
      "citing_paper_id": "259064099",
      "cited_paper_id": 258959433
    },
    {
      "context_text": "Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29, 3, 2, 33, 10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the effectiveness of incorporating human feedback into language model training.",
      "processing_time": 39.661356925964355,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29, 3, 2, 33, 10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the effectiveness of incorporating human feedback into language model training.",
      "processing_time": 39.661356925964355,
      "citing_paper_id": "259064099",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29, 3, 2, 33, 10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the effectiveness of incorporating human feedback into language model training.",
      "processing_time": 39.661356925964355,
      "citing_paper_id": "259064099",
      "cited_paper_id": 252693405
    },
    {
      "context_text": "For comparison purposes, we follow [29] to separately collect pairwise human preferences from the same group of workers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to collecting human preferences, which is a methodological step rather than a named dataset.",
      "processing_time": 37.68336081504822,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "RLHF [46, 42, 29] aims to optimize the policy language model to generate content that is desired by human.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using human feedback to optimize language models.",
      "processing_time": 39.665645360946655,
      "citing_paper_id": "259064099",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "RLHF [46, 42, 29] aims to optimize the policy language model to generate content that is desired by human.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using human feedback to optimize language models.",
      "processing_time": 39.665645360946655,
      "citing_paper_id": "259064099",
      "cited_paper_id": 251371461
    },
    {
      "context_text": "Furthermore, previous research [8, 18, 11, 45, 25, 43] into automated evaluation of generated text shows that it can be challenging for human annotators to reliably compare the overall quality of two or more model outputs when the outputs contain a mixture of diverse undesired behaviors.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses challenges in evaluating generated text.",
      "processing_time": 39.206708908081055,
      "citing_paper_id": "259064099",
      "cited_paper_id": 247315430
    },
    {
      "context_text": "Furthermore, previous research [8, 18, 11, 45, 25, 43] into automated evaluation of generated text shows that it can be challenging for human annotators to reliably compare the overall quality of two or more model outputs when the outputs contain a mixture of diverse undesired behaviors.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses challenges in evaluating generated text.",
      "processing_time": 39.206708908081055,
      "citing_paper_id": "259064099",
      "cited_paper_id": 258865710
    },
    {
      "context_text": "[5, 35, 34] use a refinement model to refine model outputs conditioning on NL human feedback and then use a reward model to select the best refined outputs for supervised fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of a refinement model and a reward model, which are not datasets.",
      "processing_time": 43.363831758499146,
      "citing_paper_id": "259064099",
      "cited_paper_id": 248492802
    },
    {
      "context_text": "[38, 14, 42] train a conversational model to predict both the response and a binary user satisfaction score in order to improve the response generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to training a conversational model. No verifiable resource names are provided.",
      "processing_time": 41.79521465301514,
      "citing_paper_id": "259064099",
      "cited_paper_id": 251371461
    },
    {
      "context_text": "[38, 14, 42] train a conversational model to predict both the response and a binary user satisfaction score in order to improve the response generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to training a conversational model. No verifiable resource names are provided.",
      "processing_time": 41.79521465301514,
      "citing_paper_id": "259064099",
      "cited_paper_id": 253224415
    },
    {
      "context_text": "This is consistent with findings from previous RLHF studies ([29], [33]).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous studies. There is no indication of a reusable resource being used.",
      "processing_time": 41.44468283653259,
      "citing_paper_id": "259064099",
      "cited_paper_id": 252693405
    },
    {
      "context_text": "We follow [33] to define language generation as a Markov Decision Process (MDP) ⟨S , A , R , P, γ, T max ⟩ with a finite vocabulary V .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework for language generation.",
      "processing_time": 39.205365896224976,
      "citing_paper_id": "259064099",
      "cited_paper_id": 252693405
    },
    {
      "context_text": "Before collecting human feedback, we follow [33] to initialize the policy model with supervised fine-tuning on a small set of examples.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'a small set of examples'. This is too generic and lacks a clear identifier.",
      "processing_time": 42.55743050575256,
      "citing_paper_id": "259064099",
      "cited_paper_id": 252693405
    },
    {
      "context_text": "[16] also explores incorporating human feedback into LM pre-training.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of incorporating human feedback into language model pre-training.",
      "processing_time": 40.907163858413696,
      "citing_paper_id": "259064099",
      "cited_paper_id": 257020046
    },
    {
      "context_text": "Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological difference in annotation. No verifiable resources are identified.",
      "processing_time": 41.77070236206055,
      "citing_paper_id": "259064099",
      "cited_paper_id": 258587884
    },
    {
      "context_text": "Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological difference in annotation. No verifiable resources are identified.",
      "processing_time": 41.77070236206055,
      "citing_paper_id": "259064099",
      "cited_paper_id": 258841470
    },
    {
      "context_text": "To get a high quality dataset, we use the highly polar movie reviews provided by Maas et al. (2011).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "highly polar movie reviews"
      ],
      "dataset_descriptions": {
        "highly polar movie reviews": "Used to train and evaluate sentiment analysis models, focusing on highly polarized opinions in movie reviews."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions using 'highly polar movie reviews' provided by Maas et al. (2011), which is a known sentiment analysis dataset.",
      "processing_time": 48.70795273780823,
      "citing_paper_id": "153313581",
      "cited_paper_id": 1428702
    },
    {
      "context_text": "In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the longterm dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to attention mechanisms in neural machine translation.",
      "processing_time": 40.6504909992218,
      "citing_paper_id": "153313581",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "In many NLP tasks, especially for text generation, attention mechanism has been proved to be an essential technique to enable the model to capture the long-term dependency (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to attention mechanisms in neural machine translation.",
      "processing_time": 40.647902965545654,
      "citing_paper_id": "153313581",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "Li et al. (2018) propose a three-stage method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method proposed by Li et al. (2018).",
      "processing_time": 42.527169942855835,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "7 10 N/A N/A N/A TemplateBased (Li et al., 2018) 84.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too limited to infer the use of any datasets.",
      "processing_time": 43.07040214538574,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "…N/A N/A TemplateBased (Li et al., 2018) 84.2 13.7 44.1 67 N/A N/A N/A DeleteOnly (Li et al., 2018) 85.5 9.7 28.6 79 N/A N/A N/A DeleteAndRetrieve (Li et al., 2018) (Fu et al., 2018) 49.9 9.2 37.9 127 N/A N/A N/A CycleRL(Xu et al., 2018) 88 Fluency Fluency is measured by the perplexity of the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 42.540377616882324,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "6 79 N/A N/A N/A DeleteAndRetrieve (Li et al., 2018) 88.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'DeleteAndRetrieve'. No verifiable resources are identified.",
      "processing_time": 42.2740421295166,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "If a human reference is available, we will calculate the BLEU RetrieveOnly (Li et al., 2018) 92.9 0.4 0.7 10 N/A N/A N/A TemplateBased (Li et al., 2018) 84.2 13.7 44.1 67 N/A N/A N/A DeleteOnly (Li et al., 2018) 85.5 9.7 28.6 79 N/A N/A N/A DeleteAndRetrieve (Li et al., 2018) (Fu et al., 2018) 49.9…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different approaches using BLEU scores.",
      "processing_time": 43.063929080963135,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "For the human evaluation, we choose two of the most well-performed models according to the automatic evaluation results as competitors: DeleteAndRetrieve (DAR) (Li et al., 2018) and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The citation is used to reference a model, not a dataset.",
      "processing_time": 16.705646753311157,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "…the BLEU RetrieveOnly (Li et al., 2018) 92.9 0.4 0.7 10 N/A N/A N/A TemplateBased (Li et al., 2018) 84.2 13.7 44.1 67 N/A N/A N/A DeleteOnly (Li et al., 2018) 85.5 9.7 28.6 79 N/A N/A N/A DeleteAndRetrieve (Li et al., 2018) (Fu et al., 2018) 49.9 9.2 37.9 127 N/A N/A N/A CycleRL(Xu et al.,…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and their performance metrics. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 49.68625068664551,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "If a human reference is available, we will calculate the BLEU RetrieveOnly (Li et al., 2018) 92.9 0.4 0.7 10 N/A N/A N/A TemplateBased (Li et al., 2018) 84.2 13.7 44.1 67 N/A N/A N/A DeleteOnly (Li et al., 2018) 85.5 9.7 28.6 79 N/A N/A N/A DeleteAndRetrieve (Li et al., 2018) (Fu et al., 2018) 49.9 9.2 37.9 127 N/A N/A N/A CycleRL(Xu et al., 2018) 88 Fluency Fluency is measured by the perplexity of the transferred sentence, and we trained a 5-gram language model on the training set of two datasets using KenLM (Heaﬁeld, 2011).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'two datasets' but does not specify their names. The citation is focused on evaluating different methods for sentiment and style transfer, using fluency as a metric.",
      "processing_time": 19.166433572769165,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "For the human evaluation , we choose two of the most well-performed models according to the automatic evaluation results as competitors: DeleteAndRetrieve (DAR) (Li et al., 2018) and Controlled Generation (CtrlGen) (Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (DeleteAndRetrieve and Controlled Generation) but does not refer to any specific datasets. The context is about model performance comparison.",
      "processing_time": 18.116605758666992,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Following previous work, we use the possessed dataset provided by Li et al. (2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "possessed dataset"
      ],
      "dataset_descriptions": {
        "possessed dataset": "Used for sentiment and style transfer experiments, specifically to evaluate the effectiveness of delete, retrieve, generate approach in modifying text attributes."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'possessed dataset' which is a plausible dataset name, and it is used in the context of sentiment and style transfer.",
      "processing_time": 22.718972206115723,
      "citing_paper_id": "153313581",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Fluency Fluency is measured by the perplexity of the transferred sentence, and we trained a 5-gram language model on the training set of two datasets using KenLM (Heafield, 2011).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'two datasets' but does not specify their names. The citation is focused on the method (KenLM) rather than the datasets themselves.",
      "processing_time": 15.604645490646362,
      "citing_paper_id": "153313581",
      "cited_paper_id": 8313873
    },
    {
      "context_text": "Hu et al. (2017) propose a new neural generative model which combines variational auto-encoders and holistic attribute discriminators for the effective imposition of semantic structures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating text with semantic structures.",
      "processing_time": 12.234374761581421,
      "citing_paper_id": "153313581",
      "cited_paper_id": 8313873
    },
    {
      "context_text": "To handle this problem, one can use REINFORCE (Williams, 1992) or the Gumbel-Softmax trick (Kusner and Hernández-Lobato, 2016) to estimates gradients from the discriminator.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for estimating gradients.",
      "processing_time": 12.112619876861572,
      "citing_paper_id": "153313581",
      "cited_paper_id": 10366219
    },
    {
      "context_text": "To handle this problem, one can use REIN-FORCE (Williams, 1992) or the Gumbel-Softmax trick (Kusner and Hern´andez-Lobato, 2016) to estimates gradients from the discriminator.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for estimating gradients.",
      "processing_time": 24.74996519088745,
      "citing_paper_id": "153313581",
      "cited_paper_id": 10366219
    },
    {
      "context_text": "Content Preservation To measure content preservation, we calculate the BLEU score (Papineni et al., 2002) between the transferred sentence and its source input using NLTK.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions calculating the BLEU score but does not reference any specific dataset. BLEU is a metric, not a dataset, and the citation is about the method itself.",
      "processing_time": 27.905352354049683,
      "citing_paper_id": "153313581",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.484269380569458,
      "citing_paper_id": "153313581",
      "cited_paper_id": 13959787
    },
    {
      "context_text": "Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.484269380569458,
      "citing_paper_id": "153313581",
      "cited_paper_id": 29165442
    },
    {
      "context_text": "Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.484269380569458,
      "citing_paper_id": "153313581",
      "cited_paper_id": 52056513
    },
    {
      "context_text": "Most of the previous methods (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Carlson et al., 2017; Zhang et al., 2018b,a; Prabhumoye et al., 2018; Jin et al., 2019; Melnyk et al., 2017; dos Santos et al., 2018) formulate the style transfer problem into the “encoder-decoder” framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and frameworks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.484269380569458,
      "citing_paper_id": "153313581",
      "cited_paper_id": 266349854
    },
    {
      "context_text": ", 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (back-translation).",
      "processing_time": 13.67821216583252,
      "citing_paper_id": "153313581",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "They employ De-noising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers do not introduce new datasets either.",
      "processing_time": 14.323864459991455,
      "citing_paper_id": "153313581",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "They employ De-noising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al., 2016) to build a translation style between different styles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers do not introduce new datasets either.",
      "processing_time": 14.323864459991455,
      "citing_paper_id": "153313581",
      "cited_paper_id": 207168299
    },
    {
      "context_text": "Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 13.402789115905762,
      "citing_paper_id": "153313581",
      "cited_paper_id": 52056513
    },
    {
      "context_text": "Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 13.402789115905762,
      "citing_paper_id": "153313581",
      "cited_paper_id": 52190376
    },
    {
      "context_text": "Following their work, many methods (Fu et al., 2018; John et al., 2018; Zhang et al., 2018a,b) has been proposed based on standard encoder-decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. No verifiable resources are identified.",
      "processing_time": 13.402789115905762,
      "citing_paper_id": "153313581",
      "cited_paper_id": 266349854
    },
    {
      "context_text": "As reported in (Elazar and Goldberg, 2018; Lample et al., 2019), the style information can be still recovered from the latent representation even the model has trained adversarially.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model training and style recovery. No verifiable resources are identified.",
      "processing_time": 15.584206342697144,
      "citing_paper_id": "153313581",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "Lample et al. (2019) reduce text style transfer to unsupervised machine translation problem (Lample et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for text style transfer.",
      "processing_time": 24.90734553337097,
      "citing_paper_id": "153313581",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "However, both lines of the previous models make few attempts to utilize the attention mechanism to refer the long-term history or the source sentence, except Lample et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a model or method. There are no verifiable resources or datasets mentioned.",
      "processing_time": 18.399254322052002,
      "citing_paper_id": "153313581",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "In the future, we are planning to adapt our Style Transformer to the multiple-attribute setting like Lample et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to adapting a method to a multiple-attribute setting.",
      "processing_time": 14.657748222351074,
      "citing_paper_id": "153313581",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "Transformer is a fully-connected selfattention neural architecture, which has achieved many exciting results on natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), language modeling (Dai et al., 2019), text classification (Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and tasks. The context focuses on the Transformer architecture and its applications.",
      "processing_time": 17.788548946380615,
      "citing_paper_id": "153313581",
      "cited_paper_id": 57759363
    },
    {
      "context_text": "They employ Denoising Auto-encoders (Vincent et al., 2008) and back-translation (Sennrich et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Denoising Auto-encoders and back-translation).",
      "processing_time": 14.654359102249146,
      "citing_paper_id": "153313581",
      "cited_paper_id": 207168299
    },
    {
      "context_text": "Base model For T RANSFORMER , we use 300-dim GloVe (Pennington et al., 2014) trained on 6B corpus as the word embeddings.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'GloVe' but does not refer to it as a dataset. It is used as word embeddings, which are not considered datasets according to the instructions.",
      "processing_time": 28.13476276397705,
      "citing_paper_id": "248299824",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Dataset The PersonaChat (Zhang et al., 2018a) data is widely used in this ﬁeld (Song et al., 2019, 2020; Wolf et al., 2019; Golovanov et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to train and evaluate personalized dialogue agents, focusing on incorporating personal information into conversational responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable dataset used in the field of personalized dialogue agents.",
      "processing_time": 34.81817364692688,
      "citing_paper_id": "248299824",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Dataset The PersonaChat (Zhang et al., 2018a) data is widely used in this ﬁeld (Song et al., 2019, 2020; Wolf et al., 2019; Golovanov et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to train and evaluate personalized dialogue agents, focusing on incorporating personal information into conversational responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable dataset used in the field of personalized dialogue agents.",
      "processing_time": 34.81817364692688,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "Dataset The PersonaChat (Zhang et al., 2018a) data is widely used in this ﬁeld (Song et al., 2019, 2020; Wolf et al., 2019; Golovanov et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to train and evaluate personalized dialogue agents, focusing on incorporating personal information into conversational responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable dataset used in the field of personalized dialogue agents.",
      "processing_time": 34.81817364692688,
      "citing_paper_id": "248299824",
      "cited_paper_id": 196184953
    },
    {
      "context_text": "A clipped persona-based dialogue from the PersonaChat (Zhang et al., 2018a) dataset is shown in Figure 1, which covers rich persona features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to demonstrate rich persona features in a clipped dialogue, focusing on personalizing dialogue agents through persona-based interactions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable dataset used for personalized dialogue systems.",
      "processing_time": 21.38352632522583,
      "citing_paper_id": "248299824",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "…focus on modifying dialogue models to condition auxiliary persona information, including extra persona embedding(Li et al., 2016b), proﬁle memory (Zhang et al., 2018a), copying from personas (Yavuz et al., 2019), CVAE with persona information (Song et al., 2019), and using meta-learning to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The focus is on modifying dialogue models to incorporate persona information.",
      "processing_time": 14.314109802246094,
      "citing_paper_id": "248299824",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Persona-based dialogue generation It sees growing interest in recent years, thanks to the released benchmark datasets such as PersonaChat/ ConvAI2 (Zhang et al., 2018a; Dinan et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat",
        "ConvAI2"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to train and evaluate persona-based dialogue systems, focusing on generating conversations that reflect personal attributes and preferences.",
        "ConvAI2": "Used to train and evaluate persona-based dialogue systems, focusing on generating conversations that reflect personal attributes and preferences."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PersonaChat/ConvAI2' as benchmark datasets used for persona-based dialogue generation. These are specific datasets with clear identifiers and are directly relevant to the research topic.",
      "processing_time": 40.34740424156189,
      "citing_paper_id": "248299824",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "To validate the effectiveness of our proposed model-agnostic data manipulation method, we ﬁrst ex-periment on two strong persona-based dialogue generation models (Transformer encoder-decoder and GPT2) on the benchmark PersonaChat (Zhang et al., 2018a) dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to experiment with persona-based dialogue generation models, specifically validating a model-agnostic data manipulation method using Transformer and GPT2 architectures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable resource used for experimenting with persona-based dialogue generation models.",
      "processing_time": 21.584046602249146,
      "citing_paper_id": "248299824",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Compared with conventional dialogue datasets such as OpenSubtitles (Lison and Tiedemann, 2016) and Weibo (Shang et al., 2015) with millions of utterances, persona-based dialogue datasets are relatively small.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles",
        "Weibo"
      ],
      "dataset_descriptions": {
        "OpenSubtitles": "Used as a conventional dialogue dataset for comparison with persona-based datasets, highlighting the size difference in terms of utterances.",
        "Weibo": "Used as a conventional dialogue dataset for comparison with persona-based datasets, emphasizing the scale of utterances available."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'OpenSubtitles' and 'Weibo' as conventional dialogue datasets, which are specific and verifiable resources. However, they are not used in the current research but compared against persona-based dialogue datasets.",
      "processing_time": 31.981223106384277,
      "citing_paper_id": "248299824",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "…are considered: • T RANSFORMER (Vaswani et al., 2017): an encoder-decoder architecture using Transformer as the backbone with pointer generator (See et al., 2017) • GPT2: one of the most powerful pretrained models on this task (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 27.033690929412842,
      "citing_paper_id": "248299824",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "…are considered: • T RANSFORMER (Vaswani et al., 2017): an encoder-decoder architecture using Transformer as the backbone with pointer generator (See et al., 2017) • GPT2: one of the most powerful pretrained models on this task (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 27.033690929412842,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "…are considered: • T RANSFORMER (Vaswani et al., 2017): an encoder-decoder architecture using Transformer as the backbone with pointer generator (See et al., 2017) • GPT2: one of the most powerful pretrained models on this task (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 27.033690929412842,
      "citing_paper_id": "248299824",
      "cited_paper_id": 196184953
    },
    {
      "context_text": "We pack two base models with our method D 3 and other data manipulation approaches for comparison: • B ACK T RANSLATION (BT) (Sennrich et al., 2016): we perform BT on all sentences in a training sample, including the persona sentences and dialogue utterances, and train the model with the augmented…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'training sample' but does not specify a named dataset. The citation is about a method (Back Translation) rather than a dataset.",
      "processing_time": 15.154801845550537,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Compared methods We pack two base models with our method D3 and other data manipulation approaches for comparison: • BACK TRANSLATION (BT) (Sennrich et al., 2016): we perform BT on all sentences in a training sample, including the persona sentences and dialogue utterances, and train the model with the augmented and original data jointly; • CVAE (Li et al.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on comparing different data manipulation approaches, particularly back translation, but does not specify a dataset.",
      "processing_time": 18.026335954666138,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Baselines We apply the same translation models as the ones used in §A.2 for the BT (Sennrich et al., 2016) baseline and augment each sample with a new sample from it.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to a baseline method. The citation is used to describe a methodological approach rather than a dataset.",
      "processing_time": 30.91967225074768,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Since the diversity scarcity issue is not severe in H̃ , we use a popular sentence-level data augmentation method, back translation (BT) (Sennrich et al., 2016), to obtain variants of dialogue utterances.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (back translation) for data augmentation.",
      "processing_time": 14.297860860824585,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "For example, if we apply back translation (Sennrich et al., 2016) to every sentence in persona-based samples, the augmented ones may not maintain the coherence between the dialogue history and the response as well as the consistency between the persona and the response simultaneously.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (back translation) applied to persona-based samples, which are not specific datasets.",
      "processing_time": 27.51007056236267,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Since the diversity scarcity issue is not severe in (cid:101) H , we use a popular sentence-level data augmentation method, back translation (BT) (Sennrich et al., 2016), to obtain variants of dialogue utterances.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (back translation) for data augmentation.",
      "processing_time": 14.293254375457764,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Text data manipulation Various data augmentation methods have been widely used in many NLP tasks (Sennrich et al., 2016; Hou et al., 2018; Guo et al., 2019; Min et al., 2020), which are also effective to boost the performance of dialogue models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses data augmentation methods in NLP tasks and their effectiveness in boosting dialogue models. No specific datasets are mentioned.",
      "processing_time": 25.938392162322998,
      "citing_paper_id": "248299824",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Text data manipulation Various data augmentation methods have been widely used in many NLP tasks (Sennrich et al., 2016; Hou et al., 2018; Guo et al., 2019; Min et al., 2020), which are also effective to boost the performance of dialogue models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses data augmentation methods in NLP tasks and their effectiveness in boosting dialogue models. No specific datasets are mentioned.",
      "processing_time": 25.938392162322998,
      "citing_paper_id": "248299824",
      "cited_paper_id": 162168620
    },
    {
      "context_text": "Some leverage BERT (Devlin et al., 2019) as backbones (Lin et al., 2021; Song et al., 2021).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (BERT) which are excluded according to the instructions.",
      "processing_time": 13.602037906646729,
      "citing_paper_id": "248299824",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "…here consider both token-level and phrase-level editing methods given a persona sentence (cid:101) P : • Token-level editing: we randomly mask a pre-deﬁned ratio of tokens in (cid:101) P , then use a pretrained BERT (Devlin et al., 2019) model to make predictions on the masked positions one by one.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BERT) which is excluded according to the rules.",
      "processing_time": 26.30487632751465,
      "citing_paper_id": "248299824",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Following previous work (Welleck et al., 2019), we cast it as a natural language inference (NLI) problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is focused on the NLI problem formulation.",
      "processing_time": 15.16566252708435,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "A trained RoBERTa (Liu et al., 2019) model is used here as the NLI model, with an accuracy of 90.8% on the DialogueNLI dev set provided in Welleck et al. (2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DialogueNLI dev set"
      ],
      "dataset_descriptions": {
        "DialogueNLI dev set": "Used to evaluate the performance of a trained RoBERTa model on natural language inference tasks, specifically focusing on dialogue contexts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DialogueNLI dev set, which is a specific dataset used for evaluating the NLI model. The dataset is clearly identified and used for evaluation.",
      "processing_time": 36.13153338432312,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "(Welleck et al., 2019) to indicate the consistency between a response and persona sentences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for indicating consistency between responses and persona sentences.",
      "processing_time": 13.574528455734253,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "Welleck et al. (2019) showed that not all responses in the PersonaChat dataset are consistent with the provided personas.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to evaluate response consistency with provided personas in dialogue systems, focusing on the quality and coherence of generated responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat dataset, which is a specific, verifiable dataset used in dialogue systems research.",
      "processing_time": 33.21183919906616,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": ", 2019) (C) is involved, where we follow the default setting and use the output of an NLI model trained on the DialogueNLI dataset (Welleck et al., 2019) to indicate the consistency between a response and persona sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DialogueNLI"
      ],
      "dataset_descriptions": {
        "DialogueNLI": "Used to train an NLI model to assess the consistency between generated responses and persona sentences, focusing on personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DialogueNLI dataset, which is a specific dataset used for training an NLI model to assess consistency between responses and persona sentences.",
      "processing_time": 35.31287622451782,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "To make the model better ﬁt the domain of PersonaChat, we ﬁnetune the model on the DialogueNLI dataset (Welleck et al., 2019) which is a part of the original PersonaChat.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DialogueNLI"
      ],
      "dataset_descriptions": {
        "DialogueNLI": "Used to fine-tune a model for the PersonaChat domain, focusing on improving the model's ability to understand and generate contextually appropriate responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the DialogueNLI dataset for fine-tuning a model to fit the domain of PersonaChat. The dataset is clearly identified and used for a specific purpose.",
      "processing_time": 23.97581386566162,
      "citing_paper_id": "248299824",
      "cited_paper_id": 53298765
    },
    {
      "context_text": ", 2017) integrated; • GPT2: one of the most powerful pretrained models on this task (Wolf et al., 2019; Golovanov et al.; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 26.599879026412964,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "A few studies have been conducted to alleviate the above data issues by ﬁnetuning existing pre-trained models such as GPT (Wolf et al., 2019; Golovanov et al., 2019) or BERT (Lin et al., 2021; Song et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models like GPT and BERT. These are excluded as per the instructions.",
      "processing_time": 17.04027009010315,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "A few studies have been conducted to alleviate the above data issues by ﬁnetuning existing pre-trained models such as GPT (Wolf et al., 2019; Golovanov et al., 2019) or BERT (Lin et al., 2021; Song et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only pre-trained models like GPT and BERT. These are excluded as per the instructions.",
      "processing_time": 17.04027009010315,
      "citing_paper_id": "248299824",
      "cited_paper_id": 196184953
    },
    {
      "context_text": "GPT/GPT2 (Radford et al., 2018, 2019) are chosen the most often and shown to improve the generation quality with different ﬁnetuning strategies (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GPT/GPT2 models and their finetuning strategies.",
      "processing_time": 30.517087697982788,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "GPT/GPT2 (Radford et al., 2018, 2019) are chosen the most often and shown to improve the generation quality with different ﬁnetuning strategies (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GPT/GPT2 models and their finetuning strategies.",
      "processing_time": 30.517087697982788,
      "citing_paper_id": "248299824",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "GPT/GPT2 (Radford et al., 2018, 2019) are chosen the most often and shown to improve the generation quality with different ﬁnetuning strategies (Wolf et al., 2019; Golovanov et al., 2019; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of GPT/GPT2 models and their finetuning strategies.",
      "processing_time": 30.517087697982788,
      "citing_paper_id": "248299824",
      "cited_paper_id": 196184953
    },
    {
      "context_text": ", 2018, 2019) are chosen the most often and shown to improve the generation quality with different finetuning strategies (Wolf et al., 2019; Golovanov et al.; Cao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and models. The context focuses on the use of certain models and strategies for improving generation quality.",
      "processing_time": 18.33540439605713,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "In past years, there has been a growing interest in introducing explicit personas in dialogue generation models (Song et al., 2019; Wolf et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a growing interest in using personas in dialogue generation models.",
      "processing_time": 27.49645233154297,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "Moreover, most of them simply concatenate persona texts and dialogue history together as a single input sequence (Wolf et al., 2019; Roller et al., 2021), highly depending on the ability of the pretrained model to fast adapt to the target data domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on how models are adapted to target data domains, but no particular dataset is named.",
      "processing_time": 16.58028244972229,
      "citing_paper_id": "248299824",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "Moreover, most of them simply concatenate persona texts and dialogue history together as a single input sequence (Wolf et al., 2019; Roller et al., 2021), highly depending on the ability of the pretrained model to fast adapt to the target data domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on how models are adapted to target data domains, but no particular dataset is named.",
      "processing_time": 16.58028244972229,
      "citing_paper_id": "248299824",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Since it can only handle pair-wise data, we concatenate all input sentences as a single input sequence in this method; • E NTROPY F ILTER ( FILTER ) (Csáky et al., 2019): it removes generic responses according to the entropy, which is calculated using the dialogue history and the response without…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for filtering generic responses in dialogue systems.",
      "processing_time": 12.795817613601685,
      "citing_paper_id": "248299824",
      "cited_paper_id": 153312586
    },
    {
      "context_text": "For example, a few approaches ﬁlter uninformative or noisy samples to enhance data quality (Csáky et al., 2019; Akama et al., 2020).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions filtering uninformative or noisy samples to enhance data quality but does not specify any datasets. The cited papers' titles suggest methods for filtering dialogue corpora, but no specific datasets are named.",
      "processing_time": 19.245920658111572,
      "citing_paper_id": "248299824",
      "cited_paper_id": 153312586
    },
    {
      "context_text": "For example, a few approaches ﬁlter uninformative or noisy samples to enhance data quality (Csáky et al., 2019; Akama et al., 2020).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions filtering uninformative or noisy samples to enhance data quality but does not specify any datasets. The cited papers' titles suggest methods for filtering dialogue corpora, but no specific datasets are named.",
      "processing_time": 19.245920658111572,
      "citing_paper_id": "248299824",
      "cited_paper_id": 222180042
    },
    {
      "context_text": "In Entropy-ﬁlter (Csáky et al., 2019), we set the threshold as 1.1 and using both source and target sequences for ﬁltering.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for filtering sequences. No verifiable resources are identified.",
      "processing_time": 13.854748964309692,
      "citing_paper_id": "248299824",
      "cited_paper_id": 153312586
    },
    {
      "context_text": "• Phrase-level editing: we remove the last few tokens in P̃ with the removal length determined by a random ratio, and utilize a pretrained GPT2 (Radford et al., 2019) to rewrite the removal part.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GPT2) which is excluded. No verifiable resources are identified.",
      "processing_time": 15.504343748092651,
      "citing_paper_id": "248299824",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "It has been applied to many NLP tasks such as machine translation (Platanios et al., 2019), reading comprehension (Tay et al., 2019) and language understanding (Xu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks. No dataset names are present in the text.",
      "processing_time": 15.101877450942993,
      "citing_paper_id": "248299824",
      "cited_paper_id": 166228313
    },
    {
      "context_text": ", 2019), reading comprehension (Tay et al., 2019) and language understanding (Xu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas. No clear identifiers for datasets are present.",
      "processing_time": 14.84086275100708,
      "citing_paper_id": "248299824",
      "cited_paper_id": 166228313
    },
    {
      "context_text": "uninformative or noisy samples to enhance data quality (Csáky et al., 2019; Akama et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to enhancing data quality through filtering noisy samples, but no named datasets are provided.",
      "processing_time": 17.35992121696472,
      "citing_paper_id": "248299824",
      "cited_paper_id": 222180042
    },
    {
      "context_text": "In this paper, we evaluate synthesized speech quality using a Mean Opinion Score (MOS) study, as in [42].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a MOS study but does not specify a dataset. The cited paper title suggests a method for conducting MOS studies, not a dataset.",
      "processing_time": 17.016425609588623,
      "citing_paper_id": "244908340",
      "cited_paper_id": 1872405
    },
    {
      "context_text": "Similarly, Tacotron 2 was used with a different speaker embeddings methods [2], with LDE embeddings [9] to improve similarity and naturalness of speech for unseen speakers [10].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of Tacotron 2 and LDE embeddings for improving speech synthesis.",
      "processing_time": 19.039500951766968,
      "citing_paper_id": "244908340",
      "cited_paper_id": 4881455
    },
    {
      "context_text": "We use global conditioning [22] in the residual blocks of the coupling layers as well as in the posterior encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (global conditioning) used in the architecture of a generative model.",
      "processing_time": 16.559727668762207,
      "citing_paper_id": "244908340",
      "cited_paper_id": 6254678
    },
    {
      "context_text": "As a decoder, we use a stack of 4 affine coupling layers [21] each layer is itself a stack of 4 WaveNet residual blocks [22], as in VITS model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers are referenced for their methods, not for providing datasets.",
      "processing_time": 17.964609146118164,
      "citing_paper_id": "244908340",
      "cited_paper_id": 6254678
    },
    {
      "context_text": "This model was chosen for achieving state-of-the-art results in VoxCeleb 1 [32] test subset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VoxCeleb 1"
      ],
      "dataset_descriptions": {
        "VoxCeleb 1": "Used to evaluate the model's performance on speaker identification, focusing on state-of-the-art results in the test subset."
      },
      "confidence_score": 1.0,
      "reasoning": "VoxCeleb 1 is mentioned as a test subset used to evaluate the model's performance. It is a specific, verifiable dataset.",
      "processing_time": 23.043471574783325,
      "citing_paper_id": "244908340",
      "cited_paper_id": 10475843
    },
    {
      "context_text": ", 2018), allowing for speech generation that resembles the target speaker (Jia et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for speech generation. No verifiable resources are identified.",
      "processing_time": 16.572883129119873,
      "citing_paper_id": "244908340",
      "cited_paper_id": 48363067
    },
    {
      "context_text": "As speaker encoder, we use the H/ASP model [29] publicly available, that was trained with the Prototypical Angular [30] plus Softmax loss functions in the VoxCeleb 2 [31] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VoxCeleb 2"
      ],
      "dataset_descriptions": {
        "VoxCeleb 2": "Used to train the H/ASP model with Prototypical Angular and Softmax loss functions, focusing on deep speaker recognition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VoxCeleb 2 dataset, which is a well-known dataset for speaker recognition. The dataset is used for training the H/ASP model with specific loss functions.",
      "processing_time": 24.955218076705933,
      "citing_paper_id": "244908340",
      "cited_paper_id": 49211906
    },
    {
      "context_text": "As speaker encoder, we use the H/ASP model [29] publicly available, that was trained with the Prototypical Angular [30] plus Softmax loss functions in the VoxCeleb 2 [31] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VoxCeleb 2"
      ],
      "dataset_descriptions": {
        "VoxCeleb 2": "Used to train the H/ASP model with Prototypical Angular and Softmax loss functions, focusing on deep speaker recognition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VoxCeleb 2 dataset, which is a well-known dataset for speaker recognition. The dataset is used for training the H/ASP model with specific loss functions.",
      "processing_time": 24.955218076705933,
      "citing_paper_id": "244908340",
      "cited_paper_id": 214667019
    },
    {
      "context_text": "For the TTS model training and for the discrimination of vocoder HiFi-GAN we use the AdamW optimizer [40] with betas 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the AdamW optimizer, which is a method, not a dataset.",
      "processing_time": 13.2844078540802,
      "citing_paper_id": "244908340",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "For efficient end-to-end vocoder training, we randomly sample constant length partial sequences from z as in [23, 27, 28, 19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for sampling sequences during training. The cited papers are about speech synthesis but do not provide specific dataset names.",
      "processing_time": 19.602277755737305,
      "citing_paper_id": "244908340",
      "cited_paper_id": 202749904
    },
    {
      "context_text": "For efficient end-to-end vocoder training, we randomly sample constant length partial sequences from z as in [23, 27, 28, 19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for sampling sequences during training. The cited papers are about speech synthesis but do not provide specific dataset names.",
      "processing_time": 19.602277755737305,
      "citing_paper_id": "244908340",
      "cited_paper_id": 222291664
    },
    {
      "context_text": "For the multilingual experiments, we use weighted random sampling [41] to guarantee a language balanced batch.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sampling languages in multilingual experiments.",
      "processing_time": 24.40971326828003,
      "citing_paper_id": "244908340",
      "cited_paper_id": 202786778
    },
    {
      "context_text": "However, we use Weighted random sampling [41] to guarantee that samples from adapted speakers appear in a quarter of the batch.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sampling. The context is about the methodology used in training models, not about datasets.",
      "processing_time": 26.703746795654297,
      "citing_paper_id": "244908340",
      "cited_paper_id": 202786778
    },
    {
      "context_text": "Meanwhile, Tacotron 2 [7] was adapted using external speaker embeddings extracted from a trained speaker encoder using a generalized end-to-end loss (GE2E) [8], allowing for speech generation that resembles the target speaker [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on adapting Tacotron 2 using speaker embeddings and GE2E loss, which are not datasets.",
      "processing_time": 29.361091375350952,
      "citing_paper_id": "244908340",
      "cited_paper_id": 206742911
    },
    {
      "context_text": "Finally, for speaker adaptation experiments, to mimic a more realistic setting, we used 4 speakers from the Common Voice dataset [38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Common Voice"
      ],
      "dataset_descriptions": {
        "Common Voice": "Used for speaker adaptation experiments to mimic a more realistic setting, focusing on adapting models to different speakers using a massively-multilingual speech corpus."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Common Voice dataset' which is a specific, verifiable dataset used for speaker adaptation experiments.",
      "processing_time": 33.57990264892578,
      "citing_paper_id": "244908340",
      "cited_paper_id": 209376338
    },
    {
      "context_text": "Nevertheless, to show the potential of our model for adaptation to new speakers/recording conditions, we selected samples from 20 to 61 seconds of speech\n9The authors presented the results in a graph without the actual figures, so the MOS scores reported here are approximations calculated considering the length in pixels of those graphs.\nfor 2 Portuguese and 2 English speakers (1M/1F) in the Common Voice [38] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Common Voice"
      ],
      "dataset_descriptions": {
        "Common Voice": "Used to adapt the model to new speakers and recording conditions, focusing on Portuguese and English speakers (1M/1F) with speech samples ranging from 20 to 61 seconds."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Common Voice dataset, which is a specific, verifiable resource used for speech data. It is used to adapt the model to new speakers and recording conditions.",
      "processing_time": 37.25178027153015,
      "citing_paper_id": "244908340",
      "cited_paper_id": 209376338
    },
    {
      "context_text": "for 2 Portuguese and 2 English speakers (1M/1F) in the Common Voice [38] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Common Voice"
      ],
      "dataset_descriptions": {
        "Common Voice": "Used to collect speech data for 2 Portuguese and 2 English speakers (1M/1F), focusing on voice diversity and quality for personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the 'Common Voice' dataset, which is a specific, verifiable resource used for speech data. It is clearly identified and used in the context of the research.",
      "processing_time": 36.6345853805542,
      "citing_paper_id": "244908340",
      "cited_paper_id": 209376338
    },
    {
      "context_text": "In this context, Attentron [3] proposed a finegrained encoder with an attention mechanism for extracting detailed styles from various reference samples and a coarsegrained encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Attentron) and its components. The context focuses on the model architecture and its capabilities.",
      "processing_time": 29.612903356552124,
      "citing_paper_id": "244908340",
      "cited_paper_id": 218673934
    },
    {
      "context_text": "Following previous works (Choi et al., 2020; Casanova et al., 2021), we compute SECS using the speaker encoder of the Resemblyzer (Jemine, 2019) package, allowing for comparison with those studies.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Resemblyzer) used for computing SECS. The context focuses on the methodology and comparison with previous studies.",
      "processing_time": 30.473392724990845,
      "citing_paper_id": "244908340",
      "cited_paper_id": 218673934
    },
    {
      "context_text": "In this context, Attentron (Choi et al., 2020) proposed a finegrained encoder with an attention mechanism for extracting detailed styles from various reference samples and a coarsegrained encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Attentron, which is a method/model, not a dataset. No specific datasets are mentioned or used in the described research.",
      "processing_time": 29.01404333114624,
      "citing_paper_id": "244908340",
      "cited_paper_id": 218673934
    },
    {
      "context_text": "[3] S. Choi, S. Han, D. Kim, and S. Ha, “Attentron: Few-shot textto-speech utilizing attention-based variable-length embedding,” arXiv preprint arXiv:2005.08484, 2020.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Attentron) for few-shot text-to-speech. No verifiable resources are identified.",
      "processing_time": 30.14875292778015,
      "citing_paper_id": "244908340",
      "cited_paper_id": 218673934
    },
    {
      "context_text": "The Attentron model achieved a SimMOS of 3.30±0.06 after being trained with approximately 100 speakers in the English language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to training data with a number of speakers, which is too generic.",
      "processing_time": 29.605976343154907,
      "citing_paper_id": "244908340",
      "cited_paper_id": 218673934
    },
    {
      "context_text": "Therefore, YourTTS can convert voices using the model’s Posterior Encoder, decoder and the HiFi-GAN Generator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a model. The context is about the functionality of YourTTS using parts of HiFi-GAN.",
      "processing_time": 30.823322057724,
      "citing_paper_id": "244908340",
      "cited_paper_id": 222291664
    },
    {
      "context_text": "For the TTS model training and for the discrimination of vocoder HiFi-GAN we use the AdamW optimizer [40] with betas 0.8 and 0.99, weight decay 0.01 and an initial learning rate of 0.0002 decaying exponentially by a gamma of 0.999875 [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization parameters and methods.",
      "processing_time": 26.207414865493774,
      "citing_paper_id": "244908340",
      "cited_paper_id": 222291664
    },
    {
      "context_text": "As a vocoder we use the HiFi-GAN [23] version 1 with the discriminator modifications introduced by [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions HiFi-GAN but does not refer to it as a dataset. It is used as a method or model for speech synthesis.",
      "processing_time": 30.141249895095825,
      "citing_paper_id": "244908340",
      "cited_paper_id": 222291664
    },
    {
      "context_text": "We used the FullSubNet model [36] as denoiser and resampled the data to 16KHz.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the FullSubNet model as a denoiser but does not reference any specific dataset. The model is not considered a dataset.",
      "processing_time": 30.139023542404175,
      "citing_paper_id": "244908340",
      "cited_paper_id": 225103151
    },
    {
      "context_text": "Furthermore, to analyze the generalization of the model for the Portuguese language, and to verify the result achieved by our model in a language where the model was trained with only one speaker, we used the 8 speakers (4M/4F) from the test subset of the MLS Portuguese dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MLS Portuguese"
      ],
      "dataset_descriptions": {
        "MLS Portuguese": "Used to analyze the generalization of a speech model for the Portuguese language, focusing on the test subset with 8 speakers (4 male, 4 female) to verify performance with limited training data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the MLS Portuguese dataset to analyze the generalization of a model for the Portuguese language, specifically using the test subset with 8 speakers.",
      "processing_time": 38.380171060562134,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "[33] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “MLS: A large-scale multilingual dataset for speech research,” in Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, H. Meng, B. Xu, and T. F. Zheng, Eds.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MLS"
      ],
      "dataset_descriptions": {
        "MLS": "Used to support speech research across multiple languages, providing a large-scale multilingual dataset for training and evaluation purposes."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'MLS' as a large-scale multilingual dataset for speech research, which is a specific, verifiable dataset.",
      "processing_time": 34.386908769607544,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "For the Portuguese MLS dataset, the highest MOS metric was achieved by experiment 3+SCL, with MOS 4.11±0.07, although the confidence intervals overlap with the other experiments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Portuguese MLS dataset"
      ],
      "dataset_descriptions": {
        "Portuguese MLS dataset": "Used to evaluate the performance of different experimental setups in speech research, focusing on the Mean Opinion Score (MOS) metric to assess quality."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Portuguese MLS dataset' which is a specific, verifiable dataset used in speech research. The dataset is used to evaluate the performance of different experimental setups.",
      "processing_time": 38.16986870765686,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "In addition, we evaluated the model in the test subset of Multilingual LibriSpeech (MLS) [33] using all languages.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multilingual LibriSpeech (MLS)"
      ],
      "dataset_descriptions": {
        "Multilingual LibriSpeech (MLS)": "Used to evaluate the model's performance across multiple languages, focusing on multilingual speech recognition capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the Multilingual LibriSpeech (MLS) dataset for evaluating the model across multiple languages.",
      "processing_time": 22.329429864883423,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "The dataset is also composed of audiobooks, therefore tending to have similar recording characteristics and prosody to the MLS dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MLS"
      ],
      "dataset_descriptions": {
        "MLS": "Used to compare recording characteristics and prosody, aiding in the composition of a new dataset with similar properties for speech research."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'MLS dataset' which is a specific, verifiable dataset. The citation is used to describe the characteristics of another dataset being composed of audiobooks with similar properties to MLS.",
      "processing_time": 36.922768354415894,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "Table 1 shows MOS and Sim-MOS with 95% confidence intervals and SECS for all of our experiments in English for the datasets VCTK and LibriTTS and in Portuguese with the Portuguese sub-set of the dataset MLS.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VCTK",
        "LibriTTS",
        "MLS"
      ],
      "dataset_descriptions": {
        "VCTK": "Used to evaluate MOS and Sim-MOS scores in English, focusing on speech quality and similarity in personalized text generation.",
        "LibriTTS": "Used to evaluate MOS and Sim-MOS scores in English, focusing on speech quality and similarity in personalized text generation.",
        "MLS": "Used to evaluate MOS and Sim-MOS scores in Portuguese, focusing on speech quality and similarity in personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for experiments in English and Portuguese, which are relevant to personalized text generation.",
      "processing_time": 47.25669717788696,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "org/ from 10 speakers (5F/5M) from the Multilingual LibriSpeech (MLS) [33] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multilingual LibriSpeech (MLS)"
      ],
      "dataset_descriptions": {
        "Multilingual LibriSpeech (MLS)": "Used to collect speech data from 10 speakers (5 female, 5 male) for speech research, focusing on multilingual speech patterns and speaker variability."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Multilingual LibriSpeech (MLS)' dataset, which is a specific, verifiable resource used for speech research. The dataset is clearly identified and used in the context of collecting speech data from 10 speakers.",
      "processing_time": 43.586726665496826,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "For the LibriTTS and MLS Portuguese, we randomly draw one sample per speaker considering only those with 5 seconds or more, to guarantee a reference with sufficient duration.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LibriTTS",
        "MLS Portuguese"
      ],
      "dataset_descriptions": {
        "LibriTTS": "Used to draw random samples per speaker with at least 5 seconds of audio, ensuring sufficient duration for speech analysis.",
        "MLS Portuguese": "Used to draw random samples per speaker with at least 5 seconds of audio, ensuring sufficient duration for speech analysis."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, LibriTTS and MLS Portuguese, which are used for drawing samples for speech research.",
      "processing_time": 42.08196496963501,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "For Portuguese we select samples\n6https://librivox.org/\nfrom 10 speakers (5F/5M) from the Multilingual LibriSpeech (MLS) [33] dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Multilingual LibriSpeech (MLS)"
      ],
      "dataset_descriptions": {
        "Multilingual LibriSpeech (MLS)": "Used to select speech samples from 10 speakers (5F/5M) for Portuguese, focusing on multilingual speech research and analysis."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Multilingual LibriSpeech (MLS)' dataset, which is a specific, verifiable resource used for selecting speech samples.",
      "processing_time": 38.1476354598999,
      "citing_paper_id": "244908340",
      "cited_paper_id": 226202134
    },
    {
      "context_text": "Portuguese: TTS-Portuguese Corpus [35], a singlespeaker dataset of the Brazilian Portuguese language with around 10 hours of speech, sampled at 48KHz.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TTS-Portuguese Corpus"
      ],
      "dataset_descriptions": {
        "TTS-Portuguese Corpus": "Used to train and evaluate speech synthesis models for Brazilian Portuguese, focusing on a single speaker with 10 hours of 48KHz audio."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, TTS-Portuguese Corpus, which is a single-speaker dataset of Brazilian Portuguese with 10 hours of speech.",
      "processing_time": 39.198978900909424,
      "citing_paper_id": "244908340",
      "cited_paper_id": 235652454
    },
    {
      "context_text": "Portuguese: TTS-Portuguese Corpus (Casanova et al., 2022), a single-speaker dataset of the Brazilian Portuguese language with around 10 hours of speech, sampled at 48KHz.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TTS-Portuguese Corpus"
      ],
      "dataset_descriptions": {
        "TTS-Portuguese Corpus": "Used to train and evaluate speech synthesis models for Brazilian Portuguese, focusing on a single speaker with approximately 10 hours of 48KHz audio."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation clearly mentions a specific dataset, TTS-Portuguese Corpus, which is used for speech synthesis in Brazilian Portuguese.",
      "processing_time": 37.97572445869446,
      "citing_paper_id": "244908340",
      "cited_paper_id": 235652454
    },
    {
      "context_text": "Also, inspired by [26], we investigated Speaker Consistency Loss (SCL) in the final loss.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Speaker Consistency Loss).",
      "processing_time": 28.701810598373413,
      "citing_paper_id": "244908340",
      "cited_paper_id": 239713366
    },
    {
      "context_text": "Voice conversion is yet another task that may benefit from multilingual contexts (Choi et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task (voice conversion) and a potential benefit (multilingual contexts).",
      "processing_time": 31.29859471321106,
      "citing_paper_id": "244908340",
      "cited_paper_id": 239998228
    },
    {
      "context_text": "Dijkstra (2005) recently demonstrated that smoking cessation materials that were minimally tailored using mostly personal identiﬁcation and demographic information led to greater self-reported quitting among college students than did more elaborately tailored materials.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the effectiveness of tailored smoking cessation materials but does not reference a named dataset.",
      "processing_time": 31.841196060180664,
      "citing_paper_id": "143407414",
      "cited_paper_id": 3384767
    },
    {
      "context_text": "By providing content of interest or concern to speciﬁc individuals, readers should be motivated to pay closer attention to the information, process it more carefully, and be more likely to use it to make decisions and take actions to improve health (Kreuter et al., 1999).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of tailoring print materials. No verifiable resources are identified.",
      "processing_time": 31.50005793571472,
      "citing_paper_id": "143407414",
      "cited_paper_id": 3679370
    },
    {
      "context_text": "Although tailored interventions now have been developed for a wide range of health problems, including injury prevention and immunization, most early tailoring studies addressed outcomes related to health promotion, prevention, and early detection of cancer (Kreuter et al., 1999).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general outcomes of tailoring studies. No clear, verifiable resource is identified.",
      "processing_time": 31.836998462677002,
      "citing_paper_id": "143407414",
      "cited_paper_id": 3679370
    },
    {
      "context_text": "…motivation and ability, people are active information processors—considering messages carefully, relating them to other information they have encountered, and comparing them to their own past experiences (Cacioppo, Harkins, & Petty, 1981; Petty, 2006; Petty, Cacioppo, Strathman, & Priester, 1994).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical works on persuasion and information processing.",
      "processing_time": 30.639722108840942,
      "citing_paper_id": "143407414",
      "cited_paper_id": 3955920
    },
    {
      "context_text": "In a multicomponent intervention, Campbell et al. (1999) provided tailored church bulletins to promote fruit and vegetable consumption to African American churchgoers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a study using tailored church bulletins but does not reference a named dataset.",
      "processing_time": 33.78825259208679,
      "citing_paper_id": "143407414",
      "cited_paper_id": 9924963
    },
    {
      "context_text": "There is little evidence to support the efficacy of HRAs in promoting behavior change ( Kreuter & Strecher, 1996 ).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a study. No verifiable resources are identified.",
      "processing_time": 35.004581689834595,
      "citing_paper_id": "143407414",
      "cited_paper_id": 15051757
    },
    {
      "context_text": "Most but not all THC studies show that THCs indeed lead to these expected outcomes (e.g., Brug et al., 1999; Campbell et al., 1994; Kreuter & Strecher, 1996; Prochaska et al., 1993; Rimer et al., 2002; Skinner et al., 1994).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 36.57621502876282,
      "citing_paper_id": "143407414",
      "cited_paper_id": 15051757
    },
    {
      "context_text": "Most but not all THC studies show that THCs indeed lead to these expected outcomes (e.g., Brug et al., 1999; Campbell et al., 1994; Kreuter & Strecher, 1996; Prochaska et al., 1993; Rimer et al., 2002; Skinner et al., 1994).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 36.57621502876282,
      "citing_paper_id": "143407414",
      "cited_paper_id": 28326665
    },
    {
      "context_text": "Most but not all THC studies show that THCs indeed lead to these expected outcomes (e.g., Brug et al., 1999; Campbell et al., 1994; Kreuter & Strecher, 1996; Prochaska et al., 1993; Rimer et al., 2002; Skinner et al., 1994).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 36.57621502876282,
      "citing_paper_id": "143407414",
      "cited_paper_id": 33538988
    },
    {
      "context_text": "There is little evidence to support the efﬁcacy of HRAs in promoting behavior change (Kreuter & Strecher, 1996).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a study. No verifiable resources are identified.",
      "processing_time": 35.61113500595093,
      "citing_paper_id": "143407414",
      "cited_paper_id": 15051757
    },
    {
      "context_text": "Most but not all THC studies show that THCs indeed lead to these expected outcomes (e.g., Brug et al., 1999; Campbell et al., 1994;  Kreuter & Strecher, 1996;  Prochaska et al., 1993; Rimer et al., 2002; Skinner et al., 1994).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies. There are no clear identifiers for datasets, and the context is focused on findings rather than reusable resources.",
      "processing_time": 38.32679581642151,
      "citing_paper_id": "143407414",
      "cited_paper_id": 15051757
    },
    {
      "context_text": "Williams-Piehota, Pizarr, Schneider, Mowad, and Salovey (2005)  found that a minimal intervention that involved brief counseling and a tailored brochure, matched/mismatched to individuals’ monitor blunting styles, increased self-reported mammography use 6 months after the intervention for blunters but not monitors.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a study involving a minimal intervention with tailored brochures. No clear, verifiable dataset is referenced.",
      "processing_time": 38.5515341758728,
      "citing_paper_id": "143407414",
      "cited_paper_id": 16556872
    },
    {
      "context_text": "Only a few authors have provided any data on cost-effectiveness (Lipkus, Rimer, Halabi, & Strigo, 2000; Saywell, et al., 2003).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'cost-effectiveness' but does not specify any datasets. The cited papers are about interventions to increase mammography adherence, which do not mention specific datasets.",
      "processing_time": 39.91075611114502,
      "citing_paper_id": "143407414",
      "cited_paper_id": 21777923
    },
    {
      "context_text": "Only a few authors have provided any data on cost-effectiveness (Lipkus, Rimer, Halabi, & Strigo, 2000; Saywell, et al., 2003).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'cost-effectiveness' but does not specify any datasets. The cited papers are about interventions to increase mammography adherence, which do not mention specific datasets.",
      "processing_time": 39.91075611114502,
      "citing_paper_id": "143407414",
      "cited_paper_id": 34344802
    },
    {
      "context_text": "This approach to tailoring has been labeled behavioral construct tailoring (Kreuter et al., 2004).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach called 'behavioral construct tailoring'. No verifiable resources are identified.",
      "processing_time": 39.158963203430176,
      "citing_paper_id": "143407414",
      "cited_paper_id": 22193473
    },
    {
      "context_text": "This approach to tailoring has been labeled behavioral construct tailoring (Kreuter et al., 2004).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach called 'behavioral construct tailoring'. No verifiable resources are identified.",
      "processing_time": 39.158963203430176,
      "citing_paper_id": "143407414",
      "cited_paper_id": 33470356
    },
    {
      "context_text": "A loss-framed message could highlight the consequences of not being screened, using individualized information about the person to make the case more salient and compelling and to facilitate behavior change (Salovey, Schneider, & Bailey, 1999).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion on persuasion for cancer risk reduction.",
      "processing_time": 12.85364055633545,
      "citing_paper_id": "143407414",
      "cited_paper_id": 22940618
    },
    {
      "context_text": "Clear Horizons , developed by Rimer et al. (1994), used a similar approach and was signiﬁcantly more effective than generic smoking cessation materials for smokers aged 50 and older.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a study and its effectiveness but does not provide details on datasets used.",
      "processing_time": 39.64396524429321,
      "citing_paper_id": "143407414",
      "cited_paper_id": 25093769
    },
    {
      "context_text": "…for blue-collar and minority smokers (Strecher, Rimer, & Monaco, 1989), African Americans (Robinson, Orleans, James, & Sutton, 1992), older smokers (Rimer et al., 1994), pregnant women (S. W. Davis, Cummings, Rimer, Sciandra, & Stone, 1992), and women with young children (Keintz, Fleisher, &…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies involving specific demographics. No clear, verifiable datasets are identified.",
      "processing_time": 39.162879943847656,
      "citing_paper_id": "143407414",
      "cited_paper_id": 25093769
    },
    {
      "context_text": "Another was the growing use of the Transtheoretical Model that described ‘‘stages of change’’ in an individual’s readiness to adopt or modify health behaviors (Prochaska, 2004; Prochaska, DiClemente, Velicer, & Rossi, 1993; Pro-chaska et al., 2002).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to the Transtheoretical Model and stages of change in health behaviors.",
      "processing_time": 39.62565016746521,
      "citing_paper_id": "143407414",
      "cited_paper_id": 28326665
    },
    {
      "context_text": "In most early studies of THC, stage of readiness was a central tailoring variable (Brug, Campbell, & van Assema, 1999; Campbell et al., 1994; Prochaska, et al., 1993; Rakowski et al., 2003; Rimer et al., 2001; Skinner, Strecher, & Hospers, 1994; Strecher, 1999).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies. No clear, verifiable datasets are identified.",
      "processing_time": 39.39588141441345,
      "citing_paper_id": "143407414",
      "cited_paper_id": 28326665
    },
    {
      "context_text": "In most early studies of THC, stage of readiness was a central tailoring variable (Brug, Campbell, & van Assema, 1999; Campbell et al., 1994; Prochaska, et al., 1993; Rakowski et al., 2003; Rimer et al., 2001; Skinner, Strecher, & Hospers, 1994; Strecher, 1999).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to studies. No clear, verifiable datasets are identified.",
      "processing_time": 39.39588141441345,
      "citing_paper_id": "143407414",
      "cited_paper_id": 33538988
    },
    {
      "context_text": "However, an important strength of the THC literature is use of process measures, such as those developed by Brug et al. (1999).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to process measures developed by Brug et al. (1999).",
      "processing_time": 40.82114839553833,
      "citing_paper_id": "143407414",
      "cited_paper_id": 33538988
    },
    {
      "context_text": "In most cases, however, the behavior change recommendations provided to HRA users had no explicit basis in theories of health behavior change (Becker & Janz, 1987).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a general observation about health risk appraisal.",
      "processing_time": 39.911622524261475,
      "citing_paper_id": "143407414",
      "cited_paper_id": 37483932
    },
    {
      "context_text": "Such an approach to message development would also be congruent with theories, such as Goal Directed Behavior (Perugini & Bagozzi, 2001) that focus on actions required to translate goals into behaviors.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 39.6321496963501,
      "citing_paper_id": "143407414",
      "cited_paper_id": 39958897
    },
    {
      "context_text": "Moreover, as Abrams, Leslie, Mermelstein, Kobus, and Clayton (2003) have argued, if tailoring permits greater population reach by getting the attention of intended recipients in a way that generic materials are unlikely to achieve, THCs could be more cost effective.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the potential benefits of tailored health communications (THCs).",
      "processing_time": 38.75496459007263,
      "citing_paper_id": "143407414",
      "cited_paper_id": 41939070
    },
    {
      "context_text": "Fishbein and colleagues (Institute of Medicine, 2003) argued that although behavioral prediction theories can help to identify speciﬁc attitudes and beliefs that may be impediments to behavior change, they do not tell us how to formulate effective messages to change these attitudes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and communication strategies.",
      "processing_time": 37.52559280395508,
      "citing_paper_id": "143407414",
      "cited_paper_id": 68783142
    },
    {
      "context_text": "…and testing messages in this manner has not been an explicit strategy for most THC programs, although it is consistent with some behavior change theories used for tailoring, for example, SOC (Prochaska, 2004; Prochaska et al., 2002) and Precaution Adoption Model (Weinstein & Sandman, 2002).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 41.17376232147217,
      "citing_paper_id": "143407414",
      "cited_paper_id": 80825276
    },
    {
      "context_text": "Such approaches are consistent with the Transtheoretical Model of Change (Prochaska et al., 2002) in helping people change the balance of pros and cons and also with ELM in encouraging more effortful processing (Petty et al., 1995).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical models. No verifiable resources are identified.",
      "processing_time": 39.625328063964844,
      "citing_paper_id": "143407414",
      "cited_paper_id": 80825276
    },
    {
      "context_text": "Such approaches are consistent with the Transtheoretical Model of Change (Prochaska et al., 2002) in helping people change the balance of pros and cons and also with ELM in encouraging more effortful processing (Petty et al., 1995).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical models. No verifiable resources are identified.",
      "processing_time": 39.625328063964844,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "Decisions about what information was provided to which groups usually was based on audience research guided by theories used most frequently in that period (e.g., stages of change [SOC; Prochaska, 2004; Prochaska et al., 2002] and the Health Belief Model [Janz, Cham-pion, & Strecher, 2002]).",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 41.51836156845093,
      "citing_paper_id": "143407414",
      "cited_paper_id": 80825276
    },
    {
      "context_text": "…of the heterogeneity within mass audiences and learned more about methods used in communication and other fields to create persuasive messages (Prochaska, Redding, & Evers, 2002; Rimer & Glassman, 1998; Slater & Flora, 1991), they began to develop different versions of materials for distinct…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses methods and approaches in communication fields but does not reference any named datasets.",
      "processing_time": 42.020707845687866,
      "citing_paper_id": "143407414",
      "cited_paper_id": 80825276
    },
    {
      "context_text": "This explanation of tailoring effects—that behavior change occurs through increasing motivation to process information—is consistent with Petty and Cacioppo’s Elaboration Likelihood Model (ELM; Petty & Cacioppo, 1981).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts. There are no verifiable resources or datasets mentioned.",
      "processing_time": 40.816906213760376,
      "citing_paper_id": "143407414",
      "cited_paper_id": 142544900
    },
    {
      "context_text": "This explanation of tailoring effects—that behavior change occurs through increasing motivation to process information—is consistent with Petty and Cacioppo’s Elaboration Likelihood Model (ELM; Petty & Cacioppo, 1981).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and concepts. There are no verifiable resources or datasets mentioned.",
      "processing_time": 40.816906213760376,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "…& Williams, 1989) MODE model, an integrative dual-process framework of the attitude–behavior relationship, expands upon ideas introduced in ELM (Petty, Haugtvedt, & Smith, 1995) and is complementary to theories of behavior change, such as the Theory of Reasoned Action and the Theory of Planned…",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 42.00210785865784,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "Attitudes that are based on thoughtful deliberation (and/or personal experience or repeated expression) are more accessible (Petty et al., 1995).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a theoretical concept about attitude strength.",
      "processing_time": 39.876506090164185,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "ELM asserts that under certain conditions like elevated motivation and ability, people are active information processors—considering messages carefully, relating them to other information they have encountered, and comparing them to their own past experiences (Cacioppo, Harkins, & Petty, 1981; Petty, 2006; Petty, Cacioppo, Strathman, & Priester, 1994).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical constructs and findings. The context is about the Elaboration Likelihood Model (ELM) and its implications for attitude strength.",
      "processing_time": 44.37925744056702,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "Fazio’s (Fazio & Towles-Schwen, 1999; Fazio, Powell, & Williams, 1989) MODE model, an integrative dual-process framework of the attitude–behavior relationship, expands upon ideas introduced in ELM (Petty, Haugtvedt, & Smith, 1995) and is complementary to theories of behavior change, such as the Theory of Reasoned Action and the Theory of Planned Behavior (Montano & Kasprzyk, 2002).",
      "catation_intent": "research work",
      "resource_type": "theory",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and frameworks. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 42.54694056510925,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": "Using ELM as an example, tailored messages also could be developed to increase time spent thinking about a topic, with the assumption that more effortful thinking is more likely to lead to behavior change.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework (ELM). No verifiable resources are identified.",
      "processing_time": 41.50431728363037,
      "citing_paper_id": "143407414",
      "cited_paper_id": 204380072
    },
    {
      "context_text": ", 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. No dataset names are present in the text.",
      "processing_time": 41.76149559020996,
      "citing_paper_id": "251253049",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 43.5089430809021,
      "citing_paper_id": "251253049",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 43.5089430809021,
      "citing_paper_id": "251253049",
      "cited_paper_id": 91183909
    },
    {
      "context_text": "Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al., 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names either.",
      "processing_time": 43.5089430809021,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231592822
    },
    {
      "context_text": "An encoder E learns to map images x ∈ D x into a spatial latent code z = E ( x ) , regularized through either a KL-divergence loss or through vector quantization (Van Den Oord et al., 2017; Agustsson et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are about vector quantization techniques, not datasets.",
      "processing_time": 42.5223228931427,
      "citing_paper_id": "251253049",
      "cited_paper_id": 9176830
    },
    {
      "context_text": "An encoder E learns to map images x ∈ Dx into a spatial latent code z = E(x), regularized through either a KL-divergence loss or through vector quantization (Van Den Oord et al., 2017; Agustsson et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the technical aspects of the encoder and regularization techniques.",
      "processing_time": 15.349832534790039,
      "citing_paper_id": "251253049",
      "cited_paper_id": 9176830
    },
    {
      "context_text": "Manipulating images with generative networks often requires one to find a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016; Xia et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and processes. The context is about generative networks and image manipulation, which does not align with the topic of personalized text generation.",
      "processing_time": 50.77009081840515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 14924561
    },
    {
      "context_text": "Manipulating images with generative networks often requires one to ﬁnd a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016; Xia et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the process of inversion in generative networks. No verifiable resources are identified.",
      "processing_time": 15.346155643463135,
      "citing_paper_id": "251253049",
      "cited_paper_id": 14924561
    },
    {
      "context_text": "Manipulating images with generative networks often requires one to ﬁnd a corresponding latent representation of the given image, a process referred to as inversion (Zhu et al., 2016; Xia et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the process of inversion in generative networks. No verifiable resources are identified.",
      "processing_time": 15.346155643463135,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231603119
    },
    {
      "context_text": "Personalized models are typically found in the realms of recommendation systems (Ben-hamdi et al., 2017; Amat et al., 2018; Martinez et al., 2009; Cho et al., 2002) or in federated learning (Mansour et al., 2020; Jiang et al., 2019; Fallah et al., 2020; Shamsian et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as recommendation systems and federated learning. No verifiable resources are identified.",
      "processing_time": 18.411263942718506,
      "citing_paper_id": "251253049",
      "cited_paper_id": 52895470
    },
    {
      "context_text": "Personalized models are typically found in the realms of recommendation systems (Ben-hamdi et al., 2017; Amat et al., 2018; Martinez et al., 2009; Cho et al., 2002) or in federated learning (Mansour et al., 2020; Jiang et al., 2019; Fallah et al., 2020; Shamsian et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as recommendation systems and federated learning. No verifiable resources are identified.",
      "processing_time": 18.411263942718506,
      "citing_paper_id": "251253049",
      "cited_paper_id": 203591432
    },
    {
      "context_text": "Personalized models are typically found in the realms of recommendation systems (Ben-hamdi et al., 2017; Amat et al., 2018; Martinez et al., 2009; Cho et al., 2002) or in federated learning (Mansour et al., 2020; Jiang et al., 2019; Fallah et al., 2020; Shamsian et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as recommendation systems and federated learning. No verifiable resources are identified.",
      "processing_time": 18.411263942718506,
      "citing_paper_id": "251253049",
      "cited_paper_id": 211171538
    },
    {
      "context_text": "Personalized models are typically found in the realms of recommendation systems (Ben-hamdi et al., 2017; Amat et al., 2018; Martinez et al., 2009; Cho et al., 2002) or in federated learning (Mansour et al., 2020; Jiang et al., 2019; Fallah et al., 2020; Shamsian et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as recommendation systems and federated learning. No verifiable resources are identified.",
      "processing_time": 18.411263942718506,
      "citing_paper_id": "251253049",
      "cited_paper_id": 211296702
    },
    {
      "context_text": "Personalized models are typically found in the realms of recommendation systems (Benhamdi et al., 2017; Amat et al., 2018; Martinez et al., 2009; Cho et al., 2002) or in federated learning (Mansour et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as recommendation systems and federated learning.",
      "processing_time": 17.20359706878662,
      "citing_paper_id": "251253049",
      "cited_paper_id": 52895470
    },
    {
      "context_text": "Typical text encoder models, such as BERT, begin with a text processing step (Figure 2, left).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT) which is excluded according to the rules.",
      "processing_time": 15.781315565109253,
      "citing_paper_id": "251253049",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Here, c θ is realized through a BERT (Devlin et al., 2018) text encoder, with y being a text prompt.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT, which is a model, not a dataset. No specific datasets are mentioned.",
      "processing_time": 25.279157876968384,
      "citing_paper_id": "251253049",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Typically, a conditional model is trained to reproduce samples from given paired image-caption datasets (Zhu et al., 2019; Tao et al., 2020), leveraging attention mechanisms (Xu et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'paired image-caption datasets' but does not specify any particular dataset name. The reference to Zhu et al. and Tao et al. suggests the use of such datasets, but no specific names are provided.",
      "processing_time": 20.768962144851685,
      "citing_paper_id": "251253049",
      "cited_paper_id": 91183909
    },
    {
      "context_text": "We represent a new embedding vector with a new pseudo-word (Rathvon, 2004) which we denote by S∗.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving a new pseudo-word representation.",
      "processing_time": 27.31934356689453,
      "citing_paper_id": "251253049",
      "cited_paper_id": 142861440
    },
    {
      "context_text": "There, it is typical to delicately tune a model to better reconstruct specific faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model tuning techniques. No verifiable resources are identified.",
      "processing_time": 16.17887282371521,
      "citing_paper_id": "251253049",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "There, it is typical to delicately tune a model to better reconstruct specific faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model tuning techniques. No verifiable resources are identified.",
      "processing_time": 16.17887282371521,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "There, it is typical to delicately tune a model to better reconstruct specific faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model tuning techniques. No verifiable resources are identified.",
      "processing_time": 16.17887282371521,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "There it is typical to apply a delicate tuning of a generative model to better reconstruct speciﬁc faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Dinh et al., 2022; Cao et al., 2022; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.099862813949585,
      "citing_paper_id": "251253049",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "There it is typical to apply a delicate tuning of a generative model to better reconstruct speciﬁc faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Dinh et al., 2022; Cao et al., 2022; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.099862813949585,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "There it is typical to apply a delicate tuning of a generative model to better reconstruct speciﬁc faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Dinh et al., 2022; Cao et al., 2022; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.099862813949585,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244772984
    },
    {
      "context_text": "There it is typical to apply a delicate tuning of a generative model to better reconstruct speciﬁc faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Dinh et al., 2022; Cao et al., 2022; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 26.099862813949585,
      "citing_paper_id": "251253049",
      "cited_paper_id": 250956830
    },
    {
      "context_text": "Pivotal Tuning In the ﬁeld of GAN inversion, it has been shown (Roich et al., 2021; Bau et al., 2019) that one may largely avoid the reconstruction-editability tradeoff using a two-stage optimization process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to GAN inversion and optimization processes.",
      "processing_time": 26.808570384979248,
      "citing_paper_id": "251253049",
      "cited_paper_id": 196834421
    },
    {
      "context_text": "Pivotal Tuning In the ﬁeld of GAN inversion, it has been shown (Roich et al., 2021; Bau et al., 2019) that one may largely avoid the reconstruction-editability tradeoff using a two-stage optimization process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to GAN inversion and optimization processes.",
      "processing_time": 26.808570384979248,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span discusses methods for GAN inversion, mentioning optimization-based techniques and encoders. No specific datasets are named or used in the described context.",
      "processing_time": 30.00397753715515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 209377041
    },
    {
      "context_text": "In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span discusses methods for GAN inversion, mentioning optimization-based techniques and encoders. No specific datasets are named or used in the described context.",
      "processing_time": 30.00397753715515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 214743564
    },
    {
      "context_text": "In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span discusses methods for GAN inversion, mentioning optimization-based techniques and encoders. No specific datasets are named or used in the described context.",
      "processing_time": 30.00397753715515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 215548657
    },
    {
      "context_text": "In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span discusses methods for GAN inversion, mentioning optimization-based techniques and encoders. No specific datasets are named or used in the described context.",
      "processing_time": 30.00397753715515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 229212848
    },
    {
      "context_text": "In the GAN literature, this inversion is done through either an optimization-based technique (Abdal et al., 2019, 2020; Zhu et al., 2020b; Gu et al., 2020) or by using an encoder (Richardson et al., 2020; Zhu et al., 2020a; Pidhorskyi et al., 2020; Tov et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span discusses methods for GAN inversion, mentioning optimization-based techniques and encoders. No specific datasets are named or used in the described context.",
      "processing_time": 30.00397753715515,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231802331
    },
    {
      "context_text": "Here, we examine these choices in light of the GAN inversion literature and discover that many core premises (such as a distortion-editability tradeoff (Tov et al., 2021; Zhu et al., 2020b)) also exist in the textual embedding space.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to literature and concepts. No verifiable resources are identified.",
      "processing_time": 13.676191568374634,
      "citing_paper_id": "251253049",
      "cited_paper_id": 214743564
    },
    {
      "context_text": "Here, we examine these choices in light of the GAN inversion literature and discover that many core premises (such as a distortion-editability tradeoff (Tov et al., 2021; Zhu et al., 2020b)) also exist in the textual embedding space.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to literature and concepts. No verifiable resources are identified.",
      "processing_time": 13.676191568374634,
      "citing_paper_id": "251253049",
      "cited_paper_id": 229212848
    },
    {
      "context_text": "Here, we examine these choices in light of the GAN inversion literature and discover that many core premises (such as a distortion-editability tradeoff (Tov et al., 2021; Zhu et al., 2020b)) also exist in the textual embedding space.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to literature and concepts. No verifiable resources are identified.",
      "processing_time": 13.676191568374634,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231802331
    },
    {
      "context_text": ", 2021), a recently introduced class of Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) that operate in the latent space of an autoencoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the use of DDPMs in the latent space of an autoencoder.",
      "processing_time": 18.87134575843811,
      "citing_paper_id": "251253049",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "We implement our method over Latent Diffusion Models (LDMs) (Rombach et al., 2021), a recently introduced class of Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) that operate in the latent space of an autoencoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (LDMs, DDPMs) but does not reference any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 17.165224313735962,
      "citing_paper_id": "251253049",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "We implement our method over Latent Diffusion Models (LDMs) (Rombach et al., 2021), a recently introduced class of Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020) that operate in the latent space of an autoencoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods (LDMs, DDPMs) but does not reference any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 17.165224313735962,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": ", 2018) or cross-modal contrastive approaches (Zhang et al., 2021; Ye et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 17.79687523841858,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231592822
    },
    {
      "context_text": "Progressive extensions We follow Tov et al. (2021) and consider a progressive multi-vector setup.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or setup from another paper.",
      "processing_time": 13.668212652206421,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231802331
    },
    {
      "context_text": "Regularization Tov et al. (2021) observed that latent codes in the space of a GAN have increased editability when they lie closer to the code distribution which was observed during training.",
      "catation_intent": "findings",
      "resource_type": "finding",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or finding related to GANs and latent codes.",
      "processing_time": 28.595600128173828,
      "citing_paper_id": "251253049",
      "cited_paper_id": 231802331
    },
    {
      "context_text": "Additional setups and experiments In appendix E, we consider two additional inversion setups: a pivotal tuning approach (Roich et al., 2021), where the model itself is optimized to improve reconstruction, and DALLE-2 (Ramesh et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on experimental setups and inversion approaches.",
      "processing_time": 27.298296451568604,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "Additional setups In the supplementary, we consider two additional setups for inversion: a pivotal tuning approach (Roich et al., 2021; Bau et al., 2020), where the model itself is optimized to improve reconstruction, and DALLE-2 (Ramesh et al., 2022)’s bipartite inversion process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on describing experimental setups and approaches.",
      "processing_time": 13.376322031021118,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "(Roich et al., 2021) (right). s is the guidance scale.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a guidance scale parameter. The cited paper title does not help in identifying a dataset.",
      "processing_time": 28.59123992919922,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "In addition to the setups outlined in the core paper, we investigated two recent approaches to inversion: Bipartite DDIM-inversion (Ramesh et al., 2022; Dhariwal & Nichol, 2021) and pivotal tuning (Roich et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and research works.",
      "processing_time": 12.184790849685669,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "Prior work has shown that this embedding space is expressive enough to capture basic image semantics (Co-hen et al., 2022; Tsimpoukelli et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior work showing the expressiveness of an embedding space.",
      "processing_time": 17.17049503326416,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235658331
    },
    {
      "context_text": "Prior work has shown that this embedding space is expressive enough to capture basic image semantics (Cohen et al., 2022; Tsimpoukelli et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to prior work showing the expressiveness of an embedding space.",
      "processing_time": 16.82623863220215,
      "citing_paper_id": "251253049",
      "cited_paper_id": 235658331
    },
    {
      "context_text": "Choi et al. (2021) improve inversion by conditioning the denoising process on noised low-pass ﬁlter data from the target image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It only refers to a method or technique using noised low-pass filter data, which is not a named dataset.",
      "processing_time": 16.82548975944519,
      "citing_paper_id": "251253049",
      "cited_paper_id": 236950721
    },
    {
      "context_text": "More measured approaches freeze the model and train transformation modules to adapt its output when faced with new concepts (Zhou et al., 2021; Gao et al., 2021; Skantze & Willemsen, 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on adapting model outputs for new concepts.",
      "processing_time": 13.639085054397583,
      "citing_paper_id": "251253049",
      "cited_paper_id": 237386023
    },
    {
      "context_text": "(2021), which was pre-trained on the LAION-400M dataset (Schuhmann et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-400M"
      ],
      "dataset_descriptions": {
        "LAION-400M": "Used for pre-training models, focusing on image-text pairs filtered by CLIP, enhancing multimodal understanding and generation capabilities."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the LAION-400M dataset, which is a specific, verifiable dataset used for pre-training models.",
      "processing_time": 21.982763051986694,
      "citing_paper_id": "251253049",
      "cited_paper_id": 241033103
    },
    {
      "context_text": "We employ the publicly available 1.4 billion parameter text-to-image model of Rombach et al. (2021), which was pre-trained on the LAION-400M dataset (Schuhmann et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-400M"
      ],
      "dataset_descriptions": {
        "LAION-400M": "Used to pre-train a high-resolution image synthesis model, focusing on generating images from textual descriptions using latent diffusion models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the LAION-400M dataset, which is a specific, verifiable dataset used for pre-training a text-to-image model.",
      "processing_time": 22.855288982391357,
      "citing_paper_id": "251253049",
      "cited_paper_id": 241033103
    },
    {
      "context_text": "We employ the publicly available 1.4 billion parameter text-to-image model of Rombach et al. (2021), which was pre-trained on the LAION-400M dataset (Schuhmann et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-400M"
      ],
      "dataset_descriptions": {
        "LAION-400M": "Used to pre-train a high-resolution image synthesis model, focusing on generating images from textual descriptions using latent diffusion models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the LAION-400M dataset, which is a specific, verifiable dataset used for pre-training a text-to-image model.",
      "processing_time": 22.855288982391357,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "…pure image generation, a large body of work explores the use of text-based interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses various works on text-based interfaces for image editing, generator domain adaptation, and video manipulation, but does not reference any datasets.",
      "processing_time": 19.026070594787598,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "We note that similar curation processes with larger batches are typically employed in text-conditioned generation works (Avrahami et al., 2022b; Ramesh et al., 2021; Yu et al., 2022), and that one can automate this selection process by using CLIP to Figure 9: Our words can be used with downstream…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general processes and methods. No clear, verifiable resource names are provided.",
      "processing_time": 26.06332039833069,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "Here, we perform localized image editing using Blended Latent Diffusion (Avrahami et al., 2022a) rank images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Blended Latent Diffusion).",
      "processing_time": 13.35582685470581,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "Speciﬁcally, we consider the recent Blended Latent Diffusion (Avrahami et al., 2022a) which enables localized text-based editing of images via a mask-based blending process in the latent space of an LDM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Blended Latent Diffusion) for text-based editing of images.",
      "processing_time": 15.29203748703003,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "There it is typical to apply a delicate tuning of a generative model to better reconstruct specific faces or scenes (Bau et al., 2019; Roich et al., 2021; Alaluf et al., 2021; Dinh et al., 2022; Cao et al., 2022; Nitzan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss generative models. No verifiable resources are identified.",
      "processing_time": 15.698916912078857,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244772984
    },
    {
      "context_text": "…domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich et al., 2022), style transfer (Kwon & Ye, 2021; Liu et al., 2022) and even texture synthesis for 3D objects (Michel et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and methods. No verifiable resources are identified.",
      "processing_time": 28.859886407852173,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244773443
    },
    {
      "context_text": "…domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich et al., 2022), style transfer (Kwon & Ye, 2021; Liu et al., 2022) and even texture synthesis for 3D objects (Michel et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and methods. No verifiable resources are identified.",
      "processing_time": 28.859886407852173,
      "citing_paper_id": "251253049",
      "cited_paper_id": 247157966
    },
    {
      "context_text": "…interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich et al., 2022),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and applications in image and video manipulation.",
      "processing_time": 14.03685998916626,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244909410
    },
    {
      "context_text": "…interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich et al., 2022),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and applications in image and video manipulation.",
      "processing_time": 14.03685998916626,
      "citing_paper_id": "251253049",
      "cited_paper_id": 250956830
    },
    {
      "context_text": ", 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 28.224645853042603,
      "citing_paper_id": "251253049",
      "cited_paper_id": 244909410
    },
    {
      "context_text": "Such guidance-dependent structure drift has also been demonstrated for GLIDE (Nichol et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GLIDE but does not refer to it as a dataset. It is a method or model, not a reusable dataset.",
      "processing_time": 16.372411012649536,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "More recently, impressive visual results were achieved by leveraging large scale auto-regressive (Ramesh et al., 2021; Yu et al., 2022) or diffusion models (Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021; Rombach et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 28.85404658317566,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "More recently, impressive visual results were achieved by leveraging large scale auto-regressive (Ramesh et al., 2021; Yu et al., 2022) or diffusion models (Ramesh et al., 2022; Saharia et al., 2022; Nichol et al., 2021; Rombach et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 28.85404658317566,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Note that our method does not involve the direct optimization of the CLIP-based objective score and, as such, is not sensitive to the adversarial scoring ﬂaws outlined by Nichol et al. (2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method and a reference to a paper discussing adversarial scoring flaws.",
      "processing_time": 15.279916048049927,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Large-scale text-to-image models (Rombach et al., 2021; Ramesh et al., 2021; 2022; Nichol et al., 2021; Yu et al., 2022; Saharia et al., 2022) have demonstrated an unprecedented capability to reason over natural language descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. The context is about the capabilities of large-scale text-to-image models, not the datasets they were trained or evaluated on.",
      "processing_time": 31.352174758911133,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Below, we outline the core details of applying our approach to a speciﬁc class of generative models — Latent Diffusion Models (Rombach et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Latent Diffusion Models).",
      "processing_time": 14.340427160263062,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Unless otherwise noted, we retain the original hyper-parameter choices of LDM (Rombach et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a method (LDM) and its hyper-parameter choices.",
      "processing_time": 27.836748838424683,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Our approach was implemented over LDM (Rombach et al., 2021), the largest publicly available text-to-image model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LDM, which is a model, not a dataset. No datasets are explicitly mentioned or used in the described research context.",
      "processing_time": 17.13416862487793,
      "citing_paper_id": "251253049",
      "cited_paper_id": 245335280
    },
    {
      "context_text": ", 2022), style transfer (Kwon & Ye, 2021; Liu et al., 2022) and even texture synthesis for 3D objects (Michel et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. No verifiable resources are identified.",
      "processing_time": 27.832919597625732,
      "citing_paper_id": "251253049",
      "cited_paper_id": 247157966
    },
    {
      "context_text": "We ﬁnd a new pseudo-word using their approach and use it to synthesize new images by leveraging VQGAN-CLIP (Crowson et al., 2022) and CLIP-Guided Diffusion (Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (VQGAN-CLIP and CLIP-Guided Diffusion) rather than datasets.",
      "processing_time": 19.789429903030396,
      "citing_paper_id": "251253049",
      "cited_paper_id": 248239727
    },
    {
      "context_text": "We ﬁnd a new pseudo-word using their approach and use it to synthesize new images by leveraging VQGAN-CLIP (Crowson et al., 2022) and CLIP-Guided Diffusion (Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (VQGAN-CLIP and CLIP-Guided Diffusion) rather than datasets.",
      "processing_time": 19.789429903030396,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "We ﬁnd a new pseudo-word using their approach and use it to synthesize new images by leveraging VQGAN-CLIP (Crowson et al., 2022) and CLIP-Guided Diffusion (Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and models (VQGAN-CLIP and CLIP-Guided Diffusion) rather than datasets.",
      "processing_time": 19.789429903030396,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "We find a new pseudo-word using their approach and use it to synthesize new images with VQGAN-CLIP (Crowson et al., 2022) and CLIP-Guided Diffusion (Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on synthesizing new images using VQGAN-CLIP and CLIP-Guided Diffusion.",
      "processing_time": 31.674678564071655,
      "citing_paper_id": "251253049",
      "cited_paper_id": 248239727
    },
    {
      "context_text": "As a second baseline, we apply the CLIP-guided models of Crowson et al . while trying to jointly minimize the CLIP-based distances to both the training set images and to the target text (VQGAN-CLIP) or by initializing the optimization with an input image from our set (Guided Diffusion).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to models and methods, which are excluded according to the instructions.",
      "processing_time": 17.477681159973145,
      "citing_paper_id": "251253049",
      "cited_paper_id": 248239727
    },
    {
      "context_text": "Alternatively, test-time optimization can be used to explore the latent space of pre-trained generators (Crowson et al., 2022; Murdock, 2021; Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is about using test-time optimization to explore the latent space of pre-trained generators.",
      "processing_time": 15.70333743095398,
      "citing_paper_id": "251253049",
      "cited_paper_id": 248239727
    },
    {
      "context_text": "Here, we observe that when using LDM’s typical guidance (Ho & Salimans, 2021) scales ( 5 - 10 ), the denoiser network is unable to maintain the original object’s structure through prompt changes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LDM’s typical guidance) and a model (denoiser network).",
      "processing_time": 14.861316442489624,
      "citing_paper_id": "251253049",
      "cited_paper_id": 249145348
    },
    {
      "context_text": "Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and ﬁne-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general problem of re-training and fine-tuning models.",
      "processing_time": 13.320810556411743,
      "citing_paper_id": "251253049",
      "cited_paper_id": 250644432
    },
    {
      "context_text": "Re-training a model with an expanded dataset for each new concept is prohibitively expensive, and fine-tuning on few examples typically leads to catastrophic forgetting (Ding et al., 2022; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about re-training and fine-tuning models.",
      "processing_time": 13.31837272644043,
      "citing_paper_id": "251253049",
      "cited_paper_id": 250644432
    },
    {
      "context_text": "…use of text-based interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and applications. There are no clear identifiers for datasets within the text.",
      "processing_time": 15.254594802856445,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "…use of text-based interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and applications. There are no clear identifiers for datasets within the text.",
      "processing_time": 15.254594802856445,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "Moving beyond pure image generation, a large body of work explores the use of text-based interfaces for image editing (Patashnik et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to a large body of work and various methods for image and video manipulation.",
      "processing_time": 17.116541385650635,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "Rather than training conditional models, several approaches employ test-time optimization to explore the latent spaces of a pre-trained generator (Crowson et al., 2022; Murdock, 2021; Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 14.832200050354004,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "Rather than training conditional models, several approaches employ test-time optimization to explore the latent spaces of a pre-trained generator (Crowson et al., 2022; Murdock, 2021; Crowson, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 14.832200050354004,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided citation span does not mention any specific datasets, only references to papers on video manipulation and motion synthesis.",
      "processing_time": 14.60568356513977,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "…et al., 2021; Abdal et al., 2021; Avrahami et al., 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022), video manipulation (Tzaban et al., 2022; Bar-Tal et al., 2022), motion synthesis (Tevet et al., 2022; Petrovich et al., 2022), style transfer (Kwon & Ye, 2021; Liu et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 17.44468855857849,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2022) and CLIP-Guided Diffusion (Crowson, 2021).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 16.091758966445923,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "These tools have been used for artistic creation, as sources of inspiration, and even to design new, physical products (Yacoubian, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to general uses of tools in artistic creation and design.",
      "processing_time": 16.76666498184204,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2022b), generator domain adaptation (Gal et al., 2021; Kim et al., 2022) and style transfer (Kwon & Ye, 2021; Liu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 17.10535454750061,
      "citing_paper_id": "251253049",
      "cited_paper_id": null
    },
    {
      "context_text": "Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neural language models and their impact on statistical machine translation.",
      "processing_time": 16.33122944831848,
      "citing_paper_id": "2955580",
      "cited_paper_id": 1245593
    },
    {
      "context_text": "…introduces two persona-based models: the Speaker Model, which models the personality of the respondent, and the Speaker-Addressee Model which models the way the respondent adapts their speech to a given addressee — a linguistic phenomenon known as lexical entrainment (Deutsch and Pechmann, 1982).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and linguistic phenomena. No verifiable resources are identified.",
      "processing_time": 16.06864619255066,
      "citing_paper_id": "2955580",
      "cited_paper_id": 2683901
    },
    {
      "context_text": "…introduces two persona-based models: the Speaker Model, which models the personality of the respondent, and the Speaker-Addressee Model which models the way the respondent adapts their speech to a given addressee — a linguistic phenomenon known as lexical entrainment (Deutsch and Pechmann, 1982).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and linguistic phenomena. No verifiable resources are identified.",
      "processing_time": 16.06864619255066,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15535101
    },
    {
      "context_text": "Our work introduces two persona-based models: the Speaker Model, which models the personality of the respondent, and the Speaker-Addressee Model which models the way the respondent adapts their speech to a given addressee — a linguistic phenomenon known as lexical entrainment (Deutsch and Pechmann, 1982).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and linguistic phenomena. The context focuses on introducing persona-based models and describing lexical entrainment.",
      "processing_time": 18.56031608581543,
      "citing_paper_id": "2955580",
      "cited_paper_id": 2683901
    },
    {
      "context_text": "et al., 2016) we used BLEU (Papineni et al., 2002) for parameter tuning and evaluation. BLEU has been shown to correlate well with human judgment on the response generation task, as demonstrated in (Galley et al., 2015). Besides BLEU scores, we also report perplexity as an indicator of model capability. 6.2 Baseline Since our main experiments are with a new dataset (the Twitter Persona Dataset), we ﬁrst show that ou",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter Persona Dataset"
      ],
      "dataset_descriptions": {
        "Twitter Persona Dataset": "Used to evaluate personalized text generation models, focusing on generating responses that align with user personas. The dataset provides context for tuning and evaluating model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a new dataset called 'Twitter Persona Dataset' which is relevant to personalized text generation. No other specific datasets are mentioned.",
      "processing_time": 23.450896739959717,
      "citing_paper_id": "2955580",
      "cited_paper_id": 6078795
    },
    {
      "context_text": "BLEU has been shown to correlate well with human judgment on the response generation task, as demonstrated in (Galley et al., 2015).",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (BLEU) which is excluded according to the instructions.",
      "processing_time": 25.185022830963135,
      "citing_paper_id": "2955580",
      "cited_paper_id": 6078795
    },
    {
      "context_text": "Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SEQ2SEQ models for generating responses in conversation systems.",
      "processing_time": 27.54200506210327,
      "citing_paper_id": "2955580",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Other researchers have recently used SEQ2SEQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of SEQ2SEQ models for generating responses in conversation systems.",
      "processing_time": 27.54200506210327,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Fortunately, neural models of conversation generation (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016) provide a straightforward mechanism for incorporating personas as embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural models and methods for conversation generation. No verifiable resources are identified.",
      "processing_time": 25.98710870742798,
      "citing_paper_id": "2955580",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Fortunately, neural models of conversation generation (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016) provide a straightforward mechanism for incorporating personas as embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural models and methods for conversation generation. No verifiable resources are identified.",
      "processing_time": 25.98710870742798,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the use of Seq2Seq models for generating responses in conversational systems.",
      "processing_time": 28.50787878036499,
      "citing_paper_id": "2955580",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the use of Seq2Seq models for generating responses in conversational systems.",
      "processing_time": 28.50787878036499,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Fortunately, neural models of conversation generation (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016) provide a straightforward mechanism for incorporating per-sonas as embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural models and methods for conversation generation. No verifiable resources are identified.",
      "processing_time": 26.705522298812866,
      "citing_paper_id": "2955580",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Fortunately, neural models of conversation generation (Sordoni et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2016) provide a straightforward mechanism for incorporating per-sonas as embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural models and methods for conversation generation. No verifiable resources are identified.",
      "processing_time": 26.705522298812866,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Yao et al. (2015) employ an intention network to maintain the relevance of responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (intention network).",
      "processing_time": 25.613385438919067,
      "citing_paper_id": "2955580",
      "cited_paper_id": 7838171
    },
    {
      "context_text": "As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. No dataset names are provided in the context.",
      "processing_time": 27.51919960975647,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "(Vinyals and Le, 2015) suggest that the lack of a coherent personality makes it impossible for current systems to pass the Turing test.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of current systems in passing the Turing test.",
      "processing_time": 12.848775625228882,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "The present work, by contrast, is in the vein of the S EQ 2S EQ models of Vinyals and Le (2015) and Li et al. (2016), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'conversational data' but does not specify a named dataset. It focuses on the methodology of training persona vectors from conversational data and side-information.",
      "processing_time": 30.855870723724365,
      "citing_paper_id": "2955580",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No verifiable resources are identified.",
      "processing_time": 15.216995000839233,
      "citing_paper_id": "2955580",
      "cited_paper_id": 13302682
    },
    {
      "context_text": "eak with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li,",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in dialog systems research.",
      "processing_time": 15.656299352645874,
      "citing_paper_id": "2955580",
      "cited_paper_id": 13302682
    },
    {
      "context_text": "nsively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation.",
      "processing_time": 13.030933856964111,
      "citing_paper_id": "2955580",
      "cited_paper_id": 14160709
    },
    {
      "context_text": "…statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation.",
      "processing_time": 15.62296748161316,
      "citing_paper_id": "2955580",
      "cited_paper_id": 14160709
    },
    {
      "context_text": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 15.18839955329895,
      "citing_paper_id": "2955580",
      "cited_paper_id": 14523418
    },
    {
      "context_text": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 15.18839955329895,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15619251
    },
    {
      "context_text": ", 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 15.18839955329895,
      "citing_paper_id": "2955580",
      "cited_paper_id": 42820567
    },
    {
      "context_text": "…et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.277028560638428,
      "citing_paper_id": "2955580",
      "cited_paper_id": 14523418
    },
    {
      "context_text": "…et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.277028560638428,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15619251
    },
    {
      "context_text": "…et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.277028560638428,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15956725
    },
    {
      "context_text": "…et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 18.277028560638428,
      "citing_paper_id": "2955580",
      "cited_paper_id": 42820567
    },
    {
      "context_text": "Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research works. The context is about modeling users and speakers within a dialog modeling framework.",
      "processing_time": 27.760822296142578,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15535101
    },
    {
      "context_text": "r et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce th",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 29.88223648071289,
      "citing_paper_id": "2955580",
      "cited_paper_id": 15956725
    },
    {
      "context_text": "We nevertheless provide this result to give a sense that these B LEU scores of 5-6% are not unreasonable. models using a much larger OpenSubtitles (OSDb) dataset (Tiedemann, 2009), and then adapting the pre-trained model to the TV series dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles (OSDb)"
      ],
      "dataset_descriptions": {
        "OpenSubtitles (OSDb)": "Used for pre-training models before adapting them to a TV series dataset, focusing on improving BLEU scores through large-scale multilingual data."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'OpenSubtitles (OSDb)' as a dataset used for pre-training models. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 23.02097773551941,
      "citing_paper_id": "2955580",
      "cited_paper_id": 17047584
    },
    {
      "context_text": "models using a much larger OpenSubtitles (OSDb) dataset (Tiedemann, 2009), and then adapting the pre-trained model to the TV series dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles (OSDb)"
      ],
      "dataset_descriptions": {
        "OpenSubtitles (OSDb)": "Used for pre-training models on a large multilingual parallel corpus before adapting to a TV series dataset, focusing on improving personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the OpenSubtitles (OSDb) dataset, which is a specific, verifiable resource. It is used for pre-training models before adapting them to a TV series dataset.",
      "processing_time": 24.363789081573486,
      "citing_paper_id": "2955580",
      "cited_paper_id": 17047584
    },
    {
      "context_text": "…typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in spoken dialog systems.",
      "processing_time": 13.017372846603394,
      "citing_paper_id": "2955580",
      "cited_paper_id": 18282840
    },
    {
      "context_text": "(2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and approaches in dialog systems research.",
      "processing_time": 27.495551347732544,
      "citing_paper_id": "2955580",
      "cited_paper_id": 18282840
    },
    {
      "context_text": "Sophisticated sampling methods (Metropolis et al., 1953) can be used to constrain the model generation to certain keywords and topics.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (sampling methods) which is not a dataset.",
      "processing_time": 26.41230845451355,
      "citing_paper_id": "208617790",
      "cited_paper_id": 1046577
    },
    {
      "context_text": "Sophisticated sampling methods(Metropolis et al., 1953) can be used to constrain the model generation to certain keywords and topics.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sampling. The context is about using sampling methods to constrain model generation, which is not directly related to a dataset.",
      "processing_time": 30.55206847190857,
      "citing_paper_id": "208617790",
      "cited_paper_id": 1046577
    },
    {
      "context_text": "We also evaluate attribute control with an external sentiment classifier trained on IMDB movie reviews (Maas et al., 2011), which is a different dataset from the one used to train the attribute model (Socher et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMDB movie reviews"
      ],
      "dataset_descriptions": {
        "IMDB movie reviews": "Used to train an external sentiment classifier for evaluating attribute control in the current research, focusing on sentiment analysis of text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of an external sentiment classifier trained on IMDB movie reviews, which is a specific dataset. The dataset is used to evaluate attribute control in the current research.",
      "processing_time": 23.410861015319824,
      "citing_paper_id": "208617790",
      "cited_paper_id": 1428702
    },
    {
      "context_text": "We also evaluate attribute control with an external sentiment classiﬁer trained on IMDB movie reviews (Maas et al., 2011), which is a different dataset from the one used to train the attribute model (Socher et al., 2013), and the same rough story holds, albeit with smaller gaps between approaches.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMDB movie reviews"
      ],
      "dataset_descriptions": {
        "IMDB movie reviews": "Used to evaluate attribute control with an external sentiment classifier, focusing on sentiment analysis performance and comparing different approaches."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the IMDB movie reviews dataset for evaluating attribute control with an external sentiment classifier. This dataset is clearly identified and used for a specific purpose in the research.",
      "processing_time": 23.917128324508667,
      "citing_paper_id": "208617790",
      "cited_paper_id": 1428702
    },
    {
      "context_text": "Possible approaches to accomplishing this are using REINFORCE (Williams, 1992) and the Gumbel-Softmax trick (Jang et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (REINFORCE and Gumbel-Softmax trick) but does not reference any specific datasets.",
      "processing_time": 15.603015422821045,
      "citing_paper_id": "208617790",
      "cited_paper_id": 2428314
    },
    {
      "context_text": ", 2019); and WD: a weighted decoding baseline in which the B model’s outputs are weighted directly toward maximizing p(a|x) (Ghazvininejad et al., 2017); see Section S6 for details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about different baselines and models used in the research.",
      "processing_time": 14.271296262741089,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3274110
    },
    {
      "context_text": "Further, Ghazvininejad et al. (2017) strongly relies on sampling from a set of keywords on a speciﬁc topic and it does not allow to bias generation towards a topic in a manner that does not necessary include a set of keywords.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses a method for generating poetry using keywords but does not reference a dataset.",
      "processing_time": 28.761480569839478,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3274110
    },
    {
      "context_text": "Weighted decoding Holtzman et al. (2018); Ghazvininejad et al. (2017) consider controlled language generation – the former with discriminators, and the latter with a bag of words – where the decoding procedure is modiﬁed to consider the scoring function used for decoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for controlled language generation.",
      "processing_time": 12.182375431060791,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3274110
    },
    {
      "context_text": "We consider a simple baseline based on a direct integration of the conditioning into the decoding procedure, similar to the approach from Ghazvininejad et al. (2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method from a cited paper. The context is about a baseline approach for conditioning in decoding, which is a methodological detail.",
      "processing_time": 19.70683002471924,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3274110
    },
    {
      "context_text": "Topic Control with Bag of Words In Ghazvininejad et al. (2017), the authors consider increasing the likelihood of sampling from a bag of key-words by performing beam-search with a modiﬁed scoring function. where ✶ BoW ( w i ) is an indicator function indicating if the token w i is present in the…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modifying the scoring function during beam search to increase the likelihood of sampling from a bag of keywords.",
      "processing_time": 16.029386520385742,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3274110
    },
    {
      "context_text": "Controlled generation Current methods for controlled text generation involve either ﬁne-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad-versarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 12.986349105834961,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "…Current methods for controlled text generation involve either ﬁne-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad-versarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 14.782650470733643,
      "citing_paper_id": "208617790",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "…Current methods for controlled text generation involve either ﬁne-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad-versarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 14.782650470733643,
      "citing_paper_id": "208617790",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "…Current methods for controlled text generation involve either ﬁne-tuning existing models with Reinforcement Learning (RL) (Ziegler et al., 2019), training Generative Ad-versarial Networks (Yu et al., 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 14.782650470733643,
      "citing_paper_id": "208617790",
      "cited_paper_id": 11157751
    },
    {
      "context_text": "In earlier works Gu et al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works and theories. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 28.45826005935669,
      "citing_paper_id": "208617790",
      "cited_paper_id": 5038627
    },
    {
      "context_text": "…al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and theories. The Shannon Noisy Channel Theory is a method, not a dataset.",
      "processing_time": 17.039706707000732,
      "citing_paper_id": "208617790",
      "cited_paper_id": 5747983
    },
    {
      "context_text": "…al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and theories. The Shannon Noisy Channel Theory is a method, not a dataset.",
      "processing_time": 17.039706707000732,
      "citing_paper_id": "208617790",
      "cited_paper_id": 196621535
    },
    {
      "context_text": "In earlier works Gu et al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works and theories. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 18.254750728607178,
      "citing_paper_id": "208617790",
      "cited_paper_id": 5747983
    },
    {
      "context_text": "(2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence modeling.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework. There are no verifiable resources or datasets mentioned.",
      "processing_time": 14.805092573165894,
      "citing_paper_id": "208617790",
      "cited_paper_id": 5747983
    },
    {
      "context_text": "We sample from the resulting combined model by following gradients in the latent representation space in a manner inspired by the approximate Metropolis-adjusted Langevin (MALA) (Roberts et al., 1996; Roberts & Rosenthal, 1998) sampler deployed in Nguyen et al. (2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and algorithms. The context focuses on sampling techniques and gradient-based methods.",
      "processing_time": 14.753740549087524,
      "citing_paper_id": "208617790",
      "cited_paper_id": 5831882
    },
    {
      "context_text": "Shen et al. (2017); Hu et al. (2017) train variational auto-encoders for style transfer that rely on learning disentangled latent representations for style and content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the methodology of training variational auto-encoders for style transfer.",
      "processing_time": 17.971731185913086,
      "citing_paper_id": "208617790",
      "cited_paper_id": 7296803
    },
    {
      "context_text": ", 2017), or training conditional generative models (Kikuchi et al., 2016; Ficler & Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods for controlling output length in neural encoder-decoders.",
      "processing_time": 28.126067399978638,
      "citing_paper_id": "208617790",
      "cited_paper_id": 11157751
    },
    {
      "context_text": "This probability can be rewritten in terms of product of conditional probabilities by recursively applying the chain-rule (Manning et al., 1999; Bengio et al., 2003) as: In this paper, we use a transformer (Vaswani et al., 2017) to model the distribution of natural language.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of a transformer model to model the distribution of natural language.",
      "processing_time": 17.98885679244995,
      "citing_paper_id": "208617790",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs) trained on a huge amount of data (Radford et al., 2019; Dai et al., 2019b; Radford et al., 2018b) to greatly improve the state-of-the-art on natural language processing tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 16.291996240615845,
      "citing_paper_id": "208617790",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs) trained on a huge amount of data (Radford et al., 2019; Dai et al., 2019b; Radford et al., 2018b) to greatly improve the state-of-the-art on natural language processing tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 16.291996240615845,
      "citing_paper_id": "208617790",
      "cited_paper_id": 57759363
    },
    {
      "context_text": "In this paper, we use a transformer (Vaswani et al., 2017) to model the distribution of natural language.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (transformer).",
      "processing_time": 13.723411321640015,
      "citing_paper_id": "208617790",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The Transformer architecture (Vaswani et al., 2017) has enabled large-scale language models (LMs) trained on a huge amount of data (Radford et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the Transformer architecture and large-scale language models. No verifiable datasets are referenced.",
      "processing_time": 29.03949213027954,
      "citing_paper_id": "208617790",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Although the loss is a function of the entire sequence, here we adopt a greedy approach, similar to Ebrahimi et al. (2018); Wallace et al. (2019), in which we optimize for Table 5: Sentence samples in triplets, generated by {baseline GPT-2, PPLM-Discrim POSITIVE , PPLM-Discrim NEGATIVE },…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the optimization approach and generated sentence samples.",
      "processing_time": 16.658647775650024,
      "citing_paper_id": "208617790",
      "cited_paper_id": 21698802
    },
    {
      "context_text": "We explore controlled generation for assistive story writing (Peng et al., 2018; Luo et al., 2019; Yao et al., 2019; Fan et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.31014919281006,
      "citing_paper_id": "208617790",
      "cited_paper_id": 44134226
    },
    {
      "context_text": "We explore controlled generation for assistive story writing (Peng et al., 2018; Luo et al., 2019; Yao et al., 2019; Fan et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 29.31014919281006,
      "citing_paper_id": "208617790",
      "cited_paper_id": 53306064
    },
    {
      "context_text": "For all results reported in this section, we use top-k sampling (Fan et al., 2018) with k = 10 to draw from the softmax distribution over the vocabulary.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (top-k sampling).",
      "processing_time": 13.93144154548645,
      "citing_paper_id": "208617790",
      "cited_paper_id": 44134226
    },
    {
      "context_text": "We train a clickbait discriminator using the dataset from Potthast et al. (2018)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Crowdsourcing a Large Corpus of Clickbait on Twitter"
      ],
      "dataset_descriptions": {
        "Crowdsourcing a Large Corpus of Clickbait on Twitter": "Used to train a clickbait discriminator, focusing on identifying clickbait content on Twitter. The dataset provides labeled examples of clickbait and non-clickbait tweets."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset from Potthast et al. (2018), which is likely the 'Crowdsourcing a Large Corpus of Clickbait on Twitter' dataset. The dataset is used to train a clickbait discriminator.",
      "processing_time": 45.178783655166626,
      "citing_paper_id": "208617790",
      "cited_paper_id": 52013710
    },
    {
      "context_text": "A key difference between the above and our approach is that we use an ofﬂine discriminator and perform optimization based on this discriminator, which as suggested by Elazar & Goldberg (2018) may outperform adversarial training approaches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The cited paper title suggests a focus on methods rather than datasets.",
      "processing_time": 17.974761486053467,
      "citing_paper_id": "208617790",
      "cited_paper_id": 52056513
    },
    {
      "context_text": "We then ask annotators to rank the pair on the desired attribute (e.g. topic relevance, sentiment strength), while allowing “neither” and “both” options to account for equally good/bad generations (Lample et al., 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for ranking text pairs on attributes.",
      "processing_time": 26.904086589813232,
      "citing_paper_id": "208617790",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "topic relevance, sentiment strength), while allowing “neither” and “both” options to account for equally good/bad generations (Lample et al., 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating text generation. No dataset names are present in the citation span.",
      "processing_time": 17.62647294998169,
      "citing_paper_id": "208617790",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "More recently, Lample et al. (2019) adapt an approach from unsupervised language translation to style transfer, where a denoised auto-encoder is trained with an objective consisting of a weighted combination of a re-construction loss and a back-translation loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for style transfer using a denoised auto-encoder.",
      "processing_time": 29.026389122009277,
      "citing_paper_id": "208617790",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "Annotators are asked to evaluate the ﬂuency of each individual sample on a scale of 1-5, with 1 being “not ﬂuent at all” and 5 being “very ﬂuent,” as done in Lample et al. (2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for evaluating fluency. The cited paper title 'Multiple-Attribute Text Rewriting' does not provide additional context to identify a dataset.",
      "processing_time": 17.968907356262207,
      "citing_paper_id": "208617790",
      "cited_paper_id": 53334018
    },
    {
      "context_text": "See et al. (2019) note that control with weighted decoding (WD) is difﬁcult and often leads to sacriﬁcing ﬂuency and coherence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (weighted decoding) and its effects on fluency and coherence.",
      "processing_time": 13.700963497161865,
      "citing_paper_id": "208617790",
      "cited_paper_id": 67855999
    },
    {
      "context_text": "For CTRL and WD, since human evaluation is performed in comparison with BCR via A/B testing, we report the numbers for BCR as well from these comparisons, for the human evaluated metrics.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only human evaluations and comparisons between models. No clear, verifiable dataset names are provided.",
      "processing_time": 14.501225471496582,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Further, we consider one sample per preﬁx for CTRL, resulting in fewer samples and higher Dist-1, 2, 3 scores as a consequence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the sampling method and evaluation metrics for a model.",
      "processing_time": 13.18725037574768,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Included also are strong baselines (CTRL, WD and GPT2-FT-RL) for each topic.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 13.89330768585205,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Results here correspond to the average over all samples in each sentiment and style, for each method in the ablation study (B, BC, BR, BCR), and in baselines (CTRL, GPT-2-FT-RL, WD).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and baselines. The context focuses on the results of an ablation study and comparisons with baselines.",
      "processing_time": 17.352946043014526,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Further, BCR slightly outperforms CTRL (51.7% & 50.0%), and signiﬁcantly outperforms WD (36 %).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. The context is focused on evaluating model performance rather than using a dataset.",
      "processing_time": 16.227749586105347,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "We compare to baselines CTRL, GPT2-FT-RL, and WD. BCR performs comparably to CTRL (73.7% and 80.0%), and BR, BC and BCR all outperform GPT2-FT-RL, the GPT-2 LM ﬁne tuned for positivity, and WD. (2019) conducted adversarial attacks that make GPT-2 produce racist output when given a carefully optimized trigger string as preﬁx.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing performance of different models.",
      "processing_time": 16.9802668094635,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "For baseline comparisons, we have (15 preﬁxes × 2 sentiments) for CTRL and (15 preﬁxes × 3 samples) for GPT2-FT-RL and (15 preﬁxes × 3 samples × 2 sentiments) for WD which each have 3 labels, resulting in 495 total annotations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the setup for generating and labeling samples using different models, but does not reference any named datasets.",
      "processing_time": 17.84097146987915,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "There has recently been a substantial discussion around the ethics of capable language models (Rad-ford et al., 2019; Keskar et al., 2019), both in their potential to recapitulate problematic social biases and for them to be directly abused for societal harm (e.g. to generate disinformation).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses ethical concerns and potential misuse of language models.",
      "processing_time": 14.084153890609741,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "The 2 topics missing a match with CTRL are: Military, Space.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only topics missing a match with a model. No verifiable resources are identified.",
      "processing_time": 15.427772045135498,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Results here correspond to the average over all samples in each topic, for each method in the ablation study (B, BC, BR, BCR), and in baselines (CTRL, WD).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and baselines. The context is focused on the results of an ablation study and baseline comparisons.",
      "processing_time": 18.96727967262268,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "The recent conditional language model, CTRL, from Keskar et al. (2019), trains a 1.6B LM conditioned on around 50 control codes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (CTRL) and control codes but does not specify a dataset. No verifiable dataset is mentioned.",
      "processing_time": 16.139812469482422,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Out of the 7 topics considered in PPLM-BoW, we found that 5 can be matched with a speciﬁc control code in CTRL.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to control codes in a model. The cited paper title confirms it's about a model, not a dataset.",
      "processing_time": 18.969953775405884,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "However, once such models are trained, controlling attributes of generated text becomes difﬁcult without modifying the model architecture to allow for extra input attributes or ﬁne-tuning with attribute-speciﬁc data (Keskar et al., 2019; Ziegler et al., 2019). and x the generated sample.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and methods. The context focuses on the challenges of controlling attributes in generated text without modifying model architecture or fine-tuning.",
      "processing_time": 19.52290439605713,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Due to the near-greedy sampling method CTRL uses, for each preﬁx it generates one sample.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CTRL) and its sampling technique. No verifiable resources are identified.",
      "processing_time": 17.518023014068604,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "BCR, CTRL and WD all score similarly on the ﬂuency metric.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model names which are excluded according to the rules.",
      "processing_time": 15.475101709365845,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "For baseline comparisons, we have (20 preﬁxes × 5 topics) for CTRL and (20 preﬁxes × 7 topics × 3 samples) for WD, each then with 3 labels, resulting in 1560 total annotations.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes the structure of the annotations used for baseline comparisons but does not reference a named dataset.",
      "processing_time": 18.746111154556274,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Keskar et al. (2019) train a large language model with over 50 different control codes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions training a large language model with control codes but does not specify a dataset. The context is about the method and not a reusable dataset.",
      "processing_time": 26.21903347969055,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "Included also are strong baselines (CTRL and WD) for each sentiment.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (CTRL and WD).",
      "processing_time": 24.15695285797119,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "• We compare PPLM with CTRL (Keskar et al., 2019) and GPT-2 ﬁnetuned for positivty (Ziegler et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different models for controllable text generation.",
      "processing_time": 26.46452784538269,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "We consider three baselines: CTRL, GPT2-FT-RL, and WD.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (CTRL, GPT2-FT-RL, WD). These are excluded as per the instructions.",
      "processing_time": 27.941859006881714,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "PPLM outperforms CTRL and WD on topic-relevance, while being comparable on ﬂuency scores. on topic than B (15.8",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance comparisons. No verifiable resources are identified.",
      "processing_time": 25.769098043441772,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "We use the ofﬁcial released codebase 2 and their open-sourced model to generate samples for the CTRL baseline.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using an open-sourced model and codebase but does not specify any dataset names. The citation is focused on the method and tools rather than a specific dataset.",
      "processing_time": 29.41885757446289,
      "citing_paper_id": "208617790",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "…Gu et al. (2016; 2017); Chen et al. (2018) explored the idea of using a small neural network to steer an LM. Yu et al. (2016), and more recently Yu et al. (2019); Yee et al. (2019); Ng et al. (2019), leveraged the Shannon Noisy Channel Theory (Shannon, 1948) for improving sequence-to-sequence…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and theories. The Shannon Noisy Channel Theory is mentioned but it is not a dataset.",
      "processing_time": 29.147191286087036,
      "citing_paper_id": "208617790",
      "cited_paper_id": 203610373
    },
    {
      "context_text": "The approach described in the previous section is able to generate text tuned for a particular discriminator, but left unchecked it will quickly result in unrealistic adversarial or fooling examples (Szegedy et al., 2013; Nguyen et al., 2015) as the text moves into low probability regions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing adversarial examples in deep neural networks.",
      "processing_time": 26.74698042869568,
      "citing_paper_id": "208617790",
      "cited_paper_id": 206592585
    },
    {
      "context_text": "Efﬁcient implementations of the trans-former (Wolf et al., 2019) use the cached H t to generate x t +1 , given x t .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (transformer) and its implementation details. There are no verifiable resources or datasets mentioned.",
      "processing_time": 28.885291814804077,
      "citing_paper_id": "208617790",
      "cited_paper_id": 269498086
    },
    {
      "context_text": "An open question is to scale progress in this task towards entire news articles, and without paired evidence (similar to open-domain QA; Chen et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to open-domain QA. No verifiable resources are identified.",
      "processing_time": 27.75179886817932,
      "citing_paper_id": "168169824",
      "cited_paper_id": 3618568
    },
    {
      "context_text": "11For each discriminator/generator pair, we search over p P t.9, .92, .94, .96, .98, 1.0u. 12Indeed, bidirectional approaches perform best on leaderboards like GLUE (Wang et al., 2018).\ndiscriminating between real and generated news articles suggests that neural fake news discrimination requires having a similar inductive bias as the generator.13\n5.3 Weak supervision: what happens if we don’t have access to Grover-Mega?",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GLUE but does not indicate it is used as a dataset. It is referenced as a leaderboard, which is excluded.",
      "processing_time": 28.54694676399231,
      "citing_paper_id": "168169824",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "…pair, we search over p P t.9, .92, .94, .96, .98, 1.0u. 12Indeed, bidirectional approaches perform best on leaderboards like GLUE (Wang et al., 2018).\ndiscriminating between real and generated news articles suggests that neural fake news discrimination requires having a similar…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GLUE but does not indicate it is used as a dataset. It is referenced as a leaderboard or benchmark, which is excluded.",
      "processing_time": 29.3962140083313,
      "citing_paper_id": "168169824",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "12Indeed, bidirectional approaches perform best on leaderboards like GLUE (Wang et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions GLUE but does not indicate it is used as a dataset. It is referenced as a leaderboard, which is excluded.",
      "processing_time": 28.88019824028015,
      "citing_paper_id": "168169824",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "We draw on recent progress in training large Transformers for language modeling (Vaswani et al., 2017), building Grover using the same architecture as for GPT2 (Radford et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures which are excluded.",
      "processing_time": 26.466301202774048,
      "citing_paper_id": "168169824",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "We draw on recent progress in training large Transformers for language modeling (Vaswani et al., 2017), building Grover using the same architecture as for GPT2 (Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures which are excluded.",
      "processing_time": 26.46469783782959,
      "citing_paper_id": "168169824",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Some users of these platforms avoid fake news with tools such as NewsGuard and Hoaxy (Shao et al., 2016) and websites like Snopes and PolitiFact.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions tools and websites but does not refer to any specific datasets. The context is about tools and platforms used to track misinformation.",
      "processing_time": 29.67848300933838,
      "citing_paper_id": "168169824",
      "cited_paper_id": 16527013
    },
    {
      "context_text": "Video-sharing platforms like YouTube use deep neural networks to scan videos while they are uploaded, to filter out content like pornography (Hosseini et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (Google Cloud Video Intelligence API) used for filtering content on video-sharing platforms.",
      "processing_time": 30.35421109199524,
      "citing_paper_id": "168169824",
      "cited_paper_id": 24677497
    },
    {
      "context_text": "What should platforms do? Video-sharing platforms like YouTube use deep neural networks to scan videos while they are uploaded, to filter out content like pornography (Hosseini et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of deep neural networks for content filtering on video-sharing platforms.",
      "processing_time": 27.940311193466187,
      "citing_paper_id": "168169824",
      "cited_paper_id": 24677497
    },
    {
      "context_text": "Efforts to automate fake news detection generally point out stylistic biases that exist in the text (Rashkin et al., 2017; Wang, 2017; Pérez-Rosas et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general efforts and findings related to fake news detection.",
      "processing_time": 27.939237594604492,
      "citing_paper_id": "168169824",
      "cited_paper_id": 27274148
    },
    {
      "context_text": "Thus, while we are excited about recent progress in text generation (Józefowicz et al., 2016; Radford et al., 2018; 2019), we are also concerned with the inevitability of AI-generated ‘neural’ fake news.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to progress in text generation and concerns about AI-generated fake news.",
      "processing_time": 29.130927801132202,
      "citing_paper_id": "168169824",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Similar to GPT (Radford et al., 2018), we place a special [CLS] token at the end of each article, and extract the final hidden state at that point.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach similar to GPT. The citation is used to reference a method, not a dataset.",
      "processing_time": 31.22760319709778,
      "citing_paper_id": "168169824",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "In this setting, the best existing fake news discriminators are, themselves, deep pretrained language models (73% accuracy) (Peters et al., 2018; Radford et al., 2018; 2019; Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to deep pretrained language models. No verifiable resources are identified.",
      "processing_time": 29.3980495929718,
      "citing_paper_id": "168169824",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Thus, while we are excited about recent progress in text generation (Józefowicz et al., 2016; Radford et al., 2018; 2019), we are also concerned with the inevitability of AI-generated ‘neural’ fake news.1\nWith this paper, we seek to understand and respond to neural fake news before it manifests at…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing advancements in text generation. No verifiable resources are identified.",
      "processing_time": 16.49885654449463,
      "citing_paper_id": "168169824",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Our smallest model, Grover-Base, has 12 layers and 124 million parameters, on par with GPT and BERT-Base (Radford et al., 2018; Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GPT and BERT-Base). The context is about model architecture and parameter counts, not dataset usage.",
      "processing_time": 31.608259677886963,
      "citing_paper_id": "168169824",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "This also suggests that recent progress on generating text in any order (Gu et al., 2019; Stern et al., 2019; Ghazvininejad et al., 2019) may lead to models that evade a Grover discriminator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about the progress in generating text in any order, which is not directly related to a specific dataset.",
      "processing_time": 32.199177503585815,
      "citing_paper_id": "168169824",
      "cited_paper_id": 60440448
    },
    {
      "context_text": "This also suggests that recent progress on generating text in any order (Gu et al., 2019; Stern et al., 2019; Ghazvininejad et al., 2019) may lead to models that evade a Grover discriminator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about the progress in generating text in any order, which is not directly related to a specific dataset.",
      "processing_time": 32.199177503585815,
      "citing_paper_id": "168169824",
      "cited_paper_id": 127955230
    },
    {
      "context_text": "…such as beam search work well for closed-ended generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during open-ended generation (Hashimoto et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 28.544482231140137,
      "citing_paper_id": "168169824",
      "cited_paper_id": 102351981
    },
    {
      "context_text": "…such as beam search work well for closed-ended generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during open-ended generation (Hashimoto et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 28.544482231140137,
      "citing_paper_id": "168169824",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "While likelihood-maximization strategies such as beam search work well for closed-ended generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during open-ended generation (Hashimoto et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 28.543720483779907,
      "citing_paper_id": "168169824",
      "cited_paper_id": 102351981
    },
    {
      "context_text": "While likelihood-maximization strategies such as beam search work well for closed-ended generation tasks where the output contains the same information as the context (like machine translation), these approaches have been shown to produce degenerate text during open-ended generation (Hashimoto et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 28.543720483779907,
      "citing_paper_id": "168169824",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "…threshold p, at each timestep we sample from the most probable words whose cumulative probability comprises the top-p% of the entire vocabulary (Holtzman et al., 2019).6\n4 Humans are Easily Fooled by Grover-written Propaganda\nWe evaluate the quality of disinformation generated by our largest…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sampling words during text generation. The context is about evaluating disinformation generated by a model, but no datasets are explicitly named.",
      "processing_time": 32.46543645858765,
      "citing_paper_id": "168169824",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "In this paper, we primarily use Nucleus Sampling (top-p): for a given threshold p, at each timestep we sample from the most probable words whose cumulative probability comprises the top-p% of the entire vocabulary (Holtzman et al., 2019).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Nucleus Sampling (top-p) as a method for generating text, but does not refer to any specific dataset. The citation is about a method, not a dataset.",
      "processing_time": 32.64385390281677,
      "citing_paper_id": "168169824",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Existing fake news is predominantly human-written, for two broad goals: monetization (ad revenue through clicks) and propaganda (communicating targeted information) (Bradshaw and Howard, 2017; Melford and Fagan, 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general goals of fake news creation. No verifiable resources are identified.",
      "processing_time": 30.591832160949707,
      "citing_paper_id": "168169824",
      "cited_paper_id": 158529031
    },
    {
      "context_text": "However, fact checking is not a panacea – cognitive biases such as the backfire e↵ect and confirmation bias make humans liable to believe fake news that fits their worldview (Swire et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only cognitive biases and human behavior in relation to fake news.",
      "processing_time": 30.33510684967041,
      "citing_paper_id": "168169824",
      "cited_paper_id": 204950945
    },
    {
      "context_text": "We thus released our models to researchers (Zellers, 2019).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the release of models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 31.38742733001709,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "We thus released our models to researchers (Zellers, 2019).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the release of models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 31.38742733001709,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "We conclude with a sketch of the ethical territory that must be mapped out in order to understand our responsibilities as researchers when studying fake news, and the potential negative implications of releasing models (Hecht et al., 2018; Zellers, 2019; Solaiman et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only ethical considerations and potential negative implications of releasing models.",
      "processing_time": 32.4588348865509,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "We conclude with a sketch of the ethical territory that must be mapped out in order to understand our responsibilities as researchers when studying fake news, and the potential negative implications of releasing models (Hecht et al., 2018; Zellers, 2019; Solaiman et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only ethical considerations and potential negative implications of releasing models.",
      "processing_time": 32.4588348865509,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "There are many types of false news, ranging from satire to propaganda (Wardle, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a type of content (false news) without providing details on data sources or methodologies.",
      "processing_time": 35.5678608417511,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "Malicious actors spread fallacious viral stories in order to gain advertising revenue, influence opinions, and even tip elections (Faris et al., 2017; Wardle and Derakhshan, 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general behavior of malicious actors. No verifiable resources are identified.",
      "processing_time": 34.55693006515503,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "Generating the first token after <startbody> results in high\n13This matches findings on the HellaSwag dataset (Zellers et al., 2019b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HellaSwag"
      ],
      "dataset_descriptions": {
        "HellaSwag": "Used to evaluate the performance of generating the first token after a start token, focusing on the initial token prediction accuracy."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the HellaSwag dataset, which is a specific, verifiable resource used in the context of generating the first token after a start token.",
      "processing_time": 45.30188345909119,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "Our study focused on detecting machine-written fake news, though the same Grover approach can be used for spotting human-written fake news as well (Zellers et al., 2019c).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Grover) for detecting fake news. No verifiable resources are identified.",
      "processing_time": 36.28128433227539,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "One additional possibility is the use of Adversarial Filtering (Zellers et al., 2018; 2019b) to oversample and then select a subset of generations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Adversarial Filtering. No verifiable resources are identified.",
      "processing_time": 37.10562205314636,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "15In additional experiments we show that accuracy increases even more – up to 98% – when the number of examples is increased (Zellers et al., 2019c).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general increase in accuracy with more examples. No verifiable resource names are present.",
      "processing_time": 37.87996578216553,
      "citing_paper_id": "168169824",
      "cited_paper_id": null
    },
    {
      "context_text": "Researchers have extensively explored and enhanced these approaches [4, 12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to general research approaches.",
      "processing_time": 37.49860334396362,
      "citing_paper_id": "269148520",
      "cited_paper_id": 24986117
    },
    {
      "context_text": "Certain studies [1, 3, 33] follow this paradigm but employ techniques like prompt learning [35] or LoRA [16] for fine-tuning the LLM and enhancing recommendation accuracy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. No verifiable resources are identified.",
      "processing_time": 37.88812828063965,
      "citing_paper_id": "269148520",
      "cited_paper_id": 248834570
    },
    {
      "context_text": "The existing personalization generation methods such as Textual Inversion [6] and DreamBooth [28] train extra embeddings for each user using their historical item images.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'historical item images' but does not specify a named dataset. The cited paper titles do not provide additional dataset names.",
      "processing_time": 39.48129677772522,
      "citing_paper_id": "269148520",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "To address it, we conduct a human evaluation to compare the images generated by our method PMG, Textual Inversion [6], and images without personalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method (Textual Inversion) and a human evaluation, which do not qualify as datasets.",
      "processing_time": 41.84883165359497,
      "citing_paper_id": "269148520",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "The current approaches to personalized generation, such as Textual Inversion [6] and DreamBooth [28], mainly focus on integrating new characters or image styles into a pre-trained diffusion model using a few images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (Textual Inversion and DreamBooth). The context focuses on the methodologies used for personalizing text-to-image generation.",
      "processing_time": 42.13583254814148,
      "citing_paper_id": "269148520",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Although it can yield favorable results even without training [7, 15, 32], this approach lacks specific optimization for recommender tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general lack of optimization for recommender tasks. No verifiable resources are identified.",
      "processing_time": 39.96567749977112,
      "citing_paper_id": "269148520",
      "cited_paper_id": 257766541
    },
    {
      "context_text": "In contrast, TANGO [10] and GILL [18] employ informative hidden embeddings but are not stable and require substantial training to align their embedding space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (TANGO and GILL). The context focuses on the methodologies and limitations of these models.",
      "processing_time": 40.74367642402649,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258352270
    },
    {
      "context_text": "In contrast, TANGO [10] and GILL [18] employ informative hidden embeddings but are not stable and require substantial training to align their embedding space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (TANGO and GILL). The context focuses on the methodologies and limitations of these models.",
      "processing_time": 40.74367642402649,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258959284
    },
    {
      "context_text": "For the other features, we convert them into text using a caption model (e.g. BLIP-2 [19], CLAP [5]) or using multimodal LLM (e.g. MiniGPT-4 [41], mPLUG-owl [39]) capable of processing multimodal inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods for converting features into text, but does not reference any specific datasets. The cited paper titles do not provide additional dataset information.",
      "processing_time": 41.35962772369385,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258352455
    },
    {
      "context_text": "As for multimodal recommendation, VIP5 [9] builds upon P5 by incorporating item images as visual features and introduces adapters to understand them.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (VIP5) that incorporates item images as visual features. No verifiable dataset names are provided.",
      "processing_time": 41.59678316116333,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258841635
    },
    {
      "context_text": "Following previous studies such as DreamBooth [28] and GILL [18], we use the similarity between the generated results and the preference keywords to measure the degree of personalization, which we call the preference score , and the accuracy score refers to the similarity with the target item…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and scores. The citation is focused on explaining the methodology for measuring personalization and accuracy in generated images.",
      "processing_time": 41.85857129096985,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258959284
    },
    {
      "context_text": "In contrast to GILL [18], which solely relies on captions for supervision, we believe that incorporating multimodal supervision (such as real images or audios) is more meaningful and helps to correct deviations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GILL) and a general approach to supervision. No verifiable resources are identified.",
      "processing_time": 41.35562562942505,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258959284
    },
    {
      "context_text": "Inspired by GILL[18], we incorporate multimodal tokens as learnable parameters into the embedding table and then utilize a linear layer to align the embedding space of the LLM with that of the generator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for incorporating multimodal tokens into language models.",
      "processing_time": 39.177945137023926,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258959284
    },
    {
      "context_text": "The user behaviors are used to produce preference conditions, including explicit keywords in natural language (named preference keywords) by a frozen LLM and implicit embeddings (named soft preference embeddings) by a tuned LLM for multimodal bias correction [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited paper title does not help in identifying a dataset.",
      "processing_time": 40.38731813430786,
      "citing_paper_id": "269148520",
      "cited_paper_id": 258959284
    },
    {
      "context_text": "Building upon these achievements, researchers have focused on expanding LLMs into the domain of multimodal understanding, with a particular emphasis on image and audio [21, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on multimodal understanding in LLMs.",
      "processing_time": 39.467342376708984,
      "citing_paper_id": "269148520",
      "cited_paper_id": 259165461
    },
    {
      "context_text": "Recommendation [29] is an important means for information retrieval and many studies aim to leverage the exceptional reasoning capabilities of LLMs for recommender systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general concept of recommendation systems and the use of LLMs. There are no clear identifiers for datasets.",
      "processing_time": 42.38170409202576,
      "citing_paper_id": "269148520",
      "cited_paper_id": 261243203
    },
    {
      "context_text": "In our implementation, we utilize a diffusion model, which contains a text encoder and a U-Net [27].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion model with a text encoder and U-Net).",
      "processing_time": 17.13161563873291,
      "citing_paper_id": "269148520",
      "cited_paper_id": null
    },
    {
      "context_text": "To enable multimodal generation tasks, LLMs can be integrated with modality-specific generators such as diffusion models [14] or multimodal LLMs [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 40.381218910217285,
      "citing_paper_id": "269148520",
      "cited_paper_id": null
    },
    {
      "context_text": "To extend NeRFs to dynamic scene modeling, previous works either learn a high-dimensional radiance ﬁeld conditioned on temporal embeddings [14, 18], or a separate deformation mapping to model motion [23, 25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for extending NeRFs to dynamic scene modeling.",
      "processing_time": 40.38080143928528,
      "citing_paper_id": "265466231",
      "cited_paper_id": 234364556
    },
    {
      "context_text": "The advent of large-scale text-conditioned diffusion-based generative models for images has ushered in a new era of imaginative, high-quality image synthesis [1, 28, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the advent of generative models. No verifiable resources are identified.",
      "processing_time": 41.07654547691345,
      "citing_paper_id": "265466231",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "The advent of large-scale text-conditioned diffusion-based generative models for images has ushered in a new era of imaginative, high-quality image synthesis [1, 28, 30].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the advent of generative models. No verifiable resources are identified.",
      "processing_time": 41.07654547691345,
      "citing_paper_id": "265466231",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Therefore, we leverage 3D-aware [16, 31] and standard image diffusion models [1, 28] in stage-one to achieve high-quality view-consistent text-to-3D synthesis, along with video diffusion guidance [3] in stage-two to learn realistic motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 40.189327239990234,
      "citing_paper_id": "265466231",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Therefore, we leverage 3D-aware [16, 31] and standard image diffusion models [1, 28] in stage-one to achieve high-quality view-consistent text-to-3D synthesis, along with video diffusion guidance [3] in stage-two to learn realistic motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 40.189327239990234,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "Seminal works such as Stable Diffusion [28] and DeepFloyd [1] take a text prompt as input and produce high-quality images that align with the prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Stable Diffusion and DeepFloyd). These are not considered datasets according to the extraction rules.",
      "processing_time": 43.081371545791626,
      "citing_paper_id": "265466231",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "In this work, we leverage a deformable NeRF representation where the canonical geometry and 4D deformation ﬁeld are both encoded by multi-resolution feature grids [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (deformable NeRF representation) and a technical detail (multi-resolution feature grids).",
      "processing_time": 43.080352544784546,
      "citing_paper_id": "265466231",
      "cited_paper_id": 246016186
    },
    {
      "context_text": "To efﬁ-ciently learn a canonical model of a 3D scene, we opt for the NeRF [21] representation with multi-resolution hash-encoded features [22], which is extensively used in previous text-to-3D methods [15, 20, 24, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of NeRF and multi-resolution hash-encoded features, which are methods, not datasets.",
      "processing_time": 43.420692443847656,
      "citing_paper_id": "265466231",
      "cited_paper_id": 246016186
    },
    {
      "context_text": "To efﬁ-ciently learn a canonical model of a 3D scene, we opt for the NeRF [21] representation with multi-resolution hash-encoded features [22], which is extensively used in previous text-to-3D methods [15, 20, 24, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of NeRF and multi-resolution hash-encoded features, which are methods, not datasets.",
      "processing_time": 43.420692443847656,
      "citing_paper_id": "265466231",
      "cited_paper_id": 253510536
    },
    {
      "context_text": "To efﬁ-ciently learn a canonical model of a 3D scene, we opt for the NeRF [21] representation with multi-resolution hash-encoded features [22], which is extensively used in previous text-to-3D methods [15, 20, 24, 37, 38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets. It focuses on the use of NeRF and multi-resolution hash-encoded features, which are methods, not datasets.",
      "processing_time": 43.420692443847656,
      "citing_paper_id": "265466231",
      "cited_paper_id": 258887357
    },
    {
      "context_text": "To speed up training and inference, plane-and voxel-based feature grids are combined with MLPs to formulate efﬁcient hybrid NeRF representations [6, 11, 22], which are extended to dynamic scene modeling by learning additional planes for the temporal dimension [5, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methodological advancements in neural representation and does not mention any specific datasets.",
      "processing_time": 39.942556619644165,
      "citing_paper_id": "265466231",
      "cited_paper_id": 246016186
    },
    {
      "context_text": "To speed up training and inference, plane-and voxel-based feature grids are combined with MLPs to formulate efﬁcient hybrid NeRF representations [6, 11, 22], which are extended to dynamic scene modeling by learning additional planes for the temporal dimension [5, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methodological advancements in neural representation and does not mention any specific datasets.",
      "processing_time": 39.942556619644165,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256105095
    },
    {
      "context_text": "To speed up training and inference, plane-and voxel-based feature grids are combined with MLPs to formulate efﬁcient hybrid NeRF representations [6, 11, 22], which are extended to dynamic scene modeling by learning additional planes for the temporal dimension [5, 12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methodological advancements in neural representation and does not mention any specific datasets.",
      "processing_time": 39.942556619644165,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256194335
    },
    {
      "context_text": "Subsequently, these developments have also led to impressive progress in (1) text-or image-conditioned static 3D content creation [15, 20, 24, 26], achieved by leveraging guidance from generic and 3D-aware [16, 17, 31] image diffusion models, and (2) video content creation via video diffusion…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in 3D content creation and video diffusion models. No verifiable resources are identified.",
      "processing_time": 43.405396699905396,
      "citing_paper_id": "265466231",
      "cited_paper_id": 253510536
    },
    {
      "context_text": "Subsequent works enhance the synthesis quality and speed by incorporating the mesh representation [15, 35], advancing score distillation [36–38], exploring representations in the latent space [20], or disentangling geometry and texture [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the text.",
      "processing_time": 42.10776162147522,
      "citing_paper_id": "265466231",
      "cited_paper_id": 253510536
    },
    {
      "context_text": "It further models the dynamic representation via a neural hexplane [5].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or model called 'neural hexplane'.",
      "processing_time": 41.0674729347229,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256105095
    },
    {
      "context_text": "D-NeRF v.s. hexplane representation We also ablate our deformable NeRF representation against the hexplane 4D representation [5] used previously in [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two representations (deformable NeRF and hexplane). No verifiable datasets are referenced.",
      "processing_time": 43.80177664756775,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256105095
    },
    {
      "context_text": "D-NeRF v.s. hexplane representation We also ablate our deformable NeRF representation against the hexplane 4D representation [5] used previously in [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two representations (deformable NeRF and hexplane). No verifiable datasets are referenced.",
      "processing_time": 43.80177664756775,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "However, this cannot be achieved with the hexplane [5] representation proposed in [33], which en-tangles the static representation and its motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HexPlane) and its limitations. No verifiable resources are identified.",
      "processing_time": 41.57058930397034,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256105095
    },
    {
      "context_text": "However, this cannot be achieved with the hexplane [5] representation proposed in [33], which en-tangles the static representation and its motion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HexPlane) and its limitations. No verifiable resources are identified.",
      "processing_time": 41.57058930397034,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "The most relevant work to ours is MAV3D [33], which optimizes a 4D scene by leveraging a pre-trained video diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MAV3D) and a pre-trained model (video diffusion model).",
      "processing_time": 43.41241979598999,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "The pioneering work MAV3D [33] is the ﬁrst attempt to address this problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system called MAV3D. The context does not provide information about the usage of any datasets.",
      "processing_time": 43.93273329734802,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "Since there is no publicly available implementation of MAV3D [33], we compare our method against the 28 visual results displayed on their web-site.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison against visual results from another method. No verifiable resources are identified.",
      "processing_time": 42.62722063064575,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "However, relying solely on guidance from image or video diffusion models for static text-to-3D synthesis, as proposed in [33], easily encounters the Janus problem (see the ﬁrst row of Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for text-to-3D synthesis. There are no clear identifiers for datasets in the context.",
      "processing_time": 44.11928749084473,
      "citing_paper_id": "265466231",
      "cited_paper_id": 256274791
    },
    {
      "context_text": "…progress in (1) text-or image-conditioned static 3D content creation [15, 20, 24, 26], achieved by leveraging guidance from generic and 3D-aware [16, 17, 31] image diffusion models, and (2) video content creation via video diffusion models [ 2–4 However, for various real-world applications such…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 43.066214084625244,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "Another line of methods [16, 17] learns 3D-aware diffusion models with images rendered from synthetic objects [8, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and synthetic objects. No clear, verifiable dataset names are present.",
      "processing_time": 42.62432241439819,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "Another line of methods [16, 17] learns 3D-aware diffusion models with images rendered from synthetic objects [8, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and synthetic objects. No clear, verifiable dataset names are present.",
      "processing_time": 42.62432241439819,
      "citing_paper_id": "265466231",
      "cited_paper_id": null
    },
    {
      "context_text": "3D diffusion models [16, 17, 31] take camera parameters with a text prompt or an image as inputs and synthesize novel view images of the target object.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for synthesizing novel view images using 3D diffusion models, which is not a dataset.",
      "processing_time": 44.71153402328491,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "3D diffusion models [16, 17, 31] take camera parameters with a text prompt or an image as inputs and synthesize novel view images of the target object.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for synthesizing novel view images using 3D diffusion models, which is not a dataset.",
      "processing_time": 44.71153402328491,
      "citing_paper_id": "265466231",
      "cited_paper_id": null
    },
    {
      "context_text": "Our approach, with both, results in the best quality (row 3). given image and its estimated foreground mask, similarly to [16, 26].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 42.851638317108154,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "The former [19, 26, 34] simultaneously utilize text-to-image and novel view diffusion models [16], while the latter [27, 31] overﬁt a diffusion model on a few images depicting the same subject to achieve personalized diffusion guidance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 41.823641777038574,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "The former [19, 26, 34] simultaneously utilize text-to-image and novel view diffusion models [16], while the latter [27, 31] overﬁt a diffusion model on a few images depicting the same subject to achieve personalized diffusion guidance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 41.823641777038574,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257757320
    },
    {
      "context_text": "Speciﬁcally, we use zero123-xl [16] as our 3D diffusion model and DeepﬂoydIF [1] as our 2D diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (zero123-xl and DeepﬂoydIF) but does not reference any specific datasets. The context is about using these models for 3D and 2D diffusion, which is not directly related to personalized text generation.",
      "processing_time": 47.226680517196655,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "To overcome this limitation, several works [13, 16, 17, 31] leverage 3D-aware diffusion models as supervisions, generating 3D objects that are consistent across multiple views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using 3D-aware diffusion models.",
      "processing_time": 42.09141278266907,
      "citing_paper_id": "265466231",
      "cited_paper_id": 257631738
    },
    {
      "context_text": "To overcome this limitation, several works [13, 16, 17, 31] leverage 3D-aware diffusion models as supervisions, generating 3D objects that are consistent across multiple views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using 3D-aware diffusion models.",
      "processing_time": 42.09141278266907,
      "citing_paper_id": "265466231",
      "cited_paper_id": null
    },
    {
      "context_text": "To overcome this limitation, several works [13, 16, 17, 31] leverage 3D-aware diffusion models as supervisions, generating 3D objects that are consistent across multiple views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using 3D-aware diffusion models.",
      "processing_time": 42.09141278266907,
      "citing_paper_id": "265466231",
      "cited_paper_id": null
    },
    {
      "context_text": "…in (1) text-or image-conditioned static 3D content creation [15, 20, 24, 26], achieved by leveraging guidance from generic and 3D-aware [16, 17, 31] image diffusion models, and (2) video content creation via video diffusion models [ 2–4 However, for various real-world applications such as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 43.78619122505188,
      "citing_paper_id": "265466231",
      "cited_paper_id": null
    },
    {
      "context_text": "Shani et al. [19] is one of the early studies of applying RL techniques for the recommender system problems, however, not in dialogue systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RL techniques) applied to recommender systems. The context is about the application of reinforcement learning, not the use of a particular dataset.",
      "processing_time": 45.80375051498413,
      "citing_paper_id": "47012216",
      "cited_paper_id": 875571
    },
    {
      "context_text": "Mesnil et al. [13] used LSTM and Conditional Random Fields networks to perform the slot filling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LSTM and Conditional Random Fields but does not specify a dataset. The context focuses on methods rather than datasets.",
      "processing_time": 41.81729197502136,
      "citing_paper_id": "47012216",
      "cited_paper_id": 1317136
    },
    {
      "context_text": "Bordes et al. [1] built an end-to-end task oriented bot based on memory network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (memory network) used to build a task-oriented bot.",
      "processing_time": 42.830079555511475,
      "citing_paper_id": "47012216",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "To tackle this problem, a common practice in the dialogue system research is using simulated users as a bootstrap to pre-train the model [1][9][23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general practice in dialogue system research. No verifiable resources are identified.",
      "processing_time": 43.0361762046814,
      "citing_paper_id": "47012216",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "To tackle this problem, a common practice in the dialogue system research is using simulated users as a bootstrap to pre-train the model [1][9][23].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general practice in dialogue system research. No verifiable resources are identified.",
      "processing_time": 43.0361762046814,
      "citing_paper_id": "47012216",
      "cited_paper_id": 17799567
    },
    {
      "context_text": "Following the REINFORCE [28] algorithm, the gradient of the learning object becomes: Here G t is the sum of rewards, or return , starting from time step t to the final time step T .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reinforcement learning algorithm. No verifiable resources are identified.",
      "processing_time": 42.34216070175171,
      "citing_paper_id": "47012216",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "After the classifier’s accuracy is stable, we keep on training the policy network using the REINFORCE algorithm [28].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reinforcement learning algorithm. No datasets are referenced in the context.",
      "processing_time": 42.839679479599,
      "citing_paper_id": "47012216",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "Most recommender systems are either content based [10], collaborative filtering (CF) based [7][14] or hybrid.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general categories of recommender systems. No verifiable resources are identified.",
      "processing_time": 43.63498830795288,
      "citing_paper_id": "47012216",
      "cited_paper_id": 6102334
    },
    {
      "context_text": "[32] was among the first works of building an end-to-end dialogue system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for building dialogue systems.",
      "processing_time": 24.845229864120483,
      "citing_paper_id": "47012216",
      "cited_paper_id": 6179947
    },
    {
      "context_text": "Deep RL has been applied for better sequential decision making in various domains, including End-to-End dialogue systems using deep RL for information access [5] [32], information extraction [16], query reformulation [17], real time ads bidding [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of deep RL in various domains. No clear, verifiable datasets are identified.",
      "processing_time": 44.09722685813904,
      "citing_paper_id": "47012216",
      "cited_paper_id": 6179947
    },
    {
      "context_text": "When trying to buy products on an e-Commerce website, users often navigate the product space through faceted search [31][6][24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of faceted search in e-Commerce. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.452582597732544,
      "citing_paper_id": "47012216",
      "cited_paper_id": 7124031
    },
    {
      "context_text": "When trying to buy products on an e-Commerce website, users often navigate the product space through faceted search [31][6][24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of faceted search in e-Commerce. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 45.452582597732544,
      "citing_paper_id": "47012216",
      "cited_paper_id": 10172217
    },
    {
      "context_text": "To avoid overwhelming users with too many facet-value pair options per conversation, a faceted search engine selects a small set of facets or facet-value pairs for a user to choose from based on context [31][6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of faceted search engines. No verifiable resources are identified.",
      "processing_time": 43.90623712539673,
      "citing_paper_id": "47012216",
      "cited_paper_id": 7124031
    },
    {
      "context_text": "To avoid overwhelming users with too many facet-value pair options per conversation, a faceted search engine selects a small set of facets or facet-value pairs for a user to choose from based on context [31][6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of faceted search engines. No verifiable resources are identified.",
      "processing_time": 43.90623712539673,
      "citing_paper_id": "47012216",
      "cited_paper_id": 10172217
    },
    {
      "context_text": "[29] developed an entropy based policy for the DS.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or framework for dialog systems.",
      "processing_time": 42.07472896575928,
      "citing_paper_id": "47012216",
      "cited_paper_id": 10319805
    },
    {
      "context_text": "us on improving rating prediction or ranking measures (learning to rank for recommendation) [7][18]. Few work has been done towards making recommendations in a dialogue system. Christakopoulou et al. [3] studied using a generative Gaussian model to recommend items to users in a conversation. However, their model does not target at maximizing the long-term benefits and their conversational agents’ act",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The focus is on the approach to conversational recommendation systems.",
      "processing_time": 44.34422588348389,
      "citing_paper_id": "47012216",
      "cited_paper_id": 11744847
    },
    {
      "context_text": "cant improvements. Mesnil et al. [13] used LSTM and Conditional Random Fields networks to perform the slot filling. Wu et al. [29] developed an entropy based policy for the DS. Christakopoulou et al. [3] used bandit machine for the decision making. Zhao et al. [32] was among the first works of building an end-to-end dialogue system. Wen et al. [26] introduced an end-to-end task oriented dialogue syst",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used in dialogue systems.",
      "processing_time": 42.829700231552124,
      "citing_paper_id": "47012216",
      "cited_paper_id": 11744847
    },
    {
      "context_text": "Chit-chat systems are focusing on information social chat and try to interact with human-like reasonable or interesting responses [8][25].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to chit-chat systems and their goals.",
      "processing_time": 43.37938737869263,
      "citing_paper_id": "47012216",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "For training games like Go or Atari [21][15], such kind of environment is easy to create based on a set of predefined rules.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to environments for training games like Go or Atari, which are not datasets.",
      "processing_time": 44.91656851768494,
      "citing_paper_id": "47012216",
      "cited_paper_id": 15238391
    },
    {
      "context_text": "nd Netflix provide customized recommendations for additional products or services based on a user’s history. Most recommender systems are either content based [10], collaborative filtering (CF) based [7][14] or hybrid. Research in recommender systems usually focus on improving rating prediction or ranking measures (learning to rank for recommendation) [7][18]. Few work has been done towards making re",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses general concepts and methods in recommender systems without referencing any particular dataset.",
      "processing_time": 44.91511011123657,
      "citing_paper_id": "47012216",
      "cited_paper_id": 58370896
    },
    {
      "context_text": "NDCG rp : The second way is following the assumption of NDCG [11].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (NDCG) which is excluded according to the instructions.",
      "processing_time": 44.073121547698975,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": "So rp = C ∗ NDCG@K , if a user finds the target.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formula for calculating a metric. There are no verifiable resources or datasets mentioned.",
      "processing_time": 44.68154692649841,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": "To explore the impact of rp , we run the experiments with the NDCG rp and the Cascade rp as discussed in Section 4.3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 44.667837619781494,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": "Linear rp and NDCG rp assume the user checks each item until threshold K, while Cascade rp assumes that the user may leave at any turn.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and assumptions about user behavior.",
      "processing_time": 42.06311798095703,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": "This is because NDCG rp and Cascade rp penalize the rewards nonlinearly with the decrease of the ranking.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses evaluation metrics (NDCG, Cascade) in the context of information retrieval.",
      "processing_time": 45.09600329399109,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": "When computing the NDCG, we use a binary relevance score.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (NDCG) and a scoring method (binary relevance).",
      "processing_time": 44.664507150650024,
      "citing_paper_id": "47012216",
      "cited_paper_id": 262553219
    },
    {
      "context_text": ", 2021) and statistical model parameters (Yu et al., 2020; Patel et al., 2021; Cao et al., 2020; Cai et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to statistical model parameters. No verifiable resources are identified.",
      "processing_time": 44.675288438797,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233476374
    },
    {
      "context_text": ", 2021) and statistical model parameters (Yu et al., 2020; Patel et al., 2021; Cao et al., 2020; Cai et al., 2021, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to statistical model parameters. No verifiable resources are identified.",
      "processing_time": 44.675288438797,
      "citing_paper_id": "251953565",
      "cited_paper_id": 215810902
    },
    {
      "context_text": "Inspiredbytherecentprogressofthetext-conditioned image generation [15], [16], [17], [18], we propose to incorporate Denoising Diffusion Probabilistic Models (DDPM) [19] into motion generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about image synthesis and text-conditional image generation, which do not directly provide dataset names.",
      "processing_time": 46.70107388496399,
      "citing_paper_id": "251953565",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Inspiredbytherecentprogressofthetext-conditioned image generation [15], [16], [17], [18], we propose to incorporate Denoising Diffusion Probabilistic Models (DDPM) [19] into motion generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about image synthesis and text-conditional image generation, which do not directly provide dataset names.",
      "processing_time": 46.70107388496399,
      "citing_paper_id": "251953565",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Inspiredbytherecentprogressofthetext-conditioned image generation [15], [16], [17], [18], we propose to incorporate Denoising Diffusion Probabilistic Models (DDPM) [19] into motion generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about image synthesis and text-conditional image generation, which do not directly provide dataset names.",
      "processing_time": 46.70107388496399,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Diffusion Models [15], [16], [17], [19] are a new class of generative models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 45.42176938056946,
      "citing_paper_id": "251953565",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Diffusion Models [15], [16], [17], [19] are a new class of generative models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 45.42176938056946,
      "citing_paper_id": "251953565",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "To simplify the problem, we follow the literature [15] to set Σ θ ( x t , t, text ) as the constant value β t , which is the same one that we used in Equation (1). μ θ ( x t , t, text ) is then estimated as below [15]: Hence, for each step, the motion sequence is initially denoised to x 0 and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and equations. There are no clear identifiers for datasets in the text.",
      "processing_time": 45.61886787414551,
      "citing_paper_id": "251953565",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "…the literature [15] to set Σ θ ( x t , t, text ) as the constant value β t , which is the same one that we used in Equation (1). μ θ ( x t , t, text ) is then estimated as below [15]: Hence, for each step, the motion sequence is initially denoised to x 0 and subsequently renoised to x t − 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and equations.",
      "processing_time": 43.87562680244446,
      "citing_paper_id": "251953565",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Previous works [15], [16], [17], [19] mainly utilize UNet-like structure as the denoising model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model structures. No verifiable resources are identified.",
      "processing_time": 45.220831632614136,
      "citing_paper_id": "251953565",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Previous works [15], [16], [17], [19] mainly utilize UNet-like structure as the denoising model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model structures. No verifiable resources are identified.",
      "processing_time": 45.220831632614136,
      "citing_paper_id": "251953565",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Implementation Details: We train our proposed MotionDif-fuse on the BABEL dataset [72] with 50 K iterations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BABEL"
      ],
      "dataset_descriptions": {
        "BABEL": "Used to train the MotionDiffuse model, focusing on bodies, action, and behavior with English labels, enhancing personalized text generation through motion data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the BABEL dataset, which is a specific, verifiable dataset with a clear name and is used for training the MotionDiffuse model.",
      "processing_time": 51.97455167770386,
      "citing_paper_id": "251953565",
      "cited_paper_id": 235457994
    },
    {
      "context_text": "Recently, BA-BEL [72] and HumanML3D [48] re-annotates AMASS [73], a large scale motion capture dataset, with English language labels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AMASS"
      ],
      "dataset_descriptions": {
        "AMASS": "Re-annotated with English language labels by BA-BEL and HumanML3D, used for generating diverse and natural 3D human motions from text, enhancing personalized text-to-motion synthesis."
      },
      "confidence_score": 0.9,
      "reasoning": "AMASS is identified as a large-scale motion capture dataset re-annotated with English language labels by BA-BEL and HumanML3D. The context indicates the dataset is used for generating diverse and natural 3D human motions from text.",
      "processing_time": 59.7751190662384,
      "citing_paper_id": "251953565",
      "cited_paper_id": 235457994
    },
    {
      "context_text": "Recently, BA-BEL [72] and HumanML3D [48] re-annotates AMASS [73], a large scale motion capture dataset, with English language labels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AMASS"
      ],
      "dataset_descriptions": {
        "AMASS": "Re-annotated with English language labels by BA-BEL and HumanML3D, used for generating diverse and natural 3D human motions from text, enhancing personalized text-to-motion synthesis."
      },
      "confidence_score": 0.9,
      "reasoning": "AMASS is identified as a large-scale motion capture dataset re-annotated with English language labels by BA-BEL and HumanML3D. The context indicates the dataset is used for generating diverse and natural 3D human motions from text.",
      "processing_time": 59.7751190662384,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model called MotionDiffuse. The cited papers' titles suggest a focus on dance generation, but do not provide specific dataset names.",
      "processing_time": 48.5721595287323,
      "citing_paper_id": "251953565",
      "cited_paper_id": 235614403
    },
    {
      "context_text": "MotionDiffuse excels at modeling complicated data distribution and generating vivid motion sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model called MotionDiffuse. The cited papers' titles suggest a focus on dance generation, but do not provide specific dataset names.",
      "processing_time": 48.5721595287323,
      "citing_paper_id": "251953565",
      "cited_paper_id": 221173065
    },
    {
      "context_text": ", 2018), 3D keypoints (Ionescu et al., 2013; Joo et al., 2015; Mehta et al., 2017; Trumble et al., 2017; Li et al., 2021) and statistical model parameters (Yu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. The cited paper title 'AI Choreographer: Music Conditioned 3D Dance Generation with AIST++' suggests a method or model rather than a dataset.",
      "processing_time": 50.61243772506714,
      "citing_paper_id": "251953565",
      "cited_paper_id": 236882798
    },
    {
      "context_text": "Similar to GLIDE [17], we ﬁrst get a text embedding e p by a linear transformation on the text features and a timestamp embedding e t by positional embedding [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is about using text and timestamp embeddings, which are not datasets.",
      "processing_time": 48.24477958679199,
      "citing_paper_id": "251953565",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Instead of predicting x t − 1 , here we follow GLIDE [17] and predict the noise term (cid:4) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GLIDE but does not indicate it is a dataset. It appears to be a method or model for image generation and editing.",
      "processing_time": 48.24329972267151,
      "citing_paper_id": "251953565",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "This architecture is also employed in later works (Tevet et al., 2022; Hong et al., 2022; Petrovich et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. There is no indication of a reusable resource being used.",
      "processing_time": 53.84122276306152,
      "citing_paper_id": "251953565",
      "cited_paper_id": 247450907
    },
    {
      "context_text": "MotionCLIP (Tevet et al., 2022) could generate stylized motions, but it is still limited to short text inputs and fails to handle complicated motion descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MotionCLIP but does not refer to it as a dataset. It is described as a method or model for generating stylized motions.",
      "processing_time": 54.312586069107056,
      "citing_paper_id": "251953565",
      "cited_paper_id": 247450907
    },
    {
      "context_text": "Most of them are deterministic generation (Ahuja and Morency, 2019; Ghosh et al., 2021; Tevet et al., 2022), which means they can only generate a single result from the given text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only deterministic generation methods. No verifiable resources are identified.",
      "processing_time": 52.757296562194824,
      "citing_paper_id": "251953565",
      "cited_paper_id": 247450907
    },
    {
      "context_text": "In addition, they (Petrovich et al., 2022; Tevet et al., 2022) typically only accept a single text prompt, which greatly limits users’ creativity.",
      "catation_intent": "limitation",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of existing methods.",
      "processing_time": 51.73602056503296,
      "citing_paper_id": "251953565",
      "cited_paper_id": 247450907
    },
    {
      "context_text": "Instead of learning a direct mapping between the text space and the motion space (Tevet et al., 2022), we pro-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is too limited to infer the use of any datasets.",
      "processing_time": 53.674909830093384,
      "citing_paper_id": "251953565",
      "cited_paper_id": 247450907
    },
    {
      "context_text": "This architecture is also employed in later works [12], [14], [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works. There is no indication of dataset usage or specific research context.",
      "processing_time": 53.389567852020264,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "We also train TEMOS [12] on both datasets from scratch based on the ofﬁcial implementation, with the same motion representation as our method for a fair comparison.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context does not provide specific dataset names, only mentions 'both datasets' without further details. The cited paper title does not help disambiguate any dataset names.",
      "processing_time": 55.03050422668457,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "TEMOS [12] introduces the VAE architecture into this task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (VAE architecture) introduced in the cited paper. The title of the cited paper suggests a focus on generating human motions from text, but no specific dataset is named.",
      "processing_time": 57.659239768981934,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "In addition, they [12], [14] typically only accept a single text prompt, which greatly limits users’ creativity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of existing methods in accepting a single text prompt.",
      "processing_time": 52.59509539604187,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "Various condition signals, including pre-deﬁned motion categories [1], [2], [3], music pieces [4], [5], [6], [7], [8], and natural language [9], [10], [11], [12], have been leveraged in previous human motion generation methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'natural language' as a condition signal for human motion generation, which aligns with the topic of personalized text generation. However, no specific dataset names are mentioned.",
      "processing_time": 55.30122113227844,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "Various condition signals, including pre-deﬁned motion categories [1], [2], [3], music pieces [4], [5], [6], [7], [8], and natural language [9], [10], [11], [12], have been leveraged in previous human motion generation methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'natural language' as a condition signal for human motion generation, which aligns with the topic of personalized text generation. However, no specific dataset names are mentioned.",
      "processing_time": 55.30122113227844,
      "citing_paper_id": "251953565",
      "cited_paper_id": 216093664
    },
    {
      "context_text": "We compare our method with TEMOS [12] and visualize two examples for each given prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method (TEMOS).",
      "processing_time": 52.05854368209839,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "Recently, TEMOS [12] utilizes KIT Motion-Language MoCap dataset [13] to demonstrate accurate human motion synthesis.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KIT Motion-Language MoCap dataset"
      ],
      "dataset_descriptions": {
        "KIT Motion-Language MoCap dataset": "Used to demonstrate accurate human motion synthesis from textual descriptions, focusing on generating diverse and realistic human motions using the TEMOS model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the KIT Motion-Language MoCap dataset, which is a specific dataset used for demonstrating accurate human motion synthesis in the TEMOS paper.",
      "processing_time": 62.17304706573486,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "Recently, TEMOS [12] utilizes KIT Motion-Language MoCap dataset [13] to demonstrate accurate human motion synthesis.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "KIT Motion-Language MoCap dataset"
      ],
      "dataset_descriptions": {
        "KIT Motion-Language MoCap dataset": "Used to demonstrate accurate human motion synthesis from textual descriptions, focusing on generating diverse and realistic human motions using the TEMOS model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the KIT Motion-Language MoCap dataset, which is a specific dataset used for demonstrating accurate human motion synthesis in the TEMOS paper.",
      "processing_time": 62.17304706573486,
      "citing_paper_id": "251953565",
      "cited_paper_id": 221173065
    },
    {
      "context_text": "Auto-Encoder (AE) and Variational Auto-Encoder (VAE) are the most widely used approaches in text-driven motion generation [11], [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about using AE and VAE for text-driven motion generation, which does not provide information on datasets.",
      "processing_time": 55.31744313240051,
      "citing_paper_id": "251953565",
      "cited_paper_id": 248476220
    },
    {
      "context_text": "HumanML3D [48] re-annotates the AMASS dataset [73] and the HumanAct12 dataset [1].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AMASS",
        "HumanAct12"
      ],
      "dataset_descriptions": {
        "AMASS": "Re-annotated to enhance the diversity and naturalness of 3D human motions generated from text, focusing on improving motion quality and realism.",
        "HumanAct12": "Re-annotated to improve the generation of diverse and natural 3D human motions from text, specifically enhancing action representation and variability."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions the re-annotation of two datasets, AMASS and HumanAct12, which are used in the context of generating diverse and natural 3D human motions from text.",
      "processing_time": 69.25568914413452,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Following Guo et al. [48], pose states in this series of experiments mainly contain seven different parts: ( r va , r vx , r vz , r h , j p , j v , j r ) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for generating 3D human motions from text. The content is focused on describing the pose states used in their experiments.",
      "processing_time": 54.90164613723755,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "[48]proposesanauto-regressive pipeline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an auto-regressive pipeline. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.039554834365845,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Following Guo et al. [48], we utilize the pretrained text-motion contrastive model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a pretrained model but does not specify a dataset. The context does not provide information about a specific dataset being used.",
      "processing_time": 53.03601551055908,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Auto-regressive methods [40], [48] have solved this problem with satisfactory performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 52.86152505874634,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Quantitative Results: We compare our proposed Motion-Diffuse with ﬁve baseline models: Language2Pose [10], Text2Gesture [80], MoCoGAN [81], Dance2Music [46], and Guo et al. [48].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.28774809837341,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Quantitative Results: We compare our proposed Motion-Diffuse with ﬁve baseline models: Language2Pose [10], Text2Gesture [80], MoCoGAN [81], Dance2Music [46], and Guo et al. [48].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods used for comparison. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.28774809837341,
      "citing_paper_id": "251953565",
      "cited_paper_id": 121033958
    },
    {
      "context_text": "Guo et al. [48] states that the results on the MultiModality metric should be larger whenever possible.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric called 'MultiModality'. Since metrics are excluded, no datasets are identified.",
      "processing_time": 53.3845272064209,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "All baselines’ performances are quoted from Guo et al. [48].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references performance results from another paper.",
      "processing_time": 51.510502338409424,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "5 shows a comparison between our method and Guo et al. [48] as a baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a baseline method.",
      "processing_time": 51.68980574607849,
      "citing_paper_id": "251953565",
      "cited_paper_id": 250602257
    },
    {
      "context_text": "Concurrent with this work, Tevet et al. [44], and Kim et al. [45] also attempted to exploit the potential of diffusion models on motion generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other research works exploring similar methodologies.",
      "processing_time": 51.512264251708984,
      "citing_paper_id": "251953565",
      "cited_paper_id": 251979380
    },
    {
      "context_text": "Concurrent with this work, Tevet et al. [44], and Kim et al. [45] also attempted to exploit the potential of diffusion models on motion generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other research works exploring similar methodologies.",
      "processing_time": 51.512264251708984,
      "citing_paper_id": "251953565",
      "cited_paper_id": 252595883
    },
    {
      "context_text": "Statistical models such as PCA [26], Motion Graph [27] are applied for these purposes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (PCA, Motion Graph). No verifiable resources are identified.",
      "processing_time": 52.61475133895874,
      "citing_paper_id": "251953565",
      "cited_paper_id": null
    },
    {
      "context_text": "Statistical models such as PCA [26], Motion Graph [27] are applied for these purposes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (PCA, Motion Graph). No verifiable resources are identified.",
      "processing_time": 52.61475133895874,
      "citing_paper_id": "251953565",
      "cited_paper_id": 9224819
    },
    {
      "context_text": ", 2005), Motion Graph (Min and Chai, 2012) are applied for these purposes.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or tools. There are no verifiable resources that meet the criteria.",
      "processing_time": 52.88742756843567,
      "citing_paper_id": "251953565",
      "cited_paper_id": null
    },
    {
      "context_text": "GANs are typically challenging to train [74], [75], especially for conditional motion generation tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the challenge of training GANs for conditional motion generation tasks.",
      "processing_time": 52.440733671188354,
      "citing_paper_id": "251953565",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, another generative model has attracted much attention with the considerable success achieved by NeRF [42], [43] in rendering realistic images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a generative model (NeRF). No verifiable resources are identified.",
      "processing_time": 52.87662100791931,
      "citing_paper_id": "251953565",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, another generative model has attracted much attention with the considerable success achieved by NeRF [42], [43] in rendering realistic images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a generative model (NeRF). No verifiable resources are identified.",
      "processing_time": 52.87662100791931,
      "citing_paper_id": "251953565",
      "cited_paper_id": 232478424
    },
    {
      "context_text": "Action labels are also a popular attribute of datasets for human action understanding that contains human-centric actions [60], [61], [62], [63], [64], [65], interaction [66], [67], [68], ﬁne-grained action understanding [63], [64], [65] and 3D data [69].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVA",
        "UCF101",
        "HACS"
      ],
      "dataset_descriptions": {
        "AVA": "Used to study spatio-temporally localized atomic visual actions in videos, focusing on human-centric actions and fine-grained action understanding.",
        "UCF101": "Used to analyze 101 human action classes from videos in the wild, emphasizing a wide range of human-centric actions and interactions.",
        "HACS": "Used for human action recognition and temporal localization, providing clips and segments of human actions in various contexts."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'datasets for human action understanding' but does not specify any dataset names. However, the cited paper titles provide specific dataset names which are relevant to the topic of personalized text generation.",
      "processing_time": 74.7044186592102,
      "citing_paper_id": "251953565",
      "cited_paper_id": 688013
    },
    {
      "context_text": "Action labels are also a popular attribute of datasets for human action understanding that contains human-centric actions [60], [61], [62], [63], [64], [65], interaction [66], [67], [68], ﬁne-grained action understanding [63], [64], [65] and 3D data [69].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVA",
        "UCF101",
        "HACS"
      ],
      "dataset_descriptions": {
        "AVA": "Used to study spatio-temporally localized atomic visual actions in videos, focusing on human-centric actions and fine-grained action understanding.",
        "UCF101": "Used to analyze 101 human action classes from videos in the wild, emphasizing a wide range of human-centric actions and interactions.",
        "HACS": "Used for human action recognition and temporal localization, providing clips and segments of human actions in various contexts."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'datasets for human action understanding' but does not specify any dataset names. However, the cited paper titles provide specific dataset names which are relevant to the topic of personalized text generation.",
      "processing_time": 74.7044186592102,
      "citing_paper_id": "251953565",
      "cited_paper_id": 7197134
    },
    {
      "context_text": "Action labels are also a popular attribute of datasets for human action understanding that contains human-centric actions [60], [61], [62], [63], [64], [65], interaction [66], [67], [68], ﬁne-grained action understanding [63], [64], [65] and 3D data [69].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AVA",
        "UCF101",
        "HACS"
      ],
      "dataset_descriptions": {
        "AVA": "Used to study spatio-temporally localized atomic visual actions in videos, focusing on human-centric actions and fine-grained action understanding.",
        "UCF101": "Used to analyze 101 human action classes from videos in the wild, emphasizing a wide range of human-centric actions and interactions.",
        "HACS": "Used for human action recognition and temporal localization, providing clips and segments of human actions in various contexts."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'datasets for human action understanding' but does not specify any dataset names. However, the cited paper titles provide specific dataset names which are relevant to the topic of personalized text generation.",
      "processing_time": 74.7044186592102,
      "citing_paper_id": "251953565",
      "cited_paper_id": 68049510
    },
    {
      "context_text": "A common form of datasets containing videos of human subjects are recorded with annotations such as 2D keypoints [49], [50], 3D keypoints [6], [51], [52], [53], [54] and statistical model parameters [55], [56], [57], [58], [59].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions various types of annotations in datasets but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 54.035422801971436,
      "citing_paper_id": "251953565",
      "cited_paper_id": 4768761
    },
    {
      "context_text": "A common form of datasets containing videos of human subjects are recorded with annotations such as 2D keypoints [49], [50], 3D keypoints [6], [51], [52], [53], [54] and statistical model parameters [55], [56], [57], [58], [59].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions various types of annotations in datasets but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 54.035422801971436,
      "citing_paper_id": "251953565",
      "cited_paper_id": 210473753
    },
    {
      "context_text": "GAN [33] introduces an auxiliary module, discriminator network, to justify the quality and validity of generated samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset. It only discusses the introduction of an auxiliary module in GANs.",
      "processing_time": 52.85562252998352,
      "citing_paper_id": "251953565",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Normalization Flow Network [37] has a long history and has been studied extensively for image synthesis [38], [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about normalization flow networks and their application in image synthesis, which does not indicate the use of a specific dataset.",
      "processing_time": 56.24577450752258,
      "citing_paper_id": "251953565",
      "cited_paper_id": 13995862
    },
    {
      "context_text": "HP-GAN [34] attempts to supervise the motion prediction results without the speciﬁc ground truth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HP-GAN). The context focuses on the method's approach to motion prediction without specific ground truth data.",
      "processing_time": 54.872734785079956,
      "citing_paper_id": "251953565",
      "cited_paper_id": 22857558
    },
    {
      "context_text": "Some works focus on proposing appropriate discriminator networks for motion generation to improve the synthesis quality [34], [35], [36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing discriminator networks for motion generation.",
      "processing_time": 52.26520323753357,
      "citing_paper_id": "251953565",
      "cited_paper_id": 22857558
    },
    {
      "context_text": "Yan et al. [29] and Aliakbarian et al. [30] regard the motion generation task as predicting a small future motion sequence with the given small current motion sequence.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the task of motion generation and prediction. No verifiable resources are identified.",
      "processing_time": 53.12094688415527,
      "citing_paper_id": "251953565",
      "cited_paper_id": 52003444
    },
    {
      "context_text": "Yan et al. [29] and Aliakbarian et al. [30] regard the motion generation task as predicting a small future motion sequence with the given small current motion sequence.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the task of motion generation and prediction. No verifiable resources are identified.",
      "processing_time": 53.12094688415527,
      "citing_paper_id": "251953565",
      "cited_paper_id": 219629756
    },
    {
      "context_text": "UESTC has a test split, so we report the FID on it, Fig.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide a clear name for a dataset, only mentioning 'UESTC' which could be a dataset, institution, or something else. There is insufficient information to confidently label it as a dataset.",
      "processing_time": 56.61056399345398,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "ACTOR [2] further uses pre-trained VIBE [83] to extract SMPL [84] sequences from the UESTC dataset and provides pretrained action recognition model for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC dataset"
      ],
      "dataset_descriptions": {
        "UESTC dataset": "Used to extract SMPL sequences and evaluate a pre-trained action recognition model, focusing on human action recognition in the context of personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the UESTC dataset, which is a specific, verifiable dataset used for human action recognition. The dataset is used to extract SMPL sequences and evaluate a pre-trained action recognition model.",
      "processing_time": 62.85357069969177,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "ACTOR [2] further uses pre-trained VIBE [83] to extract SMPL [84] sequences from the UESTC dataset and provides pretrained action recognition model for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC dataset"
      ],
      "dataset_descriptions": {
        "UESTC dataset": "Used to extract SMPL sequences and evaluate a pre-trained action recognition model, focusing on human action recognition in the context of personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the UESTC dataset, which is a specific, verifiable dataset used for human action recognition. The dataset is used to extract SMPL sequences and evaluate a pre-trained action recognition model.",
      "processing_time": 62.85357069969177,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "UESTC dataset (Ji et al., 2018) is also a significant bench-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC dataset"
      ],
      "dataset_descriptions": {
        "UESTC dataset": "Used for arbitrary-view human action recognition, focusing on large-scale RGB-D data to improve action recognition accuracy across different views."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'UESTC dataset' which is a specific dataset name. The cited paper title confirms it is a dataset used for human action recognition.",
      "processing_time": 61.009732484817505,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "For the action-conditioned motion generation task, HumanAct12 [1], UESTC [70], and NTU RGB+D [69] are three commonly used benchmarks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanAct12",
        "UESTC",
        "NTU RGB+D"
      ],
      "dataset_descriptions": {
        "HumanAct12": "Used for action-conditioned motion generation, focusing on generating realistic human motions based on specific actions.",
        "UESTC": "Used for action-conditioned motion generation, focusing on generating realistic human motions based on specific actions.",
        "NTU RGB+D": "Used for action-conditioned motion generation, focusing on generating realistic human motions based on specific actions."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for action-conditioned motion generation, which are relevant to personalized text generation in the context of human actions.",
      "processing_time": 70.90678668022156,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "HumanAct12 and UESTC are used to demonstrate the wide applicability of the proposed pipeline.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanAct12",
        "UESTC"
      ],
      "dataset_descriptions": {
        "HumanAct12": "Used to demonstrate the wide applicability of the proposed pipeline for human action recognition, focusing on arbitrary-view actions.",
        "UESTC": "Used to demonstrate the wide applicability of the proposed pipeline for human action recognition, focusing on diverse action types."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'HumanAct12' and 'UESTC' as datasets used to demonstrate the applicability of the proposed pipeline. These names are specific and plausible datasets.",
      "processing_time": 66.28249764442444,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": ", 2020), UESTC (Ji et al., 2018), and NTU RGB+D (Liu et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC",
        "NTU RGB+D"
      ],
      "dataset_descriptions": {
        "UESTC": "Used for evaluating human action recognition models, focusing on arbitrary-view actions and providing a large-scale dataset for training and testing.",
        "NTU RGB+D": "Utilized for human action recognition, specifically for multi-modal data including RGB and depth information, enhancing model performance in various view scenarios."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'UESTC' and 'NTU RGB+D', which are likely datasets given the context of human action recognition. However, 'NTU RGB+D' is incomplete and lacks a version or year, reducing confidence.",
      "processing_time": 70.89109563827515,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "UESTC dataset [70] is also a signiﬁcant benchmark for action-conditioned motion generation tasks, which includes 25 K motion sequences across 40 different action categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC dataset"
      ],
      "dataset_descriptions": {
        "UESTC dataset": "Used as a benchmark for action-conditioned motion generation tasks, including 25K motion sequences across 40 action categories, to evaluate the performance of models in generating personalized text based on actions."
      },
      "confidence_score": 0.9,
      "reasoning": "The UESTC dataset is mentioned as a significant benchmark for action-conditioned motion generation tasks, which aligns with the topic of personalized text generation, especially for generating text based on actions.",
      "processing_time": 63.87963008880615,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "We use ( r x , r y , r z , j r ) as the pose representation for the UESTC dataset, where r x , r y , r z ∈ R are Fig.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC dataset"
      ],
      "dataset_descriptions": {
        "UESTC dataset": "Used to represent human poses for action recognition, focusing on arbitrary-view actions with RGB-D data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the UESTC dataset, which is a specific, verifiable dataset used for human action recognition. The dataset is used to represent poses in the research.",
      "processing_time": 60.65866279602051,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "Each pose state is represented by ( r x , r y , r z , j r ) , which is same to the setting for the UESTC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "UESTC"
      ],
      "dataset_descriptions": {
        "UESTC": "Used to represent pose states for human action recognition, focusing on the specific parameters (r_x, r_y, r_z, j_r) for pose representation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the UESTC dataset, which is a specific, verifiable dataset used for human action recognition. The dataset is referenced for its representation of pose states.",
      "processing_time": 62.134416341781616,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "In this series of expreiments, we train 100 K iterations for the HumanAct12 dataset and 500 K for the UESTC dataset, both with a 0.0001 learning rate.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanAct12",
        "UESTC"
      ],
      "dataset_descriptions": {
        "HumanAct12": "Used to train models for 100 K iterations at a 0.0001 learning rate, focusing on arbitrary-view human action recognition.",
        "UESTC": "Used to train models for 500 K iterations at a 0.0001 learning rate, focusing on arbitrary-view human action recognition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, 'HumanAct12' and 'UESTC', which are used for training models in human action recognition experiments.",
      "processing_time": 68.94045662879944,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "Table V shows the quantitative results on the HumanAct12 dataset and the UESTC datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanAct12",
        "UESTC"
      ],
      "dataset_descriptions": {
        "HumanAct12": "Used to evaluate quantitative results in arbitrary-view human action recognition, focusing on RGB-D data.",
        "UESTC": "Used to evaluate quantitative results in human action recognition, likely providing additional data for model validation."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, 'HumanAct12' and 'UESTC', which are used for evaluating quantitative results in the context of human action recognition.",
      "processing_time": 64.14948439598083,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "Our approach outperforms all existing methods on both the HumanAct12 dataset and the UESTC dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HumanAct12",
        "UESTC"
      ],
      "dataset_descriptions": {
        "HumanAct12": "Used to evaluate the performance of the proposed approach on human action recognition, focusing on arbitrary-view actions.",
        "UESTC": "Used to evaluate the performance of the proposed approach on human action recognition, focusing on specific actions captured in a controlled environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, 'HumanAct12' and 'UESTC', which are used to evaluate the performance of the proposed approach.",
      "processing_time": 65.1011860370636,
      "citing_paper_id": "251953565",
      "cited_paper_id": 53039332
    },
    {
      "context_text": "Manyworksattempttoembedthe musicfeatureandmotionfeatureintoajointspace[5],[6],[46], [47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to works that attempt to embed music and motion features into a joint space.",
      "processing_time": 53.053465366363525,
      "citing_paper_id": "251953565",
      "cited_paper_id": 121033958
    },
    {
      "context_text": "for human action understanding that contains humancentric actions (Kuehne et al., 2011; Soomro et al., 2012; Karpathy et al., 2014; Gu et al., 2018; Shao et al., 2020; Chung et al., 2021), interaction (Carreira et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets by name. It only refers to general categories of actions and interactions in videos, which are not specific datasets.",
      "processing_time": 54.66714262962341,
      "citing_paper_id": "251953565",
      "cited_paper_id": 206592218
    },
    {
      "context_text": ", 2022), music pieces (Huang et al., 2020; Li et al., 2020, 2021; Zhuang et al., 2020; Siyao et al., 2022), and natural language (Lin et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists various types of content (music pieces, natural language) used in different studies.",
      "processing_time": 54.45011758804321,
      "citing_paper_id": "251953565",
      "cited_paper_id": 212658257
    },
    {
      "context_text": "Many works attempt to embed the music feature and motion feature into a joint space (Lee et al., 2019; Sun et al., 2020; Li et al., 2020, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or models used in embedding music and motion features.",
      "processing_time": 52.827401638031006,
      "citing_paper_id": "251953565",
      "cited_paper_id": 215896890
    },
    {
      "context_text": "We want to highlight that our results of the Human-Act12 dataset are notably close to real motions on all four metrics.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Human-Act12"
      ],
      "dataset_descriptions": {
        "Human-Act12": "Used to evaluate the realism of generated human motions, comparing results against real motions using four metrics."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Human-Act12' as a dataset used to evaluate results against real motions using four metrics.",
      "processing_time": 59.976590633392334,
      "citing_paper_id": "251953565",
      "cited_paper_id": 218508735
    },
    {
      "context_text": "Recently, another generative model has attracted much attention with the considerable success achieved by NeRF (Mildenhall et al., 2020; Jain et al., 2021) in",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a generative model (NeRF). No dataset names are present in the citation span.",
      "processing_time": 53.54203701019287,
      "citing_paper_id": "251953565",
      "cited_paper_id": 232478424
    },
    {
      "context_text": "Similar to the original NeRF, the timestamp is represented by sinusoidal values.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (NeRF) and its representation of timestamps. No verifiable resources are identified.",
      "processing_time": 53.97803974151611,
      "citing_paper_id": "251953565",
      "cited_paper_id": 232478424
    },
    {
      "context_text": "Unlike Ac-tion2Motion, ACTOR embeds the whole motion sequence into the latent space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two methods, ACTOR and Action2Motion.",
      "processing_time": 52.589256286621094,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "ACTOR [2] also uses VAE for random sampling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VAE) used in another paper. No verifiable resources are identified.",
      "processing_time": 53.5247905254364,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "Besides, ACTOR proposes a Transformer-based motion encoder and decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer-based motion encoder and decoder architecture).",
      "processing_time": 52.29179811477661,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "Quantitative Results: Following Cervantes et al. [3], three baseline models are selected: Action2Motion [1], ACTOR [2], INR [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions baseline models but does not refer to any specific datasets. The names mentioned are models, not datasets.",
      "processing_time": 52.51410698890686,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": ", 2020), ACTOR (Petrovich et al., 2021), INR (Cervantes et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable dataset names. It mentions models/methods (ACTOR, INR) which are excluded.",
      "processing_time": 53.85618758201599,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "ACTOR [2] proposes a transformer-based encoder and decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer-based encoder and decoder architecture).",
      "processing_time": 52.2703332901001,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "ACTOR (Petrovich et al., 2021) also uses VAE for random sampling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VAE) used in the cited paper. The context is about the methodology, not a dataset.",
      "processing_time": 54.98836541175842,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "ACTOR (Petrovich et al., 2021) proposes a transformer-based encoder and decoder architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transformer-based encoder and decoder architecture).",
      "processing_time": 52.49018955230713,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "However, the literature in action-conditioned motion generation task (Guo et al., 2020; Petrovich et al., 2021; Cervantes et al., 2022) argue that this metric should be close to the real motion.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric and a task. The context is about evaluating motion synthesis, but no datasets are explicitly named.",
      "processing_time": 54.34587621688843,
      "citing_paper_id": "251953565",
      "cited_paper_id": 233210075
    },
    {
      "context_text": "In dialogue systems, automated metrics tend to be borrowed from other NLP tasks such as BLEU (Papineni et al., 2002) from machine translation and ROUGE (Lin, 2004) from text summarization.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions automated metrics BLEU and ROUGE but does not refer to them as datasets. They are evaluation metrics, not datasets.",
      "processing_time": 53.43539357185364,
      "citing_paper_id": "53217693",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In dialogue systems, automated metrics tend to be borrowed from other NLP tasks such as BLEU (Papineni et al., 2002) from machine translation and ROUGE (Lin, 2004) from text summarization.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions automated metrics BLEU and ROUGE but does not refer to them as datasets. They are evaluation metrics, not datasets.",
      "processing_time": 53.43539357185364,
      "citing_paper_id": "53217693",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Examples include ADEM (Lowe et al., 2017), an evaluation model that learns to score responses from an annotated dataset of human responses scores.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "annotated dataset of human response scores"
      ],
      "dataset_descriptions": {
        "annotated dataset of human response scores": "Used to train ADEM, an evaluation model that scores dialogue responses, focusing on learning from human annotations to assess response quality."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions ADEM, which is an evaluation model, not a dataset. However, it refers to an 'annotated dataset of human response scores' used by ADEM, which fits the criteria for a dataset.",
      "processing_time": 63.635355949401855,
      "citing_paper_id": "53217693",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "This compares with values of 0.351 for Automatic User Ratings (Venkatesh et al., 2018) and 0.436 for ADEM (Lowe et al., 2017) ment ( ρ = 0 .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Automatic User Ratings' and 'ADEM', which are evaluation methods or metrics rather than datasets. No specific datasets are mentioned.",
      "processing_time": 53.6046085357666,
      "citing_paper_id": "53217693",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "This compares with values of 0.351 for Automatic User Ratings (Venkatesh et al., 2018) and 0.436 for ADEM (Lowe et al., 2017) ment ( ρ = 0 .",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Automatic User Ratings' and 'ADEM', which are evaluation methods or metrics rather than datasets. No specific datasets are mentioned.",
      "processing_time": 53.6046085357666,
      "citing_paper_id": "53217693",
      "cited_paper_id": 22328682
    },
    {
      "context_text": "We also conduct a comparative analysis with a previously re-ported metric ADEM (Lowe et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ADEM, which is a metric, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 53.179603099823,
      "citing_paper_id": "53217693",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "1) to the human ratings (i.e., horizontal axis) at the cost of lowering correlation, as done in (Lowe et al., 2017). comparison instructions are included in the supplementary material.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for evaluating dialogue responses.",
      "processing_time": 51.843404054641724,
      "citing_paper_id": "53217693",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "The model parameters are learned by optimizing the log-likelihood of the utterances via Adam optimizer with a learning rate of 0.0002; we followed (Luong et al., 2015) for decaying the learning rate.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training a model. The context is about the optimization process and learning rate decay.",
      "processing_time": 53.59305930137634,
      "citing_paper_id": "53217693",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "Our approach is close to sampling and ﬁnding the nearest neighbour in image generative models (Theis et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The cited paper title confirms the focus on generative models rather than datasets.",
      "processing_time": 54.36705470085144,
      "citing_paper_id": "53217693",
      "cited_paper_id": 2187805
    },
    {
      "context_text": "Instinctively, humans tend to adapt conversations to their interlocutor not only by looking at the last utterance but also by considering information and concepts covered in the conversation history (Danescu-Niculescu-Mizil and Lee, 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of adapting conversations. No clear, verifiable resource is identified.",
      "processing_time": 53.16506385803223,
      "citing_paper_id": "53217693",
      "cited_paper_id": 3101865
    },
    {
      "context_text": "In this work, we propose directly calculable approximations of human evaluation grounded in conversational theories of accommodation and affordance (Danescu-Niculescu-Mizil and Lee, 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework. The cited paper is used to support the methodological approach.",
      "processing_time": 53.14942407608032,
      "citing_paper_id": "53217693",
      "cited_paper_id": 3101865
    },
    {
      "context_text": "Topical Hierarchical Recurrent Encoder Decoder (THRED) can be viewed as a hybrid model that conditions the response generation on conversation history captured from previous utterances and on topic words acquired from a Latent Dirichlet Allocation (LDA) model (Blei et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LDA) and a model (THRED).",
      "processing_time": 52.72754693031311,
      "citing_paper_id": "53217693",
      "cited_paper_id": 3177797
    },
    {
      "context_text": "…set of 60 dull responses and computed the SS score by multiplying the cosine distance with the following penalty factor (akin to length penalty in (Wu et al., 2016)):\nP = 1 + log 2 + L′\n2 + L′′\nwhere L′ indicates the length of the response after dropping stop words and punctuation and L′′ stands…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing a score. The context is about a scoring mechanism, not a dataset.",
      "processing_time": 53.616743087768555,
      "citing_paper_id": "53217693",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "During inference, we experimented with the standard beam search with the beam width 5 and the length normalization α = 1 (Wu et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (beam search) and a reference to a paper describing a neural machine translation system.",
      "processing_time": 53.61449885368347,
      "citing_paper_id": "53217693",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "To render the semantic representation of an utterance, we leverage Universal Sentence En-coder (Cer et al., 2018) wherein a sentence is projected to a ﬁxed dimensional embedding vector.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Universal Sentence Encoder but does not refer to it as a dataset. It is used as a method for projecting sentences to embedding vectors.",
      "processing_time": 53.604374408721924,
      "citing_paper_id": "53217693",
      "cited_paper_id": 4494896
    },
    {
      "context_text": "Additionally, Fleiss’ Kappa score is used to gauge the reliability of the agreement between human evaluators (Shao et al., 2017).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (Fleiss’ Kappa) used for evaluating inter-rater reliability.",
      "processing_time": 53.34398913383484,
      "citing_paper_id": "53217693",
      "cited_paper_id": 5586146
    },
    {
      "context_text": "The message encoder sequentially accepts the embedding of each word in the input message Ui and updates its hidden state at every time step t by a bidirectional GRU-RNN (Cho et al., 2014) according to:",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GRU-RNN) and a paper title. The paper title is used to reference the method, not a dataset.",
      "processing_time": 55.714812994003296,
      "citing_paper_id": "53217693",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The message encoder sequentially accepts the embedding of each word in the input message U i and updates its hidden state at every time step t by a bidirectional GRU-RNN (Cho et al., 2014) according to: where h i,t − 1 represents the previous hidden state.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GRU-RNN) and a paper title. No verifiable datasets are referenced.",
      "processing_time": 54.1166729927063,
      "citing_paper_id": "53217693",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "We compare THRED against three open-source baselines, namely Standard Seq2Seq with attention mechanism (Bahdanau et al., 2015), HRED (Serban et al., 2016), and Topic-Aware (TA) Seq2Seq (Xing et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.580525636672974,
      "citing_paper_id": "53217693",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "We compare THRED against three open-source baselines, namely Standard Seq2Seq with attention mechanism (Bahdanau et al., 2015), HRED (Serban et al., 2016), and Topic-Aware (TA) Seq2Seq (Xing et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.580525636672974,
      "citing_paper_id": "53217693",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "To account further for diversity in generated responses, following (Li et al., 2016a), we calculated distinct-1 and distinct-2 by counting unique unigrams and bigrams, normalized by the number of generated words.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (distinct-1 and distinct-2) used to evaluate diversity in generated responses.",
      "processing_time": 53.59061789512634,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Instead, generated responses are dull, short and carry little information (Li et al., 2016a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with generated responses in neural conversation models.",
      "processing_time": 51.32366967201233,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "As done in (Li et al., 2016b), for Standard Seq2Seq and TA-Seq2Seq, we concatenate the dialogue history to account for context in a multi-turn conversation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for concatenating dialogue history in neural conversation models.",
      "processing_time": 53.32772493362427,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "To account further for diversity in generated responses, following (Li et al., 2016a), we calculated distinct - 1 and distinct - 2 by counting unique un-igrams and bigrams, normalized by the number of generated words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for calculating diversity in generated responses.",
      "processing_time": 51.73804187774658,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "(Li et al., 2016b) used deep reinforcement learning to generate highly-rewarded responses by considering three dialogue properties: ease of answering, informativeness and coherence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating responses in dialogues.",
      "processing_time": 51.94907331466675,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Then, we report the results based on response diversity metric, derived from (Li et al., 2016a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric derived from the cited paper.",
      "processing_time": 51.95712494850159,
      "citing_paper_id": "53217693",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Following (Papineni et al., 2002), the test dataset is randomly partitioned to 5 disjoint subsets (i.e., each one consists of 1000 test dialogues).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a test dataset but does not specify a name. It only describes how the dataset is partitioned. No specific, verifiable dataset name is provided.",
      "processing_time": 55.51693153381348,
      "citing_paper_id": "53217693",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "It computes, at step t , a weight value α i,j,t for every encoder hidden state h i,j and linearly combines them to form a vector m i,t according to Bahdanau attention mechanism (Bahdanau et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Bahdanau attention mechanism).",
      "processing_time": 51.81108856201172,
      "citing_paper_id": "53217693",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "We assign a topic T to the conversation context using a pre-trained LDA model (Hoffman et al., 2010).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a pre-trained LDA model but does not refer to a specific dataset. The LDA model is a method, not a dataset.",
      "processing_time": 54.071823835372925,
      "citing_paper_id": "53217693",
      "cited_paper_id": 15674552
    },
    {
      "context_text": "(Venkatesh et al., 2018) proposed a number of metrics based on user experience, coherence, and topical diversity and have showed that these metrics can be used as a proxy for human evaluation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and evaluation methods.",
      "processing_time": 50.5022337436676,
      "citing_paper_id": "53217693",
      "cited_paper_id": 22328682
    },
    {
      "context_text": "A good dialogue system should be capable of sustaining a coherent conversation with a human by staying on topic and by following a train of thoughts (Venkatesh et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general requirement for dialogue systems. No verifiable resources are identified.",
      "processing_time": 52.948230504989624,
      "citing_paper_id": "53217693",
      "cited_paper_id": 22328682
    },
    {
      "context_text": "El modelo de usuario está diseñado para representar los intereses del usuario desde varios puntos de vista [1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of user profile modeling. No verifiable resources are identified.",
      "processing_time": 53.18833041191101,
      "citing_paper_id": "41140640",
      "cited_paper_id": 2306382
    },
    {
      "context_text": "[5] Chen, L., Sycara, K.P.: WebMate: A Personal Agent for Browsing and Searching.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a personal agent for browsing and searching.",
      "processing_time": 51.9641637802124,
      "citing_paper_id": "41140640",
      "cited_paper_id": 3521601
    },
    {
      "context_text": "Ejemplos representativos de sistemas de acceso a la información que integran este tipo de técnicas son WebMate [5], News Dude [3] y SIFT [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions systems that integrate certain techniques but does not specify any datasets. The names mentioned are systems or tools, not datasets.",
      "processing_time": 53.4252769947052,
      "citing_paper_id": "41140640",
      "cited_paper_id": 3521601
    },
    {
      "context_text": "Para obtener las M palabras más relevantes de cada noticia indexamos todas las noticias obteniendo así el peso de cada palabra en cada documento utilizando el método tf · idf [15].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method (tf-idf) for processing text data.",
      "processing_time": 54.05922818183899,
      "citing_paper_id": "41140640",
      "cited_paper_id": 34382228
    },
    {
      "context_text": "Los cálculos de similitud que realiza nuestro sistema se basan en el modelo del espacio vectorial [15], utilizando para la representación de textos, vectores de pesos de términos.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method (vector space model) for text representation, which is not a dataset.",
      "processing_time": 54.71572971343994,
      "citing_paper_id": "41140640",
      "cited_paper_id": 34382228
    },
    {
      "context_text": "It is worth noting that, since the number of users is large in the datasets, we choose to use the dot-product to compute the similarity of the users so that the whole process can be implemented by dense retrieval libraries, such as Faiss (Johnson et al., 2021), which",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets' generically without specifying a name. No specific dataset is identified, and the focus is on the method (dot-product) and tool (Faiss) used for similarity computation.",
      "processing_time": 56.82243752479553,
      "citing_paper_id": "247693734",
      "cited_paper_id": 926364
    },
    {
      "context_text": "It is worth noting that, since the number of users is large in the datasets, we choose to use the dot-product to compute the similarity of the users so that the whole process can be implemented by dense retrieval libraries, such as Faiss (Johnson et al., 2021), which is very efﬁcient.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets' generically without specifying a name. No specific dataset is identified, only a method (Faiss) for efficient similarity search.",
      "processing_time": 54.92775559425354,
      "citing_paper_id": "247693734",
      "cited_paper_id": 926364
    },
    {
      "context_text": "(4) GPMN (Zhang et al., 2018) enhances the Seq2seq model with a memory module, which encodes and stores the persona proﬁle as memory representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GPMN) that enhances a Seq2seq model with a memory module for storing persona profiles.",
      "processing_time": 54.45197081565857,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "…dialogue system via two pathways: (1) directly modeling user personality from predeﬁned persona descriptions or user attribute (Qian et al., 2018; Zhang et al., 2018; Song et al., 2019); and (2) implicitly modeling the user personality from the user’s dialogue history (Li et al., 2016c; Ma et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for personalizing dialogue systems.",
      "processing_time": 51.324647188186646,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "…include: (1) explicitly using predeﬁned persona descriptions or attributes as users’ proﬁle to generate personalized responses (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019); (2) using user ID embeddings to enhance personalized dialogue generation (Li et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses methods and approaches for personalizing dialogue agents but does not reference any named datasets.",
      "processing_time": 54.91875863075256,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "(4) GPMN (Zhang et al., 2018) enhances the Seq2seq model with a memory module, which encodes and stores the persona profile as memory representations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (GPMN) that enhances a Seq2seq model with a memory module for storing persona profiles. No specific dataset is mentioned.",
      "processing_time": 54.08745002746582,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In general, previous work explores building a personalized dialogue system via two pathways: (1) directly modeling user personality from predefined persona descriptions or user attribute (Qian et al., 2018; Zhang et al., 2018; Song et al., 2019); and (2) implicitly modeling the user personality from the user’s dialogue history (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalizing dialogue systems. No verifiable resources are identified.",
      "processing_time": 54.069854736328125,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In general, previous work explores building a personalized dialogue system via two pathways: (1) directly modeling user personality from predefined persona descriptions or user attribute (Qian et al., 2018; Zhang et al., 2018; Song et al., 2019); and (2) implicitly modeling the user personality from the user’s dialogue history (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalizing dialogue systems. No verifiable resources are identified.",
      "processing_time": 54.069854736328125,
      "citing_paper_id": "247693734",
      "cited_paper_id": 51608471
    },
    {
      "context_text": "Typical methods include: (1) explicitly using predefined persona descriptions or attributes as users’ profile to generate personalized responses (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019); (2) using user ID embeddings to enhance personalized dialogue generation (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalized dialogue generation. The cited papers' titles also do not provide clear evidence of specific datasets.",
      "processing_time": 55.19948363304138,
      "citing_paper_id": "247693734",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Typical methods include: (1) explicitly using predefined persona descriptions or attributes as users’ profile to generate personalized responses (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019); (2) using user ID embeddings to enhance personalized dialogue generation (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalized dialogue generation. The cited papers' titles also do not provide clear evidence of specific datasets.",
      "processing_time": 55.19948363304138,
      "citing_paper_id": "247693734",
      "cited_paper_id": 51608471
    },
    {
      "context_text": "Typical methods include: (1) explicitly using predefined persona descriptions or attributes as users’ profile to generate personalized responses (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019); (2) using user ID embeddings to enhance personalized dialogue generation (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalized dialogue generation. The cited papers' titles also do not provide clear evidence of specific datasets.",
      "processing_time": 55.19948363304138,
      "citing_paper_id": "247693734",
      "cited_paper_id": 67956318
    },
    {
      "context_text": "Non-personalized Methods (1) Seq2SeqAttention (Sutskever et al., 2014) is a vanilla sequence-to-sequence model with attention mechanism (Luong et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Seq2SeqAttention).",
      "processing_time": 51.68041276931763,
      "citing_paper_id": "247693734",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "They can be categorized into four groups: Non-personalized Methods (1) Seq2Seq-Attention (Sutskever et al., 2014) is a vanilla sequence-to-sequence model with attention mechanism (Luong et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.04854226112366,
      "citing_paper_id": "247693734",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Indeed, existing studies (Xing et al., 2017; Zhu et al., 2020) have demonstrated the effectiveness of using informative tokens to improve the response generation.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the effectiveness of using informative tokens in response generation.",
      "processing_time": 52.317721128463745,
      "citing_paper_id": "247693734",
      "cited_paper_id": 9514751
    },
    {
      "context_text": "(5) PerCVAE (Zhao et al., 2017) encodes predeﬁned personalized sentences as a conditional representation and uses CVAE to generate a personalized response.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of personalized sentences in a conditional representation for generating personalized responses, but does not specify a dataset name.",
      "processing_time": 53.63092017173767,
      "citing_paper_id": "247693734",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "(5) PerCVAE (Zhao et al., 2017) encodes predefined personalized sentences as a conditional representation and uses CVAE to generate a personalized response.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PerCVAE) and a model (CVAE).",
      "processing_time": 53.880298137664795,
      "citing_paper_id": "247693734",
      "cited_paper_id": 14688760
    },
    {
      "context_text": "…prede-119 fined persona descriptions or attributes as users’120 profile to generate personalized responses (Qian121 et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019;122 Song et al., 2019); (2) using user ID embeddings123 to enhance personalized dialogue generation (Li124 et al., 2016c; Chan…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions personalized dialogue generation but does not specify any datasets. It refers to methods and models rather than datasets.",
      "processing_time": 52.99163579940796,
      "citing_paper_id": "247693734",
      "cited_paper_id": 67956318
    },
    {
      "context_text": "Retrieval-guided Natural Language Generation Retrieval-based methods can collect relevant information for language generation (Yang et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for retrieval-based language generation.",
      "processing_time": 52.08088970184326,
      "citing_paper_id": "247693734",
      "cited_paper_id": 126187120
    },
    {
      "context_text": ", 2019b) is a variant of GPT-2 (Radford et al., 2019) designed for dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model variant (GPT-2) and its application to dialogue generation.",
      "processing_time": 54.040953159332275,
      "citing_paper_id": "247693734",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "It has been widely applied in many tasks such as text style transfer (Li et al., 2018) and dialogue generation (Wu et al., 2019; Cai et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of a method in text style transfer and dialogue generation.",
      "processing_time": 53.26200032234192,
      "citing_paper_id": "247693734",
      "cited_paper_id": 202779832
    },
    {
      "context_text": "Personalized Dialogue Generation Open-domain dialogue generation has been extensively studied (Koehn et al., 2003; Vinyals and Le, 2015; Serban et al., 2016; Zhang et al., 2019a,b; Liu et al., 2020; Xiao et al., 2020; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No verifiable resources are identified.",
      "processing_time": 53.27184438705444,
      "citing_paper_id": "247693734",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "(3) DialoGPT (Zhang et al., 2019b) is a variant of GPT-2 (Radford et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.28552055358887,
      "citing_paper_id": "247693734",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue generation has been extensively studied (Koehn et al., 2003; Vinyals and Le, 2015; Serban et al., 2016; Zhang et al., 2019a,b; Liu et al., 2020; Xiao et al., 2020; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No verifiable resources are identified.",
      "processing_time": 53.26694893836975,
      "citing_paper_id": "247693734",
      "cited_paper_id": 211031876
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue generation has been extensively studied (Koehn et al., 2003; Vinyals and Le, 2015; Serban et al., 2016; Zhang et al., 2019a,b; Liu et al., 2020; Xiao et al., 2020; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No verifiable resources are identified.",
      "processing_time": 53.26694893836975,
      "citing_paper_id": "247693734",
      "cited_paper_id": 226283651
    },
    {
      "context_text": "As Table 8 shows, models trained on reviews and tips tend to generate generic phrases (such as ‘i love this place’) which often do not include information that helps users to make de-Dataset Diversity-aware NLG Diversity is an important aspect of NLG systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses generic phrases generated by models trained on reviews and tips, but does not name any particular dataset.",
      "processing_time": 55.374130725860596,
      "citing_paper_id": "202621357",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Recent works in natural language generation (NLG) tried to combine generation methods with information retrieval techniques to increase the generation diversity (Li et al., 2018; Baheti et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches in NLG.",
      "processing_time": 52.04689383506775,
      "citing_paper_id": "202621357",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Similarly, the extract-and-edit paradigm has been studied in style transfer tasks in NLG (Li et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to style transfer tasks in NLG.",
      "processing_time": 52.59947371482849,
      "citing_paper_id": "202621357",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Our user encoder and sequence encoder share the same structure, which includes an embedding layer, a two-layer bi-directional GRU (Cho et al., 2014), and a projection layer.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (bi-directional GRU) used in the model architecture.",
      "processing_time": 53.4827766418457,
      "citing_paper_id": "202621357",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The decoder is a two-layer GRU that predicts the target words given a start to-ken.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a description of a decoder architecture. No verifiable resources are identified.",
      "processing_time": 53.47052049636841,
      "citing_paper_id": "202621357",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "These justiﬁcations pass a word embedding layer, then go through the GRU and yield a sequence of hidden states e ∈ R l s × l r × n : ) where l s denotes the length of the sequence, n is the hidden size of the encoder GRU, E ∈ R l s × l r × n is the embedded sequence representation, and → e and ← e are the hidden vectors produced by a forward and a backward GRU (respectively).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the architecture of a neural network model.",
      "processing_time": 52.12287378311157,
      "citing_paper_id": "202621357",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The hidden state at time-step t is updated via the GRU unit based on the previous hidden state and the input word.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GRU unit) for updating hidden states in a neural network.",
      "processing_time": 53.894695520401,
      "citing_paper_id": "202621357",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "To improve generation quality and diversity, we pro-pose two generation models (1) a reference-based Seq2Seq model with aspect-planning, which takes previous justiﬁcations as a reference and can produce justiﬁcations based on different aspects, and (2) an aspect-conditional masked language model that can generate diverse justiﬁcations from templates extracted from previous justiﬁcations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methodologies. The context focuses on describing the proposed models for personalized text generation.",
      "processing_time": 54.298399209976196,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Attr2Seq (Dong et al., 2017) is a Seq2Seq baseline that uses attributes (i.e. user and item identity) as input.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Attr2Seq) and its use of attributes as input, which is not a dataset.",
      "processing_time": 55.575289487838745,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Attr2Seq (Dong et al., 2017) is a Seq2Seq baseline that uses attributes (i.e. user and item identity) as input.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Attr2Seq) and its use of attributes as input, which is not a dataset.",
      "processing_time": 55.575289487838745,
      "citing_paper_id": "202621357",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "Though Seq2Seq-based models can achieve high quality output, they often fail to generate diverse content.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general limitation of Seq2Seq models.",
      "processing_time": 51.450937032699585,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Our base model follows the structure of a standard Seq2Seq (Sutskever et al., 2014) model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model architecture. No dataset names are present in the citation span.",
      "processing_time": 53.62980628013611,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "As shown in Table 6, both sampling-based methods Ref2Seq (Top-k) and ACMLM achieve higher Distinct-1 and Distinct-2, while their BLEU scores are lower than Seq2Seq based models using beam search.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model performance metrics. No verifiable resources are identified.",
      "processing_time": 52.64865517616272,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Our base model follows the structure of a standard Seq2Seq (Sutskever et al., 2014) Sequence Encoders .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Seq2Seq).",
      "processing_time": 51.67501783370972,
      "citing_paper_id": "202621357",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Catherine and Cohen (2017) learn latent representations of review text to predict ratings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning latent representations from review text.",
      "processing_time": 52.62149786949158,
      "citing_paper_id": "202621357",
      "cited_paper_id": 9932413
    },
    {
      "context_text": "Attr2Seq (Dong et al., 2017) is a Seq2Seq baseline that uses attributes (i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called Attr2Seq. The title confirms that the focus is on generating product reviews from attributes, but no dataset is explicitly named.",
      "processing_time": 56.19558787345886,
      "citing_paper_id": "202621357",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "Previous work has sought to learn user preferences and writing styles from crowd-sourced reviews (Dong et al., 2017; Ni and McAuley, 2018) Review examples: I love this little stand!",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions crowd-sourced reviews but does not specify a named dataset. The reference to 'crowd-sourced reviews' is too generic and lacks a specific, identifiable dataset name.",
      "processing_time": 56.33362078666687,
      "citing_paper_id": "202621357",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "Previous work has sought to learn user preferences and writing styles from crowd-sourced reviews (Dong et al., 2017; Ni and McAuley, 2018) Review examples: I love this little stand! The coconut mocha chiller and caramel macchiato are delicious.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions crowd-sourced reviews but does not specify a named dataset. The review examples provided are illustrative and not part of a specific, verifiable dataset.",
      "processing_time": 55.337279081344604,
      "citing_paper_id": "202621357",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "We compare the performance of BERT with multiple baseline models: (1) a XGBoost model which uses Bags-of-Words as sentence features (2) a convolutional neural network (CNN) with three convolution layers and one linear layer (3) a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) network with a max-pooling layer, and a linear layer (4) a BERT sentiment classiﬁer (BERT-SA) trained on the complete Yelp dataset for one epoch and three epochs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp dataset"
      ],
      "dataset_descriptions": {
        "Yelp dataset": "Used to train a BERT sentiment classifier, comparing performance with baseline models including XGBoost, CNN, and LSTM. The dataset is used to evaluate the effectiveness of BERT in sentiment classification tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Yelp dataset, which is used to train a BERT sentiment classifier. No other datasets are explicitly named.",
      "processing_time": 61.62401008605957,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The original BERT paper applies a ﬂat rate (15",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a flat rate applied in the BERT paper. BERT itself is a model, not a dataset.",
      "processing_time": 54.980977296829224,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For ACMLM, we build our model based on the BERT implementation from HuggingFace.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using BERT but does not refer to it as a dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 55.13181519508362,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For a justiﬁcation J u,i that user u wrote about item i , we adapt the pre-trained BERT model (Devlin et al., 2019) into an encoder-decoder network with (1) an aspect encoder which encodes the user persona and item proﬁle into latent representations and (2) a masked language model sequence decoder that takes in a masked justiﬁcation and predicts the masked tokens.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT, which is a model and not a dataset.",
      "processing_time": 53.62787961959839,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For a justiﬁcation J u,i that user u wrote about item i , we adapt the pre-trained BERT model (Devlin et al., 2019) into an encoder-decoder network with (1) an aspect encoder which encodes the user persona and item proﬁle into latent representations and (2) a masked language model sequence decoder…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT, which is a model and not a dataset.",
      "processing_time": 53.613141775131226,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We split the annotated dataset into Train, Dev, and Test sets with a 0.8/0.1/0.1 ratio, ﬁne-tune the BERT classiﬁer on the Train set and choose the best model on the Dev set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions an 'annotated dataset' but does not provide a specific name. The dataset is used for training, development, and testing a BERT classifier.",
      "processing_time": 55.52144694328308,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The BERT-SA model after three epochs only achieves an F1-score of 0.491, which conﬁrms the difference be-tween sentiment analysis and our good/bad task, i.e., even if the segment has positive sentiment, it might be not suitable as a justiﬁcation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT) and a metric (F1-score). The citation is focused on the performance of the BERT-SA model rather than the use of a dataset.",
      "processing_time": 57.190287590026855,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We use the masked language model in the pre-trained BERT model as our sequence decoder and add attention over the aspect encoder’s output.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT as a pre-trained model. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 56.15538668632507,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The BERT classiﬁer has higher F1-score and precision than other classiﬁers.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison of BERT's performance against other classifiers. BERT is a model, not a dataset.",
      "processing_time": 55.112985134124756,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Here we adopt BERT (Devlin et al., 2019) to ﬁne-tune on our classiﬁcation task, where a [CLS] token is added to the beginning of each segment and the ﬁnal hidden state (i.e., output of BERT) corresponding to this token is fed into a linear layer to obtain the binary prediction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. It is used as a model for fine-tuning on a classification task.",
      "processing_time": 54.31610083580017,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Our aspect encoder shares the same WordPiece embeddings (Wu et al., 2016) as BERT.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (WordPiece embeddings) shared with BERT. BERT itself is a model, not a dataset.",
      "processing_time": 54.928242683410645,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Recent work (Wang and Cho, 2019; Mansimov et al., 2019) has shown that by sampling from the masked language model (e.g. BERT), it is able to generate coherent sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. BERT is referenced but it is a model, not a dataset.",
      "processing_time": 54.319881439208984,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Recent work (Wang and Cho, 2019; Mansimov et al., 2019) has shown that by sampling from the masked language model (e.g. BERT), it is able to generate coherent sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. BERT is referenced but it is a model, not a dataset.",
      "processing_time": 54.319881439208984,
      "citing_paper_id": "202621357",
      "cited_paper_id": 60441316
    },
    {
      "context_text": "After three epochs of ﬁne-tuning, BERT can achieve an F1-score of 0.80 on the Test set.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a performance metric on a test set. No verifiable resource is identified.",
      "processing_time": 53.97006368637085,
      "citing_paper_id": "202621357",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Prior knowledge could include story-lines in story generation (Yao et al., 2019), or historical responses in dialogue systems (We-ston et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts of prior knowledge in story generation and dialogue systems.",
      "processing_time": 53.21839165687561,
      "citing_paper_id": "202621357",
      "cited_paper_id": 53306064
    },
    {
      "context_text": "Inspired by ‘plan-and-write’ (Yao et al., 2019), we extend the base model to an Aspect-Planning Ref2Seq (AP-Ref2Seq) model where we plan a ﬁne-grained aspect before generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on extending a model rather than using a dataset.",
      "processing_time": 54.726545095443726,
      "citing_paper_id": "202621357",
      "cited_paper_id": 53306064
    },
    {
      "context_text": "Yao et al. (2019) proposed a method to incorporate planned story-lines in story generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for incorporating planned story-lines in story generation.",
      "processing_time": 51.891570806503296,
      "citing_paper_id": "202621357",
      "cited_paper_id": 53306064
    },
    {
      "context_text": "We follow the sampling strategy of Wang and Cho (2019) to generate justiﬁcations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a sampling strategy from another paper. No verifiable resources are identified.",
      "processing_time": 53.616262674331665,
      "citing_paper_id": "202621357",
      "cited_paper_id": 60441316
    },
    {
      "context_text": "Li et al. (2019a) developed an aspect-aware coarse-to-ﬁne review generation method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method developed by Li et al. (2019a).",
      "processing_time": 54.302385091781616,
      "citing_paper_id": "202621357",
      "cited_paper_id": 70350032
    },
    {
      "context_text": "Recently, there have been works showing that the generation output of sampling methods is more diverse and suitable on high-entropy tasks (Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about sampling methods in neural text generation.",
      "processing_time": 53.40121340751648,
      "citing_paper_id": "202621357",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Recently, there have been works showing that the generation output of sampling methods is more diverse and suitable on highentropy tasks (Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about sampling methods in neural text generation.",
      "processing_time": 52.94762134552002,
      "citing_paper_id": "202621357",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "To this end, we explore another decoding strategy— ‘Top-k sampling’ (Radford et al., 2019) and include a variant of our model: Ref2Ref (Top-k).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a decoding strategy and a model variant. No verifiable resources are referenced.",
      "processing_time": 54.06999754905701,
      "citing_paper_id": "202621357",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "To this end, we explore another decoding strategy— ‘Top-k sampling’ (Radford et al., 2019) in experiments",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a decoding strategy called 'Top-k sampling'. No verifiable resources are identified.",
      "processing_time": 54.27873229980469,
      "citing_paper_id": "202621357",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "1 Datasets Our datasets are built upon Amazon 5-core 2 [21] which includes user generated reviews and metadata spanning from May 1996 to July 2014 without duplicated records.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Amazon 5-core"
      ],
      "dataset_descriptions": {
        "Amazon 5-core": "Used to build datasets for personalized text generation, focusing on user-generated reviews and metadata from May 1996 to July 2014, ensuring no duplicated records."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Amazon 5-core' as a dataset used for building their datasets, which includes user-generated reviews and metadata. This fits the criteria for a specific, verifiable dataset.",
      "processing_time": 64.46379256248474,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1012652
    },
    {
      "context_text": "Since user textual reviews are informative and be er re ect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [3, 20, 22, 43, 49] and recommendation explainability [4, 6, 18, 29, 39, 47], which helps to enhance the e ectiveness, transparency, trustworthiness and persuasiveness of recommendation system [12, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to user reviews and recommendation systems. No clear, verifiable datasets are identified.",
      "processing_time": 54.79362964630127,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1378406
    },
    {
      "context_text": "Since user textual reviews are informative and be er re ect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [3, 20, 22, 43, 49] and recommendation explainability [4, 6, 18, 29, 39, 47], which helps to enhance the e ectiveness, transparency, trustworthiness and persuasiveness of recommendation system [12, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to user reviews and recommendation systems. No clear, verifiable datasets are identified.",
      "processing_time": 54.79362964630127,
      "citing_paper_id": "204874165",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "Since user textual reviews are informative and be er re ect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [3, 20, 22, 43, 49] and recommendation explainability [4, 6, 18, 29, 39, 47], which helps to enhance the e ectiveness, transparency, trustworthiness and persuasiveness of recommendation system [12, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to user reviews and recommendation systems. No clear, verifiable datasets are identified.",
      "processing_time": 54.79362964630127,
      "citing_paper_id": "204874165",
      "cited_paper_id": 17792584
    },
    {
      "context_text": "Later approaches to CF more and more advanced to more accurate but less transparent latent factor approaches, beginning from various matrix factorization algorithms [15, 26, 34, 36], to more recent deep learning and neural modeling approaches [44, 45, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.79033589363098,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1401406
    },
    {
      "context_text": "Later approaches to CF more and more advanced to more accurate but less transparent latent factor approaches, beginning from various matrix factorization algorithms [15, 26, 34, 36], to more recent deep learning and neural modeling approaches [44, 45, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.79033589363098,
      "citing_paper_id": "204874165",
      "cited_paper_id": 3890973
    },
    {
      "context_text": "Later approaches to CF more and more advanced to more accurate but less transparent latent factor approaches, beginning from various matrix factorization algorithms [15, 26, 34, 36], to more recent deep learning and neural modeling approaches [44, 45, 49].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.79033589363098,
      "citing_paper_id": "204874165",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "It depends on the previous hidden state hwn,t−1 and the current input wn,t :\nhwn,t = f (hwn,t−1,wn,t ) (16)\ne f (·) can be LSTM, GRU or Vanilla RNN.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model architectures. The citation is about the function used in the model, not a dataset.",
      "processing_time": 54.39252305030823,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "e long short-term memory unit (LSTM) [12] and gated recurrent unit (GRU) [4] are among the most commonly used neural networks for natural language modeling to avoid the gradient vanishing problem when dealing with long sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only neural network architectures. No dataset names are present in the citation span.",
      "processing_time": 53.75796389579773,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "In our implementation, we also use the two-level stacked LSTM for text generation as the paper proposed. ere are three reasons for choosing this model as our baseline:\n• Similar input: both our HSS and their Att2SeqA would learn user and item latent factor as the input for text generation. e di erence is that their model would take rating information as the direct input while our model would learn to predict the rating score.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture (LSTM) and a method (text generation). There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.68458914756775,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "In our implementation, we also use the two-level stacked LSTM for text generation as the paper proposed. ere are three reasons for choosing this model as our baseline:\n• Similar input: both our HSS and their Att2SeqA would learn user and item latent factor as the input for text generation. e di erence is that their model would take rating information as the direct input while our model would learn to predict the rating score.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture (LSTM) and a method (text generation). There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.68458914756775,
      "citing_paper_id": "204874165",
      "cited_paper_id": null
    },
    {
      "context_text": "A demonstration of potential utility of recurrent networks for natural language generation was provided by [33], which used a character-level LSTM model for the generation of grammatical English sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) used for generating grammatical English sentences.",
      "processing_time": 53.969027042388916,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "To evaluate the performance of text generation module, we compare our work with Att2SeqA[15]. is work is to automatically generate product reviews by given user, item and corresponding rating information. eir model treat user, item and rating as a ributes and encode the three a ributes into latent factors through multi-layer perceptron. en the decoder would take the encoded latent factor as the initial hidden state of LSTM for review generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Att2SeqA) and its usage but does not reference a named dataset.",
      "processing_time": 55.41135120391846,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "To evaluate the performance of text generation module, we compare our work with Att2SeqA[15]. is work is to automatically generate product reviews by given user, item and corresponding rating information. eir model treat user, item and rating as a ributes and encode the three a ributes into latent factors through multi-layer perceptron. en the decoder would take the encoded latent factor as the initial hidden state of LSTM for review generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Att2SeqA) and its usage but does not reference a named dataset.",
      "processing_time": 55.41135120391846,
      "citing_paper_id": "204874165",
      "cited_paper_id": null
    },
    {
      "context_text": "Recurrent neural network (RNN) [22] has shown notable success in sequential modeling tasks. e long short-term memory unit (LSTM) [12] and gated recurrent unit (GRU) [4] are among the most commonly used neural networks for natural language modeling to avoid the gradient vanishing problem when dealing with long sequences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 53.455204010009766,
      "citing_paper_id": "204874165",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Later approaches to CF more and more advanced to more accurate but less transparent latent factor approaches, beginning from various matrix factorization algorithms [14, 24, 32, 34], to more recent deep learning and neural modeling approaches [9, 10, 37, 42, 43, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 54.563432693481445,
      "citing_paper_id": "204874165",
      "cited_paper_id": 5048382
    },
    {
      "context_text": "Later approaches to CF more and more advanced to more accurate but less transparent latent factor approaches, beginning from various matrix factorization algorithms [14, 24, 32, 34], to more recent deep learning and neural modeling approaches [9, 10, 37, 42, 43, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not provide specific dataset names.",
      "processing_time": 54.563432693481445,
      "citing_paper_id": "204874165",
      "cited_paper_id": 58370896
    },
    {
      "context_text": "• DeepCoNN[49]: DeepCooperative Neural Networks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DeepCoNN. The context and title do not provide information about datasets used.",
      "processing_time": 54.96034598350525,
      "citing_paper_id": "204874165",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "Since user textual reviews are informative and beer reect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [1, 18, 20, 41, 46] and recommendation explainability [2, 3, 8, 16, 27, 38, 45], which helps to enhance the eectiveness, transparency, trustworthiness and persuasiveness of recommendation system [11, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and goals. The cited papers' titles do not provide additional information about specific datasets.",
      "processing_time": 55.22897410392761,
      "citing_paper_id": "204874165",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Since user textual reviews are informative and beer reect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [1, 18, 20, 41, 46] and recommendation explainability [2, 3, 8, 16, 27, 38, 45], which helps to enhance the eectiveness, transparency, trustworthiness and persuasiveness of recommendation system [11, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and goals. The cited papers' titles do not provide additional information about specific datasets.",
      "processing_time": 55.22897410392761,
      "citing_paper_id": "204874165",
      "cited_paper_id": 8400616
    },
    {
      "context_text": "Since user textual reviews are informative and beer reect user preferences, a lot of research explored the possibility of incorporating user reviews to improve the recommendation quality [1, 18, 20, 41, 46] and recommendation explainability [2, 3, 8, 16, 27, 38, 45], which helps to enhance the eectiveness, transparency, trustworthiness and persuasiveness of recommendation system [11, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and goals. The cited papers' titles do not provide additional information about specific datasets.",
      "processing_time": 55.22897410392761,
      "citing_paper_id": "204874165",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "author has shown that their model signicantly outperforms some strong topic modeling based methods such as HFT[20], CTR[36] and CDL[37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.54184103012085,
      "citing_paper_id": "204874165",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "• DeepCoNN[46]: Deep Cooperative Neural Networks. is is a state-of-art deep learning method that exploits user reviews information to jointly model user and item. e\n2h p://jmcauley.ucsd.edu/data/amazon\nauthor has shown that their model signi cantly outperforms some strong topic modeling based methods such as HFT[20], CTR[36] and CDL[37].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only models and methods. The URL provided points to a general Amazon dataset page, which is not a specific dataset name.",
      "processing_time": 55.64671301841736,
      "citing_paper_id": "204874165",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "As a result, researchers have looked into explainable recommendation systems in the recent years [2, 3, 8, 16, 27, 38, 39, 44, 45], which can not only provide users with the recommendation lists, but also intutive explanations about why these items are recommended.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research towards explainable recommendation systems.",
      "processing_time": 53.41636776924133,
      "citing_paper_id": "204874165",
      "cited_paper_id": 8400616
    },
    {
      "context_text": "As a result, researchers have looked into explainable recommendation systems in the recent years [2, 3, 8, 16, 27, 38, 39, 44, 45], which can not only provide users with the recommendation lists, but also intutive explanations about why these items are recommended.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research towards explainable recommendation systems.",
      "processing_time": 53.41636776924133,
      "citing_paper_id": "204874165",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "To implement this idea, we rst extract all the feature words by using toolkit Sentires1[47, 48], represented as K and K ⊆ V , from the data set.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'the data set' without providing a specific name or identifier. It is a generic reference and does not meet the criteria for inclusion.",
      "processing_time": 54.80917048454285,
      "citing_paper_id": "204874165",
      "cited_paper_id": 8467898
    },
    {
      "context_text": "•We adopt three o ine metrics – BLEU score, ROUGE score and feature coverage – to evaluate the quality of the generated explanations. e rst two metrics are classical measures for neural machine translation and text summarization.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (BLEU, ROUGE) but does not refer to any specific datasets. These metrics are used to evaluate the quality of generated explanations.",
      "processing_time": 55.45918917655945,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We use three evaluation metrics to evaluate generated explanation sentence quality: BLEU[28], ROUGE[19] and feature words coverage.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics but does not refer to any specific datasets. The cited papers are methods for evaluating machine translation, not datasets.",
      "processing_time": 54.418556690216064,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "For more details, please refer to the paper [28].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific information about datasets, methods, or findings. It merely refers to another paper for more details.",
      "processing_time": 54.22280192375183,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "As shown in Table 4, the BLEU-score, which is a precision-based metric for text generation evaluation, also gives a higher score to HSS than A 2SeqA.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (BLEU-score) used for evaluating text generation models.",
      "processing_time": 54.032843351364136,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "BLEU score is precision-based while ROUGE score is relatively recall-based. ey are complement to each other and it would be reasonable to report both scores to re ect the quality of machine generated text.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU and ROUGE). These are excluded as per the instructions.",
      "processing_time": 54.4107232093811,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We use three evaluation metrics to evaluate generated explanation sentence quality: BLEU[26], ROUGE[17] and feature words coverage.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics but does not refer to any specific datasets. The context is about evaluating generated explanations using BLEU, ROUGE, and feature words coverage.",
      "processing_time": 55.451446294784546,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "During the testing stage, we only generate one sentence and calculate ROUGE and BLEU score\nTable 5: ROUGE score on Electronics dataset (in percentage)\nROUGE-1 ROUGE-2 ROUGE-L ROUGE-SU4 recall precision F1 recall precision F1 recall precision F1 recall precision F1\nA 2SeqA 22.80 7.79 10.19 0.45 0.14 0.18 19.93 6.77 8.85 9.26 1.07 1.38 HSS 26.76 15.72 18.36 3.01 1.77 2.05 22.51 13.31 15.47 9.69 3.51 4.10\nTable 6: ROUGE score on Beauty dataset (in percentage)\nbased on the rst sentence in the human reference text.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Electronics dataset",
        "Beauty dataset"
      ],
      "dataset_descriptions": {
        "Electronics dataset": "Used to evaluate the performance of text generation models on electronics-related content, focusing on ROUGE scores to measure the quality of generated sentences.",
        "Beauty dataset": "Used to evaluate the performance of text generation models on beauty-related content, focusing on ROUGE scores to measure the quality of generated sentences."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Electronics dataset' and 'Beauty dataset' which are specific datasets used for evaluating the performance of text generation models using ROUGE scores.",
      "processing_time": 68.10617589950562,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "• BLEU : this is a precision-based measure which is used for automatically evaluating machine generated text quality.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLEU as a metric, which is excluded according to the instructions. No datasets are mentioned.",
      "processing_time": 53.3610565662384,
      "citing_paper_id": "204874165",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Template-based models, such as [8, 38, 45], dene one or more explanation sentence templates, and then ll dierent words into the templates according to the corresponding recommendation so as to generate dierent explanations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using template-based models for generating explanations.",
      "processing_time": 53.60965633392334,
      "citing_paper_id": "204874165",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "• BiasedMF [14]: Biased Matrix Factorization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method called Biased Matrix Factorization, which is not a dataset.",
      "processing_time": 54.639307737350464,
      "citing_paper_id": "204874165",
      "cited_paper_id": 58370896
    },
    {
      "context_text": "• SVD++ [14]: It extends Singular Value Decomposition by integrating implicit feedback into latent factor modeling.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (SVD++). The cited paper title also does not provide any additional information about a dataset.",
      "processing_time": 55.3618483543396,
      "citing_paper_id": "204874165",
      "cited_paper_id": 207168823
    },
    {
      "context_text": "A demonstration of potential utility of recurrent networks for natural language generation was provided by [35], which used a character-level LSTM model for the generation of grammatical English sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM) used for generating grammatical English sentences.",
      "processing_time": 54.358596324920654,
      "citing_paper_id": "204874165",
      "cited_paper_id": 263865370
    },
    {
      "context_text": "However, A2SeqA model obtains the aentive context vector with user, item and rating latent factors, which are the aributes as mentioned in the paper [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to latent factors and attributes, which are not datasets.",
      "processing_time": 54.63072204589844,
      "citing_paper_id": "204874165",
      "cited_paper_id": null
    },
    {
      "context_text": "To evaluate the performance of text generation module, we compare our work with Att2SeqA[15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with another method.",
      "processing_time": 52.82741332054138,
      "citing_paper_id": "204874165",
      "cited_paper_id": null
    },
    {
      "context_text": "Following previous works (Zhou and Bhat, 2021; Panthaplackel et al., 2022), we use Rouge-1/Rouge-L (Lin, 2004) as evaluation metrics for generation tasks (LaMP-4 to 7).",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (Rouge-1/Rouge-L) but does not reference any specific datasets. The context is focused on the evaluation of generation tasks, not on the datasets used.",
      "processing_time": 56.70809197425842,
      "citing_paper_id": "258298303",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Following previous works (Zhou and Bhat, 2021; Panthaplackel et al., 2022), we use Rouge-1/Rouge-L (Lin, 2004) as evaluation metrics for generation tasks (LaMP-4 to 7).",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (Rouge-1/Rouge-L) but does not reference any specific datasets. The context is focused on the evaluation of generation tasks, not on the datasets used.",
      "processing_time": 56.70809197425842,
      "citing_paper_id": "258298303",
      "cited_paper_id": 243865349
    },
    {
      "context_text": "Following previous works (Zhou and Bhat, 2021; Panthaplackel et al., 2022), we use Rouge-1/Rouge-L (Lin, 2004) as evaluation metrics for generation tasks (LaMP-4 to 7).",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (Rouge-1/Rouge-L) but does not reference any specific datasets. The context is focused on the evaluation of generation tasks, not on the datasets used.",
      "processing_time": 56.70809197425842,
      "citing_paper_id": "258298303",
      "cited_paper_id": 248780169
    },
    {
      "context_text": "In line with prior research [56], we utilize Rouge-1 and Rouge-L [22] as the evaluation metrics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (Rouge-1 and Rouge-L) but does not refer to any specific dataset. Metrics are excluded according to the instructions.",
      "processing_time": 55.354947090148926,
      "citing_paper_id": "258298303",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Similar to the Task 4, we adopt Rouge-1/Rouge-L [22] as evaluation metrics.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (Rouge-1/Rouge-L) but does not refer to any specific dataset. The cited paper title confirms that ROUGE is a package for evaluation, not a dataset.",
      "processing_time": 57.434147119522095,
      "citing_paper_id": "258298303",
      "cited_paper_id": 964287
    },
    {
      "context_text": "The commonly used evaluation metrics for text generation, whether syntactical (Lin, 2004; Banerjee and Lavie, 2005; Papineni et al., 2002) or semantical (Zhang* et al., 2020), do not incorporate the user into their evaluation process.",
      "catation_intent": "findings",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. No dataset names are present in the text.",
      "processing_time": 54.6049325466156,
      "citing_paper_id": "258298303",
      "cited_paper_id": 964287
    },
    {
      "context_text": "We adopt the Rouge-1 and Rouge-L metrics [22] for evaluation.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (Rouge-1 and Rouge-L) but does not refer to any specific dataset. Metrics are excluded according to the instructions.",
      "processing_time": 55.50713038444519,
      "citing_paper_id": "258298303",
      "cited_paper_id": 964287
    },
    {
      "context_text": ", 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001), in contexts ranging from query auto-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of search personalization. No clear, verifiable resources are identified.",
      "processing_time": 54.869099378585815,
      "citing_paper_id": "258298303",
      "cited_paper_id": 1292249
    },
    {
      "context_text": "…recommender systems (Davidson et al., 2010; Das et al., 2007; Xu et al., 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al., 2023), in contexts ranging from query auto-completion…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications and contexts. No clear, verifiable datasets are identified.",
      "processing_time": 54.739222288131714,
      "citing_paper_id": "258298303",
      "cited_paper_id": 1292249
    },
    {
      "context_text": "We employ beam search (Freitag and Al-Onaizan, 2017) with a beam size of 4 in all experiments to improve the model’s ability to generate high-quality predictions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (beam search) but does not reference any specific dataset. The context is about improving model performance using a particular technique.",
      "processing_time": 55.20416736602783,
      "citing_paper_id": "258298303",
      "cited_paper_id": 2229477
    },
    {
      "context_text": "While many existing well-known NLP benchmarks, such as GLUE (Wang et al., 2018), Super-GLUE (Wang et al., 2019), KILT (Petroni et al., 2021), and GEM (Gehrmann et al., 2021) have led to significant progress in various NLP tasks, they have often taken the dominant NLP approach of “one-size-fits-all” to modeling and evaluation, and do not allow the development of models that adapt to the specific needs of end users – limiting extensive research on personalization in NLP tasks.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not indicate their use as datasets. It criticizes them for not supporting personalization, which is the focus of the research.",
      "processing_time": 56.047250747680664,
      "citing_paper_id": "258298303",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "1 While existing NLP benchmarks, such as (Super)GLUE [44, 45], KILT [34], and GEM [12] have significantly progressed the NLP frontier, they have often taken the dominant NLP approach of “one-size-fits-all” to modeling and evaluation, and do not allow the development of models that adapt to the specific needs of end users – limiting extensive research on personalization in NLP tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several benchmarks but does not indicate their use as datasets. It focuses on the limitations of these benchmarks in supporting personalization research.",
      "processing_time": 55.343260049819946,
      "citing_paper_id": "258298303",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "While many existing well-known NLP benchmarks, such as GLUE (Wang et al., 2018), Super-GLUE (Wang et al., 2019), KILT (Petroni et al., 2021), and GEM (Gehrmann et al., 2021) have led to significant progress in various NLP tasks, they have often taken the dominant NLP approach of “one-size-fits-all”…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several benchmarks but does not indicate their use as datasets in the current research context. These benchmarks are excluded as they are primarily used for score comparison.",
      "processing_time": 56.034021615982056,
      "citing_paper_id": "258298303",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "…et al., 2010; Das et al., 2007; Xu et al., 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al., 2023), in contexts ranging from query auto-completion (Jaech and Ostendorf, 2018) to…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and applications. There are no clear identifiers for datasets, models, or other resources.",
      "processing_time": 56.03880310058594,
      "citing_paper_id": "258298303",
      "cited_paper_id": 6245052
    },
    {
      "context_text": "It also represents an important element of large-scale industry recommender systems [5, 6, 49] and has also been studied for search applications [2, 4, 8], in contexts ranging from query auto-completion [17] to collaborative search [50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and contexts. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.64814901351929,
      "citing_paper_id": "258298303",
      "cited_paper_id": 6245052
    },
    {
      "context_text": "It also represents an important element of large-scale industry recommender systems [5, 6, 49] and has also been studied for search applications [2, 4, 8], in contexts ranging from query auto-completion [17] to collaborative search [50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications and contexts. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.64814901351929,
      "citing_paper_id": "258298303",
      "cited_paper_id": 6338654
    },
    {
      "context_text": "…(Davidson et al., 2010; Das et al., 2007; Xu et al., 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al., 2023), in contexts ranging from query auto-completion (Jaech and Ostendorf,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and applications. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.03393244743347,
      "citing_paper_id": "258298303",
      "cited_paper_id": 6338654
    },
    {
      "context_text": "the similarity between each class label and the generated output utilizing BERTScore [53].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERTScore but does not refer to a dataset. BERTScore is a metric, which is excluded according to the instructions.",
      "processing_time": 55.63905119895935,
      "citing_paper_id": "258298303",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "In classification tasks, if the produced output does not correspond to a valid class, we resort to calculating the similarity between each class label and the generated output utilizing BERTScore (Zhang* et al., 2020).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERTScore but does not refer to it as a dataset. It is used as a method for evaluating text generation.",
      "processing_time": 55.32280611991882,
      "citing_paper_id": "258298303",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "Personalization has been widely studied by various communities, including the information retrieval (IR) and human-computer interaction (HCI) communities, often with applications to search engines and recommender systems (Fowler et al., 2015; Xue et al., 2009; Naumov et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalization in IR and HCI. No verifiable resources are named.",
      "processing_time": 55.40228247642517,
      "citing_paper_id": "258298303",
      "cited_paper_id": 173990641
    },
    {
      "context_text": "While personalization has been widely studied by various communities, including the information retrieval (IR) and humancomputer interaction (HCI) communities, often with applications to search engines and recommender systems [10, 29, 50] – its exploration in NLP has been limited.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of research and application domains.",
      "processing_time": 54.051029205322266,
      "citing_paper_id": "258298303",
      "cited_paper_id": 173990641
    },
    {
      "context_text": "Besides exploring text generation for dialogues, other work has also leveraged publicly available reviews and recipes to explore personalization for review [21] and recipe generation tasks [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'publicly available reviews and recipes' but does not specify any named datasets. The cited paper title suggests the use of reviews for personalization, but no specific dataset is named.",
      "processing_time": 57.60977816581726,
      "citing_paper_id": "258298303",
      "cited_paper_id": 202767450
    },
    {
      "context_text": "Other work has also leveraged publicly available reviews and recipes to explore personalization for review (Li and Tuzhilin, 2019) and recipe generation (Ma-jumder et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'publicly available reviews and recipes' but does not specify a named dataset. The context is too generic to identify a specific, verifiable dataset.",
      "processing_time": 56.39857292175293,
      "citing_paper_id": "258298303",
      "cited_paper_id": 202767450
    },
    {
      "context_text": "It also represents an important element of large-scale industry recommender systems (Davidson et al., 2010; Das et al., 2007; Xu et al., 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and applications in recommender systems and search. No verifiable resources are identified.",
      "processing_time": 56.546618700027466,
      "citing_paper_id": "258298303",
      "cited_paper_id": 207163129
    },
    {
      "context_text": "…work has constructed dialogue data for users by promoting crowd-workers to author dialogues based on specific personas (Zhang et al., 2018), and through extracting user attributes and utterances from Red-dit (Mazaré et al., 2018; Wu et al., 2021) and Weibo (Zhong et al., 2022; Qian et al., 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions constructing dialogue data for users, but does not specify a named dataset. It refers to Reddit and Weibo as sources of data, which are platforms, not specific datasets.",
      "processing_time": 57.35580897331238,
      "citing_paper_id": "258298303",
      "cited_paper_id": 221969995
    },
    {
      "context_text": "Additionally, employing soft prompts (Lester et al., 2021) can be further explored in the context of personalizing",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (soft prompts) from a cited paper. The context is too limited to infer any dataset usage.",
      "processing_time": 56.31176161766052,
      "citing_paper_id": "258298303",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Furthermore, the use of soft prompts (Lester et al., 2021) can be helpful for personalizing language models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (soft prompts) for personalizing language models.",
      "processing_time": 55.15133738517761,
      "citing_paper_id": "258298303",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Finally, Ao et al. (2021) presents a personalized headline generation dataset constructed from realistic user interaction data on Microsoft News.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PENS"
      ],
      "dataset_descriptions": {
        "PENS": "Used to construct a personalized headline generation system, focusing on realistic user interaction data on Microsoft News to enhance personalization in news headlines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset 'PENS' used for personalized news headline generation, which is directly relevant to the research topic.",
      "processing_time": 60.52166390419006,
      "citing_paper_id": "258298303",
      "cited_paper_id": 236460075
    },
    {
      "context_text": "[1] presents a personalized headline generation dataset constructed from realistic user interaction data on Microsoft News.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PENS"
      ],
      "dataset_descriptions": {
        "PENS": "Used to construct a personalized headline generation dataset from realistic user interaction data on Microsoft News, focusing on improving user engagement through tailored headlines."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset 'PENS' which is used for personalized news headline generation, aligning with the research topic.",
      "processing_time": 61.30156850814819,
      "citing_paper_id": "258298303",
      "cited_paper_id": 236460075
    },
    {
      "context_text": "Similarly, other work has explored personalized sentiment prediction on publicly available Yelp and IMDB data (Mireshghallah et al., 2022; Zhong et al., 2021) – this work bears a resemblance to the LaMP-3 task and ties back to rating prediction explored in recommendation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in reviews.",
        "IMDB": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in movie reviews."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp and IMDB data' as publicly available datasets used for personalized sentiment prediction. These datasets are specific and widely recognized.",
      "processing_time": 65.7573344707489,
      "citing_paper_id": "258298303",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "Similarly, other work has explored personalized sentiment prediction on publicly available Yelp and IMDB data (Mireshghallah et al., 2022; Zhong et al., 2021) – this work bears a resemblance to the LaMP-3 task and ties back to rating prediction explored in recommendation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in reviews.",
        "IMDB": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in movie reviews."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp and IMDB data' as publicly available datasets used for personalized sentiment prediction. These datasets are specific and widely recognized.",
      "processing_time": 65.7573344707489,
      "citing_paper_id": "258298303",
      "cited_paper_id": 238252929
    },
    {
      "context_text": "Similarly, other work has explored personalized sentiment prediction on publicly available Yelp and IMDB data (Mireshghallah et al., 2022; Zhong et al., 2021) – this work bears a resemblance to the LaMP-3 task and ties back to rating prediction explored in recommendation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in reviews.",
        "IMDB": "Used to explore personalized sentiment prediction, focusing on user-specific sentiment patterns in movie reviews."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp and IMDB data' as publicly available datasets used for personalized sentiment prediction. These datasets are specific and widely recognized.",
      "processing_time": 65.7573344707489,
      "citing_paper_id": "258298303",
      "cited_paper_id": null
    },
    {
      "context_text": "ment prediction tasks on publicly available Yelp and IMDB data (Mireshghallah et al., 2022; Zhong et al., 2021) – this work bears a resemblance to the personalized product rating task in LaMP and ties back to rating prediction tasks explored in rec-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Yelp": "Used for sentiment prediction tasks, focusing on user reviews to explore personalized product rating predictions.",
        "IMDB": "Used for sentiment prediction tasks, focusing on movie reviews to explore personalized product rating predictions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp and IMDB data' which are well-known datasets used for sentiment analysis and personalized text generation tasks.",
      "processing_time": 65.7113790512085,
      "citing_paper_id": "258298303",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "Authors writing displays distinct stylistic elements influenced by both personal and social factors (Zhu and Jurgens, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to stylistic elements in authors' writing. No clear, verifiable resource is identified.",
      "processing_time": 56.74269104003906,
      "citing_paper_id": "258298303",
      "cited_paper_id": 237431146
    },
    {
      "context_text": "Social media posts adhere strongly to various personal stylistic patterns of authors (Zhu and Jurgens, 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions personal stylistic patterns in social media posts, which is relevant to personalized text generation, but does not specify a dataset.",
      "processing_time": 56.394256830215454,
      "citing_paper_id": "258298303",
      "cited_paper_id": 237431146
    },
    {
      "context_text": "Optimizing ranking models that select personalized entries for the sake of personalized text classification and/or generation would be a potentially impactful research direction.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general research direction. No verifiable resources are identified.",
      "processing_time": 55.922497272491455,
      "citing_paper_id": "258298303",
      "cited_paper_id": 237431146
    },
    {
      "context_text": "With the widespread adoption of employing LLMs with no fine-tuning in contemporary research, we conduct an evaluation of two such models on our benchmark.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a benchmark which is not considered a dataset according to the guidelines.",
      "processing_time": 55.90519714355469,
      "citing_paper_id": "258298303",
      "cited_paper_id": 237431146
    },
    {
      "context_text": "quality of the generated headlines, we adopt Rouge-1 and Rouge-L as evaluation metrics, which have been commonly used in previous work for headline generation tasks [33, 51].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics (Rouge-1 and Rouge-L). These metrics are excluded as per the instructions.",
      "processing_time": 57.077778577804565,
      "citing_paper_id": "258298303",
      "cited_paper_id": 237513592
    },
    {
      "context_text": "quality of the generated headlines, we adopt Rouge-1 and Rouge-L as evaluation metrics, which have been commonly used in previous work for headline generation tasks [33, 51].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics (Rouge-1 and Rouge-L). These metrics are excluded as per the instructions.",
      "processing_time": 57.077778577804565,
      "citing_paper_id": "258298303",
      "cited_paper_id": 248780169
    },
    {
      "context_text": "Predicting ratings based on user reviews has been studied extensively in personalized sentiment prediction tasks (Mireshghallah et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task area. No verifiable resources are identified.",
      "processing_time": 56.04610013961792,
      "citing_paper_id": "258298303",
      "cited_paper_id": 238252929
    },
    {
      "context_text": "In line with prior research on paraphrase generation (Zhou and Bhat, 2021), we utilize Rouge-1 and Rouge-L (Lin, 2004) as the primary evaluation metrics for this task.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No datasets are identified that meet the criteria.",
      "processing_time": 56.19787001609802,
      "citing_paper_id": "258298303",
      "cited_paper_id": 243865349
    },
    {
      "context_text": "This optimism has also been reflected in the recent UserNLP’22 workshop [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a workshop, which is not a verifiable resource according to the given criteria.",
      "processing_time": 57.4511456489563,
      "citing_paper_id": "258298303",
      "cited_paper_id": 251597891
    },
    {
      "context_text": "As the use of LLMs, such as GPT-4 (OpenAI, 2023), in real-world applications evolves, personalization emerges as a key factor in meeting the user’s expectations for tailored experiences that align with their unique needs and preferences (Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evolution of LLMs and the importance of personalization. No verifiable resources are identified.",
      "processing_time": 57.03838658332825,
      "citing_paper_id": "258298303",
      "cited_paper_id": 251597891
    },
    {
      "context_text": "As the use of LLMs, such as GPT-4 (OpenAI, 2023), in real-world applications evolves, personalization emerges as a key factor in meeting the user’s expectations for tailored experiences that align with their unique needs and preferences (Huang et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the evolution of LLMs and the importance of personalization. No verifiable resources are identified.",
      "processing_time": 57.03838658332825,
      "citing_paper_id": "258298303",
      "cited_paper_id": null
    },
    {
      "context_text": "To create a dataset, we use a collection of Huffington Post articles (Misra, 2022; Misra and Grover, 2021).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Huffington Post articles"
      ],
      "dataset_descriptions": {
        "Huffington Post articles": "Used to create a dataset for personalized text generation, focusing on news articles to study category-specific language patterns and content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions using a collection of Huffington Post articles, which is a specific, verifiable dataset. The cited paper titles confirm the nature of the dataset.",
      "processing_time": 64.9299144744873,
      "citing_paper_id": "258298303",
      "cited_paper_id": 252519330
    },
    {
      "context_text": "To construct our dataset for this task, we leverage the News Categorization dataset (Misra, 2022; Misra and Grover, 2021) from the HuffPost website.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "News Categorization dataset"
      ],
      "dataset_descriptions": {
        "News Categorization dataset": "Used to construct a dataset for personalized text generation, leveraging news articles categorized from the HuffPost website."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'News Categorization dataset' which is a specific, verifiable dataset used for constructing a dataset for the task at hand.",
      "processing_time": 63.98663139343262,
      "citing_paper_id": "258298303",
      "cited_paper_id": 252519330
    },
    {
      "context_text": "To construct our dataset for this task, we leverage the News Categorization dataset [27, 28] from the HuffPost website3.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "News Categorization dataset"
      ],
      "dataset_descriptions": {
        "News Categorization dataset": "Used to construct a dataset for personalized text generation, leveraging news articles categorized by topic from the HuffPost website."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'News Categorization dataset' from the HuffPost website, which is a specific, verifiable dataset.",
      "processing_time": 63.35972595214844,
      "citing_paper_id": "258298303",
      "cited_paper_id": 252519330
    },
    {
      "context_text": "To construct our dataset for this task, we leverage the news categorization dataset [27, 28] obtained from the HuffPost website2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "news categorization dataset"
      ],
      "dataset_descriptions": {
        "news categorization dataset": "Used to construct a dataset for personalized text generation, leveraging news articles categorized by topic from the HuffPost website."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset called 'news categorization dataset' obtained from the HuffPost website, which is relevant to personalized text generation.",
      "processing_time": 63.806743144989014,
      "citing_paper_id": "258298303",
      "cited_paper_id": 252519330
    },
    {
      "context_text": "5-turbo or ChatGPT 5 ) and FlanT5-XXL (Chung et al., 2022).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.53782296180725,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "Conversely, FlanT5-XXL predictions are consistently among the questioned labels.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (FlanT5-XXL). No verifiable resources are identified.",
      "processing_time": 57.170045137405396,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "5-turbo or ChatGPT5 and FlanT5XXL [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.477343797683716,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "FlanT5-XXL comprises 11B parameters, however, the size of GPT-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model sizes. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.29246139526367,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "Even in zero-shot settings where an off-the-shelf LLM without fine-tuning (e.g., FlanT5-XXL) is used, utilizing our proposed method results in a relative average improvement of 12.2% across the tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (FlanT5-XXL) and a method (instruction-finetuning). There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.94046664237976,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "These findings indicate that fine-tuning smaller models on downstream tasks leads to enhanced performance in comparison to zero-shot performance of LLMs. Finally, it is crucial to highlight that the observed outcomes, which indicate superior performance of FlanT5-XXL over GPT-3.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance comparisons.",
      "processing_time": 55.77076554298401,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253018554
    },
    {
      "context_text": "[36] examines the application of personalization methods to modeling annotators in a classification task reliant on modeling social norms – making an important connection between personalization and an emerging body of work on accommodating human label variation in NLP research [14, 35, 40].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a connection between personalization and human label variation in NLP research.",
      "processing_time": 56.67239689826965,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253370768
    },
    {
      "context_text": "…of personalization methods to modeling annotators in a classification task reliant on modeling social norms – making an important connection between personalization and an emerging body of work on accommodating human label variation in NLP (Rottger et al., 2022; Gordon et al., 2022; Plank, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing personalization methods and human label variation in NLP.",
      "processing_time": 56.25379514694214,
      "citing_paper_id": "258298303",
      "cited_paper_id": 253370768
    },
    {
      "context_text": "Recent work has shown the benefits (and harms) of personalizing LLMs [19].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of personalizing LLMs. No verifiable resources are identified.",
      "processing_time": 56.127695083618164,
      "citing_paper_id": "258298303",
      "cited_paper_id": 257427629
    },
    {
      "context_text": "Recent work has also highlighted the potential impact and considerations of personalizing LLMs (Kirk et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion on personalizing LLMs and associated risks.",
      "processing_time": 55.42051649093628,
      "citing_paper_id": "258298303",
      "cited_paper_id": 257427629
    },
    {
      "context_text": "…2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al., 2023), in contexts ranging from query auto-completion (Jaech and Ostendorf, 2018) to collaborative personalized search (Xue et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and applications. No verifiable resources are identified.",
      "processing_time": 55.010117530822754,
      "citing_paper_id": "258298303",
      "cited_paper_id": 258332095
    },
    {
      "context_text": ", 2022) and has also been extensively studied for search applications (Bennett et al., 2012; Dumais, 2016; Croft et al., 2001; Tabrizi et al., 2018; Zeng et al., 2023), in contexts ranging from query auto-completion",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas and applications. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.659637451171875,
      "citing_paper_id": "258298303",
      "cited_paper_id": 258332095
    },
    {
      "context_text": "Similarly, other work has explored personalized sentiment prediction tasks on publicly available Yelp and IMDB data [26, 55] – this work bears a resemblance to the LaMP-3 task in LaMP and ties back to rating prediction tasks explored in recommendation tasks.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp",
        "IMDB"
      ],
      "dataset_descriptions": {
        "Yelp": "Used for personalized sentiment prediction tasks, focusing on user reviews and ratings to explore recommendation systems.",
        "IMDB": "Used for personalized sentiment prediction tasks, focusing on movie reviews and ratings to explore recommendation systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp and IMDB data' which are specific, publicly available datasets used for personalized sentiment prediction tasks.",
      "processing_time": 65.45643949508667,
      "citing_paper_id": "258298303",
      "cited_paper_id": null
    },
    {
      "context_text": "This typically leads stylized chatbot models to employ complex, multi-step setups, potentially involving reinforcement learning (Niu and Bansal, 2018; Sun et al., 2022; Firdaus et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the complexity of setups in stylized chatbot models.",
      "processing_time": 55.447285890579224,
      "citing_paper_id": "258378197",
      "cited_paper_id": 13690180
    },
    {
      "context_text": "Niu and Bansal (2018) used a politeness classifier and a language model trained on polite utterances to generate polite dialog responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a language model trained on polite utterances, but does not specify a named dataset. The focus is on the method and findings rather than a reusable dataset.",
      "processing_time": 56.34856867790222,
      "citing_paper_id": "258378197",
      "cited_paper_id": 13690180
    },
    {
      "context_text": "Previous works focus on personalized (Li et al., 2016; Luan et al., 2017; Su et al., 2019), polite Niu and Bansal (2018) or emotional (Zhou et al., 2018) dialogues.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works focusing on different aspects of dialogue generation. No verifiable resources are named.",
      "processing_time": 55.46043682098389,
      "citing_paper_id": "258378197",
      "cited_paper_id": 13690180
    },
    {
      "context_text": "Generating stylized dialogue responses has been investigated in various studies, with a broad understanding of style covering emotion (Zhou et al., 2018), personality (Li et al., 2016) or politeness (Niu and Bansal, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies that investigate stylized dialogue responses. No clear, verifiable datasets are named.",
      "processing_time": 55.3467276096344,
      "citing_paper_id": "258378197",
      "cited_paper_id": 13690180
    },
    {
      "context_text": "Following prior work (Madaan et al., 2020; Niu and Bansal, 2018), we use automatic metrics for the evaluation of the models along two major dimensions: (1) style transfer and (2) content preservation and relevance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of automatic metrics for evaluating models on style transfer and content preservation.",
      "processing_time": 54.59324789047241,
      "citing_paper_id": "258378197",
      "cited_paper_id": 13690180
    },
    {
      "context_text": "For politeness, traditionally, polite chatbot responses are accomplished by manual dialogue de-sign, where predefined rules or templates are used to generate responses based on certain keywords or scenarios (André et al., 2004; Gupta et al., 2007; de Jong et al., 2008).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for generating polite chatbot responses.",
      "processing_time": 54.120909690856934,
      "citing_paper_id": "258378197",
      "cited_paper_id": 15026443
    },
    {
      "context_text": "Building a chatbot agent that produces stylized and coherent responses can yield more engaging conversations (Niederhoffer and Pennebaker, 2002).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to building chatbots. The cited paper title does not help in identifying a dataset.",
      "processing_time": 55.327921867370605,
      "citing_paper_id": "258378197",
      "cited_paper_id": 16228951
    },
    {
      "context_text": "To measure politeness transfer quality, we compute Polite Score , which is defined as the average score given to the generated sequences by our politeness classifier, which we created by finetuning BERT (Devlin et al., 2019) on Madaan et al. (2020)’s Enron data (see Section 4.1).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Enron data"
      ],
      "dataset_descriptions": {
        "Enron data": "Used to fine-tune a politeness classifier for measuring politeness transfer quality in generated sequences, focusing on the average score given by the classifier."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Enron data' as the dataset used to fine-tune a politeness classifier. This dataset is specific and relevant to the research on politeness transfer quality.",
      "processing_time": 64.39905643463135,
      "citing_paper_id": "258378197",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The accuracy of our BERT-based politeness classification model is 83.27% on the politeness transfer data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'politeness transfer data' which appears to be a specific dataset used for evaluating a BERT-based politeness classification model. However, the name is too generic and lacks a clear identifier.",
      "processing_time": 57.72847056388855,
      "citing_paper_id": "258378197",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "4 Although the scale of politeness classes is not necessarily linear, we believe that this is still a good indicator of the overall politeness of the data. bedding similarity, we use a pre-trained Sentence-BERT model (Reimers and Gurevych, 2019) and cosine similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of a pre-trained Sentence-BERT model for cosine similarity calculations.",
      "processing_time": 54.06694483757019,
      "citing_paper_id": "258378197",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "4 Although the scale of politeness classes is not necessarily linear, we believe that this is still a good indicator of the overall politeness of the data. bedding similarity, we use a pre-trained Sentence-BERT model (Reimers and Gurevych, 2019) and cosine similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of a pre-trained Sentence-BERT model for cosine similarity calculations.",
      "processing_time": 54.06694483757019,
      "citing_paper_id": "258378197",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "Polite Chatbot Response Generation Stylized dialogue generation attracted a lot of attention in recent years (Gao et al., 2019; Zheng et al., 2021; Zeng and Nie, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attention towards stylized dialogue generation. No verifiable resources are identified.",
      "processing_time": 54.331887006759644,
      "citing_paper_id": "258378197",
      "cited_paper_id": 202565872
    },
    {
      "context_text": "Polite Chatbot Response Generation Stylized dialogue generation attracted a lot of attention in recent years (Gao et al., 2019; Zheng et al., 2021; Zeng and Nie, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attention towards stylized dialogue generation. No verifiable resources are identified.",
      "processing_time": 54.331887006759644,
      "citing_paper_id": "258378197",
      "cited_paper_id": 221970895
    },
    {
      "context_text": "Polite Chatbot Response Generation Stylized dialogue generation attracted a lot of attention in recent years (Gao et al., 2019; Zheng et al., 2021; Zeng and Nie, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attention towards stylized dialogue generation. No verifiable resources are identified.",
      "processing_time": 54.331887006759644,
      "citing_paper_id": "258378197",
      "cited_paper_id": 235097623
    },
    {
      "context_text": "In most cases, the stylistic features we want to capture are embedded in unpaired texts that cannot be directly utilized by supervised models (Gao et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to unpaired texts. No clear, verifiable resource names are provided.",
      "processing_time": 54.6976158618927,
      "citing_paper_id": "258378197",
      "cited_paper_id": 202565872
    },
    {
      "context_text": "For dialogue modeling, we use multiple pre-trained models: (1) GPT-2 (Radford et al., 2019), which is a Transformer decoder trained for general language modeling (including dialogues), (2) DialoGPT (Zhang et al., 2020), which shares GPT-2’s architecture but was pre-trained specifically on dialogue data, (3) BlenderBot (Roller et al., 2021), which is an encoder-decoder Transformer specifically trained to learn dialogue skills such as empathy or engagement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models but does not specify any datasets. The models are described, but no datasets are named or used directly.",
      "processing_time": 54.30273938179016,
      "citing_paper_id": "258378197",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "For dialogue modeling, we use multiple pre-trained models: (1) GPT-2 (Radford et al., 2019), which is a Transformer decoder trained for general language modeling (including dialogues), (2) DialoGPT (Zhang et al., 2020), which shares GPT-2’s architecture but was pre-trained specifically on dialogue data, (3) BlenderBot (Roller et al., 2021), which is an encoder-decoder Transformer specifically trained to learn dialogue skills such as empathy or engagement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models but does not specify any datasets. The models are described, but no datasets are named or used directly.",
      "processing_time": 54.30273938179016,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "…models: (1) GPT-2 (Radford et al., 2019), which is a Transformer decoder trained for general language modeling (including dialogues), (2) DialoGPT (Zhang et al., 2020), which shares GPT-2’s architecture but was pre-trained specifically on dialogue data, (3) BlenderBot (Roller et al., 2021), which…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and their architectures but does not reference any specific datasets. The focus is on the models themselves rather than the datasets they were trained on.",
      "processing_time": 55.12527847290039,
      "citing_paper_id": "258378197",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "The human evaluation results mostly agree with our automatic evaluation results: our data preparation method performs better than Madaan et al. (2020)’s transfer in terms of polite-ness and is able to improve the base BlenderBot model.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No verifiable resources are identified.",
      "processing_time": 53.3433780670166,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "We randomly select 50 context-utterance pairs from the DailyDi-alog test set for all models based on the strongest BlenderBot language model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DailyDialog test set"
      ],
      "dataset_descriptions": {
        "DailyDialog test set": "Used to select context-utterance pairs for evaluating chatbot models, focusing on open-domain conversation quality and coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DailyDialog test set', which is a specific dataset used for evaluating chatbot performance. The dataset is used to select context-utterance pairs for model evaluation.",
      "processing_time": 62.5463080406189,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "We have evaluated 50 model outputs for each variant of the BlenderBot model (see Table 5 for a sample).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to model outputs, which are not considered datasets according to the extraction rules.",
      "processing_time": 54.69050574302673,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "…(including dialogues), (2) DialoGPT (Zhang et al., 2020), which shares GPT-2’s architecture but was pre-trained specifically on dialogue data, (3) BlenderBot (Roller et al., 2021), which is an encoder-decoder Transformer specifically trained to learn dialogue skills such as empathy or engagement.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and architectures but does not refer to any specific datasets. The focus is on describing different chatbot models and their training approaches.",
      "processing_time": 54.67295217514038,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Moreover, our method is the only one that improves the Polite Score over the vanilla BlenderBot model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (Polite Score) and a model (BlenderBot).",
      "processing_time": 54.13575315475464,
      "citing_paper_id": "258378197",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Identity preservation is determined by detecting faces in the reference and generated images using MTCNN [41], and then calculating a pairwise identity similarity using FaceNet [34].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MTCNN and FaceNet, but these are models or methods, not datasets. No datasets are explicitly mentioned or used in the described process.",
      "processing_time": 55.12207865715027,
      "citing_paper_id": "258740710",
      "cited_paper_id": 10585115
    },
    {
      "context_text": "Inspired by Style Mixing [18], we propose a simple delayed subject conditioning, which allows for inferenceonly subject conditioning while striking a balance between identity preservation and editability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Style Mixing) from the cited paper. The context is about proposing a new technique inspired by the cited method.",
      "processing_time": 55.78732085227966,
      "citing_paper_id": "258740710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "We built a subject-augmented image-text paired dataset based on the FFHQ-wild [18] dataset to train our models.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FFHQ-wild"
      ],
      "dataset_descriptions": {
        "FFHQ-wild": "Used as a basis to create a subject-augmented image-text paired dataset for training models, focusing on enhancing the diversity and quality of generated images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the creation of a subject-augmented image-text paired dataset based on the FFHQ-wild dataset. FFHQ-wild is a specific dataset used as a basis for training models.",
      "processing_time": 64.7757248878479,
      "citing_paper_id": "258740710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "First, the current training set is FFHQ [18] which is small and primarily contains headshots of human faces.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FFHQ"
      ],
      "dataset_descriptions": {
        "FFHQ": "Used as a training set for generative models, primarily containing headshots of human faces, to evaluate the performance of style-based generator architectures."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions FFHQ as a training set, which is a specific dataset used for training generative models, particularly for human face images.",
      "processing_time": 62.21835780143738,
      "citing_paper_id": "258740710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "Finally, our model, built on the foundation of Stable Diffusion and FFHQ, also inherits their biases.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Stable Diffusion' and 'FFHQ', but these are models or datasets used for image generation and face recognition, not directly related to personalized text generation. No specific datasets are mentioned for the target topic.",
      "processing_time": 57.7198851108551,
      "citing_paper_id": "258740710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "Inspired by Style Mixing [18], we propose a simple delayed subject conditioning , which allows for inference-only subject conditioning while striking a balance between identity preservation and editability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Style Mixing) from the cited paper. The context focuses on proposing a new technique inspired by the cited work.",
      "processing_time": 55.617302894592285,
      "citing_paper_id": "258740710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "Recent advancements in text-to-image generation [4, 8, 17, 28], particularly diffusion models [13, 27, 31, 36, 37], have opened new frontiers in content creation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image generation and diffusion models. No verifiable resources are identified.",
      "processing_time": 54.141510248184204,
      "citing_paper_id": "258740710",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Recent advancements in text-to-image generation [4, 8, 17, 28], particularly diffusion models [13, 27, 31, 36, 37], have opened new frontiers in content creation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image generation and diffusion models. No verifiable resources are identified.",
      "processing_time": 54.141510248184204,
      "citing_paper_id": "258740710",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Recent advancements in text-to-image generation [4, 8, 17, 28], particularly diffusion models [13, 27, 31, 36, 37], have opened new frontiers in content creation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image generation and diffusion models. No verifiable resources are identified.",
      "processing_time": 54.141510248184204,
      "citing_paper_id": "258740710",
      "cited_paper_id": 255372955
    },
    {
      "context_text": "Recent advancements in text-to-image generation [4, 8, 17, 28], particularly diffusion models [13, 27, 31, 36, 37], have opened new frontiers in content creation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image generation and diffusion models. No verifiable resources are identified.",
      "processing_time": 54.141510248184204,
      "citing_paper_id": "258740710",
      "cited_paper_id": 257427461
    },
    {
      "context_text": "Subject-driven text-to-image generation permits the personalization to new individuals given a few sample images [3, 9, 20, 25, 32], allowing the generation of images featuring specific subjects in novel scenes, styles, and actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses subject-driven text-to-image generation but does not cite any particular dataset.",
      "processing_time": 54.1368305683136,
      "citing_paper_id": "258740710",
      "cited_paper_id": 237491885
    },
    {
      "context_text": "Subject-driven text-to-image generation permits the personalization to new individuals given a few sample images [3, 9, 20, 25, 32], allowing the generation of images featuring specific subjects in novel scenes, styles, and actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses subject-driven text-to-image generation but does not cite any particular dataset.",
      "processing_time": 54.1368305683136,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Subject-driven text-to-image generation permits the personalization to new individuals given a few sample images [3, 9, 20, 25, 32], allowing the generation of images featuring specific subjects in novel scenes, styles, and actions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses subject-driven text-to-image generation but does not cite any particular dataset.",
      "processing_time": 54.1368305683136,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "We start training from the StableDiffusion v1-5 [31] model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (StableDiffusion v1-5) but does not refer to any specific dataset. The citation is used to reference the model, not a dataset.",
      "processing_time": 56.077370405197144,
      "citing_paper_id": "258740710",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Given the lack of published baselines in our tuning-free environment, we compare with leading optimization-based approaches, including DreamBooth [32], Textual-Inversion [9], and Custom Diffusion [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimization-based approaches but does not refer to any specific datasets. The cited papers are methods/models, not datasets.",
      "processing_time": 53.49291682243347,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Given the lack of published baselines in our tuning-free environment, we compare with leading optimization-based approaches, including DreamBooth [32], Textual-Inversion [9], and Custom Diffusion [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimization-based approaches but does not refer to any specific datasets. The cited papers are methods/models, not datasets.",
      "processing_time": 53.49291682243347,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "However, all these tuning-based methods [9, 10, 20, 32] require resource-intensive backpropagation, and the hardware must be capable of fine-tuning the model, which is neither feasible on edge devices such as smartphones, nor scalable for cloud-based applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses the limitations of tuning-based methods for model fine-tuning.",
      "processing_time": 53.20503807067871,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "However, all these tuning-based methods [9, 10, 20, 32] require resource-intensive backpropagation, and the hardware must be capable of fine-tuning the model, which is neither feasible on edge devices such as smartphones, nor scalable for cloud-based applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses the limitations of tuning-based methods for model fine-tuning.",
      "processing_time": 53.20503807067871,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Optimizationbased methods [9, 20, 32] frequently falter in maintaining identity preservation, often generating generic images or images that blend identities among different reference subjects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization-based methods and their limitations in identity preservation.",
      "processing_time": 52.51473689079285,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Optimizationbased methods [9, 20, 32] frequently falter in maintaining identity preservation, often generating generic images or images that blend identities among different reference subjects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization-based methods and their limitations in identity preservation.",
      "processing_time": 52.51473689079285,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "This is achieved by either fine-tuning the model weights [20, 32] or inverting the subject image into a text token that encodes the subject identity [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for fine-tuning models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.43832731246948,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "This is achieved by either fine-tuning the model weights [20, 32] or inverting the subject image into a text token that encodes the subject identity [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for fine-tuning models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.43832731246948,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "DreamBooth [32], textual-inversion [9], and custom-diffusion [20] use optimization-based methods to embed subjects into diffusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and models (DreamBooth, textual-inversion, custom-diffusion) but does not reference any specific datasets. The focus is on the methodologies used to embed subjects into diffusion models.",
      "processing_time": 56.678632736206055,
      "citing_paper_id": "258740710",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "DreamBooth [32], textual-inversion [9], and custom-diffusion [20] use optimization-based methods to embed subjects into diffusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions methods and models (DreamBooth, textual-inversion, custom-diffusion) but does not reference any specific datasets. The focus is on the methodologies used to embed subjects into diffusion models.",
      "processing_time": 56.678632736206055,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Our design allows the generation of images featuring specified subjects with only forward passes and can be further integrated with model compression techniques [2, 11, 40] to boost deployment efficiency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. The context is about model design and integration with compression techniques.",
      "processing_time": 53.433678150177,
      "citing_paper_id": "258740710",
      "cited_paper_id": 253708271
    },
    {
      "context_text": "Custom-Diffusion [20] enables multi-concepts composition by jointly fine-tuning the diffusion model for multiple concepts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for customizing text-to-image diffusion models.",
      "processing_time": 52.30707859992981,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Furthermore, our prompt consistency is on par with tuning-based approaches [9, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison to tuning-based approaches. No verifiable resources are identified.",
      "processing_time": 52.8481867313385,
      "citing_paper_id": "258740710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "First, we use the BLIP-2 model [21] blip2-opt-6.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (BLIP-2) and does not refer to any specific dataset. The context does not provide information about a dataset being used.",
      "processing_time": 54.27345585823059,
      "citing_paper_id": "258740710",
      "cited_paper_id": 256390509
    },
    {
      "context_text": "SpaText [1], and Collage Diffusion [33] enable multi-object composition through a layout to image generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models. The context focuses on the capabilities of SpaText and Collage Diffusion, which are not datasets.",
      "processing_time": 55.05566191673279,
      "citing_paper_id": "258740710",
      "cited_paper_id": 257255250
    },
    {
      "context_text": "Nevertheless, these techniques either compose generic objects without customization [1] or demand the costly textual-inversion process to encode instance-speciﬁc details [33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and processes. There are no verifiable resources or datasets mentioned.",
      "processing_time": 52.88673520088196,
      "citing_paper_id": "258740710",
      "cited_paper_id": 257255250
    },
    {
      "context_text": "We use the implementations from diffusers library [38].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a library, which is not a dataset, and does not refer to any specific, verifiable dataset.",
      "processing_time": 53.52319383621216,
      "citing_paper_id": "258740710",
      "cited_paper_id": null
    },
    {
      "context_text": "We then leverage the spaCy [15] library to chunk all noun phrases in the image captions and expand numbered plural phrases (e.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (spaCy) used for processing text. No verifiable resources are identified.",
      "processing_time": 53.52004528045654,
      "citing_paper_id": "258740710",
      "cited_paper_id": null
    },
    {
      "context_text": "Over the years, many effective recommendation algorithms, such as user/item-based collaborative filtering [33, 34], matrix factorization [20, 21] and deep neural networks [15, 43, 48] have been proposed to improve recommendation accuracy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only recommendation algorithms and techniques. No verifiable resources are identified.",
      "processing_time": 52.8785502910614,
      "citing_paper_id": "224270828",
      "cited_paper_id": 2616594
    },
    {
      "context_text": "Over the years, many effective recommendation algorithms, such as user/item-based collaborative filtering [33, 34], matrix factorization [20, 21] and deep neural networks [15, 43, 48] have been proposed to improve recommendation accuracy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only recommendation algorithms and techniques. No verifiable resources are identified.",
      "processing_time": 52.8785502910614,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "Over the years, many effective recommendation algorithms, such as user/item-based collaborative filtering [33, 34], matrix factorization [20, 21] and deep neural networks [15, 43, 48] have been proposed to improve recommendation accuracy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only recommendation algorithms and techniques. No verifiable resources are identified.",
      "processing_time": 52.8785502910614,
      "citing_paper_id": "224270828",
      "cited_paper_id": 58370896
    },
    {
      "context_text": "In addition, in terms of feature matching ratio (FMR), approximately Notably, we observe that NETE-PMI performs better than NETE in terms of USR and FCR.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses performance metrics (FMR, USR, FCR) and compares two approaches (NETE-PMI and NETE).",
      "processing_time": 57.088653802871704,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "As for the other two metrics, i.e., FMR and DIV, NETE-PMI’s performance is also competitive to the baselines.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. There are no clear identifiers for datasets in the given context.",
      "processing_time": 53.014129400253296,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "Formally, given a user’s feature f u and an item’s feature f i , the PMI is computed as: Then, from item i ’s feature set F i , we select the feature ˆ f i with the highest PMI score against all the features in user u ’s feature set F u as the predicted one, i.e., ˆ f i The approximation in Eq.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a method for computing PMI scores in a recommendation system.",
      "processing_time": 52.57858920097351,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "In natural language processing, the encoder-decoder framework has been widely used in different tasks, such as machine translation [10], conversational systems [30, 42], and text summarization [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications of the encoder-decoder framework.",
      "processing_time": 52.57552480697632,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "In natural language processing, the encoder-decoder framework has been widely used in different tasks, such as machine translation [10], conversational systems [30, 42], and text summarization [2].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications of the encoder-decoder framework.",
      "processing_time": 52.57552480697632,
      "citing_paper_id": "224270828",
      "cited_paper_id": 51878811
    },
    {
      "context_text": "PMI penalizes a frequently occurring feature by dividing its prior probability, which helps us filter out less informative features for producing better explanations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PMI) used for filtering features. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.062877893447876,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "Despite its popularity, researchers have pointed out that they tend to generate too general sentences, which lack concrete meanings and therefore are less useful to users [30, 42, 47].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about generated sentences.",
      "processing_time": 51.523534059524536,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "Despite its popularity, researchers have pointed out that they tend to generate too general sentences, which lack concrete meanings and therefore are less useful to users [30, 42, 47].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about generated sentences.",
      "processing_time": 51.523534059524536,
      "citing_paper_id": "224270828",
      "cited_paper_id": 86726183
    },
    {
      "context_text": "To address this problem, researchers have introduced keywords [30, 42] or a group of attributes [47] as input to the generation model, so as to improve the expressiveness of the generated contents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches to improving content generation using keywords or attributes.",
      "processing_time": 52.62741231918335,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "To address this problem, researchers have introduced keywords [30, 42] or a group of attributes [47] as input to the generation model, so as to improve the expressiveness of the generated contents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches to improving content generation using keywords or attributes.",
      "processing_time": 52.62741231918335,
      "citing_paper_id": "224270828",
      "cited_paper_id": 86726183
    },
    {
      "context_text": "In section 4, we will provide a simple point-wise mutual information (PMI)-based approach for feature prediction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method (PMI-based approach).",
      "processing_time": 51.92536282539368,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "It might be because that NETE-PMI’s input features are predicted ones that may not exactly match those in the testing set, so the user-item-feature combination may not be commonly seen in the training data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a potential issue with input features in a model, but does not reference any dataset by name.",
      "processing_time": 55.02268838882446,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "The results show that our NETE model and its variant NETE-PMI generally perform better on all metrics, especially on USR, which measures the uniqueness of the generated sentences.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 53.21887230873108,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "By comparing predicted features with those in the testing reviews in terms of Precision and Recall, we find that the PMI-based method performs two times better than randomly selecting features from the target item’s feature set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. No verifiable resource names are present in the context.",
      "processing_time": 53.396671533584595,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "The two assumptions may not be true, but we use them in a pragmatic way, so that feature-level PMI on user u ’s feature set is additive.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It discusses assumptions and feature-level PMI, which are methodological aspects.",
      "processing_time": 54.32509922981262,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "• NETE-PMI : The only difference of this from NETE is that the input features are predicted by the PMI method introduced in Section 4, while the standard NETE model uses the feature given by the ground-truth explanation to see if our model can generate a similar sentence to comment about the feature.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a model variant (NETE-PMI) and a method (PMI) but does not reference any dataset.",
      "processing_time": 56.0284161567688,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "This is as expected, because the former takes users’ preferences on features into consideration when computing PMI values, while the latter arbitrarily selects features of the target item without considering these information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological difference between two approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.936912059783936,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "As point-wise mutual information (PMI) [30, 42] has been widely used in computational linguistics to find the association between words/features, we utilize it to predict a user’s interest to a feature by measuring its relationship with the user’s preferred features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PMI) used for predicting user interest. There are no clear identifiers for datasets.",
      "processing_time": 54.32918310165405,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "To evaluate the recommendation performance, in additon to some of the above methods, i.e., NRT, NETE-GM and NETE-MUL (NETE-GRU and NETE-PMI are excluded because their recommendation module is the same as NETE’s), we compare with the following three typical rating prediction methods: • MF : Matrix Factorization [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on comparing different recommendation and rating prediction methods.",
      "processing_time": 53.78968048095703,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "On the other hand, NETE-PMI, which takes the features predicted by the PMI method as input, performs similar with the GRU-based methods, i.e., NRT, Att2Seq, NETE-GM and NETE-GRU, because some of the predicted features may not match the ones in the testing set, and thus the generated contents may diverge from the ground-truth.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 53.54992389678955,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "• DeepCoNN: Deep Cooperative Neural Networks [48].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DeepCoNN. The context is about a recommendation system using reviews, but no dataset names are provided.",
      "processing_time": 55.39147472381592,
      "citing_paper_id": "224270828",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "Moreover, because the outputs from neural generation methods may not be easy to control [41], some works have leveraged attention mechanism [28] or copy mechanism [14] to force the models to include some specific words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and mechanisms. The cited papers do not introduce new dataset names either.",
      "processing_time": 53.34162664413452,
      "citing_paper_id": "224270828",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "Moreover, because the outputs from neural generation methods may not be easy to control [41], some works have leveraged attention mechanism [28] or copy mechanism [14] to force the models to include some specific words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and mechanisms. The cited papers do not introduce new dataset names either.",
      "processing_time": 53.34162664413452,
      "citing_paper_id": "224270828",
      "cited_paper_id": 52135124
    },
    {
      "context_text": "Recently, increasing attention has been paid to generating explanations for recommendations [3–5, 7–9, 12, 39, 44, 45], because it has been shown that providing explanations can help users to make better and/or faster decisions, increase the system’s ease of use and enjoyment, and gain user trust in the system [36, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of generating explanations for recommendations. No verifiable resources are identified.",
      "processing_time": 53.77848148345947,
      "citing_paper_id": "224270828",
      "cited_paper_id": 8407569
    },
    {
      "context_text": "Recently, increasing attention has been paid to generating explanations for recommendations [3–5, 7–9, 12, 39, 44, 45], because it has been shown that providing explanations can help users to make better and/or faster decisions, increase the system’s ease of use and enjoyment, and gain user trust in the system [36, 44].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of generating explanations for recommendations. No verifiable resources are identified.",
      "processing_time": 53.77848148345947,
      "citing_paper_id": "224270828",
      "cited_paper_id": 13752895
    },
    {
      "context_text": "These problems amount to the importance of quality control in natural language generation approaches to explainable recommendation, since poor explanations may bring negative effects to the user receptiveness of recommendations and the overall user experience in recommender systems [36].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the importance of quality control in explanations for recommendation systems.",
      "processing_time": 54.53583860397339,
      "citing_paper_id": "224270828",
      "cited_paper_id": 8407569
    },
    {
      "context_text": "We treat the feature and the neural template as two types of information, so we use two GRUs to process them, respectively, which are finally merged by GFU.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for processing information using GRUs and GFU.",
      "processing_time": 52.70280385017395,
      "citing_paper_id": "224270828",
      "cited_paper_id": 9401721
    },
    {
      "context_text": "The GFU then combines the outputs from the two GRUs to emit a final hidden state that is used to predict the next word.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the architecture of a neural network component.",
      "processing_time": 52.091389179229736,
      "citing_paper_id": "224270828",
      "cited_paper_id": 9401721
    },
    {
      "context_text": "Specifically, the GFRU in our NETE model consists of three components: two Gated Recurrent Units (GRU) that are responsible for generating the item feature word and the sentence context words (i.e., the template), respectively, as well as a Gated Fusion Unit (GFU) that decides which GRU’s word to be emitted at each time step.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only components of a model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.51168370246887,
      "citing_paper_id": "224270828",
      "cited_paper_id": 9401721
    },
    {
      "context_text": "Specifically, our GFRU consists of three components: two GRUs and a gated fusion unit (GFU) [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components of a model.",
      "processing_time": 51.55916738510132,
      "citing_paper_id": "224270828",
      "cited_paper_id": 9401721
    },
    {
      "context_text": "also adopt table-to-text generation frameworks [41], such as review generation [11, 37, 40], tips generation [25], and explanation generation [5, 9], where the input data include users, items and ratings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'input data' which includes users, items, and ratings, but does not specify a named dataset. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 56.4150013923645,
      "citing_paper_id": "224270828",
      "cited_paper_id": 10154792
    },
    {
      "context_text": "also adopt table-to-text generation frameworks [41], such as review generation [11, 37, 40], tips generation [25], and explanation generation [5, 9], where the input data include users, items and ratings.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'input data' which includes users, items, and ratings, but does not specify a named dataset. The cited papers do not provide additional context to identify a specific dataset.",
      "processing_time": 56.4150013923645,
      "citing_paper_id": "224270828",
      "cited_paper_id": 52135124
    },
    {
      "context_text": "Specifically, our method generates an explanation that discusses about at least one concrete feature of the item, while previous methods usually adopt a textual review [11, 37, 40], its first sentence [9], or the review title [25] for generation, which could be irrelevant to the recommended item.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for generating explanations or recommendations.",
      "processing_time": 52.12176251411438,
      "citing_paper_id": "224270828",
      "cited_paper_id": 10154792
    },
    {
      "context_text": "For example, Visually Explainable Collaborative Filtering (VECF) [7] and Multimodal Review Generation (MRG) [37] generate descriptions based on image features, while the Neural Memory Model (NMM) [40] considers the neighborhood relation between reviews for review generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing different approaches to generating descriptions and reviews.",
      "processing_time": 54.037981033325195,
      "citing_paper_id": "224270828",
      "cited_paper_id": 10154792
    },
    {
      "context_text": "(MRG) [37] generate descriptions based on image features, while the Neural Memory Model (NMM) [40] considers the neighborhood relation between reviews for review generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing different approaches to text generation.",
      "processing_time": 53.568400859832764,
      "citing_paper_id": "224270828",
      "cited_paper_id": 10154792
    },
    {
      "context_text": "However, the selected reviews could be too long and may contain information that are irrelevant to the item or the user’s interests [44], which may confuse the user.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue with review length and relevance in recommendation systems.",
      "processing_time": 52.852022886276245,
      "citing_paper_id": "224270828",
      "cited_paper_id": 13752895
    },
    {
      "context_text": ", human-computer interaction approaches that investigate human perception on different types of explanations [6, 13, 23], and machine learning approaches, which design algorithms to provide recommendation explanations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 53.03169250488281,
      "citing_paper_id": "224270828",
      "cited_paper_id": 17471203
    },
    {
      "context_text": ", human-computer interaction approaches that investigate human perception on different types of explanations [6, 13, 23], and machine learning approaches, which design algorithms to provide recommendation explanations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 53.03169250488281,
      "citing_paper_id": "224270828",
      "cited_paper_id": 18712907
    },
    {
      "context_text": "On one hand, template-based explanation methods [12, 38, 45] have beenwidely used, which adopt a predefined template to create explanations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only template-based explanation methods. No verifiable resources are identified.",
      "processing_time": 53.02863621711731,
      "citing_paper_id": "224270828",
      "cited_paper_id": 47019137
    },
    {
      "context_text": ", “You might be interested in [feature], on which this product performs well”) [45], with the slot(s) to be filled by means of matrix/tensor factorization [38, 45] or attention mechanism [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 54.035414934158325,
      "citing_paper_id": "224270828",
      "cited_paper_id": 47019137
    },
    {
      "context_text": "The explanation generation problem can be formulated as a table-to-text generation task [41], where the table contains a user, an item, and possibly other attributes, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a formulation of a problem. No clear identifiers for datasets are present.",
      "processing_time": 53.55449461936951,
      "citing_paper_id": "224270828",
      "cited_paper_id": 52135124
    },
    {
      "context_text": "…performance, in additon to some of the above methods, i.e., NRT, NETE-GM and NETE-MUL (NETE-GRU and NETE-PMI are excluded because their recommendation module is the same as NETE’s), we compare with the following three typical rating prediction methods: • MF : Matrix Factorization [21].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is used to reference a method, not a dataset.",
      "processing_time": 54.24049973487854,
      "citing_paper_id": "224270828",
      "cited_paper_id": 58370896
    },
    {
      "context_text": "Traditionally, rating prediction task is achieved by the inner product between the user and item latent factors [21], but its bi-linear nature may make it difficult to model complex user-item interactions [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (matrix factorization) and a limitation of the method. No verifiable resources are identified.",
      "processing_time": 54.87489175796509,
      "citing_paper_id": "224270828",
      "cited_paper_id": 58370896
    },
    {
      "context_text": ", images [7, 18] and item neighbors [26, 32]), text explanation [3, 5, 8, 9, 12, 39, 45] has been widely studied because of the abundant textual data that online marketplace websites, such as TripAdvisor, Yelp and Amazon, could offer.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of data (images, item neighbors, text explanation) and mentions online marketplace websites (TripAdvisor, Yelp, Amazon) as sources of textual data.",
      "processing_time": 58.09735298156738,
      "citing_paper_id": "224270828",
      "cited_paper_id": 170078790
    },
    {
      "context_text": ", images [7, 18] and item neighbors [26, 32]), text explanation [3, 5, 8, 9, 12, 39, 45] has been widely studied because of the abundant textual data that online marketplace websites, such as TripAdvisor, Yelp and Amazon, could offer.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of data (images, item neighbors, text explanation) and mentions online marketplace websites (TripAdvisor, Yelp, Amazon) as sources of textual data.",
      "processing_time": 58.09735298156738,
      "citing_paper_id": "224270828",
      "cited_paper_id": 197640939
    },
    {
      "context_text": "Therefore, we adopt non-linear transformations that have been shown to have better representation ability in different fields, such as computer vision [22] and natural language processing [29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to non-linear transformations in computer vision and NLP.",
      "processing_time": 52.72881603240967,
      "citing_paper_id": "224270828",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "Therefore, some research works have shifted to retrieve sentences instead of the whole review as explanations [8, 39], but they are still limited to adopting existing sentences and cannot create new contents.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a shift in research approach. No verifiable resources are identified.",
      "processing_time": 52.960739612579346,
      "citing_paper_id": "224270828",
      "cited_paper_id": 197640939
    },
    {
      "context_text": "DIN\nsegments each text into phases and stores the phases as integer IDs, while M6-Rec directly operates on the text.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 53.49387192726135,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "DIN stores most features values\nas integer IDs, learns the embeddings of the IDs, and models the interaction between features via attention modules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Deep Interest Network) and its components. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.561522483825684,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "DIN additionally uses item IDs as features, which are widely deemed useful.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to item IDs as features, which is too generic.",
      "processing_time": 53.712249755859375,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Therefore, M6-Rec uses only a small subset of the training data when running on TaoProduct, AlipayQuery, and AlipayMiniApp, even though the baselines such as DIN and YouTubeDNN do use all training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'TaoProduct', 'AlipayQuery', and 'AlipayMiniApp' as datasets but does not provide enough detail to confirm their nature as specific, verifiable datasets. The context also mentions 'training data' generically.",
      "processing_time": 58.77355074882507,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Therefore, M6-Rec uses only a small subset of the training data when running on TaoProduct, AlipayQuery, and AlipayMiniApp, even though the baselines such as DIN and YouTubeDNN do use all training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'TaoProduct', 'AlipayQuery', and 'AlipayMiniApp' as datasets but does not provide enough detail to confirm their nature as specific, verifiable datasets. The context also mentions 'training data' generically.",
      "processing_time": 58.77355074882507,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "Infrequent items are filtered out, otherwise ID-based methods such as DIN will perform terribly on these cold items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a methodological consideration about filtering infrequent items, which is not tied to a specific dataset.",
      "processing_time": 55.173585176467896,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Table 2 shows thatM6-Rec outperformsDIN onAlipayQuery and TaoProduct, two large-scale datasets collected in Alipay’s search query recommender and Taobao’s product recommender.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AlipayQuery",
        "TaoProduct"
      ],
      "dataset_descriptions": {
        "AlipayQuery": "Used to evaluate M6-Rec's performance in Alipay’s search query recommender, focusing on click-through rate prediction.",
        "TaoProduct": "Used to evaluate M6-Rec's performance in Taobao’s product recommender, focusing on click-through rate prediction."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two datasets, 'AlipayQuery' and 'TaoProduct', which are specific and domain-qualified. These datasets are used to evaluate the performance of M6-Rec compared to DIN.",
      "processing_time": 70.2834119796753,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "We compare M6-Rec with DIN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two models. The cited paper title confirms that DIN is a method, not a dataset.",
      "processing_time": 54.52675771713257,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Each training step of a dense deep model such as M6-Rec takes far more time than that of a sparse shallow model such as DIN and YouTubeDNN.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (M6-Rec, DIN, YouTubeDNN). The citation is comparing the training times of dense deep models versus sparse shallow models.",
      "processing_time": 56.11009240150452,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Each training step of a dense deep model such as M6-Rec takes far more time than that of a sparse shallow model such as DIN and YouTubeDNN.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (M6-Rec, DIN, YouTubeDNN). The citation is comparing the training times of dense deep models versus sparse shallow models.",
      "processing_time": 56.11009240150452,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "Most systems use a deep sequential model such as DIN [65] for CTR prediction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DIN) for CTR prediction.",
      "processing_time": 52.468504428863525,
      "citing_paper_id": "248834570",
      "cited_paper_id": 1637394
    },
    {
      "context_text": "Finally, to make M6-Rec deployable on edge devices such as mobile phones, we further employ techniques such as parameter sharing [20], pruning [58], quantization [57], and early-exiting [16, 48] to reduce the model size and accelerate the inference speed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques for reducing model size and accelerating inference speed. No verifiable resources are identified.",
      "processing_time": 54.10191369056702,
      "citing_paper_id": "248834570",
      "cited_paper_id": 2916466
    },
    {
      "context_text": "Finally, to make M6-Rec deployable on edge devices such as mobile phones, we further employ techniques such as parameter sharing [20], pruning [58], quantization [57], and early-exiting [16, 48] to reduce the model size and accelerate the inference speed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques for reducing model size and accelerating inference speed. No verifiable resources are identified.",
      "processing_time": 54.10191369056702,
      "citing_paper_id": "248834570",
      "cited_paper_id": 3475998
    },
    {
      "context_text": "Finally, to make M6-Rec deployable on edge devices such as mobile phones, we further employ techniques such as parameter sharing [20], pruning [58], quantization [57], and early-exiting [16, 48] to reduce the model size and accelerate the inference speed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques for reducing model size and accelerating inference speed. No verifiable resources are identified.",
      "processing_time": 54.10191369056702,
      "citing_paper_id": "248834570",
      "cited_paper_id": 243938339
    },
    {
      "context_text": "Early exiting [16, 48] is a common technique for reducing the latency of deep models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a technique for reducing latency in deep models. The cited papers do not provide additional information about datasets.",
      "processing_time": 54.325276136398315,
      "citing_paper_id": "248834570",
      "cited_paper_id": 2916466
    },
    {
      "context_text": "Early exiting [16, 48] is a common technique for reducing the latency of deep models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a technique for reducing latency in deep models. The cited papers do not provide additional information about datasets.",
      "processing_time": 54.325276136398315,
      "citing_paper_id": "248834570",
      "cited_paper_id": 3475998
    },
    {
      "context_text": "M6-Rec uses the following plain text format to support both personalized product design [18] and explainable recommendation [61]: [BOS′] .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a format used for personalized product design and explainable recommendation. No verifiable resources are identified.",
      "processing_time": 53.90775990486145,
      "citing_paper_id": "248834570",
      "cited_paper_id": 13752895
    },
    {
      "context_text": "M6-Rec uses the following plain text format to support both personalized product design [18] and explainable recommendation [61]: [BOS′] .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a format used for personalized product design and explainable recommendation. No verifiable resources are identified.",
      "processing_time": 53.90775990486145,
      "citing_paper_id": "248834570",
      "cited_paper_id": 23787675
    },
    {
      "context_text": "For example, there are tasks such as retrieval and ranking for deciding which contents to deliver to a user [6], tasks such as keyword highlighting and explanation generation for polishing the way the contents are presented [61], and even tasks such as trend forecasting and AI-assisted content production for enriching the supply of contents [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. No verifiable resources are named.",
      "processing_time": 53.05267333984375,
      "citing_paper_id": "248834570",
      "cited_paper_id": 13752895
    },
    {
      "context_text": "For example, there are tasks such as retrieval and ranking for deciding which contents to deliver to a user [6], tasks such as keyword highlighting and explanation generation for polishing the way the contents are presented [61], and even tasks such as trend forecasting and AI-assisted content production for enriching the supply of contents [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. No verifiable resources are named.",
      "processing_time": 53.05267333984375,
      "citing_paper_id": "248834570",
      "cited_paper_id": 23787675
    },
    {
      "context_text": "For example, there are tasks such as retrieval and ranking for deciding which contents to deliver to a user [6], tasks such as keyword highlighting and explanation generation for polishing the way the contents are presented [61], and even tasks such as trend forecasting and AI-assisted content production for enriching the supply of contents [18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks and applications. No verifiable resources are named.",
      "processing_time": 53.05267333984375,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "It uses a sequence-to-sequence attention mask similar to UniLM [10], where the source sentence falls within a region that uses the bidirectional mask and the target sentence is in a follow-up region that uses the unidirectional mask (see Figure 1).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (UniLM) and its usage in a sequence-to-sequence attention mask.",
      "processing_time": 53.762375354766846,
      "citing_paper_id": "248834570",
      "cited_paper_id": 147704286
    },
    {
      "context_text": "Previous works [14, 53, 67] have adopted early exiting for BERT-like bidirectional language models, and we show in this work that it is also applicable to GPT-like autoregressive language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 53.58719730377197,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Previous works [14, 53, 67] have adopted early exiting for BERT-like bidirectional language models, and we show in this work that it is also applicable to GPT-like autoregressive language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 53.58719730377197,
      "citing_paper_id": "248834570",
      "cited_paper_id": 215415863
    },
    {
      "context_text": "Previous works [14, 53, 67] have adopted early exiting for BERT-like bidirectional language models, and we show in this work that it is also applicable to GPT-like autoregressive language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 53.58719730377197,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216552850
    },
    {
      "context_text": "Previous works [14, 53, 67] have adopted early exiting for BERT-like bidirectional language models, and we show in this work that it is also applicable to GPT-like autoregressive language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 53.58719730377197,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions language models but does not refer to any specific datasets. It focuses on the capabilities of these models rather than the datasets they are trained or evaluated on.",
      "processing_time": 54.98205041885376,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions language models but does not refer to any specific datasets. It focuses on the capabilities of these models rather than the datasets they are trained or evaluated on.",
      "processing_time": 54.98205041885376,
      "citing_paper_id": "248834570",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions language models but does not refer to any specific datasets. It focuses on the capabilities of these models rather than the datasets they are trained or evaluated on.",
      "processing_time": 54.98205041885376,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions language models but does not refer to any specific datasets. It focuses on the capabilities of these models rather than the datasets they are trained or evaluated on.",
      "processing_time": 54.98205041885376,
      "citing_paper_id": "248834570",
      "cited_paper_id": 237091588
    },
    {
      "context_text": "2.2 Language Models as Foundations Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.9101939201355,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "2.2 Language Models as Foundations Language models such as BERT [7], GPT [2, 38] and T5 [39] transfer knowledge from web corpora to downstream tasks and are thus deemed foundation models [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.9101939201355,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "WebGPT [34] demonstrates that user behavior data are valuable for pretrained language models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'user behavior data'. This is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 54.797040939331055,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "WebGPT [34] demonstrates that user behavior data are valuable for pretrained language models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to 'user behavior data'. This is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 54.797040939331055,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Specifically, WebGPT collects human behavior data in a simplified text-based web browsing environment, and use the behavior data for teaching GPT-3 [2] how to browse the web when answering questions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WebGPT' and 'behavior data', but does not specify a named dataset. The focus is on the method or approach rather than a reusable dataset.",
      "processing_time": 55.46241521835327,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Specifically, WebGPT collects human behavior data in a simplified text-based web browsing environment, and use the behavior data for teaching GPT-3 [2] how to browse the web when answering questions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WebGPT' and 'behavior data', but does not specify a named dataset. The focus is on the method or approach rather than a reusable dataset.",
      "processing_time": 55.46241521835327,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "The concurrent proof-of-concept work of Sileo et al. [43] shows that a pretrained model such as GPT-2 [38] does contain knowledge useful for recommendation, by using a text prompt to guide the non-finetuned GPT-2 to rank movies.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on the capability of pretrained models like GPT-2 to perform recommendation tasks using text prompts.",
      "processing_time": 55.92129611968994,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "The concurrent proof-of-concept work of Sileo et al. [43] shows that a pretrained model such as GPT-2 [38] does contain knowledge useful for recommendation, by using a text prompt to guide the non-finetuned GPT-2 to rank movies.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The focus is on the capability of pretrained models like GPT-2 to perform recommendation tasks using text prompts.",
      "processing_time": 55.92129611968994,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "[43] shows that a pretrained model such as GPT-2 [38] does contain knowledge useful for recommendation, by using a text prompt to guide the non-finetuned GPT-2 to rank movies.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GPT-2 but does not refer to it as a dataset. It is used as a model to demonstrate zero-shot recommendation capabilities.",
      "processing_time": 54.333608865737915,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "[43] shows that a pretrained model such as GPT-2 [38] does contain knowledge useful for recommendation, by using a text prompt to guide the non-finetuned GPT-2 to rank movies.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GPT-2 but does not refer to it as a dataset. It is used as a model to demonstrate zero-shot recommendation capabilities.",
      "processing_time": 54.333608865737915,
      "citing_paper_id": "248834570",
      "cited_paper_id": 244954768
    },
    {
      "context_text": "Prompt tuning prepends the input sequence with a predetermined number of trainable embeddings that serve as “soft prompts” in place of the discrete text prompts used by GPT-3, and tunes the soft prompts for downstream adaptation while freezing the pretrained model’s parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses a method for adapting language models using prompt tuning.",
      "processing_time": 52.319236516952515,
      "citing_paper_id": "248834570",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Prompt tuning prepends the input sequence with a predetermined number of trainable embeddings that serve as “soft prompts” in place of the discrete text prompts used by GPT-3, and tunes the soft prompts for downstream adaptation while freezing the pretrained model’s parameters.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses a method for adapting language models using prompt tuning.",
      "processing_time": 52.319236516952515,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "There have been two recent trends in making language models more powerful foundations: (i) some researchers explore the scaling law and construct increasingly larger-scale models [11, 21, 30, 42, 46, 59, 62], and (ii) the others strive for the unification of multiple modalities [8, 28, 32, 37, 40, 44, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only trends in language model research. No verifiable resources are identified.",
      "processing_time": 53.266154527664185,
      "citing_paper_id": "248834570",
      "cited_paper_id": 199453025
    },
    {
      "context_text": "There have been two recent trends in making language models more powerful foundations: (i) some researchers explore the scaling law and construct increasingly larger-scale models [11, 21, 30, 42, 46, 59, 62], and (ii) the others strive for the unification of multiple modalities [8, 28, 32, 37, 40, 44, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only trends in language model research. No verifiable resources are identified.",
      "processing_time": 53.266154527664185,
      "citing_paper_id": "248834570",
      "cited_paper_id": 201317624
    },
    {
      "context_text": "There have been two recent trends in making language models more powerful foundations: (i) some researchers explore the scaling law and construct increasingly larger-scale models [11, 21, 30, 42, 46, 59, 62], and (ii) the others strive for the unification of multiple modalities [8, 28, 32, 37, 40, 44, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only trends in language model research. No verifiable resources are identified.",
      "processing_time": 53.266154527664185,
      "citing_paper_id": "248834570",
      "cited_paper_id": 232075617
    },
    {
      "context_text": "There have been two recent trends in making language models more powerful foundations: (i) some researchers explore the scaling law and construct increasingly larger-scale models [11, 21, 30, 42, 46, 59, 62], and (ii) the others strive for the unification of multiple modalities [8, 28, 32, 37, 40, 44, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only trends in language model research. No verifiable resources are identified.",
      "processing_time": 53.266154527664185,
      "citing_paper_id": "248834570",
      "cited_paper_id": 233394012
    },
    {
      "context_text": "There have been two recent trends in making language models more powerful foundations: (i) some researchers explore the scaling law and construct increasingly larger-scale models [11, 21, 30, 42, 46, 59, 62], and (ii) the others strive for the unification of multiple modalities [8, 28, 32, 37, 40, 44, 63].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only trends in language model research. No verifiable resources are identified.",
      "processing_time": 53.266154527664185,
      "citing_paper_id": "248834570",
      "cited_paper_id": 235731579
    },
    {
      "context_text": "Weexperimentwith dialogbased recommendation following the settings of KBRD [3] on dataset ReDial .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ReDial"
      ],
      "dataset_descriptions": {
        "ReDial": "Used to experiment with dialog-based recommendation, focusing on knowledge-based interactions in a conversational setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ReDial' as a dataset used for experimenting with dialog-based recommendation. The cited paper title does not provide additional disambiguation but confirms the relevance to recommender systems.",
      "processing_time": 61.580305099487305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 199668753
    },
    {
      "context_text": "Techniques such as distillation [17, 41, 47, 50] and early exiting [53, 67] can avoid high latency, but cannot fully leverage a giant foundation model’s capacity since they reduce or use only a subset of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses techniques like distillation and early exiting but does not mention any specific datasets. The cited papers are about model compression techniques, not datasets.",
      "processing_time": 54.483166456222534,
      "citing_paper_id": "248834570",
      "cited_paper_id": 202719327
    },
    {
      "context_text": "Techniques such as distillation [17, 41, 47, 50] and early exiting [53, 67] can avoid high latency, but cannot fully leverage a giant foundation model’s capacity since they reduce or use only a subset of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses techniques like distillation and early exiting but does not mention any specific datasets. The cited papers are about model compression techniques, not datasets.",
      "processing_time": 54.483166456222534,
      "citing_paper_id": "248834570",
      "cited_paper_id": 203626972
    },
    {
      "context_text": "Techniques such as distillation [17, 41, 47, 50] and early exiting [53, 67] can avoid high latency, but cannot fully leverage a giant foundation model’s capacity since they reduce or use only a subset of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses techniques like distillation and early exiting but does not mention any specific datasets. The cited papers are about model compression techniques, not datasets.",
      "processing_time": 54.483166456222534,
      "citing_paper_id": "248834570",
      "cited_paper_id": 211296536
    },
    {
      "context_text": "Techniques such as distillation [17, 41, 47, 50] and early exiting [53, 67] can avoid high latency, but cannot fully leverage a giant foundation model’s capacity since they reduce or use only a subset of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses techniques like distillation and early exiting but does not mention any specific datasets. The cited papers are about model compression techniques, not datasets.",
      "processing_time": 54.483166456222534,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216552850
    },
    {
      "context_text": ", parameter sharing [20], distillation [17, 41, 47, 50], pruning [5, 12, 58], and quantization [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various techniques such as parameter sharing, distillation, pruning, and quantization. The cited papers also do not provide any specific dataset names.",
      "processing_time": 56.204328536987305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 202719327
    },
    {
      "context_text": ", parameter sharing [20], distillation [17, 41, 47, 50], pruning [5, 12, 58], and quantization [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various techniques such as parameter sharing, distillation, pruning, and quantization. The cited papers also do not provide any specific dataset names.",
      "processing_time": 56.204328536987305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 203626972
    },
    {
      "context_text": ", parameter sharing [20], distillation [17, 41, 47, 50], pruning [5, 12, 58], and quantization [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various techniques such as parameter sharing, distillation, pruning, and quantization. The cited papers also do not provide any specific dataset names.",
      "processing_time": 56.204328536987305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 211171709
    },
    {
      "context_text": ", parameter sharing [20], distillation [17, 41, 47, 50], pruning [5, 12, 58], and quantization [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various techniques such as parameter sharing, distillation, pruning, and quantization. The cited papers also do not provide any specific dataset names.",
      "processing_time": 56.204328536987305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 211296536
    },
    {
      "context_text": ", parameter sharing [20], distillation [17, 41, 47, 50], pruning [5, 12, 58], and quantization [57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various techniques such as parameter sharing, distillation, pruning, and quantization. The cited papers also do not provide any specific dataset names.",
      "processing_time": 56.204328536987305,
      "citing_paper_id": "248834570",
      "cited_paper_id": 243938339
    },
    {
      "context_text": "And YouTubeDNN is a traditional baseline that maps feature values to IDs and produces user/item embeddings based on the IDs’ embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (YouTubeDNN) and its functionality.",
      "processing_time": 53.376638889312744,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "We compare M6-Rec with TwinBERT [33] and YouTubeDNN [6].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions YouTubeDNN, which is a method/model, not a dataset. No datasets are explicitly mentioned or used in the given context.",
      "processing_time": 54.21460676193237,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "Industrial recommender systems include a task called retrieval, also known as matching or candidate generation, which is executed prior to ranking tasks such as CTR and CVR prediction [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task in recommender systems. No verifiable resources are identified.",
      "processing_time": 53.38288187980652,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "Moreover, when the test cases contain feature values unseen during training, such as new search queries and new mini-apps, the IDbased YouTubeDNN fails, while M6-Rec still performs well.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the performance of models (YouTubeDNN and M6-Rec) on test cases with unseen feature values, but does not name any datasets.",
      "processing_time": 57.10136890411377,
      "citing_paper_id": "248834570",
      "cited_paper_id": 207240067
    },
    {
      "context_text": "The existing works [52, 55, 56] on pretraining for recommendation are mainly about transferable user modeling, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts related to pretraining and user modeling.",
      "processing_time": 52.738152503967285,
      "citing_paper_id": "248834570",
      "cited_paper_id": 210164320
    },
    {
      "context_text": "The existing works [52, 55, 56] on pretraining for recommendation are mainly about transferable user modeling, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts related to pretraining and user modeling.",
      "processing_time": 52.738152503967285,
      "citing_paper_id": "248834570",
      "cited_paper_id": 222133055
    },
    {
      "context_text": "To reduce the model size, we use MiniLM’s relation-based approach [50] to distill the 300M M6-base into a 10M tiny model which is named M6-Edge.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for compressing pre-trained transformers. The citation is focused on the technique used rather than a dataset.",
      "processing_time": 54.88517475128174,
      "citing_paper_id": "248834570",
      "cited_paper_id": 211296536
    },
    {
      "context_text": "Previous works [19, 69] consider the late interaction between two coarse-grained entities, and M6-Rec extends it to handle the interaction among multiple finer-grained segments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (M6-Rec) that extends previous work. The cited paper title 'ColBERT' is a method, not a dataset.",
      "processing_time": 56.27373933792114,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "Second, we implement a multi-segment variant of late interaction [19] when adapting M6Rec to tasks that require low-latency real-time inference, where most of the heavy computation is pre-computed offline and cached.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ColBERT) and a model (M6Rec). The context focuses on the implementation details and computational efficiency.",
      "processing_time": 55.50953006744385,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "We thus design multi-segment late interaction, an extended variant of the original late interaction [19], to cope with the issue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'late interaction'. The context is about extending a method, not using a dataset.",
      "processing_time": 53.728286027908325,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "Late interaction [19] is another alternative in settings where caching intermediate results are possible, which segments Transformer into a deep stage that precomputes intermediate representations and a shallow stage for performing feature interaction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ColBERT) and its application in passage search. No verifiable datasets are referenced.",
      "processing_time": 54.60461187362671,
      "citing_paper_id": "248834570",
      "cited_paper_id": 216553223
    },
    {
      "context_text": "Zero-parameter methods that use text prompts perform worse than full-model fine-tuning, but enjoy the merits of fewand zero-shot learning [2].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between zero-parameter methods and full-model fine-tuning.",
      "processing_time": 53.091660022735596,
      "citing_paper_id": "248834570",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Following the literature [64], we use the items in the same mini-batches to form Y ′ and set the temperature hyperparameter τ to 0.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is too generic to infer the use of a specific dataset.",
      "processing_time": 54.1763379573822,
      "citing_paper_id": "248834570",
      "cited_paper_id": 219434009
    },
    {
      "context_text": "However, it is straightforward to support images by converting an image into a sequence of tokens, as in DALL-E [40] and M6 [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (DALL-E and M6). The context is about converting images into token sequences, which is a methodological approach rather than a dataset.",
      "processing_time": 56.44811296463013,
      "citing_paper_id": "248834570",
      "cited_paper_id": 232075617
    },
    {
      "context_text": "We thus build M6-Rec upon M6 [28, 29], an existing language model pretrained on web corpora.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'web corpora' but does not specify a named dataset. The citation is about using an existing language model (M6) pretrained on web data, which is too generic.",
      "processing_time": 56.449830055236816,
      "citing_paper_id": "248834570",
      "cited_paper_id": 232075617
    },
    {
      "context_text": "We thus build M6-Rec upon M6 [28, 29], an existing language model pretrained on web corpora.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'web corpora' but does not specify a named dataset. The citation is about using an existing language model (M6) pretrained on web data, which is too generic.",
      "processing_time": 56.449830055236816,
      "citing_paper_id": "248834570",
      "cited_paper_id": 236980215
    },
    {
      "context_text": "First, we propose option tuning, an improved variant of prompt tuning [22], in place of fine-tuning.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (prompt tuning). There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.48651623725891,
      "citing_paper_id": "248834570",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Wepropose option tuning, which improves upon prompt tuning [22] by addressing the slow convergence issue reported by some researchers [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to prompt tuning.",
      "processing_time": 52.2273736000061,
      "citing_paper_id": "248834570",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "Wepropose option tuning, which improves upon prompt tuning [22] by addressing the slow convergence issue reported by some researchers [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings related to prompt tuning.",
      "processing_time": 52.2273736000061,
      "citing_paper_id": "248834570",
      "cited_paper_id": 244102838
    },
    {
      "context_text": "Other methods, such as adapters [15], prefix tuning [27], and prompt tuning [22], attempt to minimize the performance gap when reducing the number of task-specific parameters, and it is convenient to deploy one single model for serving multiple tasks simultaneously with these methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 53.6160089969635,
      "citing_paper_id": "248834570",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "a limited hardware budget via mixed-task inference [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for parameter-efficient prompt tuning.",
      "processing_time": 52.144339084625244,
      "citing_paper_id": "248834570",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "[13] unify adapters and prefix tuning, and discover that combining the two brings performance comparable to fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings.",
      "processing_time": 51.14269018173218,
      "citing_paper_id": "248834570",
      "cited_paper_id": 238583580
    },
    {
      "context_text": "Some researchers [45] have reported the slow convergence issue of prompt tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue related to prompt tuning.",
      "processing_time": 51.37781500816345,
      "citing_paper_id": "248834570",
      "cited_paper_id": 244102838
    },
    {
      "context_text": "Particularly, question generation has been studied using template-based (Heilman and Smith, 2010; Chali and Hasan, 2015; Fabbri et al., 2020) and neural seq2seq models (Du and Cardie, 2017; Duan et al., 2017; Kim et al., 2018; Sultan et al., 2020; Shwartz et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.27098894119263,
      "citing_paper_id": "261046857",
      "cited_paper_id": 427742
    },
    {
      "context_text": "Particularly, question generation has been studied using template-based (Heilman and Smith, 2010; Chali and Hasan, 2015; Fabbri et al., 2020) and neural seq2seq models (Du and Cardie, 2017; Duan et al., 2017; Kim et al., 2018; Sultan et al., 2020; Shwartz et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.27098894119263,
      "citing_paper_id": "261046857",
      "cited_paper_id": 20969045
    },
    {
      "context_text": "For example, research shows that patients only retain a minimal amount of information from discharge instructions, with an immediate forgetting rate of up to 80% (Kessels, 2003; Richard et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general findings about patient recall of information.",
      "processing_time": 52.35046100616455,
      "citing_paper_id": "261046857",
      "cited_paper_id": 1446184
    },
    {
      "context_text": "There is a growing need to improve patients’ understanding regarding their hospital experiences (Federman et al., 2018; Weerahandi et al., 2018; Kwon et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general need to improve patient understanding of hospital experiences.",
      "processing_time": 54.27866339683533,
      "citing_paper_id": "261046857",
      "cited_paper_id": 13725401
    },
    {
      "context_text": "There is a growing need to improve patients’ understanding regarding their hospital experiences (Federman et al., 2018; Weerahandi et al., 2018; Kwon et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the general need to improve patient understanding of hospital experiences.",
      "processing_time": 54.27866339683533,
      "citing_paper_id": "261046857",
      "cited_paper_id": 53241185
    },
    {
      "context_text": "Previous research has attempted to generate hospital course summaries for patients using lay language (Di Eugenio et al., 2014; Acharya et al., 2018; Adams et al., 2021; Cai et al., 2022a; Hartman and Campion, 2022; Adams et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous research efforts. No clear identifiers for datasets are present.",
      "processing_time": 53.57938265800476,
      "citing_paper_id": "261046857",
      "cited_paper_id": 46919376
    },
    {
      "context_text": "Learning through conversation can improve education outcomes (Golinkoff et al., 2019; Zhang et al., 2020; Cai et al., 2022b; Yao et al., 2022a,a; Xu et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 55.11662745475769,
      "citing_paper_id": "261046857",
      "cited_paper_id": 51969141
    },
    {
      "context_text": "Upon hospital discharge, physicians often provide discharge instructions to aid in patients’ recovery and disease self-management (Federman et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general practice of providing discharge instructions. No verifiable resources are identified.",
      "processing_time": 52.73263239860535,
      "citing_paper_id": "261046857",
      "cited_paper_id": 53241185
    },
    {
      "context_text": ", 2019); 2) BioBERT (Lee et al., 2020); 3) PubmedBERT (Gu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (BioBERT, PubmedBERT) but does not reference any specific datasets. The context is about using these models, not datasets.",
      "processing_time": 54.92741298675537,
      "citing_paper_id": "261046857",
      "cited_paper_id": 59291975
    },
    {
      "context_text": "These models are obtained from HuggingFace: (1) BERT-large (Devlin et al., 2019); (2) BioBERT (Lee et al., 2020); (3) PubmedBERT (Gu et al., 2020); (4) ClinicalRoBERTa (Lewis et al., 2020); All four pre-trained models have the same scale of parameters (345 million).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models from HuggingFace but does not refer to any specific datasets. The context is about pre-trained models, not datasets.",
      "processing_time": 54.28285002708435,
      "citing_paper_id": "261046857",
      "cited_paper_id": 59291975
    },
    {
      "context_text": "Successful patient education requires effective questioning (Pylman and Ward, 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a paper providing tips for effective questioning in medical education.",
      "processing_time": 54.505056858062744,
      "citing_paper_id": "261046857",
      "cited_paper_id": 215795632
    },
    {
      "context_text": "Considering the factuality issues of neural language models (Maynez et al., 2020; Pagnoni et al., 2021), question generation in the medical domain remains challenging.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in the medical domain for question generation.",
      "processing_time": 52.927079916000366,
      "citing_paper_id": "261046857",
      "cited_paper_id": 218487034
    },
    {
      "context_text": "These systems focus on improving the accuracy of their answers (Soni and Roberts, 2020; Rawat et al., 2020; Yue et al., 2020a,b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to improving answer accuracy in systems. No verifiable resources are identified.",
      "processing_time": 54.275877714157104,
      "citing_paper_id": "261046857",
      "cited_paper_id": 218630366
    },
    {
      "context_text": "These systems focus on improving the accuracy of their answers (Soni and Roberts, 2020; Rawat et al., 2020; Yue et al., 2020a,b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to improving answer accuracy in systems. No verifiable resources are identified.",
      "processing_time": 54.275877714157104,
      "citing_paper_id": "261046857",
      "cited_paper_id": 218974465
    },
    {
      "context_text": "Most clinical QAs are designed to satisfy individuals’ information needs, with questions modeled after those that can be asked by physicians (Pampari et al., 2018; Jin et al., 2019; Raghavan et al., 2021; Lehman et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to clinical QAs and physician-modeled questions. No verifiable resources are identified.",
      "processing_time": 55.095582723617554,
      "citing_paper_id": "261046857",
      "cited_paper_id": 235097193
    },
    {
      "context_text": "Common human–LLM interactions include (a) zero-shot prompting , where users provide a prompt for the LLM to complete, and (b) in-context learning , where users give task examples and ask the LLM to solve a new case, potentially involving a multi-step reasoning process (Wei et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only interaction methods with LLMs. No verifiable resources are identified.",
      "processing_time": 53.77902674674988,
      "citing_paper_id": "261046857",
      "cited_paper_id": 246411621
    },
    {
      "context_text": "3 Question Answering in the GPT Era LLMs such as ChatGPT have led to significant advancements in generative AI (Brown et al., 2020; Sanh et al., 2021; Chowdhery et al., 2022; Longpre et al., 2023; OpenAI, 2023; Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their advancements in generative AI. No verifiable resources are identified.",
      "processing_time": 55.247249126434326,
      "citing_paper_id": "261046857",
      "cited_paper_id": 259287344
    },
    {
      "context_text": "3 Question Answering in the GPT Era LLMs such as ChatGPT have led to significant advancements in generative AI (Brown et al., 2020; Sanh et al., 2021; Chowdhery et al., 2022; Longpre et al., 2023; OpenAI, 2023; Wang et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their advancements in generative AI. No verifiable resources are identified.",
      "processing_time": 55.247249126434326,
      "citing_paper_id": "261046857",
      "cited_paper_id": null
    },
    {
      "context_text": "This result is consistent with other recent discussions that incorporate the copying mechanism into LM or LLM by modifying the model structure, loss function, or prompting (Wang et al., 2023b; Chang et al., 2023; Eremeev et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and modifications to model structures, loss functions, or prompting.",
      "processing_time": 54.02189254760742,
      "citing_paper_id": "261046857",
      "cited_paper_id": null
    },
    {
      "context_text": "Despite the success of the latter, simpler, domain, it is well-known that a large quantity of human dialogue centers on socialization, personal interests and chit-chat (Dunbar et al., 1997).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to general human conversational behavior without naming a particular dataset.",
      "processing_time": 54.2529718875885,
      "citing_paper_id": "6869582",
      "cited_paper_id": 1151885
    },
    {
      "context_text": "Instead, the research community has focused on task-oriented communication, such as airline or restaurant booking (Bordes and Weston, 2016), or else single-turn informa-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of research focus.",
      "processing_time": 52.23651456832886,
      "citing_paper_id": "6869582",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "The most successful goal-oriented dialogue systems model conversation as partially observable Markov decision processes (POMDPs) (Young et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (POMDPs) used in dialogue systems.",
      "processing_time": 53.52084827423096,
      "citing_paper_id": "6869582",
      "cited_paper_id": 2364633
    },
    {
      "context_text": "For the chit-chat setting, the most relevant work is (Li et al., 2016a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper. The context is about dialogue generation, but no dataset names are provided.",
      "processing_time": 55.04759359359741,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.317145586013794,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.317145586013794,
      "citing_paper_id": "6869582",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "A popular class of methods are generative recurrent systems like seq2seq applied to dialogue (Sutskever et al., 2014; Vinyals and Le, 2015; Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.317145586013794,
      "citing_paper_id": "6869582",
      "cited_paper_id": 14857825
    },
    {
      "context_text": "…models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles",
        "Cornell Movie-Dialogue Corpus",
        "Reddit",
        "Twitter"
      ],
      "dataset_descriptions": {
        "OpenSubtitles": "Used to train neural models for dialogue generation, focusing on natural language interactions in movie scripts.",
        "Cornell Movie-Dialogue Corpus": "Utilized for training neural dialogue systems, emphasizing conversational exchanges from movies.",
        "Reddit": "Employed to train neural models for generating dialogues, capturing diverse and informal online conversations.",
        "Twitter": "Used to train neural dialogue systems, focusing on short, real-time interactions and social media language."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training neural approaches in dialogue generation, which aligns with the topic of personalized text generation.",
      "processing_time": 73.99679446220398,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "…models are ones based on movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles",
        "Cornell Movie-Dialogue Corpus",
        "Reddit",
        "Twitter"
      ],
      "dataset_descriptions": {
        "OpenSubtitles": "Used to train neural models for dialogue generation, focusing on natural language interactions in movie scripts.",
        "Cornell Movie-Dialogue Corpus": "Utilized for training neural dialogue systems, emphasizing conversational exchanges from movies.",
        "Reddit": "Employed to train neural models for generating dialogues, capturing diverse and informal online conversations.",
        "Twitter": "Used to train neural dialogue systems, focusing on short, real-time interactions and social media language."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training neural approaches in dialogue generation, which aligns with the topic of personalized text generation.",
      "processing_time": 73.99679446220398,
      "citing_paper_id": "6869582",
      "cited_paper_id": 14857825
    },
    {
      "context_text": "Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many di-alogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with chit-chat models. No verifiable resources are identified.",
      "processing_time": 53.98824071884155,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SQuAD"
      ],
      "dataset_descriptions": {
        "SQuAD": "Used to highlight issues in dataset curation, specifically the word overlap problem, which affects the quality of training data for question-answering systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the SQuAD dataset, which is a specific, verifiable resource. It discusses the word overlap problem in dataset curation, which is relevant to the research topic of personalized text generation.",
      "processing_time": 64.01086950302124,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3618568
    },
    {
      "context_text": "cent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SQuAD"
      ],
      "dataset_descriptions": {
        "SQuAD": "Used to highlight limitations in dataset curation techniques, specifically the word overlap problem in question answering datasets."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions the SQuAD dataset, which is a well-known resource for question answering. However, it is mentioned in the context of a limitation regarding word overlap problems.",
      "processing_time": 61.35029101371765,
      "citing_paper_id": "6869582",
      "cited_paper_id": 3618568
    },
    {
      "context_text": "Serban et al. (2015) list available corpora for training dialogue systems.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'available corpora' but does not specify any particular dataset names. The cited paper is a survey, which suggests it lists multiple datasets without focusing on a single one.",
      "processing_time": 55.888590812683105,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "According to Serban et al. (2015)’s survey, personalization of dialogue systems is “an important task, which so far has not received much atten-tion”.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general lack of attention to personalization in dialogue systems.",
      "processing_time": 53.73889446258545,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "According to the survey (Serban et al., 2015) personalization of dialogue systems is “an important task, which so far has not received much attention”.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about personalization in dialogue systems.",
      "processing_time": 53.056474924087524,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "(Serban et al., 2015) list available corpora for training dialogue systems.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'corpora' but does not specify any particular dataset names. The cited paper is a survey, which suggests it lists multiple datasets without focusing on a single one.",
      "processing_time": 55.87174081802368,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "In particular, many of the tasks and datasets available are constrained to narrow domains ( Serban et al., 2015).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'tasks and datasets' but does not specify any particular dataset names. The reference is too generic and lacks specific identifiers.",
      "processing_time": 54.382256269454956,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "In particular, many of the tasks and datasets available are constrained to narrow domains (Serban et al., 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'tasks and datasets' but does not specify any particular dataset names. The reference to 'narrow domains' suggests a general discussion rather than specific datasets.",
      "processing_time": 56.06129717826843,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5415347
    },
    {
      "context_text": "For example, less than 5% of posts on Twitter are questions, whereas around 80% are about personal emotional state, thoughts or activities, authored by so called “Meformers” (Naaman et al., 2010).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general observations about Twitter post content. No clear, verifiable resource is identified.",
      "processing_time": 54.27105641365051,
      "citing_paper_id": "6869582",
      "cited_paper_id": 5556711
    },
    {
      "context_text": "As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016) we also perform human evaluation using crowdsourced workers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of crowdsourced workers for human evaluation. No verifiable datasets are identified.",
      "processing_time": 54.47568106651306,
      "citing_paper_id": "6869582",
      "cited_paper_id": 9197196
    },
    {
      "context_text": ", 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015);",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of neural conversational models and their training.",
      "processing_time": 53.16941046714783,
      "citing_paper_id": "6869582",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "movie scripts such as OpenSubtitles and Cornell Movie-Dialogue Corpus, and dialogue from web platforms such as Reddit and Twitter, all of which have been used for training neural approaches (Vinyals and Le, 2015; Dodge et al., 2015; Li et al., 2016b; Serban et al., 2017b).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenSubtitles",
        "Cornell Movie-Dialogue Corpus",
        "Reddit",
        "Twitter"
      ],
      "dataset_descriptions": {
        "OpenSubtitles": "Used to train neural conversational models, providing a large corpus of movie subtitles for generating natural dialogues.",
        "Cornell Movie-Dialogue Corpus": "Utilized for training neural approaches, offering a structured collection of movie dialogues to enhance conversational skills.",
        "Reddit": "Employed to train neural models, leveraging user-generated dialogues from various subreddits to improve conversational fluency and context awareness.",
        "Twitter": "Used to train neural models, utilizing real-time, public conversations to enhance the responsiveness and naturalness of generated text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for training neural approaches in conversational models, which are directly relevant to personalized text generation.",
      "processing_time": 77.55925679206848,
      "citing_paper_id": "6869582",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and their limitations.",
      "processing_time": 52.500502824783325,
      "citing_paper_id": "6869582",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "For example, modern solutions that build an open-ended dialogue system to the Alexa challenge combine hand-coded and machine-learned elements (Serban et al., 2017a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for generating dialogues. The context focuses on the combination of hand-coded and machine-learned elements in dialogue systems.",
      "processing_time": 56.502522468566895,
      "citing_paper_id": "6869582",
      "cited_paper_id": 14857825
    },
    {
      "context_text": "In the case of goal-oriented dialogue some work has focused on the agent being aware of the human’s proﬁle and adjusting the dialogue accordingly, but without a personality to the agent itself (Lucas et al., 2009; Joshi et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research work. The context focuses on the concept of personalization in goal-oriented dialog systems.",
      "processing_time": 55.01689600944519,
      "citing_paper_id": "6869582",
      "cited_paper_id": 29473470
    },
    {
      "context_text": "In the case of goal-oriented dialogue some work has focused on the agent being aware of the human’s profile and adjusting the dialogue accordingly, but without a personality to the agent itself (Lucas et al., 2009; Joshi et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about personalization in goal-oriented dialog systems.",
      "processing_time": 53.07442259788513,
      "citing_paper_id": "6869582",
      "cited_paper_id": 29473470
    },
    {
      "context_text": "Non-goal driven dialogue systems go back to Weizenbaum’s famous program ELIZA (Weizenbaum, 1966), and hand-coded systems have con-\ntinued to be used in applications to this day.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references a historical program and general applications.",
      "processing_time": 52.57989263534546,
      "citing_paper_id": "6869582",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, end-to-end methods [22,24] are proposed to combine deep convolutional networks and recurrent neural networks as autoregressive models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 54.46781396865845,
      "citing_paper_id": "92996193",
      "cited_paper_id": 652921
    },
    {
      "context_text": "On the other hand, it avoids the discrete problem when training a GAN in generating sequential data.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses a general issue with training GANs for sequential data.",
      "processing_time": 55.02582240104675,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "In detail, we choose to utilize our GAN as a Wasserstein GAN [2].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Wasserstein GAN).",
      "processing_time": 53.060657024383545,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "GANs are methods to generate synthetic data with similar statistical properties as the real one [9].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to GANs as a method for generating synthetic data.",
      "processing_time": 54.28832697868347,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Not surprisingly, all models except Simple-GAN achieve high performance on Fluency, since for those straightforward RNN-based models, it is easier to imitate human style languages, while simple implemented GAN fails due to the discreteness output in the task.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model performance on a fluency task.",
      "processing_time": 53.291311264038086,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "For the GAN part, we take the RMSProp2 algorithm with learning rate 10−5 and decay 0.9.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm (RMSProp2) which is excluded according to the instructions.",
      "processing_time": 54.44703412055969,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Thus, requirements of fluency could be unsatisfied with directly using the GAN [28].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a general reference to GANs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 54.76990270614624,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Instead of explicitly defining a loss from a target distribution, GANs train a generator by receiving a loss from a discriminator which tries to differentiate between real and generated data.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept about GANs. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 54.20584034919739,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "At the same time, we have observed that our model outperforms the others with significant margin on Human-Recall and Diversity due to the excellent generative ability of GAN.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and a model (GAN).",
      "processing_time": 53.25755524635315,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "On one hand, the embedding technology simplifies the GAN structure into DNNs which are more easy to learn.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about embedding technology and GAN structure.",
      "processing_time": 53.44940185546875,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Though GANs and its variants have shown great success in Computer Vision domain [7,19], there are still challenges in applying them to the traditional NLP tasks [26–28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general challenges in applying GANs to NLP tasks.",
      "processing_time": 53.01927924156189,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Then, considering the requirements on semantic relation, we adopt the Generative Adversarial Networks (GANs) structure [16], and further introduce a noise vector τ , so that the requirements on diversity could also be satisfied.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GANs). There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.592496395111084,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Thus, it is proved to be reasonable that the combination of embedding method and GAN is suitable for Danmu generation task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GAN) and a task (Danmu generation).",
      "processing_time": 53.60299587249756,
      "citing_paper_id": "92996193",
      "cited_paper_id": null
    },
    {
      "context_text": "Data were tokenized using the Moses tokenizer (Koehn et al., 2007).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the Moses tokenizer, which is a tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 54.359471797943115,
      "citing_paper_id": "13959787",
      "cited_paper_id": 794019
    },
    {
      "context_text": "We also use global attention of (Luong et al., 2015) to aid our generators.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'global attention' from Luong et al., 2015, which is a method, not a dataset. No specific dataset is mentioned.",
      "processing_time": 55.9648814201355,
      "citing_paper_id": "13959787",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts related to conversational agents and modulating politeness styles. No verifiable resources are identified.",
      "processing_time": 55.640106201171875,
      "citing_paper_id": "13959787",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general concepts related to conversational agents and modulating politeness styles. No verifiable resources are identified.",
      "processing_time": 55.640106201171875,
      "citing_paper_id": "13959787",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only examples of conversational styles in different settings. No verifiable resources are identified.",
      "processing_time": 54.69993233680725,
      "citing_paper_id": "13959787",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "In conversational agents (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016), for example, modulating the politeness style, to sound natural depending upon a situation: at a party with friends “Shut up! the video is starting!”, or in a professional setting “Please be quiet, the video will begin shortly.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only examples of conversational styles in different settings. No verifiable resources are identified.",
      "processing_time": 54.69993233680725,
      "citing_paper_id": "13959787",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Li et al. (2018) first extract words or phrases associated with the original style of the sentence, delete them from the original sentence and then replace them with new phrases associated with the target style.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation describes a method for style transfer but does not mention any specific datasets. The focus is on the technique rather than a particular dataset.",
      "processing_time": 54.504005908966064,
      "citing_paper_id": "13959787",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and related works. The context focuses on paraphrase generation methods and back-translation techniques.",
      "processing_time": 55.28359532356262,
      "citing_paper_id": "13959787",
      "cited_paper_id": 7421838
    },
    {
      "context_text": ", 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. The cited paper title 'The Multilingual Paraphrase Database' suggests a potential dataset, but it is not explicitly mentioned in the citation context.",
      "processing_time": 57.56683588027954,
      "citing_paper_id": "13959787",
      "cited_paper_id": 7421838
    },
    {
      "context_text": "We set up a manual pairwise comparison following Bennett (2005). The test presents the original sentence and then, in random order, its corresponding sentences produced by the baseline and our models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating synthesized sentences.",
      "processing_time": 53.010172843933105,
      "citing_paper_id": "13959787",
      "cited_paper_id": 7948174
    },
    {
      "context_text": "We set up a manual pairwise comparison following Bennett (2005).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for manual pairwise comparison. The cited paper title suggests a corpus-based synthesizer evaluation, but no specific dataset is named in the citation context.",
      "processing_time": 56.80693984031677,
      "citing_paper_id": "13959787",
      "cited_paper_id": 7948174
    },
    {
      "context_text": "While generating sentences, we use the attention vector to replace unknown characters (UNK) using the copy mechanism in (See et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method (copy mechanism) used in sentence generation.",
      "processing_time": 53.40416765213013,
      "citing_paper_id": "13959787",
      "cited_paper_id": 8314118
    },
    {
      "context_text": ", 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to natural language generation works. No verifiable resources are identified.",
      "processing_time": 54.80084443092346,
      "citing_paper_id": "13959787",
      "cited_paper_id": 9818013
    },
    {
      "context_text": ", 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to natural language generation works. No verifiable resources are identified.",
      "processing_time": 54.80084443092346,
      "citing_paper_id": "13959787",
      "cited_paper_id": 10565222
    },
    {
      "context_text": "This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general language generation tasks. No verifiable resources are identified.",
      "processing_time": 54.528578996658325,
      "citing_paper_id": "13959787",
      "cited_paper_id": 9818013
    },
    {
      "context_text": "This is particularly relevant for language generation tasks such as machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), caption generation (Karpathy and Fei-Fei, 2015; Xu et al., 2015), and natural language generation (Wen et al., 2017; Kiddon et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general language generation tasks. No verifiable resources are identified.",
      "processing_time": 54.528578996658325,
      "citing_paper_id": "13959787",
      "cited_paper_id": 10565222
    },
    {
      "context_text": "Our second dataset is comprised of top-level comments on Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages (Voigt et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages"
      ],
      "dataset_descriptions": {
        "Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages": "Used to analyze top-level comments on Facebook posts from US political figures, focusing on public engagement and response patterns in social media interactions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset of Facebook comments from US political figures, which is relevant to personalized text generation.",
      "processing_time": 68.29244899749756,
      "citing_paper_id": "13959787",
      "cited_paper_id": 13723880
    },
    {
      "context_text": "of top-level comments on Facebook posts from all 412 current members of the United States Senate and House who have public Facebook pages (Voigt et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'top-level comments on Facebook posts' which could be part of a dataset, but it does not specify a named dataset. The cited paper title 'RtGender' suggests a corpus, but it is not explicitly mentioned in the context.",
      "processing_time": 59.12370443344116,
      "citing_paper_id": "13959787",
      "cited_paper_id": 13723880
    },
    {
      "context_text": "More recently, Mallinson et al. (2017) and Wieting et al. (2017) showed how neural back-translation can be used to generate paraphrases.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (neural back-translation) used for generating paraphrases.",
      "processing_time": 54.9428915977478,
      "citing_paper_id": "13959787",
      "cited_paper_id": 23144639
    },
    {
      "context_text": "Our work is also closely-related to a problem of paraphrase generation (Madnani and Dorr, 2010; Dong et al., 2017), including methods relying on (phrase-based) back-translation (Ganitkevitch et al., 2011; Ganitkevitch and Callison-Burch, 2014). More recently, Mallinson et al. (2017) and Wieting et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works. The context is about paraphrase generation and back-translation methods.",
      "processing_time": 55.92889928817749,
      "citing_paper_id": "13959787",
      "cited_paper_id": 202928534
    },
    {
      "context_text": "In sociolinguistics, gender is known to be one of the most important social categories driving language choice (Eckert and McConnellGinet, 2003; Lakoff and Bucholtz, 2004; Coates, 2015). Reddy and Knight (2016) proposed a heuristic-based method to obfuscate gender of a writer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to sociolinguistic studies and a method for obfuscating gender. No verifiable resources are identified.",
      "processing_time": 56.132736921310425,
      "citing_paper_id": "13959787",
      "cited_paper_id": 202928534
    },
    {
      "context_text": "In sociolinguistics, gender is known to be one of the most important social categories driving language choice (Eckert and McConnellGinet, 2003; Lakoff and Bucholtz, 2004; Coates, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to sociolinguistic studies. No verifiable resources are identified.",
      "processing_time": 55.26239275932312,
      "citing_paper_id": "13959787",
      "cited_paper_id": 202928534
    },
    {
      "context_text": "In particular, it would be interesting to back-translate through multiple target languages with a single source language (Johnson et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for multilingual neural machine translation.",
      "processing_time": 54.583881855010986,
      "citing_paper_id": "13959787",
      "cited_paper_id": 260464809
    },
    {
      "context_text": "Other researchers have used reinforcement learning methods to generate emotional text [8], [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only reinforcement learning methods for generating emotional text.",
      "processing_time": 54.01722002029419,
      "citing_paper_id": "119304814",
      "cited_paper_id": 966278
    },
    {
      "context_text": "This approach also limits the variety and ﬂuency of the generated text [11], [12].",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation in the variety and fluency of generated text.",
      "processing_time": 54.774410247802734,
      "citing_paper_id": "119304814",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "The model considers two cases: when only one keyword exists, an asynchronous decoder similar to [11] is used to generate the reply.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is about the model's architecture and how it handles different cases.",
      "processing_time": 56.136789083480835,
      "citing_paper_id": "119304814",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "Recently, a sequence-to-sequence model based on sequence prediction problems, which can be applied to large-scale datasets [16], has been widely used in machine translation [17] and conversation generation [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to large-scale datasets and applications in machine translation and conversation generation.",
      "processing_time": 55.32591772079468,
      "citing_paper_id": "119304814",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Recently, a sequence-to-sequence model based on sequence prediction problems, which can be applied to large-scale datasets [16], has been widely used in machine translation [17] and conversation generation [18].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to large-scale datasets and applications in machine translation and conversation generation.",
      "processing_time": 55.32591772079468,
      "citing_paper_id": "119304814",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Later, a large number of variant models based on that were proposed, focusing on improving the quality of text in terms of grammar and sentence patterns, including increasing the diversity of generated text [3], introducing additional prior knowledge to generate more meaningful text [4], [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses improvements in model variants for text generation.",
      "processing_time": 54.483357667922974,
      "citing_paper_id": "119304814",
      "cited_paper_id": 9514751
    },
    {
      "context_text": "In recent years, most research efforts are focused on improving the quality of conversational content (e.g., ﬂuency, diversity) [4], [5] while ignoring the generation of ﬁne-grained emotional factors in text.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research trends. No verifiable resources are identified.",
      "processing_time": 54.769246339797974,
      "citing_paper_id": "119304814",
      "cited_paper_id": 9514751
    },
    {
      "context_text": "Artiﬁcial inte[1][4]lligence with both emotion and intelligence has higher practical value and signiﬁcance [1], [2].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the value of AI with emotion and intelligence.",
      "processing_time": 55.538370847702026,
      "citing_paper_id": "119304814",
      "cited_paper_id": 9514751
    },
    {
      "context_text": "The emotion dictionary and topic dictionary used are the same as the work in [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions dictionaries but does not specify their names or provide enough detail to identify them as verifiable datasets. The context is too vague to confidently extract specific dataset names.",
      "processing_time": 56.90549993515015,
      "citing_paper_id": "119304814",
      "cited_paper_id": 52094979
    },
    {
      "context_text": "In [24], the researchers introduced topic information and emotional information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of topic and emotional information. No verifiable resources are identified.",
      "processing_time": 55.26987361907959,
      "citing_paper_id": "119304814",
      "cited_paper_id": 52094979
    },
    {
      "context_text": "[70] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GP-GAN) for image blending. No datasets are referenced for training or evaluation.",
      "processing_time": 55.43653988838196,
      "citing_paper_id": "251800180",
      "cited_paper_id": 10804187
    },
    {
      "context_text": "Image composition techniques [13, 38, 70] aim to clone a given subject into a new background such that the subject melds into the scene.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques for image composition. No verifiable resources are identified.",
      "processing_time": 54.7346625328064,
      "citing_paper_id": "251800180",
      "cited_paper_id": 10804187
    },
    {
      "context_text": "Following [61], we use the SentencePiece tokenizer [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the SentencePiece tokenizer but does not refer to any specific dataset. The tokenizer is a tool, not a dataset.",
      "processing_time": 55.67719507217407,
      "citing_paper_id": "251800180",
      "cited_paper_id": 52051958
    },
    {
      "context_text": "Personalization In recent years, personalization has become a prominent factor in various fields within Machine Learning such as recommendation systems [2], language models [11], and Federated Learning [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas where personalization is applied. No verifiable resources are named.",
      "processing_time": 55.386465072631836,
      "citing_paper_id": "251800180",
      "cited_paper_id": 52895470
    },
    {
      "context_text": "Personalization In recent years, personalization has become a prominent factor in various fields within Machine Learning such as recommendation systems [2], language models [11], and Federated Learning [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas where personalization is applied. No verifiable resources are named.",
      "processing_time": 55.386465072631836,
      "citing_paper_id": "251800180",
      "cited_paper_id": 203591432
    },
    {
      "context_text": "Personalization In recent years, personalization has become a prominent factor in various fields within Machine Learning such as recommendation systems [2], language models [11], and Federated Learning [28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas where personalization is applied. No verifiable resources are named.",
      "processing_time": 55.386465072631836,
      "citing_paper_id": "251800180",
      "cited_paper_id": null
    },
    {
      "context_text": "There also exists work on text-to-image synthesis [14, 16, 19, 24, 27, 35, 36, 50, 51, 55, 58, 67, 74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works on text-to-image synthesis.",
      "processing_time": 54.67770576477051,
      "citing_paper_id": "251800180",
      "cited_paper_id": 67855413
    },
    {
      "context_text": "[36] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and Jianfeng Gao.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only lists authors and does not provide context on how any resources were used.",
      "processing_time": 55.696683406829834,
      "citing_paper_id": "251800180",
      "cited_paper_id": 67855413
    },
    {
      "context_text": "Other works [4, 32] exploit the recent Diffusion models [25, 58, 60, 25, 59, 54, 44, 61, 55, 57], which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that diffusion models are used over 'highly diverse datasets'. No specific, verifiable datasets are named.",
      "processing_time": 55.779481649398804,
      "citing_paper_id": "251800180",
      "cited_paper_id": 196470871
    },
    {
      "context_text": "In order to prevent language drift [32, 38] that causes the model to associate the class name (e.g., “dog”) with the specific instance, we propose an autogenous, class-specific prior preservation loss , which leverages the semantic prior on the class that is embedded in the model, and encourages it…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method to prevent language drift. No verifiable resources are identified.",
      "processing_time": 54.97234869003296,
      "citing_paper_id": "251800180",
      "cited_paper_id": 214713859
    },
    {
      "context_text": "Language drift has been an observed problem in language models [32, 38], where a model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general issue of language drift in language models. No verifiable resources are identified.",
      "processing_time": 55.219499349594116,
      "citing_paper_id": "251800180",
      "cited_paper_id": 214713859
    },
    {
      "context_text": "Other works [4, 30] exploit the recent diffusion models [25, 25, 43, 55, 57, 59–63], which achieve state-of-the-art generation quality over highly diverse datasets, often sur-passing GANs [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'highly diverse datasets'. No specific, verifiable datasets are named.",
      "processing_time": 55.33241319656372,
      "citing_paper_id": "251800180",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Other works [4, 30] exploit the recent diffusion models [25, 25, 43, 55, 57, 59–63], which achieve state-of-the-art generation quality over highly diverse datasets, often sur-passing GANs [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'highly diverse datasets'. No specific, verifiable datasets are named.",
      "processing_time": 55.33241319656372,
      "citing_paper_id": "251800180",
      "cited_paper_id": 231979499
    },
    {
      "context_text": "Other works [4, 30] exploit the recent diffusion models [25, 25, 43, 55, 57, 59–63], which achieve state-of-the-art generation quality over highly diverse datasets, often sur-passing GANs [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'highly diverse datasets'. No specific, verifiable datasets are named.",
      "processing_time": 55.33241319656372,
      "citing_paper_id": "251800180",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "[18] Patrick Esser, Robin Rombach, and Bjorn Ommer.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only authors and a paper title. No verifiable resources are identified.",
      "processing_time": 54.710434436798096,
      "citing_paper_id": "251800180",
      "cited_paper_id": 229297973
    },
    {
      "context_text": "[14] use VQ-GAN [18] and train over more diverse data to alleviate this concern.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a reference to 'more diverse data'. This is too generic and lacks a clear identifier.",
      "processing_time": 55.15199565887451,
      "citing_paper_id": "251800180",
      "cited_paper_id": 229297973
    },
    {
      "context_text": "Crowson et al. [14] use VQ-GAN [18] and train over more diverse data to alleviate this concern.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (VQ-GAN) and a general statement about using more diverse data.",
      "processing_time": 56.089345932006836,
      "citing_paper_id": "251800180",
      "cited_paper_id": 229297973
    },
    {
      "context_text": "There also exists work on text-to-image synthesis [14, 16, 19, 24, 26, 33, 34, 48, 49, 52, 55, 64, 71].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works on text-to-image synthesis.",
      "processing_time": 53.915504693984985,
      "citing_paper_id": "251800180",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "To consider composition in novel poses, one may apply 3D reconstruction techniques [6, 8, 41, 49, 68] which usually works on rigid objects and require a larger number of views.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general techniques and requirements for 3D reconstruction. No verifiable resources are identified.",
      "processing_time": 54.980782985687256,
      "citing_paper_id": "251800180",
      "cited_paper_id": 232352655
    },
    {
      "context_text": "To consider composition in novel poses, one may apply 3D reconstruction techniques [6, 8, 41, 49, 68] which usually works on rigid objects and require a larger number of views.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general techniques and requirements for 3D reconstruction. No verifiable resources are identified.",
      "processing_time": 54.980782985687256,
      "citing_paper_id": "251800180",
      "cited_paper_id": 249209647
    },
    {
      "context_text": "Recent work [46, 21, 65, 1] combines these components to produce highly realistic manipulations using text without needing extra manual labor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the combination of components for text-driven image manipulation.",
      "processing_time": 53.90816879272461,
      "citing_paper_id": "251800180",
      "cited_paper_id": 232428282
    },
    {
      "context_text": "[62] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only authors and their work. There is no indication of dataset usage or relevance to personalized text generation.",
      "processing_time": 55.26279306411743,
      "citing_paper_id": "251800180",
      "cited_paper_id": 233241040
    },
    {
      "context_text": "Inversion [12, 15, 51] can be used to preserve a subject while modifying context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique (inversion) and its application. No verifiable resources are identified.",
      "processing_time": 55.07616853713989,
      "citing_paper_id": "251800180",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Text-driven image manipulation has recently achieved significant progress using GANs [9,22,27–29] combined with image-text representations such as CLIP [50], yielding realistic manipulations using text [2, 7, 21, 41, 46, 68].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on the use of GANs and CLIP for text-driven image manipulation.",
      "processing_time": 56.147247314453125,
      "citing_paper_id": "251800180",
      "cited_paper_id": 235606261
    },
    {
      "context_text": "Text-driven image manipulation has recently achieved significant progress using GANs [9,22,27–29] combined with image-text representations such as CLIP [50], yielding realistic manipulations using text [2, 7, 21, 41, 46, 68].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on the use of GANs and CLIP for text-driven image manipulation.",
      "processing_time": 56.147247314453125,
      "citing_paper_id": "251800180",
      "cited_paper_id": 245117814
    },
    {
      "context_text": "Text-driven image manipulation has recently achieved significant progress using GANs [9,22,27–29] combined with image-text representations such as CLIP [50], yielding realistic manipulations using text [2, 7, 21, 41, 46, 68].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on the use of GANs and CLIP for text-driven image manipulation.",
      "processing_time": 56.147247314453125,
      "citing_paper_id": "251800180",
      "cited_paper_id": null
    },
    {
      "context_text": "[26] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only authors and a paper title. There is no indication of dataset usage.",
      "processing_time": 54.82015323638916,
      "citing_paper_id": "251800180",
      "cited_paper_id": 235619773
    },
    {
      "context_text": "A key component of high-quality sample generations from [61] is the use of noise conditioning augmentation [26] for the two SR modules.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.04564070701599,
      "citing_paper_id": "251800180",
      "cited_paper_id": 235619773
    },
    {
      "context_text": "[4] Omri Avrahami, Dani Lischinski, and Ohad Fried.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only lists authors and does not provide context on usage or research.",
      "processing_time": 55.2717969417572,
      "citing_paper_id": "251800180",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "Other works [4, 31] exploit the recent diffusion models [25, 25, 45, 58, 60, 62–66], which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'highly diverse datasets'. No specific dataset names are provided.",
      "processing_time": 55.10221266746521,
      "citing_paper_id": "251800180",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "Other works [4, 31] exploit the recent diffusion models [25, 25, 45, 58, 60, 62–66], which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs [15].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to 'highly diverse datasets'. No specific dataset names are provided.",
      "processing_time": 55.10221266746521,
      "citing_paper_id": "251800180",
      "cited_paper_id": 244909410
    },
    {
      "context_text": "[31] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide any information about datasets, methods, or specific research findings. It only lists authors and a reference number.",
      "processing_time": 54.51345109939575,
      "citing_paper_id": "251800180",
      "cited_paper_id": 244909410
    },
    {
      "context_text": "[39] propose a diffusion-based technique allowing for image variations guided by reference image or text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image synthesis using semantic diffusion guidance.",
      "processing_time": 53.95788264274597,
      "citing_paper_id": "251800180",
      "cited_paper_id": 245117331
    },
    {
      "context_text": "[39] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang,",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific dataset names or verifiable resources. It only lists authors' names.",
      "processing_time": 54.26186966896057,
      "citing_paper_id": "251800180",
      "cited_paper_id": 245117331
    },
    {
      "context_text": "Text-to-Image Synthesis There also exists work on generation of images conditioned on text [16, 24, 62, 36, 37, 47, 48, 52, 67, 14, 19, 54, 27], also known as text-to-image synthesis.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to text-to-image synthesis work. No verifiable resources are identified.",
      "processing_time": 55.82175588607788,
      "citing_paper_id": "251800180",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "While most works that require only text are limited to global editing [14, 31], Bar-Tal et al. [5] proposed a text-based localized editing technique without using masks, showing impressive results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for text-based localized editing. No verifiable resources are identified.",
      "processing_time": 55.44668197631836,
      "citing_paper_id": "251800180",
      "cited_paper_id": 247996703
    },
    {
      "context_text": "[8] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T Barron, Hendrik Lensch, and Varun Jampani.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only authors and a paper title. The paper title suggests a method for extracting shape and material from images, which is not directly related to personalized text generation.",
      "processing_time": 57.58622336387634,
      "citing_paper_id": "251800180",
      "cited_paper_id": 249209647
    },
    {
      "context_text": "To overcome subject modification, several works [3, 42] assume a user-provided mask to restrict the modified area.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving user-provided masks.",
      "processing_time": 54.262439489364624,
      "citing_paper_id": "251800180",
      "cited_paper_id": 249394540
    },
    {
      "context_text": ", 2019) and models (Li et al., 2016a; Zhang et al., 2017; Qian et al., 2018) have greatly facilitated the chatbot with configurable and persistent personalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their contributions to chatbot development. No verifiable resources are identified.",
      "processing_time": 55.529805421829224,
      "citing_paper_id": "247411350",
      "cited_paper_id": 6862403
    },
    {
      "context_text": ", 2019) and models (Li et al., 2016a; Zhang et al., 2017; Qian et al., 2018) have greatly facilitated the chatbot with configurable and persistent personalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and their contributions to chatbot development. No verifiable resources are identified.",
      "processing_time": 55.529805421829224,
      "citing_paper_id": "247411350",
      "cited_paper_id": 51608471
    },
    {
      "context_text": "Existing persona dialogue datasets such as PersonaChat (Zhang et al., 2018; Dinan et al., 2019) and models (Li et al., 2016a; Zhang et al., 2017; Qian et al., 2018) have greatly facilitated the chatbot with conﬁgurable and persistent personalities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to facilitate chatbots with configurable and persistent personalities, enhancing coherence and personalization in dialogue systems."
      },
      "confidence_score": 1.0,
      "reasoning": "PersonaChat is mentioned as a specific dataset used in the field of persona dialogue, which is relevant to personalized text generation.",
      "processing_time": 61.530205965042114,
      "citing_paper_id": "247411350",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "Existing persona dialogue datasets such as PersonaChat (Zhang et al., 2018; Dinan et al., 2019) and models (Li et al., 2016a; Zhang et al., 2017; Qian et al., 2018) have greatly facilitated the chatbot with conﬁgurable and persistent personalities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to facilitate chatbots with configurable and persistent personalities, enhancing coherence and personalization in dialogue systems."
      },
      "confidence_score": 1.0,
      "reasoning": "PersonaChat is mentioned as a specific dataset used in the field of persona dialogue, which is relevant to personalized text generation.",
      "processing_time": 61.530205965042114,
      "citing_paper_id": "247411350",
      "cited_paper_id": 51608471
    },
    {
      "context_text": ", 2019) network for sentence representation, and another CNN model (Kim, 2014)",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (CNN) but does not refer to any specific datasets. No verifiable resources are identified.",
      "processing_time": 54.72980833053589,
      "citing_paper_id": "247411350",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "In Qian et al. (2018), an explicit persona model is proposed to generate consistent responses for given persona information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating consistent responses using persona information.",
      "processing_time": 54.37914967536926,
      "citing_paper_id": "247411350",
      "cited_paper_id": 51608471
    },
    {
      "context_text": ", 2020) Text 7 355,000 English Yes PCR (Mazaré et al., 2018) Text 7 700,000,000 English Yes MSC (Xu et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'PCR' and 'MSC', which could be datasets, but without additional context or disambiguation from the title, it is unclear if they are datasets or other types of resources. The numbers and languages suggest they might be datasets, but the names are too ambiguous.",
      "processing_time": 61.9758415222168,
      "citing_paper_id": "247411350",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "Persona is crucial for open-domain dialogue systems to establish long-term intimacy with users (Huang et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the importance of persona in dialogue systems.",
      "processing_time": 53.813892126083374,
      "citing_paper_id": "247411350",
      "cited_paper_id": 153312680
    },
    {
      "context_text": "Persona Dialogue: As described in Huang et al. (2020), there is much work related to persona dialogue.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general work related to persona dialogue.",
      "processing_time": 53.822259187698364,
      "citing_paper_id": "247411350",
      "cited_paper_id": 153312680
    },
    {
      "context_text": "Specifically, the ERNIE-CNN network employs a pre-trained ERNIE2 (Sun et al., 2019) network for sentence representation, and another CNN model (Kim, 2014)",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ERNIE2 and a CNN model but does not refer to any specific datasets. These are models, not datasets.",
      "processing_time": 55.19265341758728,
      "citing_paper_id": "247411350",
      "cited_paper_id": 198968327
    },
    {
      "context_text": "Speciﬁcally, the ERNIE-CNN network employs a pre-trained ERNIE 2 (Sun et al., 2019) for classiﬁcation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ERNIE 2, but it is used as a pre-trained model, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 55.6711699962616,
      "citing_paper_id": "247411350",
      "cited_paper_id": 198968327
    },
    {
      "context_text": "Previous long-term persona dialogue systems (Kim et al., 2014; Bang et al., 2015) are mainly rule-based systems without large-scale pre-training models, in which researchers proposed various episodic mem-a ory architectures to extract, store and manage relevant facts in prior interactions for use…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only rule-based systems and memory architectures. No verifiable resources are identified.",
      "processing_time": 54.760411977767944,
      "citing_paper_id": "247411350",
      "cited_paper_id": 206422013
    },
    {
      "context_text": "3 Human Evaluation Self-chat has been widely used in the evaluation of dialogue systems (Li et al., 2016b; Roller et al., 2021; Bao et al., 2020), where the model plays the roles of both parties in the dialogue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (self-chat) used in evaluating dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.409483909606934,
      "citing_paper_id": "247411350",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "3 Human Evaluation Self-chat has been widely used in the evaluation of dialogue systems (Li et al., 2016b; Roller et al., 2021; Bao et al., 2020), where the model plays the roles of both parties in the dialogue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (self-chat) used in evaluating dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.409483909606934,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "With-out this ability, the current state-of-the-art models, such as Meena (Adiwardana et al., 2020), Blender (Roller et al., 2021), and PLATO (Bao et al., 2020), tend to talk to people like strangers in long-term conversations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The citation intent is to discuss the limitations of current chatbot models in maintaining long-term conversations.",
      "processing_time": 55.58756160736084,
      "citing_paper_id": "247411350",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "With-out this ability, the current state-of-the-art models, such as Meena (Adiwardana et al., 2020), Blender (Roller et al., 2021), and PLATO (Bao et al., 2020), tend to talk to people like strangers in long-term conversations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models. The citation intent is to discuss the limitations of current chatbot models in maintaining long-term conversations.",
      "processing_time": 55.58756160736084,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "Self-chat has been widely used in the evaluation of dialogue systems (Li et al., 2016b; Roller et al., 2021; Bao et al., 2020), where the model plays the roles of both parties in the dialogue.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the method of self-chat for evaluating dialogue systems.",
      "processing_time": 53.72466325759888,
      "citing_paper_id": "247411350",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Self-chat has been widely used in the evaluation of dialogue systems (Li et al., 2016b; Roller et al., 2021; Bao et al., 2020), where the model plays the roles of both parties in the dialogue.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the method of self-chat for evaluating dialogue systems.",
      "processing_time": 53.72466325759888,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": ", 2020), Blender (Roller et al., 2021), and PLATO (Bao et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 54.84727764129639,
      "citing_paper_id": "247411350",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Based on our collected dataset, we carefully design a novel PLATO-LTM framework for the longterm persona dialogue setting by adding a plug-andplay long-term memory (LTM) to the state-of-theart open-domain dialogue model (Bao et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our collected dataset' which is a generic reference and does not provide a specific, identifiable dataset name.",
      "processing_time": 54.66854190826416,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "Generation Model For the Generation model, We follow PLATO-2 (Bao et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PLATO-2 but does not refer to it as a dataset. It is cited as a method or model for building a chatbot.",
      "processing_time": 55.57745909690857,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "As in Bao et al. (2020), we do not impose any restrictions on the chats except for specifying session openings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to chat sessions.",
      "processing_time": 52.90215873718262,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "Based on our collected dataset, we carefully de-sign a novel PLATO-LTM framework for the long-term persona dialogue setting by adding a plug-and-play long-term memory (LTM) to the state-of-the-art open-domain dialogue model (Bao et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'our collected dataset' which is too generic and does not meet the criteria for a specific, verifiable dataset.",
      "processing_time": 54.94276428222656,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "We trained our model on the basis of the PLATO2 (Bao et al., 2020) architecture which adopts the generic transformer language model (Vaswani et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions PLATO2 but it is described as an architecture, not a dataset. No other datasets are mentioned.",
      "processing_time": 54.45198917388916,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "• PLATO-2 (Bao et al., 2020): The SOTA opendomain dialogue model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PLATO-2 as a state-of-the-art open-domain dialogue model but does not refer to it as a dataset. It is clearly a model or method.",
      "processing_time": 56.36919665336609,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "We trained our model on the basis of the PLATO-2 (Bao et al., 2020) architecture which adopts the generic transformer language model (Vaswani et al., 2017) and leverages a stack of masked multi-head self-attention layers to train on massive dialogue data 3 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'massive dialogue data' but does not specify a named dataset. The citation is focused on the model architecture and training methodology rather than a specific dataset.",
      "processing_time": 55.98699474334717,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": "• PLATO-2 (Bao et al., 2020): The SOTA open-domain dialogue model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions PLATO-2 as a state-of-the-art open-domain dialogue model but does not refer to it as a dataset. It is clearly a model or method.",
      "processing_time": 56.184117555618286,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": ", 2021), and PLATO (Bao et al., 2020), tend to talk to people like strangers in long-term conversations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The citation is used to describe the behavior of chatbots in long-term conversations.",
      "processing_time": 55.362013816833496,
      "citing_paper_id": "247411350",
      "cited_paper_id": 220265679
    },
    {
      "context_text": ", 2018) Text 7 700,000,000 English Yes MSC (Xu et al., 2021) Text 3 5,001 English Yes",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSC"
      ],
      "dataset_descriptions": {
        "MSC": "Used to train and evaluate models for multi-domain sentiment analysis, focusing on the ability to generalize across different domains and contexts."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MSC (Xu et al., 2021)', which likely stands for 'Multi-Domain Sentiment Corpus'. However, the context does not provide enough information to confirm its usage or relevance to personalized text generation.",
      "processing_time": 64.80724501609802,
      "citing_paper_id": "247411350",
      "cited_paper_id": 236034497
    },
    {
      "context_text": "The stored documents in MSC will not be dynamically modi-ﬁed and will increase inﬁnitely as the conversation progresses.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a characteristic of stored documents in a system called MSC.",
      "processing_time": 54.3781852722168,
      "citing_paper_id": "247411350",
      "cited_paper_id": 236034497
    },
    {
      "context_text": "Recently, Xu et al. (2021) proposed MSC dataset as a multi-session extension of PersonaChat, and its sessions are additionally annotated with summaries of important personal points.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSC"
      ],
      "dataset_descriptions": {
        "MSC": "Used to extend PersonaChat with multi-session conversations, annotated with summaries of important personal points for personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MSC dataset, which is a specific, verifiable dataset used for multi-session conversations with personal point summaries.",
      "processing_time": 60.3543598651886,
      "citing_paper_id": "247411350",
      "cited_paper_id": 236034497
    },
    {
      "context_text": "Similar to the previous episodic memory architecture, Xu et al. (2021) summarize and recall previous conversations for future dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for summarizing and recalling previous conversations.",
      "processing_time": 54.07167673110962,
      "citing_paper_id": "247411350",
      "cited_paper_id": 236034497
    },
    {
      "context_text": ", 2020) Text 7 16,878 Multilingual Yes PEC (Zhong et al., 2020) Text 7 355,000 English Yes PCR (Mazaré et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PEC",
        "PCR"
      ],
      "dataset_descriptions": {
        "PEC": "Used to train empathetic conversational models, focusing on persona-based interactions in multilingual settings.",
        "PCR": "Used to train conversational models, focusing on persona-based interactions in English."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'PEC' and 'PCR' which appear to be dataset acronyms. The cited paper title suggests these are related to persona-based conversational models.",
      "processing_time": 65.21651792526245,
      "citing_paper_id": "247411350",
      "cited_paper_id": 271403894
    },
    {
      "context_text": "Early approaches [19, 20, 21, 22] employed GANs [23] and VQ-VAE [24], while more recent works have explored diffusion models to generate high-quality videos [25, 26, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 54.82690620422363,
      "citing_paper_id": "267035298",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Early approaches [19, 20, 21, 22] employed GANs [23] and VQ-VAE [24], while more recent works have explored diffusion models to generate high-quality videos [25, 26, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 54.82690620422363,
      "citing_paper_id": "267035298",
      "cited_paper_id": 20282961
    },
    {
      "context_text": "Early approaches [19, 20, 21, 22] employed GANs [23] and VQ-VAE [24], while more recent works have explored diffusion models to generate high-quality videos [25, 26, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 54.82690620422363,
      "citing_paper_id": "267035298",
      "cited_paper_id": 199465721
    },
    {
      "context_text": "Early approaches [19, 20, 21, 22] employed GANs [23] and VQ-VAE [24], while more recent works have explored diffusion models to generate high-quality videos [25, 26, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 54.82690620422363,
      "citing_paper_id": "267035298",
      "cited_paper_id": 249209614
    },
    {
      "context_text": "Early approaches [19, 20, 21, 22] employed GANs [23] and VQ-VAE [24], while more recent works have explored diffusion models to generate high-quality videos [25, 26, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 54.82690620422363,
      "citing_paper_id": "267035298",
      "cited_paper_id": 262823915
    },
    {
      "context_text": "The AdamW [53] optimizer is used with learning rate 5e-5, weight decay 1e-2, batch size 2, and training steps 500.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the optimizer used during training.",
      "processing_time": 52.7150399684906,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "We use AdamW [53] optimizer, with learning rate 4e-5, weight decay 1e-2, batch size 2, and training steps 500.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization parameters and methods.",
      "processing_time": 52.54385852813721,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "We use AdamW [53] optimizer, with learning rate 4e-4, weight decay 1e-2, batch size 2, and training steps 1000.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization parameters and methods.",
      "processing_time": 52.546056032180786,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "• DisenDiff + SVD-XT [14, 15]: DisenDiff is trained for 250 steps with AdamW [53] optimizer (learning rate 4e-5, weight decay 1e-2, batch size 2).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and training parameters.",
      "processing_time": 52.491684913635254,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "We use AdamW [53] optimizer with a batch size of 2.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an optimizer. There are no verifiable resources that meet the criteria.",
      "processing_time": 53.7753427028656,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "We train CustomVideo for 500 steps with AdamW [53] optimizer (batch size 2, learning rate 4e-5, and weight decay 1e-2).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a training setup and optimizer details.",
      "processing_time": 52.71557068824768,
      "citing_paper_id": "267035298",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "…4 metrics: 1) CLIP Textual Alignment computes the average cosine similarity between the generated frames and text prompt with CLIP [58] ViT-B/32 [59] image and text models; 2) CLIP Image Alignment calculates the average cosine similarity between the generated frames and subject images with CLIP…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and metrics. The context focuses on evaluation metrics using CLIP, which is a model, not a dataset.",
      "processing_time": 55.363712310791016,
      "citing_paper_id": "267035298",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "…with CLIP ViT-B/32 image model; 3) DINO Image Alignment measures the average visual similarity between generated frames and reference images with DINO [60] ViT-S/16 model; 4) Temporal Consistency [61] evaluates the average cosine similarity of all consecutive frame pairs in the generated videos.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and metrics. The context focuses on evaluation methods for video synthesis, which are not considered datasets.",
      "processing_time": 54.849199533462524,
      "citing_paper_id": "267035298",
      "cited_paper_id": 233444273
    },
    {
      "context_text": "…with CLIP ViT-B/32 image model; 3) DINO Image Alignment measures the average visual similarity between generated frames and reference images with DINO [60] ViT-S/16 model; 4) Temporal Consistency [61] evaluates the average cosine similarity of all consecutive frame pairs in the generated videos.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and metrics. The context focuses on evaluation methods for video synthesis, which are not considered datasets.",
      "processing_time": 54.849199533462524,
      "citing_paper_id": "267035298",
      "cited_paper_id": 256615582
    },
    {
      "context_text": "We adopt the follow setting for in our experiments, including the previous SOTA methods for comparison and our CustomVideo: • DreamBooth [45]: For efficient fine-tuning, we utilize LoRA [50] to adapt the U-Net under DreamBooth [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (LoRA and DreamBooth) rather than datasets.",
      "processing_time": 55.15478730201721,
      "citing_paper_id": "267035298",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "We adopt the follow setting for in our experiments, including the previous SOTA methods for comparison and our CustomVideo: • DreamBooth [45]: For efficient fine-tuning, we utilize LoRA [50] to adapt the U-Net under DreamBooth [45].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about methods (LoRA and DreamBooth) rather than datasets.",
      "processing_time": 55.15478730201721,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "A recent work, VideoDreamer [12], proposes multi-subject driven video customization through disen-mix finetuning strategy with LoRA [50].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LoRA) and a system (VideoDreamer).",
      "processing_time": 53.99409532546997,
      "citing_paper_id": "267035298",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "• VideoDreamer [12]: LoRA [50] is utilized for fine-tuning the U-Net and text encoder in VideoDreamer [12].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of LoRA for fine-tuning models. LoRA is a method, not a dataset.",
      "processing_time": 55.42319297790527,
      "citing_paper_id": "267035298",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "During inference, we perform 50 steps denoising with DPM-Solver [8] sampler and classifier-free guidance [55].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models.",
      "processing_time": 52.26680135726929,
      "citing_paper_id": "267035298",
      "cited_paper_id": 249145348
    },
    {
      "context_text": "Inspired by previous T2I personalization [45, 40], we conduct class-specific prior preservation to improve the diversity of generated videos and alleviate the issue of language drift.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous work on T2I personalization. No verifiable resources are identified.",
      "processing_time": 54.63114142417908,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Inspired by previous T2I personalization [45, 40], we conduct class-specific prior preservation to improve the diversity of generated videos and alleviate the issue of language drift.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous work on T2I personalization. No verifiable resources are identified.",
      "processing_time": 54.63114142417908,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Except for most relevant VideoDreamer [12], we also consider adapting previous SOTA image-based multi-subject driven methods to a video scenario for comparison, including DreamBooth [45] and CustomDiffusion [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to compare different methods, not to reference datasets.",
      "processing_time": 54.42116069793701,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Except for most relevant VideoDreamer [12], we also consider adapting previous SOTA image-based multi-subject driven methods to a video scenario for comparison, including DreamBooth [45] and CustomDiffusion [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is used to compare different methods, not to reference datasets.",
      "processing_time": 54.42116069793701,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Additionally, DreamBooth [45] fine-tunes the weights of the diffusion model to improve fidelity in image generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DreamBooth) for fine-tuning a diffusion model. No verifiable datasets are referenced.",
      "processing_time": 55.855342864990234,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "These subjects are adapted from DreamBooth [45], CustomDiffusion [40], and Mix-of-Show [52], in which they cover a wide range of 13 categories.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DreamBooth', 'CustomDiffusion', and 'Mix-of-Show' but does not refer to them as datasets. They are likely methods or models used for fine-tuning text-to-image diffusion models.",
      "processing_time": 59.01700210571289,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "These subjects are adapted from DreamBooth [45], CustomDiffusion [40], and Mix-of-Show [52], in which they cover a wide range of 13 categories.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DreamBooth', 'CustomDiffusion', and 'Mix-of-Show' but does not refer to them as datasets. They are likely methods or models used for fine-tuning text-to-image diffusion models.",
      "processing_time": 59.01700210571289,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "The images are adopted from DreamBooth [45], CustomDiffusion [40] and Mix-of-Show [52].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DreamBooth', 'CustomDiffusion', and 'Mix-of-Show' but does not refer to them as datasets. They are likely methods or tools used for generating images.",
      "processing_time": 57.651243686676025,
      "citing_paper_id": "267035298",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "The images are adopted from DreamBooth [45], CustomDiffusion [40] and Mix-of-Show [52].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DreamBooth', 'CustomDiffusion', and 'Mix-of-Show' but does not refer to them as datasets. They are likely methods or tools used for generating images.",
      "processing_time": 57.651243686676025,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "There has been a growing interest in customizing pre-trained image and video diffusion models for personalized generation [39, 40, 41, 42, 43, 44, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in customizing pre-trained models for personalized generation. No verifiable resources are identified.",
      "processing_time": 55.594568967819214,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "There has been a growing interest in customizing pre-trained image and video diffusion models for personalized generation [39, 40, 41, 42, 43, 44, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in customizing pre-trained models for personalized generation. No verifiable resources are identified.",
      "processing_time": 55.594568967819214,
      "citing_paper_id": "267035298",
      "cited_paper_id": 257427549
    },
    {
      "context_text": "There has been a growing interest in customizing pre-trained image and video diffusion models for personalized generation [39, 40, 41, 42, 43, 44, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in customizing pre-trained models for personalized generation. No verifiable resources are identified.",
      "processing_time": 55.594568967819214,
      "citing_paper_id": "267035298",
      "cited_paper_id": 260091569
    },
    {
      "context_text": "There has been a growing interest in customizing pre-trained image and video diffusion models for personalized generation [39, 40, 41, 42, 43, 44, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in customizing pre-trained models for personalized generation. No verifiable resources are identified.",
      "processing_time": 55.594568967819214,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "• CustomDiffusion [40]: The weights of query and value in all of the cross-attention layers of U-Net are fine-tuned in CustomDiffusion [40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called CustomDiffusion. The context focuses on the fine-tuning of weights in a neural network model.",
      "processing_time": 56.1462984085083,
      "citing_paper_id": "267035298",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Several works [46, 14, 47, 48] have explored personalized image diffusion with multiple subjects, focusing on parameter-efficient finetuning and text embedding learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.94965648651123,
      "citing_paper_id": "267035298",
      "cited_paper_id": 258740710
    },
    {
      "context_text": "Several works [46, 14, 47, 48] have explored personalized image diffusion with multiple subjects, focusing on parameter-efficient finetuning and text embedding learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.94965648651123,
      "citing_paper_id": "267035298",
      "cited_paper_id": 269005013
    },
    {
      "context_text": "To enhance controllability, VideoComposer [33] incorporates additional guidance signals, such as depth maps, to produce desired videos alongside text inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VideoComposer) and its capabilities. No verifiable resources are identified.",
      "processing_time": 54.66782832145691,
      "citing_paper_id": "267035298",
      "cited_paper_id": 259075720
    },
    {
      "context_text": "Text-to-video generation has made significant advancements in recent years [17, 18, 1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in text-to-video generation. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.926568031311035,
      "citing_paper_id": "267035298",
      "cited_paper_id": 260680830
    },
    {
      "context_text": "Text-to-video generation has made significant advancements in recent years [17, 18, 1].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only advancements in text-to-video generation. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.926568031311035,
      "citing_paper_id": "267035298",
      "cited_paper_id": 264803867
    },
    {
      "context_text": "Video diffusion models (VDMs) [1, 18, 17] generate videos by gradually denoising a randomly sampled Gaussian noise ϵ , following an iterative denoising process that resembles a reverse procedure of a fixed-length Markov Chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.93874168395996,
      "citing_paper_id": "267035298",
      "cited_paper_id": 260680830
    },
    {
      "context_text": "Video diffusion models (VDMs) [1, 18, 17] generate videos by gradually denoising a randomly sampled Gaussian noise ϵ , following an iterative denoising process that resembles a reverse procedure of a fixed-length Markov Chain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.93874168395996,
      "citing_paper_id": "267035298",
      "cited_paper_id": 264803867
    },
    {
      "context_text": "GEST [35] models the representation between the text and video via a graph of events in the spatial and temporal space, in which the video is generated following a timeline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (GEST) for representing text and video interactions.",
      "processing_time": 53.9826934337616,
      "citing_paper_id": "267035298",
      "cited_paper_id": 262044764
    },
    {
      "context_text": "VideoDirectorGPT [34] aims to control the video generation with the guidance from different scenes and specific layouts generated by GPT4 in the temporal axis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VideoDirectorGPT) and a model (GPT4). No verifiable datasets are referenced.",
      "processing_time": 56.31425929069519,
      "citing_paper_id": "267035298",
      "cited_paper_id": 262825203
    },
    {
      "context_text": "Also, we compare our method with single-subject driven video personalization method DreamVideo [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DreamVideo' but does not refer to it as a dataset. It is a method or tool for video personalization.",
      "processing_time": 54.675715923309326,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "Although existing methods [9, 10, 11] have been proposed to generate videos from a single object, tackling multiple objects still remains a difficult scenario.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.88477945327759,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "Following previous works [11, 12], we quantitatively evaluate our CustomVideo with the following 4 metrics: 1) CLIP Textual Alignment computes the average cosine similarity between the generated frames and text prompt with CLIP [58] ViT-B/32 [59] image and text models; 2) CLIP Image Alignment…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context focuses on evaluation metrics and does not reference any data sources.",
      "processing_time": 55.33095407485962,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "…[10], and ID-Animator [49], which use reference images to personalize the video diffusion model while preserving subject identity, and DreamVideo [11], which decouples the learning process for subject and motion customization, these methods are designed for single objects and cannot handle…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the capabilities and limitations of various video generation models.",
      "processing_time": 55.316596031188965,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "• DreamVideo [11]: We train the learnable textual token and identity adapter for 3000 and 800 steps, respectively, with learning rate 1e-4 and 1e-5.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only training parameters and steps. The cited paper title does not provide additional information about datasets.",
      "processing_time": 54.66077256202698,
      "citing_paper_id": "267035298",
      "cited_paper_id": 266053833
    },
    {
      "context_text": "All of the training process is under fp16 mixed precision with accelerate package [62].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a training process detail.",
      "processing_time": 52.29342341423035,
      "citing_paper_id": "267035298",
      "cited_paper_id": null
    },
    {
      "context_text": "BT-family produces textual reports for parents that their infant is under intensive care, using affective natural language generation [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating affective natural language reports.",
      "processing_time": 52.61386275291443,
      "citing_paper_id": "207821484",
      "cited_paper_id": 947719
    },
    {
      "context_text": "Data-to-text generation is a sub-field of Natural Language Processing (NLP) and Natural Language Generation (NLG) in particular, that copes with the task of automatically generating text from nonlinguistic input, such as time-series data [10].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to data-to-text generation. No verifiable resources are identified.",
      "processing_time": 54.37674403190613,
      "citing_paper_id": "207821484",
      "cited_paper_id": 3116153
    },
    {
      "context_text": "Of these, a series of works (Chunseong Park et al., 2017; Denton et al., 2015; Feng et al., 2017; Del Chiaro et al., 2020; Jia et al., 2020; Long et al., 2020; Shuster et al., 2019) focus on personalizing image captions according to a user writing style.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a series of works focusing on personalizing image captions according to user writing style.",
      "processing_time": 54.67250370979309,
      "citing_paper_id": "247939764",
      "cited_paper_id": 88898
    },
    {
      "context_text": "Of these, a series of works (Chunseong Park et al., 2017; Denton et al., 2015; Feng et al., 2017; Del Chiaro et al., 2020; Jia et al., 2020; Long et al., 2020; Shuster et al., 2019) focus on personalizing image captions according to a user writing style.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a series of works focusing on personalizing image captions according to user writing style.",
      "processing_time": 54.67250370979309,
      "citing_paper_id": "247939764",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Of these, a series of works (Chunseong Park et al., 2017; Denton et al., 2015; Feng et al., 2017; Del Chiaro et al., 2020; Jia et al., 2020; Long et al., 2020; Shuster et al., 2019) focus on personalizing image captions according to a user writing style.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a series of works focusing on personalizing image captions according to user writing style.",
      "processing_time": 54.67250370979309,
      "citing_paper_id": "247939764",
      "cited_paper_id": 207227390
    },
    {
      "context_text": "Of these, a series of works (Chunseong Park et al., 2017; Denton et al., 2015; Feng et al., 2017; Del Chiaro et al., 2020; Jia et al., 2020; Long et al., 2020; Shuster et al., 2019) focus on personalizing image captions according to a user writing style.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a series of works focusing on personalizing image captions according to user writing style.",
      "processing_time": 54.67250370979309,
      "citing_paper_id": "247939764",
      "cited_paper_id": 221191625
    },
    {
      "context_text": "When compared to zero and few-shot learning (FSL) models based on meta-learning (Finn et al., 2017; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017; Chen et al., 2021; Lampert et al., 2009; Akata et al., 2016; Xian et al., 2017; Atzmon & Chechik, 2018; 2019; Paz-Argaman et al., 2020) or to few-shot incremental learning (Tao et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. No verifiable resources are identified.",
      "processing_time": 53.63123083114624,
      "citing_paper_id": "247939764",
      "cited_paper_id": 523388
    },
    {
      "context_text": "When compared to zero and few-shot learning (FSL) models based on meta-learning (Finn et al., 2017; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017; Chen et al., 2021; Lampert et al., 2009; Akata et al., 2016; Xian et al., 2017; Atzmon & Chechik, 2018; 2019; Paz-Argaman et al., 2020) or to few-shot incremental learning (Tao et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. No verifiable resources are identified.",
      "processing_time": 53.63123083114624,
      "citing_paper_id": "247939764",
      "cited_paper_id": 739861
    },
    {
      "context_text": "When compared to zero and few-shot learning (FSL) models based on meta-learning (Finn et al., 2017; Vinyals et al., 2016; Sung et al., 2018; Snell et al., 2017; Chen et al., 2021; Lampert et al., 2009; Akata et al., 2016; Xian et al., 2017; Atzmon & Chechik, 2018; 2019; Paz-Argaman et al., 2020) or to few-shot incremental learning (Tao et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. No verifiable resources are identified.",
      "processing_time": 53.63123083114624,
      "citing_paper_id": "247939764",
      "cited_paper_id": 4412459
    },
    {
      "context_text": "Alternatively, (Hendricks et al., 2016; Venugopalan et al., 2017; Wu et al., 2018; Zheng et al., 2019; Lu et al., 2018; Demirel et al., 2019) extend image captions with novel concepts using “slot filling”, which are placeholders for nouns that are filled using object detector predictions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method ('slot filling') used across multiple studies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.27954435348511,
      "citing_paper_id": "247939764",
      "cited_paper_id": 4406645
    },
    {
      "context_text": "Alternatively, (Hendricks et al., 2016; Venugopalan et al., 2017; Wu et al., 2018; Zheng et al., 2019; Lu et al., 2018; Demirel et al., 2019) extend image captions with novel concepts using “slot filling”, which are placeholders for nouns that are filled using object detector predictions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method ('slot filling') used across multiple studies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.27954435348511,
      "citing_paper_id": "247939764",
      "cited_paper_id": 8457705
    },
    {
      "context_text": "Alternatively, (Hendricks et al., 2016; Venugopalan et al., 2017; Wu et al., 2018; Zheng et al., 2019; Lu et al., 2018; Demirel et al., 2019) extend image captions with novel concepts using “slot filling”, which are placeholders for nouns that are filled using object detector predictions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method ('slot filling') used across multiple studies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.27954435348511,
      "citing_paper_id": "247939764",
      "cited_paper_id": 53720830
    },
    {
      "context_text": "Alternatively, (Hendricks et al., 2016; Venugopalan et al., 2017; Wu et al., 2018; Zheng et al., 2019; Lu et al., 2018; Demirel et al., 2019) extend image captions with novel concepts using “slot filling”, which are placeholders for nouns that are filled using object detector predictions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method ('slot filling') used across multiple studies. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.27954435348511,
      "citing_paper_id": "247939764",
      "cited_paper_id": 199064261
    },
    {
      "context_text": "Our model differs from zero and few-shot learning (FSL) based on metalearning [21, 63, 59, 57, 11, 35, 1, 68, 4, 5, 49] or incremental learning [60, 18, 51, 12, 31, 66] in three aspects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 55.792444944381714,
      "citing_paper_id": "247939764",
      "cited_paper_id": 6719686
    },
    {
      "context_text": "Our model differs from zero and few-shot learning (FSL) based on metalearning [21, 63, 59, 57, 11, 35, 1, 68, 4, 5, 49] or incremental learning [60, 18, 51, 12, 31, 66] in three aspects.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works and methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 55.792444944381714,
      "citing_paper_id": "247939764",
      "cited_paper_id": 237213737
    },
    {
      "context_text": "People frequently learn novel concepts from a few examples (Malaviya et al., 2022; Carey & Bartlett, 1978; Lake & Piantadosi, 2020; Markman, 1990; Markman et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies about learning novel concepts. No verifiable resources are identified.",
      "processing_time": 54.43003177642822,
      "citing_paper_id": "247939764",
      "cited_paper_id": 12166077
    },
    {
      "context_text": "People frequently learn novel concepts from a few examples (Malaviya et al., 2022; Carey & Bartlett, 1978; Lake & Piantadosi, 2020; Markman, 1990; Markman et al., 2003).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies about learning novel concepts. No verifiable resources are identified.",
      "processing_time": 54.43003177642822,
      "citing_paper_id": "247939764",
      "cited_paper_id": 42489580
    },
    {
      "context_text": "When people learn novel concepts from a few examples [50, 10, 40, 51, 52], they can seamlessly employ them in their semantic mental state and reason jointly both over the personalized concepts and over a large body of prior knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of learning from a few examples. There are no clear identifiers for datasets in the text.",
      "processing_time": 55.37173390388489,
      "citing_paper_id": "247939764",
      "cited_paper_id": 50145091
    },
    {
      "context_text": "In inversion, models typically aim to identify codes in the latent spaces of pre-trained generators, which represent specific images or identities [1, 61].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to latent spaces and pre-trained generators. No verifiable resource names are present.",
      "processing_time": 54.20463681221008,
      "citing_paper_id": "247939764",
      "cited_paper_id": 102350964
    },
    {
      "context_text": "Another line of work (Tsimpoukelli et al., 2021; Hill et al., 2020), studies “fast-learning” combined with “slow-learned” concepts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a line of research. No clear identifiers for datasets are present.",
      "processing_time": 53.690473794937134,
      "citing_paper_id": "247939764",
      "cited_paper_id": 221516648
    },
    {
      "context_text": "In brief, Zabari & Hoshen (2021) create a set of query-driven relevance maps for an image, coupled with transformer interpretability methods (Chefer et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on creating relevance maps and using transformer interpretability methods.",
      "processing_time": 55.16054630279541,
      "citing_paper_id": "247939764",
      "cited_paper_id": 229297908
    },
    {
      "context_text": "This is equivalent to the FSL baseline in [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a baseline method in a meta-learning context.",
      "processing_time": 53.37687277793884,
      "citing_paper_id": "247939764",
      "cited_paper_id": 237213737
    },
    {
      "context_text": "These models provide a multimodal vision-language representation, and are used in a multitude of downstream tasks, from image captioning [54] and video retrieval [23], through image generation [26, 56] and segmentation [84, 42], to robotic manipulation [65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various applications of multimodal models. No dataset names are present in the citation span.",
      "processing_time": 54.833943128585815,
      "citing_paper_id": "247939764",
      "cited_paper_id": 237396838
    },
    {
      "context_text": "…in a visual specific context [13, 5]; in human-robot interaction, a worker may show a specific tool to a robotic arm, and instruct how to use it [65, 77, 48]; in video applications, an operator may search for a specific known item in the context of other items or people doing activities that…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses various contexts where personalized text generation could be applied but does not reference any named datasets.",
      "processing_time": 55.5894672870636,
      "citing_paper_id": "247939764",
      "cited_paper_id": 237396838
    },
    {
      "context_text": "COLLIE [67]: Learn an adapter module over the output of CLIP text encoder, with an additional scaler function Scaler h T ( S ) ∈ [0 , 1] that softly applies the adapter layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CoLLIE) and a model (CLIP).",
      "processing_time": 54.35354566574097,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "(1) Text (CLIP) , using both the rich and concept only queries, (2) AvgIM , (3) IM&Text and (4) COLLIE .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. The cited paper title 'CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings' confirms that CoLLIE is a method, not a dataset.",
      "processing_time": 59.441588163375854,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Here we describe results showing that COLLIE is even sensitive to much simpler queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model called COLLIE. No verifiable resources are identified.",
      "processing_time": 53.477800369262695,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "COLLIE’s segmentation performance in the two scenarios is shown in Fig.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a figure showing performance. No verifiable resources are identified.",
      "processing_time": 53.249897956848145,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Most works [56, 73, 23, 43] are based on learning a residual",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to learning a residual. No verifiable resources are identified.",
      "processing_time": 54.15803050994873,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "6.1 we demonstrated that COLLIE performance degrades when using rich textual queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CoLLIE) and its performance degradation with rich textual queries.",
      "processing_time": 54.39520287513733,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "COLLIE:Text : COLLIE, when the text query uses the concept type for [CONCEPT], rather than the trained concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model called 'CoLLIE'. There are no clear identifiers for datasets.",
      "processing_time": 54.92461085319519,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "When the text query includes only the [CONCEPT] tag, as in COLLIE’s training procedure, COLLIE achieves a 13.4% average MRR score on DeepFashion2 retrieval test set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DeepFashion2"
      ],
      "dataset_descriptions": {
        "DeepFashion2": "Used to evaluate COLLIE's performance in retrieving fashion images using text queries, focusing on the effectiveness of language grounding in image retrieval tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DeepFashion2 retrieval test set' which is a specific dataset used for evaluating retrieval performance. The dataset is used to measure the effectiveness of COLLIE in retrieving fashion images using text queries.",
      "processing_time": 63.529011487960815,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Previous efforts [56, 73, 23, 76] focused on learning a transformation module on top of CLIP’s output space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (CLIP). There are no clear identifiers for datasets in the context.",
      "processing_time": 55.057642459869385,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "COLLIE is closest to our method, because it may preserve some capabilities of the underlying pretrained model, when Scaler ( · ) = 0.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called COLLIE. No verifiable resources are identified.",
      "processing_time": 53.39899706840515,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "We observe that, in contrast to our own approach and the baseline CLIP, COLLIE’s performance on the segmentation task does not appear to be sensitive to the level of detail in the query.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between models (CLIP and COLLIE) on a segmentation task.",
      "processing_time": 54.37126159667969,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Several studies extend CLIP by learning an “Adapter” module on top of the CLIP representation [23, 56, 73, 43, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers extending CLIP. No verifiable resources are identified.",
      "processing_time": 54.05689716339111,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Adapter : As in COLLIE, but replace the scaler with a constant value of 1, making the “Adapter” layer always active.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model called 'Adapter'. The citation is about modifying a component of a model, not using a dataset.",
      "processing_time": 56.03197717666626,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "To compare with COLLIE, we generate the embeddings using their adapter setup.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (adapter setup) from the cited paper CoLLIE. The citation is used to reference a method, not a dataset.",
      "processing_time": 57.217596769332886,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "Last, we further investigated whether COLLIE demonstrates a similar sensitivity to rich queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model or method called CoLLIE. No verifiable datasets are referenced.",
      "processing_time": 54.782591581344604,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "In fact, we find that Adapter and COLLIE are even sensitive to the prompt prefix of the query.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods (Adapter and COLLIE). The citation is about the sensitivity of these models to the prompt prefix, not about a dataset.",
      "processing_time": 57.21096920967102,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "COLLIE [56]: Learn an adapter module over the output of CLIP text encoder, with an additional scaler function Scaler ( hT (S) ) ∈ [0, 1] that softly applies the adapter layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CoLLIE) and a model (CLIP).",
      "processing_time": 54.455448150634766,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "(2) Adapter baselines (Adapter & COLLIE) improve over vanilla CLIP when only the concept is queried.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods (Adapter, COLLIE, CLIP).",
      "processing_time": 54.03070545196533,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": "We compare our results with the recent baseline COLLIE.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'COLLIE' but does not indicate it is a dataset. It appears to be a method or model used as a baseline for comparison.",
      "processing_time": 56.103039503097534,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244117869
    },
    {
      "context_text": ", 2020) or to few-shot incremental learning (Tao et al., 2020; Fan et al., 2021; Ren et al., 2020; Cheraghian et al., 2021; Khan et al., 2021; Wu et al., 2021), our model differs in three key aspects: First, we impose stronger generalization requirements.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works. There is no indication of a reusable resource being discussed.",
      "processing_time": 54.69142532348633,
      "citing_paper_id": "247939764",
      "cited_paper_id": 244302919
    },
    {
      "context_text": ", 2021) and segmentation (Zabari & Hoshen, 2021; Li et al., 2022), to robotic manipulation (Shridhar et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers on segmentation and robotic manipulation.",
      "processing_time": 53.03214359283447,
      "citing_paper_id": "247939764",
      "cited_paper_id": 245836975
    },
    {
      "context_text": "When people learn novel concepts from a few examples [44, 7, 34, 45, 46], they can seamlessly employ them in their semantic mental state and reason jointly both over the personalized concepts and over a large body of prior knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of learning from a few examples. There are no clear identifiers for datasets in the text.",
      "processing_time": 55.6783173084259,
      "citing_paper_id": "247939764",
      "cited_paper_id": 246705829
    },
    {
      "context_text": "As in [37], we observed that the encoding distribution of text and images does not overlap in CLIP space.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about the encoding distribution of text and images in CLIP space.",
      "processing_time": 54.92131495475769,
      "citing_paper_id": "247939764",
      "cited_paper_id": 247244904
    },
    {
      "context_text": "Taking another approach for leveraging pretrained models, recent papers study the effects of fine-tuning (Wortsman et al., 2021; Kumar et al., 2022; Hewitt et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers studying the effects of fine-tuning pretrained models.",
      "processing_time": 54.17026448249817,
      "citing_paper_id": "247939764",
      "cited_paper_id": 248941551
    },
    {
      "context_text": ", 2021), through image generation (Gal et al., 2021; Patashnik et al., 2021) and segmentation (Zabari & Hoshen, 2021; Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing various techniques in image generation and segmentation.",
      "processing_time": 53.979801416397095,
      "citing_paper_id": "247939764",
      "cited_paper_id": null
    },
    {
      "context_text": "In image retrieval, a user may tag a few of their images and wish to retrieve other photos of that concept in a visual specific context [13, 5]; in human-robot interaction, a worker may show a specific tool to a robotic arm, and instruct how to use it [65, 77, 48]; in video applications, an…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes potential applications of personalized text generation in various contexts.",
      "processing_time": 54.436572551727295,
      "citing_paper_id": "247939764",
      "cited_paper_id": null
    },
    {
      "context_text": "These can then be used in downstream tasks such as editing [63, 2] or super resolution [53].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks. There are no verifiable resources or specific dataset names mentioned.",
      "processing_time": 54.43407845497131,
      "citing_paper_id": "247939764",
      "cited_paper_id": null
    },
    {
      "context_text": "Conversation modeling is one such domain where end-to-end trained systems have matched or surpassed traditional dialog systems in both open-ended [Dodge et al., 2015] and goal-oriented applications [Bordes and Weston, 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing end-to-end dialog systems. No verifiable resources are identified.",
      "processing_time": 53.92109131813049,
      "citing_paper_id": "29473470",
      "cited_paper_id": 2239496
    },
    {
      "context_text": "The closest work to ours is by Li et al. (2016b) who encoded speaker personas into SEQ2SEQ dialog models (Vinyals and Le, 2015; Li et al., 2016a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the approach of encoding speaker personas into dialog models.",
      "processing_time": 55.004292726516724,
      "citing_paper_id": "29473470",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "…mechanisms for neural networks architectures have led to remarkable progress in machine translation (Bahdanau et al., 2014; Johnson et al., 2016), question answering (Sukhbaatar et al. 2015, Graves et al., 2016) and other language understanding tasks which require an element of logical reasoning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of neural network mechanisms. No verifiable resources are identified.",
      "processing_time": 54.81528902053833,
      "citing_paper_id": "29473470",
      "cited_paper_id": 195345948
    },
    {
      "context_text": "nses yare sampled per example. 5 4.3 Memory Networks Memory Networks [Weston et al., 2014] are a recent class of models that have proven successful for a variety of tasks such as question answering [Sukhbaatar et al., 2015] and conducting dialog [Dodge et al., 2015]. For dialogs, the entire conversation history is stored in the memory component of the model. It can be iteratively read from to perform reasoning and sele",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing Memory Networks and their applications.",
      "processing_time": 54.392117738723755,
      "citing_paper_id": "29473470",
      "cited_paper_id": 195345948
    },
    {
      "context_text": "4.3 Memory Networks Memory Networks (Weston et al., 2015a; Sukhbaatar et al., 2015) are a recent class of models that have proven successful for a variety of language understanding tasks such as question answering (Weston et al., 2015b), language modelling (Sukhbataar et al., 2015) and conducting…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing Memory Networks and their applications.",
      "processing_time": 54.40021586418152,
      "citing_paper_id": "29473470",
      "cited_paper_id": 195345948
    },
    {
      "context_text": "…Drawing inspiration from adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on evaluating sentence completions using adversarial training methods and non-parametric tests.",
      "processing_time": 56.165501832962036,
      "citing_paper_id": "748227",
      "cited_paper_id": 536962
    },
    {
      "context_text": "In supervised settings, rnnlm decoders conditioned on task-speciﬁc features are the state of the art in tasks like machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) and image captioning (Vinyals et al., 2015; Mao et al., 2015; Donahue et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and models. The context focuses on the use of RNNLM decoders in various NLP tasks, but does not specify any datasets.",
      "processing_time": 57.16404843330383,
      "citing_paper_id": "748227",
      "cited_paper_id": 1169492
    },
    {
      "context_text": "Adversarial evaluation Drawing inspiration from adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from the true sentence endings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on evaluating sentence completions using adversarial methods and two-sample tests.",
      "processing_time": 55.819416761398315,
      "citing_paper_id": "748227",
      "cited_paper_id": 1282515
    },
    {
      "context_text": "Adversarial evaluation Drawing inspiration from adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from the true sentence endings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on evaluating sentence completions using adversarial methods and two-sample tests.",
      "processing_time": 55.819416761398315,
      "citing_paper_id": "748227",
      "cited_paper_id": 10742222
    },
    {
      "context_text": "…from adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from the true sentence…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on evaluating imputed sentence completions using adversarial training methods and non-parametric tests.",
      "processing_time": 56.79784941673279,
      "citing_paper_id": "748227",
      "cited_paper_id": 1282515
    },
    {
      "context_text": "We adapt the variational autoencoder to text by using single-layer lstm rnns (Hochreiter and Schmidhuber, 1997) for both the encoder and the decoder, essentially forming a sequence autoencoder with the Gaussian prior acting as a regularizer on the hidden code.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of LSTM RNNs in a variational autoencoder framework.",
      "processing_time": 54.57184457778931,
      "citing_paper_id": "748227",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Drawing inspiration from recent successes in modeling images (Gregor et al., 2015), handwriting, and natural speech (Chung et al., 2015), our model circum-vents these diﬃculties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture and inference techniques.",
      "processing_time": 54.43356943130493,
      "citing_paper_id": "748227",
      "cited_paper_id": 1930231
    },
    {
      "context_text": "Drawing inspiration from recent successes in modeling images (Gregor et al., 2015), handwriting, and natural speech (Chung et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to modeling images, handwriting, and natural speech. No verifiable resources are identified.",
      "processing_time": 55.07344460487366,
      "citing_paper_id": "748227",
      "cited_paper_id": 1930231
    },
    {
      "context_text": "We also experimented with more sophisticated recognition models q(~z|x), including a multistep sampling model styled after draw (Gregor et al., 2015), and a posterior approximation using normalizing flows (Rezende and Mohamed, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are referenced for their models, not for datasets.",
      "processing_time": 54.919246196746826,
      "citing_paper_id": "748227",
      "cited_paper_id": 1930231
    },
    {
      "context_text": "We also experimented with more sophisticated recognition models q ( (cid:126)z | x ), including a multistep sampling model styled after draw (Gregor et al., 2015), and a posterior approximation using normalizing ﬂows (Rezende and Mohamed, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not refer to any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 54.548128843307495,
      "citing_paper_id": "748227",
      "cited_paper_id": 1930231
    },
    {
      "context_text": "This technique is a variant of word dropout (Iyyer et al., 2015; Kumar et al., 2016), applied not to a feature extractor but to a decoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique called word dropout. No verifiable resources are identified.",
      "processing_time": 54.135828733444214,
      "citing_paper_id": "748227",
      "cited_paper_id": 2319779
    },
    {
      "context_text": "This technique is a variant of word dropout (Iyyer et al., 2015; Kumar et al., 2015), applied not to a feature extractor but to a decoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a technique called word dropout. No verifiable resources are identified.",
      "processing_time": 54.13391900062561,
      "citing_paper_id": "748227",
      "cited_paper_id": 2722012
    },
    {
      "context_text": "In a related result, recent approaches to image generation that use lstm decoders are able to do well without vae -style global latent variables (Theis and Bethge, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.664806604385376,
      "citing_paper_id": "748227",
      "cited_paper_id": 2865509
    },
    {
      "context_text": "For this experiment and subsequent analysis, we train our models on the Books Corpus introduced in Kiros et al. (2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Books Corpus"
      ],
      "dataset_descriptions": {
        "Books Corpus": "Used to train models for personalized text generation, focusing on capturing narrative and stylistic elements in the text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Books Corpus' as a training dataset for the models. The corpus is introduced in Kiros et al. (2015), which aligns with the cited paper title 'Skip-Thought Vectors'.",
      "processing_time": 63.156394243240356,
      "citing_paper_id": "748227",
      "cited_paper_id": 9126867
    },
    {
      "context_text": "Two other models have shown promise in learning sentence encodings, but cannot be used in a generative setting: Skip-thought models (Kiros et al., 2015) are unsupervised learning models that take the same model structure as a sequence au-toencoder, but generate text conditioned on a neighboring…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their limitations. The context focuses on describing the Skip-thought model and its inability to be used in a generative setting.",
      "processing_time": 56.775808334350586,
      "citing_paper_id": "748227",
      "cited_paper_id": 9126867
    },
    {
      "context_text": "Two other models have shown promise in learning sentence encodings, but cannot be used in a generative setting: Skip-thought models (Kiros et al., 2015) are unsupervised learning models that take the same model structure as a sequence autoencoder, but generate text conditioned on a neighboring sentence from the target text, instead of on the target sentence itself.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. The cited paper title 'Skip-Thought Vectors' confirms that the resource discussed is a model, not a dataset.",
      "processing_time": 57.10244965553284,
      "citing_paper_id": "748227",
      "cited_paper_id": 9126867
    },
    {
      "context_text": ", 2015) and a cnn classifier (Kim, 2014) both of which, unlike our model, are optimized end-to-end.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about comparing models, not using datasets.",
      "processing_time": 54.47023057937622,
      "citing_paper_id": "748227",
      "cited_paper_id": 9672033
    },
    {
      "context_text": "…adversarial training methods for generative models as well as non-parametric two-sample tests (Goodfellow et al., 2014; Li et al., 2015b; Denton et al., 2015; Gretton et al., 2012), we evaluate the imputed sentence completions by examining their distinguishability from the true sentence endings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on evaluating imputed sentence completions using adversarial training methods and two-sample tests.",
      "processing_time": 56.493255853652954,
      "citing_paper_id": "748227",
      "cited_paper_id": 10742222
    },
    {
      "context_text": "…has been some work on discrete sequences: a technique for doing this using rnn encoders and de-coders, which shares the same high-level architecture as our model, was proposed under the name Variational Recurrent Autoencoder ( vrae ) for the modeling of music in Fabius and van Amersfoort (2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Variational Recurrent Autoencoder). The context focuses on the architectural similarity to the cited work.",
      "processing_time": 56.093650341033936,
      "citing_paper_id": "748227",
      "cited_paper_id": 14282237
    },
    {
      "context_text": "Iterated conditional modes is a technique for finding the maximum joint assignment of a set of variables by alternately maximizing conditional distributions, and is a generalization of “hard-em” algorithms like k-means (Kearns et al., 1998).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (iterated conditional modes) and its relation to other algorithms. No verifiable resources are identified.",
      "processing_time": 55.644662380218506,
      "citing_paper_id": "748227",
      "cited_paper_id": 15295061
    },
    {
      "context_text": "Iterated conditional modes is a technique for ﬁnding the maximum joint assignment of a set of variables by alternately maximizing conditional distributions, and is a generalization of “hard-em ” algorithms like k-means (Kearns et al., 1998).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (iterated conditional modes) and a generalization to 'hard-em' algorithms. No verifiable resources are identified.",
      "processing_time": 56.48532724380493,
      "citing_paper_id": "748227",
      "cited_paper_id": 15295061
    },
    {
      "context_text": "…circum-vents these diﬃculties using the architecture of a variational autoencoder and takes advantage of recent advances in variational inference (Kingma and Welling, 2015; Rezende et al., 2014) that introduce a practical training technique for powerful neural network generative models with…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the architecture and training techniques of variational autoencoders.",
      "processing_time": 55.63890552520752,
      "citing_paper_id": "748227",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "We train our models with stochastic gradient descent, and at each gradient step we estimate the reconstruction cost using a single sample from q ( (cid:126)z | x ), but compute the kl divergence term of the cost function in closed form, again following Kingma and Welling (2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training models. The cited paper 'Auto-Encoding Variational Bayes' is referenced for a specific technique, not for a dataset.",
      "processing_time": 57.08103823661804,
      "citing_paper_id": "748227",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "Previous work on vae s for image modeling (Kingma and Welling, 2015) used a much weaker independent pixel de-coder model p ( x | (cid:126)z ), forcing the model to use the global latent variable to achieve good likelihoods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (VAE) and a model (independent pixel decoder).",
      "processing_time": 54.823028564453125,
      "citing_paper_id": "748227",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "In the experiments presented below using vae models, we use diagonal Gaussians for the prior and posterior distributions p ( (cid:126)z ) and q ( (cid:126)z | x ), using the Gaussian reparameterization trick of Kingma and Welling (2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of VAE models and Gaussian distributions. No verifiable resources are identified.",
      "processing_time": 55.04775333404541,
      "citing_paper_id": "748227",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "We argue the reason that the previous methods fail to preserve identity details, e.g. text and logos, as shown in Figure 1, Figure 14, 15, 16 in Appendix F, is because they cannot capture enough training information [9,12,13,21,30, 32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and figures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.010183334350586,
      "citing_paper_id": "265050824",
      "cited_paper_id": 3692201
    },
    {
      "context_text": "We argue the reason that the previous methods fail to preserve identity details, e.g. text and logos, as shown in Figure 1, Figure 14, 15, 16 in Appendix F, is because they cannot capture enough training information [9,12,13,21,30, 32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and figures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.010183334350586,
      "citing_paper_id": "265050824",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "We argue the reason that the previous methods fail to preserve identity details, e.g. text and logos, as shown in Figure 1, Figure 14, 15, 16 in Appendix F, is because they cannot capture enough training information [9,12,13,21,30, 32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and figures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.010183334350586,
      "citing_paper_id": "265050824",
      "cited_paper_id": 258999204
    },
    {
      "context_text": "We argue the reason that the previous methods fail to preserve identity details, e.g. text and logos, as shown in Figure 1, Figure 14, 15, 16 in Appendix F, is because they cannot capture enough training information [9,12,13,21,30, 32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to previous methods and figures. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.010183334350586,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259138650
    },
    {
      "context_text": "Diffusion Based Text-to-Image Models have recently demonstrated remarkable progress, primarily through the utilization of diffusion models [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion models).",
      "processing_time": 53.12901186943054,
      "citing_paper_id": "265050824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "[5], on the other hand, embark on the task of disentangling subjects from backgrounds by employing a CLIP image encoder for encoding backgrounds [22].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP image encoder) which is not included as per instructions.",
      "processing_time": 54.55053663253784,
      "citing_paper_id": "265050824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "In our quantitative assessment, we follow DreamBooth to employ DINO [4] and CLIP-I [22], to evaluate subject in a sea of roses, a <new> toy sitting 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on using DINO and CLIP-I for evaluation, which are not datasets.",
      "processing_time": 56.22326374053955,
      "citing_paper_id": "265050824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Recent advancements in text-to-image models have greatly improved the creation of diverse and high-quality images based on text input [20, 24, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image models. No verifiable resources are identified.",
      "processing_time": 54.80320620536804,
      "citing_paper_id": "265050824",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Recent advancements in text-to-image models have greatly improved the creation of diverse and high-quality images based on text input [20, 24, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in text-to-image models. No verifiable resources are identified.",
      "processing_time": 54.80320620536804,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "These achievements are particularly pronounced when employing large models trained on extensive datasets [3, 18, 20, 23, 24, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only refers to 'extensive datasets' in a generic manner. No specific names or identifiers are provided.",
      "processing_time": 55.55558919906616,
      "citing_paper_id": "265050824",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "These achievements are particularly pronounced when employing large models trained on extensive datasets [3, 18, 20, 23, 24, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only refers to 'extensive datasets' in a generic manner. No specific names or identifiers are provided.",
      "processing_time": 55.55558919906616,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "This not only preserves subject identity but also exhibits impressive generalization across various text inputs [7, 25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of a method. No verifiable resources are identified.",
      "processing_time": 54.296635150909424,
      "citing_paper_id": "265050824",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "[6] speed up customization with a feed forward network trained on many datasets of custom objects, but they prioritize speed over quality improvements.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'many datasets of custom objects' which is too generic.",
      "processing_time": 55.33670234680176,
      "citing_paper_id": "265050824",
      "cited_paper_id": 257913352
    },
    {
      "context_text": "To combat the issue of overﬁtting, researchers have explored diverse strategies, mostly by ﬁne-tuning different decomposition parts of the model [9,12,13,21,30,32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general strategies for combating overfitting. No verifiable resources are identified.",
      "processing_time": 54.775704860687256,
      "citing_paper_id": "265050824",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "To combat the issue of overﬁtting, researchers have explored diverse strategies, mostly by ﬁne-tuning different decomposition parts of the model [9,12,13,21,30,32,33].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general strategies for combating overfitting. No verifiable resources are identified.",
      "processing_time": 54.775704860687256,
      "citing_paper_id": "265050824",
      "cited_paper_id": 258999204
    },
    {
      "context_text": "[32] point out that the model is prone to overﬁt to the image layout when attempting to learn per-Methods sonalized concepts from a limited set of examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the model's tendency to overfit to image layouts when learning personalized concepts.",
      "processing_time": 56.238863706588745,
      "citing_paper_id": "265050824",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "Adata-oriented approach to tackle overﬁtting issues during diffusion model ﬁne-tuning.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to fine-tuning diffusion models. No verifiable resources are identified.",
      "processing_time": 55.11060428619385,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259138650
    },
    {
      "context_text": "It is worth noting that for SDXL, we incorporate cropping coordinates as described by [20], where we increase the original image size to achieve a cropped image size of 1024x1024 pixels.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for increasing image resolution using cropping coordinates.",
      "processing_time": 53.34072518348694,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "We apply this method to well-established models, such as StableDiffusion [24] and Sta-bleDiffusionXL [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context focuses on applying methods to models rather than using datasets.",
      "processing_time": 54.52551198005676,
      "citing_paper_id": "265050824",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "A systematic exploration of diverse methods for generating a regularization dataset, with the intention of providing valuable insights to the broader research community.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses the creation of a regularization dataset but does not name any existing datasets.",
      "processing_time": 55.35753512382507,
      "citing_paper_id": "265050824",
      "cited_paper_id": 260091569
    },
    {
      "context_text": "One way to improve MPC is to consider additional signals such as temporal information (Shokouhi and Radinsky, 2012; Whiting and Jose, 2014) or information gleaned from a users’ past queries (Shokouhi, 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches to improve query auto-completion. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 56.02239108085632,
      "citing_paper_id": "13742826",
      "cited_paper_id": 883098
    },
    {
      "context_text": "One way to improve MPC is to consider additional signals such as temporal information (Shokouhi and Radinsky, 2012; Whiting and Jose, 2014) or information gleaned from a users’ past queries (Shokouhi, 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches to improve query auto-completion. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 56.02239108085632,
      "citing_paper_id": "19096382",
      "cited_paper_id": 207204302
    },
    {
      "context_text": "The second architecture was\nintroduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectures and methods for language model personalization.",
      "processing_time": 53.401190996170044,
      "citing_paper_id": "13742826",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The second architecture was\nintroduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectures and methods for language model personalization.",
      "processing_time": 53.401190996170044,
      "citing_paper_id": "13742826",
      "cited_paper_id": 7595933
    },
    {
      "context_text": "The second architecture was introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectures and methods for language model personalization.",
      "processing_time": 53.532538414001465,
      "citing_paper_id": "13742826",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The second architecture was introduced by Mikolov and Zweig (2012) and has been used multiple times for LM personalization (Wen et al., 2013; Huang et al., 2014; Li et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only architectures and methods for language model personalization.",
      "processing_time": 53.532538414001465,
      "citing_paper_id": "13742826",
      "cited_paper_id": 7595933
    },
    {
      "context_text": "The Adam optimizer is used during training with a learning rate of 10−3 (Kingma and Ba, 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the context.",
      "processing_time": 54.04098868370056,
      "citing_paper_id": "13742826",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions tuning the learning rate on a held-out set of 12,000 queries, which is a specific dataset, but it does not provide a clear name or identifier for this dataset.",
      "processing_time": 57.36754369735718,
      "citing_paper_id": "13742826",
      "cited_paper_id": 7365802
    },
    {
      "context_text": "We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.2\nThe language model is a single-layer characterlevel LSTM with coupled input and forget gates and layer normalization (Melis et al., 2018; Ba et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a held-out set of 12,000 queries but does not provide a specific name for this dataset. No other datasets are mentioned.",
      "processing_time": 56.33641529083252,
      "citing_paper_id": "13742826",
      "cited_paper_id": 7365802
    },
    {
      "context_text": "We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified that perplexity is a good indicator of performance on the QAC task.2\nThe language model is a single-layer characterlevel LSTM with coupled input and forget gates and layer normalization (Melis et al., 2018; Ba et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a held-out set of 12,000 queries but does not provide a specific name for this dataset. No other datasets are mentioned.",
      "processing_time": 56.33641529083252,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "The first six of the ten files were used for\n1In the case of an LSTM, W′ is extended to incorporate all of the gates.\ntraining.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'files' used for training, which is too generic.",
      "processing_time": 54.88031554222107,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "Our experiments show that the LSTM model can be improved using personalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about improving LSTM models with personalization.",
      "processing_time": 53.85170388221741,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "By allowing a greater fraction of the weights to change during personalization, the FactorCell model has advantages over the traditional approach to adaptation of concatenating a context vector to the input of the LSTM (Mikolov and Zweig, 2012).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context is about the FactorCell model and its advantages over traditional LSTM adaptation methods.",
      "processing_time": 55.96146106719971,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "As a result, for a given latency budget, the FactorCell\nmodel outperforms the Mikolov and Zweig (2012) model for LSTM adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models. No verifiable resources are identified.",
      "processing_time": 54.32102870941162,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "The small models use an LSTM hidden state size of 300 and 20 dimensional user embeddings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architecture details.",
      "processing_time": 52.43448185920715,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "In their\nwork, completions are generated from a character LSTM language model instead of by ranking completions retrieved from a database, as in the MPC algorithm.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating completions using a character LSTM language model.",
      "processing_time": 54.31578803062439,
      "citing_paper_id": "13742826",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "The FactorCell model, (Jaech and Ostendorf, 2017), remedies this by letting the user embedding transform the weights of the recurrent layer via the use of a low-rank adaptation matrix.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (FactorCell) and its mechanism. No verifiable resources are referenced.",
      "processing_time": 55.28113770484924,
      "citing_paper_id": "13742826",
      "cited_paper_id": 19096382
    },
    {
      "context_text": "In particular, we make use of the recently introduced FactorCell model that uses an embedding vector to additively transform the weights of the language model’s recurrent layer with a low-rank matrix (Jaech and Ostendorf, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (FactorCell) rather than a dataset. No specific dataset is referenced.",
      "processing_time": 53.8335325717926,
      "citing_paper_id": "13742826",
      "cited_paper_id": 19096382
    },
    {
      "context_text": "Recently, Park and Chiba (2017) suggested a significantly different approach to QAC.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a different approach to query auto-completion.",
      "processing_time": 52.90821123123169,
      "citing_paper_id": "13742826",
      "cited_paper_id": 27562075
    },
    {
      "context_text": "Our experiments make use of the AOL Query data collected over three months in 2006 (Pass et al., 2006).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AOL Query data"
      ],
      "dataset_descriptions": {
        "AOL Query data": "Used to analyze user search behavior, specifically focusing on queries over a three-month period in 2006 to understand search patterns and trends."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'AOL Query data' which is a specific dataset used for search query analysis. It is clearly identified and used in the research.",
      "processing_time": 61.98289227485657,
      "citing_paper_id": "13742826",
      "cited_paper_id": 28918994
    },
    {
      "context_text": "Combined and Contextual Topic Models Combined topic models [1] extend the Neural-ProdLDA [18] variational approach by concatenating the input BoW document representation with pre-trained contextual embeddings produced by a neural language model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on combining topic models and neural language models, which are not datasets.",
      "processing_time": 55.616432905197144,
      "citing_paper_id": "221293335",
      "cited_paper_id": 29842525
    },
    {
      "context_text": "Combined topic models (Bianchi, Terragni, and Hovy 2021) extend the Neural-ProdLDA (Srivastava and Sutton 2017) variational approach by concatenating the input BoW document representation with pre-trained contextual embeddings produced by a neural language model after a pre-training procedure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 53.764811515808105,
      "citing_paper_id": "221293335",
      "cited_paper_id": 29842525
    },
    {
      "context_text": "Each sequence is annotated with one or more topics by two domain experts using a set of five main topics (family, literature, work, travel, health) and five subtopics that were found during a previous analysis of the corpus [17].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'corpus' but does not provide a specific name or identifier. The corpus is used for annotation by domain experts, but it lacks the specificity required to be considered a verifiable dataset.",
      "processing_time": 57.31719732284546,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "We produce a total of 72 sentences on the Svevo Corpus (plus another 72 on gold labels for human evaluation) and 120 sentences each for both EuroParl-IT and EuroParl-EN.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "EuroParl-IT",
        "EuroParl-EN"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to generate 72 sentences for personalized text generation, with additional 72 gold-labeled sentences for human evaluation.",
        "EuroParl-IT": "Used to generate 120 sentences for personalized text generation, focusing on Italian-English parallel text.",
        "EuroParl-EN": "Used to generate 120 sentences for personalized text generation, focusing on English-Italian parallel text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for generating sentences, which are relevant to personalized text generation.",
      "processing_time": 72.11430716514587,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "We produce a total of 72 sentences on the Svevo Corpus (plus another 72 on gold labels for human evaluation) and 120 sentences each for both EuroParl-IT and EuroParl-EN.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "EuroParl-IT",
        "EuroParl-EN"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to generate 72 sentences for personalized text generation, with additional 72 gold-labeled sentences for human evaluation.",
        "EuroParl-IT": "Used to generate 120 sentences for personalized text generation, focusing on Italian-English parallel text.",
        "EuroParl-EN": "Used to generate 120 sentences for personalized text generation, focusing on English-Italian parallel text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for generating sentences, which are relevant to personalized text generation.",
      "processing_time": 72.11430716514587,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Readers are referred to (Fenu 2017; Sarti 2019) for additional details on the latter.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers readers to other sources for additional details.",
      "processing_time": 54.523027658462524,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Readers are referred to (Fenu 2017; Sarti 2019) for additional details on the latter.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers readers to other sources for additional details.",
      "processing_time": 54.523027658462524,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "1 We begin by evaluating the effectiveness of ETC-NLG on the Svevo Corpus [7,17], a topic-annotated Italian epistolary corpus containing archaic and dialectal terms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to evaluate ETC-NLG's effectiveness on a topic-annotated Italian epistolary corpus containing archaic and dialectal terms, focusing on personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Svevo Corpus, which is a specific, named dataset used for evaluating the effectiveness of ETC-NLG. The corpus is described as topic-annotated and containing archaic and dialectal terms.",
      "processing_time": 65.81040644645691,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "1 We begin by evaluating the effectiveness of ETC-NLG on the Svevo Corpus [7,17], a topic-annotated Italian epistolary corpus containing archaic and dialectal terms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to evaluate ETC-NLG's effectiveness on a topic-annotated Italian epistolary corpus containing archaic and dialectal terms, focusing on personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Svevo Corpus, which is a specific, named dataset used for evaluating the effectiveness of ETC-NLG. The corpus is described as topic-annotated and containing archaic and dialectal terms.",
      "processing_time": 65.81040644645691,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "The Svevo Corpus contains 5419 sequences ranging from few words to multiple sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to study sequences ranging from few words to multiple sentences, focusing on the structure and variability of text for personalized generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'The Svevo Corpus' which is a specific dataset with a clear identifier and is relevant to the topic of personalized text generation.",
      "processing_time": 61.016780853271484,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "The Svevo Corpus contains 5419 sequences ranging from few words to multiple sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to study sequences ranging from few words to multiple sentences, focusing on the structure and variability of text for personalized generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'The Svevo Corpus' which is a specific dataset with a clear identifier and is relevant to the topic of personalized text generation.",
      "processing_time": 61.016780853271484,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experiments start from the evaluation of ETC-NLG on the Svevo Corpus (Fenu 2017; Sarti 2019), a topic-annotated Italian epistolary corpus containing archaic and dialectal terms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to evaluate ETC-NLG on a topic-annotated Italian epistolary corpus containing archaic and dialectal terms, focusing on personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The Svevo Corpus is a specific, named dataset that fits the criteria for inclusion. It is used for evaluating ETC-NLG on a topic-annotated Italian epistolary corpus.",
      "processing_time": 64.26037120819092,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experiments start from the evaluation of ETC-NLG on the Svevo Corpus (Fenu 2017; Sarti 2019), a topic-annotated Italian epistolary corpus containing archaic and dialectal terms.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to evaluate ETC-NLG on a topic-annotated Italian epistolary corpus containing archaic and dialectal terms, focusing on personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The Svevo Corpus is a specific, named dataset that fits the criteria for inclusion. It is used for evaluating ETC-NLG on a topic-annotated Italian epistolary corpus.",
      "processing_time": 64.26037120819092,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples of conditioned generation on the Svevo Corpus, presented in Table 5, show how ETC-NLG can produce meaningful sentences despite the relatively high perplexity achieved by GePpeTto generator on the epistolary corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "epistolary corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to demonstrate conditioned text generation capabilities of ETC-NLG, focusing on producing meaningful sentences despite high perplexity.",
        "epistolary corpus": "Used to evaluate the performance of the GePpeTto generator, specifically assessing its ability to generate text in an epistolary style."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Svevo Corpus' and 'epistolary corpus', which are specific datasets used for conditioned text generation. The usage is clear and relevant to personalized text generation.",
      "processing_time": 69.8661437034607,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples of conditioned generation on the Svevo Corpus, presented in Table 5, show how ETC-NLG can produce meaningful sentences despite the relatively high perplexity achieved by GePpeTto generator on the epistolary corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "epistolary corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used to demonstrate conditioned text generation capabilities of ETC-NLG, focusing on producing meaningful sentences despite high perplexity.",
        "epistolary corpus": "Used to evaluate the performance of the GePpeTto generator, specifically assessing its ability to generate text in an epistolary style."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Svevo Corpus' and 'epistolary corpus', which are specific datasets used for conditioned text generation. The usage is clear and relevant to personalized text generation.",
      "processing_time": 69.8661437034607,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Each sequence is annotated with one or more topics by two domain experts using a set of ﬁve main topics (family, literature, work, travel, health) and ﬁve sub-topics that were found during a previous analysis of the corpus (Sarti 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a corpus but does not provide a specific name. It describes the annotation process and topics used, which suggests a custom dataset rather than a publicly known one.",
      "processing_time": 56.06894516944885,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples from the Svevo Corpus with gold annotations (top), with each keyword representing a different topic, and from the English Europarl corpus with contextual annotations (bottom), where the top 5 topic-related keywords from the prevalent topic extracted through topic modeling are presented.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "English Europarl corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used for gold annotations with keywords representing different topics, aiding in the study of personalized text generation.",
        "English Europarl corpus": "Utilized for contextual annotations, presenting top 5 topic-related keywords from the prevalent topic extracted through topic modeling."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific corpora, 'Svevo Corpus' and 'English Europarl corpus', which are used for annotation and topic modeling respectively.",
      "processing_time": 67.31374740600586,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples from the Svevo Corpus with gold annotations (top), with each keyword representing a different topic, and from the English Europarl corpus with contextual annotations (bottom), where the top 5 topic-related keywords from the prevalent topic extracted through topic modeling are presented.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Svevo Corpus",
        "English Europarl corpus"
      ],
      "dataset_descriptions": {
        "Svevo Corpus": "Used for gold annotations with keywords representing different topics, aiding in the study of personalized text generation.",
        "English Europarl corpus": "Utilized for contextual annotations, presenting top 5 topic-related keywords from the prevalent topic extracted through topic modeling."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific corpora, 'Svevo Corpus' and 'English Europarl corpus', which are used for annotation and topic modeling respectively.",
      "processing_time": 67.31374740600586,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experimental objectives are three-fold: ﬁrst, we test ETC-NLG on the Italian subset of the epistolary corpus of Italo Svevo (Fenu 2017), a famous Italian author of the early 20th century, to quantify the impact of dialectal and archaic expressions on the quality of generated sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "epistolary corpus of Italo Svevo"
      ],
      "dataset_descriptions": {
        "epistolary corpus of Italo Svevo": "Used to test ETC-NLG on the Italian subset, focusing on the impact of dialectal and archaic expressions on the quality of generated sentences."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific subset of a corpus, which is used to test the impact of dialectal and archaic expressions on text generation quality.",
      "processing_time": 63.33439326286316,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "Our experimental objectives are three-fold: first, we test ETC-NLG on the Italian subset of the epistolary corpus of Italo Svevo [7], a famous Italian author of the early 20th century, to quantify the impact of dialectal and archaic expressions on the quality of generated sentences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "epistolary corpus of Italo Svevo"
      ],
      "dataset_descriptions": {
        "epistolary corpus of Italo Svevo": "Used to test ETC-NLG on the Italian subset, focusing on the impact of dialectal and archaic expressions on the quality of generated sentences."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific subset of a corpus, which is the Italian subset of the epistolary corpus of Italo Svevo. This corpus is used to test the impact of dialectal and archaic expressions on the quality of generated sentences.",
      "processing_time": 68.35170912742615,
      "citing_paper_id": "221293335",
      "cited_paper_id": null
    },
    {
      "context_text": "As in much of the existing work on data collection for REG (Gatt et al., 2007; Dale and Viethen, 2009; Paraboni et al., 2017a), the b5-ref corpus was implemented as a language production task in which subjects were requested to distinguish a certain target from distractor objects in a given context…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref corpus"
      ],
      "dataset_descriptions": {
        "b5-ref corpus": "Used to implement a language production task where subjects distinguish a target from distractor objects, focusing on the generation of referring expressions in a given context."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'b5-ref corpus' as a specific dataset used for a language production task in referring expression generation (REG) research.",
      "processing_time": 61.93227028846741,
      "citing_paper_id": "21691164",
      "cited_paper_id": 323921
    },
    {
      "context_text": "As in much of the existing work on data collection for REG (Gatt et al., 2007; Dale and Viethen, 2009; Paraboni et al., 2017a), the b5-ref corpus was implemented as a language production task in which subjects were requested to distinguish a certain target from distractor objects in a given context…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref corpus"
      ],
      "dataset_descriptions": {
        "b5-ref corpus": "Used to implement a language production task where subjects distinguish a target from distractor objects, focusing on the generation of referring expressions in a given context."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'b5-ref corpus' as a specific dataset used for a language production task in referring expression generation (REG) research.",
      "processing_time": 61.93227028846741,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29272723
    },
    {
      "context_text": "As in much of the existing work on data collection for REG (Gatt et al., 2007; Dale and Viethen, 2009; Paraboni et al., 2017a), the b5-ref corpus was implemented as a language production task in which subjects were requested to distinguish a certain target from distractor objects in a given context…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref corpus"
      ],
      "dataset_descriptions": {
        "b5-ref corpus": "Used to implement a language production task where subjects distinguish a target from distractor objects, focusing on the generation of referring expressions in a given context."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'b5-ref corpus' as a specific dataset used for a language production task in referring expression generation (REG) research.",
      "processing_time": 61.93227028846741,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29746672
    },
    {
      "context_text": ", 2013; Nowson and Gill, 2014), but also for the study of other forms of author profiling, as in (Schwartz et al., 2013; Marquardt et al., 2014; ÁlvarezCarmona et al., 2015; González-Gallardo et al., 2015; S. ulea and Dichiu, 2015; Najib et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. There are no clear identifiers for datasets within the given context.",
      "processing_time": 55.08455753326416,
      "citing_paper_id": "21691164",
      "cited_paper_id": 5772411
    },
    {
      "context_text": "…as in (Mairesse et al., 2007; Farnadi et al., 2013; Nowson and Gill, 2014), but also for the study of other forms of author proﬁling, as in (Schwartz et al., 2013; Marquardt et al., 2014; ´Alvarez-Carmona et al., 2015; Gonz´alez-Gallardo et al., 2015; S.ulea and Dichiu, 2015; Najib et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 55.386798620224,
      "citing_paper_id": "21691164",
      "cited_paper_id": 5772411
    },
    {
      "context_text": "…as in (Mairesse et al., 2007; Farnadi et al., 2013; Nowson and Gill, 2014), but also for the study of other forms of author proﬁling, as in (Schwartz et al., 2013; Marquardt et al., 2014; ´Alvarez-Carmona et al., 2015; Gonz´alez-Gallardo et al., 2015; S.ulea and Dichiu, 2015; Najib et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 55.386798620224,
      "citing_paper_id": "21691164",
      "cited_paper_id": 9099819
    },
    {
      "context_text": "Following much of the work on NLG, controlled text was elicited from visual stimuli represented by images widely used in Psycholinguistics.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to visual stimuli used in psycholinguistics. No clear, verifiable dataset names are provided.",
      "processing_time": 55.87745785713196,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "The b5-ref subcorpus was built for the study of the effects of human personality on the generation of referring expressions (REG), which is an active research topic in NLG (Krahmer and van Deemter, 2012).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref subcorpus"
      ],
      "dataset_descriptions": {
        "b5-ref subcorpus": "Used to study the effects of human personality on the generation of referring expressions, focusing on how personality traits influence the creation of REGs in NLG."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'b5-ref subcorpus' as a specific dataset used for studying the effects of human personality on the generation of referring expressions.",
      "processing_time": 62.97788381576538,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "2 Although certain spelling mistakes might be indicative of personality traits, this possible source of knowledge was discarded as a means to enable the use of the corpus in studies of NLG as well, whose focus is usually the generation of correct text. ing author’s identiﬁers.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the decision to discard spelling mistakes as a source of knowledge for NLG studies, but no dataset is named.",
      "processing_time": 56.110814571380615,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "The use of free text is mainly motivated by the speciﬁc needs of certain types of application, such as the recognition of personality from text on social networks, whereas the use of controlled text is required for a range of Natural Language Generation (NLG) studies.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of free text and controlled text in applications and NLG studies, but does not provide names of datasets.",
      "processing_time": 56.603935956954956,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "Generally speaking, the original and parsed formats are potentially more useful to NLG studies since they indicate what each subject wrote in response to each of the visual stimuli, whereas per-speaker is more useful to language understanding studies (for example, for author proﬁling or document classiﬁcation).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general formats of data that could be useful for NLG studies. No clear, verifiable resource names are provided.",
      "processing_time": 55.86422848701477,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "As future work, we intend to provide the semantic annotation of the stimulus scenes in b5-text and b5-caption , so that these datasets may be explored more fully in subsequent NLG studies.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-text",
        "b5-caption"
      ],
      "dataset_descriptions": {
        "b5-text": "Intended for future semantic annotation to enhance NLG studies, focusing on generating referring expressions in text.",
        "b5-caption": "Intended for future semantic annotation to enhance NLG studies, focusing on generating referring expressions in image captions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'b5-text' and 'b5-caption' as datasets intended for future work in semantic annotation and NLG studies. These names are specific and plausible.",
      "processing_time": 67.68017268180847,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "Given its linguistic motivation, the Big Five model provides a theoretical basis for the computational treatment of personality on at least two fronts: the automatic recognition of personality traits from text (which is a language understanding task), and the generation of text in order to reproduce certain personality traits of interest (which is a natural language generation (NLG) task).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical models and tasks. The context focuses on the Big Five model's application in personality recognition and text generation.",
      "processing_time": 55.628735303878784,
      "citing_paper_id": "21691164",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "…nat-ural language is the focus of a large body of work in the Psychology ﬁeld, and it is perhaps best summarised by the Big Five personality factors (Goldberg, 1990) - Openness to experience, Conscientiousness, Extraversion, Agreeable-ness and Neuroticism - which are widely accepted as an adequate…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework (Big Five personality factors).",
      "processing_time": 53.996079206466675,
      "citing_paper_id": "21691164",
      "cited_paper_id": 9034636
    },
    {
      "context_text": "…language is the focus of a large body of work in the Psychology ﬁeld, and it is perhaps best summarised by the Big Five personality factors (Goldberg, 1990) - Openness to experience, Conscientiousness, Extraversion, Agreeable-ness and Neuroticism - which are widely accepted as an adequate…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework (Big Five personality factors).",
      "processing_time": 53.99307990074158,
      "citing_paper_id": "21691164",
      "cited_paper_id": 9034636
    },
    {
      "context_text": "The Big Five model (Goldberg, 1990) comprises ﬁve fundamental dimensions of the human personality - Openness to experience, Conscientiousness, Extraversion, Agreeable-ness and Neuroticism - that may be estimated by using a wide range of methods, the most common being the use of personality…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the Big Five personality model and its dimensions, which is a method or theoretical framework rather than a dataset.",
      "processing_time": 56.20821213722229,
      "citing_paper_id": "21691164",
      "cited_paper_id": 9034636
    },
    {
      "context_text": "Moreover, results from a personality-dependent lexical choice model built from b5ref in (Lan and Paraboni, 2018) showed that the lexicalisation of the most frequent properties (i.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5ref"
      ],
      "dataset_descriptions": {
        "b5ref": "Used to build a personality-dependent lexical choice model, focusing on the lexicalisation of frequent properties in personalized text generation."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'b5ref' which appears to be a dataset used for building a personality-dependent lexical choice model. However, there is no clear indication of the full name or specific details about the dataset.",
      "processing_time": 62.737247943878174,
      "citing_paper_id": "21691164",
      "cited_paper_id": 21699698
    },
    {
      "context_text": "Moreover, results from a personality-dependent lexical choice model built from b5-ref in (Lan and Paraboni, 2018) showed that the lexicali-sation of the most frequent properties (i.e., those for which there is sufﬁcient data in the corpus) greatly improves when personality information is taken into…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref"
      ],
      "dataset_descriptions": {
        "b5-ref": "Used to build a personality-dependent lexical choice model, focusing on the lexicalization of frequent properties, demonstrating improvements when personality information is included."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'b5-ref' as a corpus used for building a personality-dependent lexical choice model. The corpus name is specific and plausible, and the usage is clear.",
      "processing_time": 62.231380462646484,
      "citing_paper_id": "21691164",
      "cited_paper_id": 21699698
    },
    {
      "context_text": "Preliminary results of a machine learning REG model based on b5-ref data in (Paraboni et al., 2017c) suggest that the selection of non-discriminatory attributes (e.g., the property of ‘being young’, which is shared by all objects in the b5-ref domain and it is therefore not discriminatory) is…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref"
      ],
      "dataset_descriptions": {
        "b5-ref": "Used to train and evaluate a machine learning model for referring expression generation, focusing on non-discriminatory attribute selection in the b5-ref domain."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'b5-ref data' which seems to be a specific dataset used in the research. The cited papers do not provide additional clarity on whether 'b5-ref' is a dataset or something else, but the context suggests it is used for training and evaluating a machine learning model.",
      "processing_time": 67.57139778137207,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29272723
    },
    {
      "context_text": "Preliminary results of a machine learning REG model based on b5-ref data in (Paraboni et al., 2017c) suggest that the selection of non-discriminatory attributes (e.g., the property of ‘being young’, which is shared by all objects in the b5-ref domain and it is therefore not discriminatory) is…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "b5-ref"
      ],
      "dataset_descriptions": {
        "b5-ref": "Used to train and evaluate a machine learning model for referring expression generation, focusing on non-discriminatory attribute selection in the b5-ref domain."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'b5-ref data' which seems to be a specific dataset used in the research. The cited papers do not provide additional clarity on whether 'b5-ref' is a dataset or something else, but the context suggests it is used for training and evaluating a machine learning model.",
      "processing_time": 67.57139778137207,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29746672
    },
    {
      "context_text": "…the faces presented as stimulus may be considered, to some extent, as having a round shape), and it carries non-trivial consequences for the design of REG algorithms that favour the selection of discriminatory information and/or pay regard to referential overspeciﬁcation (Paraboni et al., 2017b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses implications for algorithm design.",
      "processing_time": 53.22510528564453,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29272723
    },
    {
      "context_text": "…the faces presented as stimulus may be considered, to some extent, as having a round shape), and it carries non-trivial consequences for the design of REG algorithms that favour the selection of discriminatory information and/or pay regard to referential overspeciﬁcation (Paraboni et al., 2017b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses implications for algorithm design.",
      "processing_time": 53.22510528564453,
      "citing_paper_id": "21691164",
      "cited_paper_id": 29746672
    },
    {
      "context_text": "Studies of this kind may be seen as a possible generalisation of models of human variation for this task (Viethen and Dale, 2010; Ferreira and Paraboni, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies and models. No verifiable resources are identified.",
      "processing_time": 54.31112051010132,
      "citing_paper_id": "21691164",
      "cited_paper_id": 35932997
    },
    {
      "context_text": "…of these applications would be beyond the scope of this paper, but includes the recognition of personality traits from text on social networks (Iacobelli et al., 2011; Celli, 2012; ´Alvarez-Carmona et al., 2015) and the generation of text based on a target personality (Mairesse and Walker, 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the generation of text based on a target personality, which is relevant to personalized text generation. However, no specific dataset names are mentioned.",
      "processing_time": 55.66309952735901,
      "citing_paper_id": "21691164",
      "cited_paper_id": 141211891
    },
    {
      "context_text": "Studies of this kind would typically take the form of a supervised (Mairesse et al., 2007), or semi-supervised (Celli, 2012) learning task.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of learning tasks. No verifiable resources are identified.",
      "processing_time": 54.62419080734253,
      "citing_paper_id": "21691164",
      "cited_paper_id": 141211891
    },
    {
      "context_text": ", 2001), four additional, MRC-like (Coltheart, 1981) psycholinguistic properties and further 60 dictionary attributes.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'MRC-like' psycholinguistic properties, which suggests a reference to the MRC Psycholinguistic Database. However, the exact dataset name is not directly mentioned in the citation span.",
      "processing_time": 57.354161500930786,
      "citing_paper_id": "21691164",
      "cited_paper_id": 142718266
    },
    {
      "context_text": "The image description task made use of 10 GAPED images with valence values selected at regular intervals (from 3 to 54 degrees).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAPED"
      ],
      "dataset_descriptions": {
        "GAPED": "Used to select images with specific valence values for the image description task, focusing on emotional content and normative significance."
      },
      "confidence_score": 1.0,
      "reasoning": "GAPED is identified as a specific dataset used for the image description task, focusing on valence values.",
      "processing_time": 59.90386939048767,
      "citing_paper_id": "21691164",
      "cited_paper_id": 207655542
    },
    {
      "context_text": "In the present work, images were taken from the GAPED (Dan-Glauser and Scherer, 2011), Face Place (Righi et al., 2012) and Greebles (Gauthier and Tarr, 1997) image databases 1 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAPED",
        "Face Place",
        "Greebles"
      ],
      "dataset_descriptions": {
        "GAPED": "Used to study valence and normative significance in images, focusing on emotional responses and affective processing.",
        "Face Place": "Used to analyze facial recognition and place categorization, focusing on the neural mechanisms underlying face and place perception.",
        "Greebles": "Used to investigate object recognition and perceptual expertise, focusing on the ability to distinguish between highly similar objects."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific image databases: GAPED, Face Place, and Greebles. These are clearly identified as datasets used for image-based research.",
      "processing_time": 71.8518180847168,
      "citing_paper_id": "21691164",
      "cited_paper_id": 207655542
    },
    {
      "context_text": "The visual stimuli employed in both cases were taken from GAPED (Dan-Glauser and Scherer, 2011), a database conveying images classiﬁed by valence and normative significance, and designed to arouse different degrees and types of reaction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAPED"
      ],
      "dataset_descriptions": {
        "GAPED": "Used to evoke different emotional reactions through images classified by valence and normative significance, focusing on the impact of visual stimuli on emotional responses."
      },
      "confidence_score": 1.0,
      "reasoning": "GAPED is identified as a specific database of images classified by valence and normative significance, which is used to evoke different emotional reactions.",
      "processing_time": 61.989813804626465,
      "citing_paper_id": "21691164",
      "cited_paper_id": 207655542
    },
    {
      "context_text": "The visual stimuli employed in both cases were taken from GAPED (Dan-Glauser and Scherer, 2011), a database conveying images classified by valence and normative significance, and designed to arouse different degrees and types of reaction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAPED"
      ],
      "dataset_descriptions": {
        "GAPED": "Used to provide visual stimuli classified by valence and normative significance, designed to elicit varying emotional reactions in participants."
      },
      "confidence_score": 1.0,
      "reasoning": "GAPED is identified as a specific database of images classified by valence and normative significance, which is used to arouse different degrees and types of reaction.",
      "processing_time": 61.75413370132446,
      "citing_paper_id": "21691164",
      "cited_paper_id": 207655542
    },
    {
      "context_text": "In the present work, images were taken from the GAPED (Dan-Glauser and Scherer, 2011), Face Place (Righi et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GAPED"
      ],
      "dataset_descriptions": {
        "GAPED": "Used to study valence and normative significance in images, focusing on emotional responses and affective processing in personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "GAPED is identified as a specific dataset from the citation context and the title confirms it is a picture database. Face Place is mentioned but cut off, so it is excluded due to lack of context.",
      "processing_time": 63.69974374771118,
      "citing_paper_id": "21691164",
      "cited_paper_id": 207655542
    },
    {
      "context_text": "Dictionary features were obtained from Unitex-PB (Muniz, 2004) by computing word classes and a range of morphological features.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Unitex-PB' but does not refer to it as a dataset. It is described as a tool or method for obtaining dictionary features, which is not a dataset.",
      "processing_time": 56.5183379650116,
      "citing_paper_id": "21691164",
      "cited_paper_id": 221247466
    },
    {
      "context_text": "As learning features, we computed 64 LIWC categories (Pennebaker et al., 2001), four additional, MRC-like (Coltheart, 1981) psycholinguistic properties and further 60 dictionary attributes.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LIWC categories and MRC-like psycholinguistic properties, which are not datasets but rather tools or methods for text analysis. No specific, verifiable datasets are mentioned.",
      "processing_time": 56.39204406738281,
      "citing_paper_id": "21691164",
      "cited_paper_id": null
    },
    {
      "context_text": "Among many inventories developed for the Big Five model, the need for a fast assessment tool led to the proposal of the BFI inventory (John et al., 1991).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a personality inventory which is not a dataset but a method or tool.",
      "processing_time": 54.355711221694946,
      "citing_paper_id": "21691164",
      "cited_paper_id": null
    },
    {
      "context_text": "The personality inventory that accompanies the collected texts is a Brazilian Portuguese version of the 44-item BFI developed for the English language (John et al., 1991), and presented in (de Andrade, 2008).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions a personality inventory, which is a specific, verifiable resource. However, it does not mention a dataset, corpus, or similar data collection.",
      "processing_time": 56.11662435531616,
      "citing_paper_id": "21691164",
      "cited_paper_id": null
    },
    {
      "context_text": "Implementation details We implemented f θ using a standard Transformer architecture (Vaswani et al., 2017) with pre-trained Glove embedding (Pennington et al., 2014) 1 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained GloVe embeddings but does not refer to a specific dataset. GloVe is a method for word representation, not a dataset.",
      "processing_time": 55.46957731246948,
      "citing_paper_id": "165163819",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Recently, several meta-learning models has been proposed for solving few-shot image classiﬁcation (Ravi and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about meta-learning models and their applications, which are not considered datasets.",
      "processing_time": 55.91847538948059,
      "citing_paper_id": "165163819",
      "cited_paper_id": 2928017
    },
    {
      "context_text": "Recently, several meta-learning models has been proposed for solving few-shot image classiﬁcation (Ravi and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about meta-learning models and their applications, which are not considered datasets.",
      "processing_time": 55.91847538948059,
      "citing_paper_id": "165163819",
      "cited_paper_id": 6466088
    },
    {
      "context_text": "Recently, several meta-learning models has been proposed for solving few-shot image classiﬁcation (Ravi and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about meta-learning models and their applications, which are not considered datasets.",
      "processing_time": 55.91847538948059,
      "citing_paper_id": "165163819",
      "cited_paper_id": 8909022
    },
    {
      "context_text": "For the standard training, we used Adam (Kingma and Ba, 2014) optimizer with a warm-up learning rate strategy, and a batch size of 32.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 55.09341549873352,
      "citing_paper_id": "165163819",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Instead, in meta-training, we used SGD for the inner loop and Adam for the outer loop with learning rate α = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only optimization methods (SGD and Adam).",
      "processing_time": 54.39263033866882,
      "citing_paper_id": "165163819",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "This conﬁrms that these measures are not correlated to human judgment (Liu et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper discussing evaluation metrics for dialogue systems.",
      "processing_time": 54.913689374923706,
      "citing_paper_id": "165163819",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "Human Since automatic evaluation performs poorly in this task (Liu et al., 2016), we perform a human evaluation using crowd-sourced workers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to human evaluation using crowd-sourced workers.",
      "processing_time": 54.90388321876526,
      "citing_paper_id": "165163819",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "Meta-Learning Meta-learning (Thrun and Pratt, 1998; Schmidhuber, 1987, 1992; Naik and Mammone, 1992; Bengio et al., 1992) is sub-ﬁeld of machine learning with the aim of learning the learning algorithm itself.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to meta-learning concepts and related works. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.87784433364868,
      "citing_paper_id": "165163819",
      "cited_paper_id": 11974053
    },
    {
      "context_text": "Meta-Learning Meta-learning (Thrun and Pratt, 1998; Schmidhuber, 1987, 1992; Naik and Mammone, 1992; Bengio et al., 1992) is sub-ﬁeld of machine learning with the aim of learning the learning algorithm itself.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to meta-learning concepts and related works. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.87784433364868,
      "citing_paper_id": "165163819",
      "cited_paper_id": 28783413
    },
    {
      "context_text": "Meta-Learning Meta-learning (Thrun and Pratt, 1998; Schmidhuber, 1987, 1992; Naik and Mammone, 1992; Bengio et al., 1992) is sub-ﬁeld of machine learning with the aim of learning the learning algorithm itself.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to meta-learning concepts and related works. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.87784433364868,
      "citing_paper_id": "165163819",
      "cited_paper_id": 264351059
    },
    {
      "context_text": "Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classiﬁcation (Yu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of meta-learning in NLP tasks. No verifiable resources are named.",
      "processing_time": 55.505393505096436,
      "citing_paper_id": "165163819",
      "cited_paper_id": 11974053
    },
    {
      "context_text": "Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classiﬁcation (Yu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of meta-learning in NLP tasks. No verifiable resources are named.",
      "processing_time": 55.505393505096436,
      "citing_paper_id": "165163819",
      "cited_paper_id": 29162291
    },
    {
      "context_text": "Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classiﬁcation (Yu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of meta-learning in NLP tasks. No verifiable resources are named.",
      "processing_time": 55.505393505096436,
      "citing_paper_id": "165163819",
      "cited_paper_id": 52100101
    },
    {
      "context_text": "Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methodologies and works. No verifiable resources are identified.",
      "processing_time": 55.35075569152832,
      "citing_paper_id": "165163819",
      "cited_paper_id": 29473470
    },
    {
      "context_text": "Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methodologies and works. No verifiable resources are identified.",
      "processing_time": 55.35075569152832,
      "citing_paper_id": "165163819",
      "cited_paper_id": 53297919
    },
    {
      "context_text": "Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various methodologies and works. No verifiable resources are identified.",
      "processing_time": 55.35075569152832,
      "citing_paper_id": "165163819",
      "cited_paper_id": 68167178
    },
    {
      "context_text": "…to capture a persona just by using few sentences, and collecting a non-synthetic set of persona descriptions from a real human-human conversation, e.g., Reddit, is challenging as well since it requires hand-crafted fea-†† These two authors contributed equally. ture designs (Mazare et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the challenge of collecting real human-human conversation data for persona descriptions, but does not specify a named dataset. The reference to Reddit is too generic.",
      "processing_time": 56.20562028884888,
      "citing_paper_id": "165163819",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "Aside of standards evaluation metrics, we also train a Natural Language Inference (NLI) model using Dialog NLI (Sean et al., 2018) dataset, a recently proposed corpus based on Persona dataset, with NLI annotation between persona description sentences and dialogues utterance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dialog NLI"
      ],
      "dataset_descriptions": {
        "Dialog NLI": "Used to train a Natural Language Inference model, focusing on NLI annotations between persona description sentences and dialogue utterances."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Dialog NLI dataset, which is a specific corpus used for training an NLI model. The dataset is based on the Persona dataset and includes NLI annotations.",
      "processing_time": 62.56884407997131,
      "citing_paper_id": "165163819",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "We ﬁne-tune a pre-trained BERT model (Devlin et al., 2018) using the DNLI corpus and achieve a test set accuracy of 88.43%, which is aligned to the best-reported model ESIM (Chen et al., 2017) in Sean et al. (2018) (with 88.20% accuracy).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DNLI corpus"
      ],
      "dataset_descriptions": {
        "DNLI corpus": "Used to fine-tune a pre-trained BERT model for dialogue natural language inference, achieving high test set accuracy and aligning with state-of-the-art models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DNLI corpus, which is a specific dataset used for fine-tuning a pre-trained BERT model. The usage is clear and relevant to the research topic of personalized text generation.",
      "processing_time": 65.66717052459717,
      "citing_paper_id": "165163819",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "This agrees with results of Sean et al. (2018), where the authors showed that by re-ranking the beam search hypothesis using the DNLI score (i.e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DNLI score) used in re-ranking beam search hypotheses.",
      "processing_time": 54.641836166381836,
      "citing_paper_id": "165163819",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "For any i, j ∈ Z ≥ 0 , δ ij represents the Kronecker delta function [15], which output 1 if i = j otherwise 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the Kronecker delta function, which is a mathematical concept.",
      "processing_time": 55.25295925140381,
      "citing_paper_id": "270764846",
      "cited_paper_id": 1300625
    },
    {
      "context_text": "This is important for managing tradeoffs among different dimensions [44, 3] and catering to the diverse needs of users [21, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to managing tradeoffs and catering to user needs, which are general statements.",
      "processing_time": 55.794705867767334,
      "citing_paper_id": "270764846",
      "cited_paper_id": 3696067
    },
    {
      "context_text": "This is important for managing tradeoffs among different dimensions [44, 3] and catering to the diverse needs of users [21, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to managing tradeoffs and catering to user needs, which are general statements.",
      "processing_time": 55.794705867767334,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263830508
    },
    {
      "context_text": "This is important for managing tradeoffs among different dimensions [44, 3] and catering to the diverse needs of users [21, 13].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to managing tradeoffs and catering to user needs, which are general statements.",
      "processing_time": 55.794705867767334,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives [44, 21, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general need for methods to adapt LMs on-the-fly. No verifiable resources are identified.",
      "processing_time": 55.5278103351593,
      "citing_paper_id": "270764846",
      "cited_paper_id": 3696067
    },
    {
      "context_text": "Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives [44, 21, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general need for methods to adapt LMs on-the-fly. No verifiable resources are identified.",
      "processing_time": 55.5278103351593,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263830508
    },
    {
      "context_text": "Therefore, there is a growing need for methods of adapting LMs on-the-fly toward different combinations of objectives [44, 21, 13].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general need for methods to adapt LMs on-the-fly. No verifiable resources are identified.",
      "processing_time": 55.5278103351593,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Definition 1 ( f -divergence [1, 10, 11]) .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a definition of f-divergence, which is a concept rather than a reusable resource.",
      "processing_time": 56.454352140426636,
      "citing_paper_id": "270764846",
      "cited_paper_id": 117000403
    },
    {
      "context_text": "Definition 1 ( f -divergence [1, 10, 11]) .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a definition of f-divergence, which is a concept rather than a reusable resource.",
      "processing_time": 56.454352140426636,
      "citing_paper_id": "270764846",
      "cited_paper_id": null
    },
    {
      "context_text": "For any f -divergence satisfying one of the following conditions: (i) f is not a barrier function; (ii) I f is Reverse KL-divergence; (iii) f is a strong-barrier function, with finite roots of It is commonly adopted in previous studies [65, 42] that the network receives the same inputs z 0 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only conditions for f-divergences and network inputs. No verifiable resources are identified.",
      "processing_time": 55.03678822517395,
      "citing_paper_id": "270764846",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "1) The analysis is primarily based on tabular setting [55], not taking function approximation error into consideration.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a general setting which is not a verifiable resource.",
      "processing_time": 55.113588094711304,
      "citing_paper_id": "270764846",
      "cited_paper_id": 219708846
    },
    {
      "context_text": "Naive methods such as prompt adjustment for particular styles [6, 38] fail to provide precise control over the nuanced weighting of output characteristics [67].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the limitations of naive methods for style control in text generation.",
      "processing_time": 55.32975721359253,
      "citing_paper_id": "270764846",
      "cited_paper_id": 232290492
    },
    {
      "context_text": "Kumar et al. [24] view this task as maximizing log p ( y ) while satisfying some constraints, and use simultaneous gradient descent to solve it.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for controlled text generation.",
      "processing_time": 53.49565100669861,
      "citing_paper_id": "270764846",
      "cited_paper_id": 236912674
    },
    {
      "context_text": "2) Decoding from a response-level probability distribution at the token level may lead to degraded performance, which is likely to be alleviated by energy-based approaches [37, 24, 58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological approaches to text generation.",
      "processing_time": 53.487388610839844,
      "citing_paper_id": "270764846",
      "cited_paper_id": 236912674
    },
    {
      "context_text": "2) Decoding from a response-level probability distribution at the token level may lead to degraded performance, which is likely to be alleviated by energy-based approaches [37, 24, 58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological approaches to text generation.",
      "processing_time": 53.487388610839844,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269430591
    },
    {
      "context_text": "Despite the competitive results exhibited in prior works [51, 40, 21], this theorem reveals that parameter-merging lacks a theoretical guarantee in practical scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses parameter-merging in models.",
      "processing_time": 53.333131074905396,
      "citing_paper_id": "270764846",
      "cited_paper_id": 247362886
    },
    {
      "context_text": "Despite the competitive results exhibited in prior works [51, 40, 21], this theorem reveals that parameter-merging lacks a theoretical guarantee in practical scenarios.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses parameter-merging in models.",
      "processing_time": 53.333131074905396,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 54.888407945632935,
      "citing_paper_id": "270764846",
      "cited_paper_id": 247362886
    },
    {
      "context_text": "This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 54.888407945632935,
      "citing_paper_id": "270764846",
      "cited_paper_id": 257505247
    },
    {
      "context_text": "This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 54.888407945632935,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 54.888407945632935,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267068615
    },
    {
      "context_text": "This line of work [40, 21, 27], represented by rewarded soups (RS), aims at providing a training-free solution which obtains weights of the policy as a linear combination of weights of trained policies for each single objective, inspired by [51] and its other applications [41, 26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 54.888407945632935,
      "citing_paper_id": "270764846",
      "cited_paper_id": null
    },
    {
      "context_text": "The expected calibration error of a stochastic policy π is defined as Hypothesis 1 (Reduced reward mis-specification [51, 40, 21]) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a hypothesis about reduced reward mis-specification. No verifiable resources are identified.",
      "processing_time": 54.45891880989075,
      "citing_paper_id": "270764846",
      "cited_paper_id": 247362886
    },
    {
      "context_text": "The expected calibration error of a stochastic policy π is defined as Hypothesis 1 (Reduced reward mis-specification [51, 40, 21]) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a hypothesis about reduced reward mis-specification. No verifiable resources are identified.",
      "processing_time": 54.45891880989075,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "In addition, there are other efforts exploring alternative objectives and frameworks: SLiC-HF [61, 60] refer to the alignment process as sequence likelihood calibration; SPIN [8] iteratively improves the model by leveraging synthetically generated data, thereby circumventing the need for human…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing different frameworks and objectives in personalized text generation.",
      "processing_time": 54.586430311203,
      "citing_paper_id": "270764846",
      "cited_paper_id": 252683988
    },
    {
      "context_text": "To achieve this goal, energy-based methods are adopted in many works [37, 25], which involves continuous optimization for LMs to obtain gradient information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on energy-based methods and gradient information in language models.",
      "processing_time": 54.75084209442139,
      "citing_paper_id": "270764846",
      "cited_paper_id": 253397955
    },
    {
      "context_text": "Since MOD does not yield a sampling policy, which make it impossible to directly measure KL ( ·∥ π ref ) as prior work [52], we demonstrate example generations in Appendix F.6 Moreover, we can apply not-all-positive preference weightings w ∈ R M as long as =1 w i = 1 , thus allowing us to optimize…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It discusses a technical aspect of the research methodology.",
      "processing_time": 54.301377296447754,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Some efforts (e.g., MORLHF [52, 3] MODPO [62]) match varying personal preferences 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is too vague to identify any verifiable resources.",
      "processing_time": 54.43047094345093,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Some efforts (e.g., MORLHF [52, 3] MODPO [62]) match varying personal preferences 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context is too vague to identify any verifiable resources.",
      "processing_time": 54.43047094345093,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "MORLHF [52] optimizes for the weighted multi-objective reward function =1 w i · R i using PPO, with the same configurations as training for single objective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PPO) and a concept (multi-objective reward function).",
      "processing_time": 54.453696966171265,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Learning from human feedback [36, 35] has gained significant attention due to its potential for using human-labeled datasets to align language models to human preferences [42, 52, 39, 8, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of 'human-labeled datasets' which is too generic.",
      "processing_time": 54.26978516578674,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Learning from human feedback [36, 35] has gained significant attention due to its potential for using human-labeled datasets to align language models to human preferences [42, 52, 39, 8, 60].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of 'human-labeled datasets' which is too generic.",
      "processing_time": 54.26978516578674,
      "citing_paper_id": "270764846",
      "cited_paper_id": 266725672
    },
    {
      "context_text": "This motivating experiment is based on FineGrainedRLHF [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The title of the cited paper confirms it is about a method for training language models using human feedback.",
      "processing_time": 55.91887903213501,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Free from Free from Requirement trained LLMs RM prompting MORLHF [52, 3] # preferences ✗ ✔ MODPO [62] # preferences ✔ ✔ DPA [48], CPO [18], RiC [56] 1 ✗ ✗ RS [40, 21] # objectives ✔ ✔ same arch.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.04204535484314,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Free from Free from Requirement trained LLMs RM prompting MORLHF [52, 3] # preferences ✗ ✔ MODPO [62] # preferences ✔ ✔ DPA [48], CPO [18], RiC [56] 1 ✗ ✗ RS [40, 21] # objectives ✔ ✔ same arch.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.04204535484314,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Free from Free from Requirement trained LLMs RM prompting MORLHF [52, 3] # preferences ✗ ✔ MODPO [62] # preferences ✔ ✔ DPA [48], CPO [18], RiC [56] 1 ✗ ✗ RS [40, 21] # objectives ✔ ✔ same arch.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.04204535484314,
      "citing_paper_id": "270764846",
      "cited_paper_id": 268063181
    },
    {
      "context_text": "Free from Free from Requirement trained LLMs RM prompting MORLHF [52, 3] # preferences ✗ ✔ MODPO [62] # preferences ✔ ✔ DPA [48], CPO [18], RiC [56] 1 ✗ ✗ RS [40, 21] # objectives ✔ ✔ same arch.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.04204535484314,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "Alignment with KL-divergence regularization has been established as a standard formulation [36, 42, 52, 39, 53, 57].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of a reusable resource or dataset being used.",
      "processing_time": 54.7254741191864,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "For instance, dialogue agents need to trade off between helpfulness and harmlessness [3, 22], while question-answering systems can have attributes of relevance, verbosity, and completeness [52].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general attributes of dialogue agents and question-answering systems. No verifiable resources are identified.",
      "processing_time": 55.077958822250366,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "The most natural approach to solve multi-objective alignment is to retrain for a linearly combined multiple reward functions (MORLHF [52, 3]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The term 'MORLHF' is likely an acronym for a method or framework, not a dataset.",
      "processing_time": 55.859843492507935,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "The widely used RLHF (PPO) approach [36, 42, 52] optimizes over rewards with Reverse KL-divergence as a penalty, where the reward models are learned from human preference datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'human preference datasets' but does not specify a particular dataset name. The term is too generic and lacks a specific identifier.",
      "processing_time": 54.67337727546692,
      "citing_paper_id": "270764846",
      "cited_paper_id": 259064099
    },
    {
      "context_text": "Definition 3 (Expected calibration error [17, 47]) .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (Expected calibration error). No datasets are referenced or used in the context provided.",
      "processing_time": 54.34068560600281,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "…differentiable and strongly convex on R + , we can obtain a closed-form bijection between any single-objective aligned LM π i and the corresponding R i as shown below (initially proposed in [47], see detailed proof in Lemma 1): where Z i ( x ) is the normalization factor with respect to x .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a mathematical concept and a reference to a proof. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.88829231262207,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "Ψ -PO [2] further modifies the reward term to be optimized as other mappings from preference pairs; f-DPO [47] replaces Reverse KL-divergence with other divergence measures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and modifications to optimization techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.909424781799316,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "Ψ -PO [2] further modifies the reward term to be optimized as other mappings from preference pairs; f-DPO [47] replaces Reverse KL-divergence with other divergence measures.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and modifications to optimization techniques. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 54.909424781799316,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264288854
    },
    {
      "context_text": "We acknowledge that commonly used f -divergence measures have been introduced in [47] and show them here for completeness: Here we show the optimal sampling policies for multi-objective w.r.t. these divergence measures: Divergence measure Optimal policy Reverse KL-divergence And we show the optimal…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only divergence measures and policies. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.277771949768066,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "…by Legendre transform in convex optimization [34], which allows us to derive a closed-form solution from a family of f -divergence regularized optimization approaches [9, 39, 47] (e.g., PPO, DPO are optimizing for the reward function with KL-divergence penalty), and its efficient approximation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization methods and concepts. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.87150430679321,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "(17), (19), (21)) Finally we have Lemma 3 (Theorem 2 in [47]) .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references lemmas and theorems from other papers.",
      "processing_time": 53.71248388290405,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "Recently, a sequential line of work [47, 43] has proposed replacing Reverse KL-divergence with a set of f -divergences such as Forward KL-divergence, JSD, and α -divergence, which they claim can enhance generation diversity and decrease the expected calibration error [17] empirically.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses divergence measures and their impact on generation diversity and calibration error.",
      "processing_time": 53.33548450469971,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "Recently, a sequential line of work [47, 43] has proposed replacing Reverse KL-divergence with a set of f -divergences such as Forward KL-divergence, JSD, and α -divergence, which they claim can enhance generation diversity and decrease the expected calibration error [17] empirically.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses divergence measures and their impact on generation diversity and calibration error.",
      "processing_time": 53.33548450469971,
      "citing_paper_id": "270764846",
      "cited_paper_id": 268358469
    },
    {
      "context_text": "Extending the results of [47] to the multi-objective setting, we prove the necessity of f being barrier functions to find an optimal policy π ⋆ for multi-objective alignment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and methods.",
      "processing_time": 52.28388023376465,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "The lemma is revealed by Theorem 1 in [47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a theorem, which is not a verifiable resource.",
      "processing_time": 53.73334813117981,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "Additionally, the same as a general barrier in interior point methods [34], it obviates the need for introducing slack variables as in [47].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or resources. It refers to methodological aspects of optimization techniques.",
      "processing_time": 53.31467628479004,
      "citing_paper_id": "270764846",
      "cited_paper_id": 263142109
    },
    {
      "context_text": "We reveal the sub-optimality of the parameter-merging paradigm [40, 21] under a common setting, showing that for most f -divergence regularization, including the commonly-used KL-divergence, the optimal policy is not guaranteed to lie in the interpolation region of the weights of base policies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a methodological approach and theoretical findings.",
      "processing_time": 52.555808782577515,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "The optimality of the parameter-merging paradigm [40, 21] primarily relies on reduced reward mis-specification hypothesis (see Hypothesis 1 in Appendix D.1 for definition).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a hypothesis and references to other works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.31660175323486,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "The latter two paradigms are more efficient, while relying heavily on either the reduced mis-specification hypothesis [40] or unguaranteed OOD generalization ability [63], posing challenges in terms of interpretability and robustness.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses paradigms and their challenges, which are not directly linked to any verifiable resource.",
      "processing_time": 55.02121043205261,
      "citing_paper_id": "270764846",
      "cited_paper_id": 264812661
    },
    {
      "context_text": "In practice, the number of objectives is easily enumerable (e.g.,   5 in [50, 12]), making it feasible to finetune an LM for each objective.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to objectives. The cited paper title suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 54.5297212600708,
      "citing_paper_id": "270764846",
      "cited_paper_id": 265220723
    },
    {
      "context_text": "Additional experiments on the HelpSteer [50] task are provided in Appendix F.4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "HelpSteer"
      ],
      "dataset_descriptions": {
        "HelpSteer": "Used to conduct additional experiments on multi-attribute helpfulness, focusing on the effectiveness of SteerLM in generating personalized text."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'HelpSteer' in the context of additional experiments, and the title confirms it is a dataset.",
      "processing_time": 60.573920488357544,
      "citing_paper_id": "270764846",
      "cited_paper_id": 265220723
    },
    {
      "context_text": "…efforts exploring alternative objectives and frameworks: SLiC-HF [61, 60] refer to the alignment process as sequence likelihood calibration; SPIN [8] iteratively improves the model by leveraging synthetically generated data, thereby circumventing the need for human feedback; OPO [54] employs…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing different techniques and frameworks for improving language models.",
      "processing_time": 53.71628165245056,
      "citing_paper_id": "270764846",
      "cited_paper_id": 266725672
    },
    {
      "context_text": "Proxy-tuning [28] & jail-breaking [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 53.99734306335449,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "Proxy-tuning [28] & jail-breaking [59].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 53.99734306335449,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "Moreover, it is worth noting that, our method can be applied as a lightweight approach for large-scale models, as a multi-objective extension of proxy-tuning [28].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the application of a method for large-scale models.",
      "processing_time": 53.64797353744507,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "The resulting method extends prior work employing logit arithmetic for decoding-time alignment [28, 59, 19, 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and prior work. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.306291341781616,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "The resulting method extends prior work employing logit arithmetic for decoding-time alignment [28, 59, 19, 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and prior work. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.306291341781616,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "The resulting method extends prior work employing logit arithmetic for decoding-time alignment [28, 59, 19, 30], but we are the first to successfully achieve decoding towards multiple objectives simultaneously.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and prior work. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.306291341781616,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269187650
    },
    {
      "context_text": "Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28, 59, 19, 30, 62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about explaining existing tuning approaches and decoding strategies, which are methods rather than datasets.",
      "processing_time": 54.97024345397949,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28, 59, 19, 30, 62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about explaining existing tuning approaches and decoding strategies, which are methods rather than datasets.",
      "processing_time": 54.97024345397949,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28, 59, 19, 30, 62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about explaining existing tuning approaches and decoding strategies, which are methods rather than datasets.",
      "processing_time": 54.97024345397949,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269187650
    },
    {
      "context_text": "Notably, our comprehensive framework generalizes and explains many existing tuning approaches and decoding strategies [28, 59, 19, 30, 62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about explaining existing tuning approaches and decoding strategies, which are methods rather than datasets.",
      "processing_time": 54.97024345397949,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "For example, proxy-tuning [28] tunes only a smaller LM, then applies the difference between the logits of the small tuned and untuned LMs to shift the predictions of a larger untuned model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'proxy-tuning'. No verifiable resources are identified.",
      "processing_time": 53.29937720298767,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "We provide insights on this line of work [28, 59, 19] and derivations of some other related works [30, 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to related works and their derivations.",
      "processing_time": 53.47358012199402,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "We provide insights on this line of work [28, 59, 19] and derivations of some other related works [30, 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to related works and their derivations.",
      "processing_time": 53.47358012199402,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "We provide insights on this line of work [28, 59, 19] and derivations of some other related works [30, 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to related works and their derivations.",
      "processing_time": 53.47358012199402,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269187650
    },
    {
      "context_text": "We provide insights on this line of work [28, 59, 19] and derivations of some other related works [30, 62] in Appendix C.3, further demonstrating the potential for universally applying our approach.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It only refers to related works and their derivations.",
      "processing_time": 53.47358012199402,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "…guide the decoding process; DeRa [30] works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28, 59, 19] tunes a small model and applies it to steer a larger base model by operating on logits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 53.897706031799316,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "…guide the decoding process; DeRa [30] works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28, 59, 19] tunes a small model and applies it to steer a larger base model by operating on logits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 53.897706031799316,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "…guide the decoding process; DeRa [30] works on hyperparameter re-alignment and proposes the potential of a special case of MOD, while introducing a per-token distribution approximation; proxy-tuning [28, 59, 19] tunes a small model and applies it to steer a larger base model by operating on logits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 53.897706031799316,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269187650
    },
    {
      "context_text": "And this is exactly the proxy-tuning approach, validated by extensive experiments in [28].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The title 'Tuning Language Models by Proxy' suggests a methodological focus rather than a dataset.",
      "processing_time": 55.38625407218933,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267028120
    },
    {
      "context_text": "This phenomenon indicates that we do not even need to specifically tune an unsafe model as in [59], since the knowledge of −R is indeed learned when being tuned for R .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper discussing model tuning. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.26494336128235,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "Reversing the position of P M + and P M − yields jail-breaking [59]. δ -unlearning [19] is the same.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or techniques. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 54.30617618560791,
      "citing_paper_id": "270764846",
      "cited_paper_id": 267320277
    },
    {
      "context_text": "Reversing the position of P M + and P M − yields jail-breaking [59]. δ -unlearning [19] is the same.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or techniques. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 54.30617618560791,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269187650
    },
    {
      "context_text": "This concept is reflected in LMs alignment as preference-conditioned prompting: this line of work [56, 48, 18] directly presents the preference weightings in prompts after a finetuning process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving preference-conditioned prompting in language models.",
      "processing_time": 53.12683129310608,
      "citing_paper_id": "270764846",
      "cited_paper_id": 268063181
    },
    {
      "context_text": "(6) reduces to a simple token-wise decoding rule: equivalent to linearly combining logits [32, 30] of each model with preference weightings.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.467321157455444,
      "citing_paper_id": "270764846",
      "cited_paper_id": 269188153
    },
    {
      "context_text": "MODPO [62] uses π 1 ’s output as an implicit reward signal of R 1 and inserts it into the DPO objective for R 2 to optimize for w 1 R 1 + w 2 R 2 , with the same configurations as training for single objective.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach called MODPO. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 54.19896388053894,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "MODPO [62].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not provide enough context to identify a specific dataset. The title suggests a method or approach rather than a dataset.",
      "processing_time": 53.483322620391846,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "And MODPO [62] retrains the model in a reward-model-free way, by learning a flexible reward representation and directly training on a fixed preference dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "fixed preference dataset"
      ],
      "dataset_descriptions": {
        "fixed preference dataset": "Used to train a reward representation in a reward-model-free manner, focusing on multi-objective direct preference optimization for language models."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'fixed preference dataset' which is a plausible dataset name, but lacks specificity. The title does not provide additional clarity.",
      "processing_time": 61.07388114929199,
      "citing_paper_id": "270764846",
      "cited_paper_id": 270227321
    },
    {
      "context_text": "Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers that inspired work on dialogue generation.",
      "processing_time": 53.127012968063354,
      "citing_paper_id": "224705337",
      "cited_paper_id": 780171
    },
    {
      "context_text": "Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers that inspired work on dialogue generation.",
      "processing_time": 53.127012968063354,
      "citing_paper_id": "224705337",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers that inspired work on dialogue generation.",
      "processing_time": 53.127012968063354,
      "citing_paper_id": "224705337",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "We exploit the policy-gradient method (Sutton et al., 2000) to continue-train g ( U, D ) by which g ( U, D ) is further “supervised” by the GPT-2 model and is directly optimized for a target metric (e.g., F1 in the experiments).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (policy-gradient) and a model (GPT-2).",
      "processing_time": 53.46651029586792,
      "citing_paper_id": "224705337",
      "cited_paper_id": 1211821
    },
    {
      "context_text": "We exploit the policy-gradient method (Sutton et al., 2000) to continue-train g(U,D) by which g(U,D) is further “supervised” by the GPT-2 model and is directly optimized for a target metric (e.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (policy-gradient) and a model (GPT-2).",
      "processing_time": 53.49963116645813,
      "citing_paper_id": "224705337",
      "cited_paper_id": 1211821
    },
    {
      "context_text": "R̃i = R(D̃i)− b, (6) where R(D̃i) = Sim(r′ i, ri) with r ′ i the response generated by the GPT-2 model given Ui and D̃i, and b = ∑N i=1R(D̃i)/N is the baseline that is used to reduce the variance of gradient estimation(Clark and Manning, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GPT-2) and a baseline calculation. No verifiable resources are identified.",
      "processing_time": 54.33459544181824,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2012188
    },
    {
      "context_text": "…to ¯ D at each time step, and deﬁne the loss function as where R ( ˜ D i ) = Sim( r i , r i ) with r i the response generated by the GPT-2 model given U i and ˜ D i , and b = =1 R ( ˜ D i ) /N is the baseline that is used to reduce the variance of gradient estimation(Clark and Manning, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GPT-2) and a technique (Deep Reinforcement Learning).",
      "processing_time": 54.05824065208435,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2012188
    },
    {
      "context_text": "…of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some speciﬁc personas (Li et al., 2016; Zhang…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It primarily cites papers for methods and approaches in conversation generation and persona-based response generation.",
      "processing_time": 54.49315810203552,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2024574
    },
    {
      "context_text": ", 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to controlling attributes of responses and biasing responses to specific personas. No verifiable resources are named.",
      "processing_time": 55.07551884651184,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2024574
    },
    {
      "context_text": ", 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some specific personas (Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to controlling attributes of responses and biasing responses to specific personas. No verifiable resources are named.",
      "processing_time": 55.07551884651184,
      "citing_paper_id": "224705337",
      "cited_paper_id": 67855999
    },
    {
      "context_text": "…contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some speciﬁc personas (Li et al., 2016; Zhang et al., 2018b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches for personalizing dialogue systems.",
      "processing_time": 53.349451541900635,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "…contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some speciﬁc personas (Li et al., 2016; Zhang et al., 2018b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches for personalizing dialogue systems.",
      "processing_time": 53.349451541900635,
      "citing_paper_id": "224705337",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "…contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019); and to bias responses to some speciﬁc personas (Li et al., 2016; Zhang et al., 2018b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches for personalizing dialogue systems.",
      "processing_time": 53.349451541900635,
      "citing_paper_id": "224705337",
      "cited_paper_id": 67855999
    },
    {
      "context_text": ", 2019); and to bias responses to some specific personas (Li et al., 2016; Zhang et al., 2018b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing persona-based models and biases in responses.",
      "processing_time": 53.563305616378784,
      "citing_paper_id": "224705337",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 55.68091917037964,
      "citing_paper_id": "224705337",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 55.68091917037964,
      "citing_paper_id": "224705337",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 55.68091917037964,
      "citing_paper_id": "224705337",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al., 2020), building an open domain dialogue system with data-driven approaches has attracted…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 56.14402985572815,
      "citing_paper_id": "224705337",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al., 2020), building an open domain dialogue system with data-driven approaches has attracted…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 56.14402985572815,
      "citing_paper_id": "224705337",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "With advances in neural machine learning (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al., 2020), building an open domain dialogue system with data-driven approaches has attracted…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in neural machine learning and the availability of human conversations on social media. No verifiable resource names are provided.",
      "processing_time": 56.14402985572815,
      "citing_paper_id": "224705337",
      "cited_paper_id": 13756489
    },
    {
      "context_text": ", 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'extracted from visual background' which suggests the use of datasets containing images and dialogues. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 56.36821126937866,
      "citing_paper_id": "224705337",
      "cited_paper_id": 5065277
    },
    {
      "context_text": ", 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'extracted from visual background' which suggests the use of datasets containing images and dialogues. However, no specific dataset names are mentioned in the citation span.",
      "processing_time": 56.36821126937866,
      "citing_paper_id": "224705337",
      "cited_paper_id": 53297976
    },
    {
      "context_text": "…knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and approaches but does not specify any named datasets. The cited papers' titles do not provide additional clarity on specific datasets.",
      "processing_time": 55.020078897476196,
      "citing_paper_id": "224705337",
      "cited_paper_id": 5065277
    },
    {
      "context_text": "…knowledge graphs (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019), retrieved from unstructured documents (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020), or extracted from visual background (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various methods and approaches but does not specify any named datasets. The cited papers' titles do not provide additional clarity on specific datasets.",
      "processing_time": 55.020078897476196,
      "citing_paper_id": "224705337",
      "cited_paper_id": 53297976
    },
    {
      "context_text": "All models are learned with Adam (Kingma and Ba, 2015) optimizer with β 1 = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 54.112180948257446,
      "citing_paper_id": "224705337",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "All models are learned with Adam (Kingma and Ba, 2015) optimizer with β1 = 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 53.93420696258545,
      "citing_paper_id": "224705337",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "…et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.498600244522095,
      "citing_paper_id": "224705337",
      "cited_paper_id": 14247119
    },
    {
      "context_text": ", 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.653218269348145,
      "citing_paper_id": "224705337",
      "cited_paper_id": 14247119
    },
    {
      "context_text": ", 2018); to model the structure of conversation contexts (Serban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 53.653218269348145,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "Later, the vanilla encoder-decoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets within the text.",
      "processing_time": 54.29280471801758,
      "citing_paper_id": "224705337",
      "cited_paper_id": 14247119
    },
    {
      "context_text": "…2019; Ren et al., 2019) and thus is incompatible with the pre-trained language models, or it is learned with human annotations (Dinan et al., 2019; Kim et al., 2018) which are difﬁcult to obtain in practice (e.g., the dataset in (Zhou et al., 2018b) does not contain annotations for knowledge…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a dataset from Zhou et al., 2018b, but does not provide a specific name. The reference is too vague to identify a specific dataset.",
      "processing_time": 56.38335418701172,
      "citing_paper_id": "224705337",
      "cited_paper_id": 44132329
    },
    {
      "context_text": "…vanilla encoder-decoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and architectures. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.8474657535553,
      "citing_paper_id": "224705337",
      "cited_paper_id": 51609155
    },
    {
      "context_text": "Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Serban et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and architectures. There are no clear identifiers for datasets.",
      "processing_time": 54.0606107711792,
      "citing_paper_id": "224705337",
      "cited_paper_id": 51609155
    },
    {
      "context_text": "We choose BERT (Devlin et al., 2018) as the backbone of the encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 55.26491451263428,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Thus, the encoder can take advantage of pre-training, and the multi-layer bi-directional attention mechanism in BERT allows a dialogue context and the associated knowledge to sufﬁciently interact with each other, resulting in context-aware knowledge representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for pre-training and context-aware knowledge representations.",
      "processing_time": 53.84878134727478,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We choose BERT (110M) and GPT-2 (117M) as the pre-trained language models in KnowledGPT, and implement the models with the code in https://github.com/huggingface/ transformers .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained language models and a code repository. No verifiable datasets are referenced.",
      "processing_time": 54.29116630554199,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and benchmarks but does not specify any particular datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 54.8203821182251,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and benchmarks but does not specify any particular datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 54.8203821182251,
      "citing_paper_id": "224705337",
      "cited_paper_id": 146808476
    },
    {
      "context_text": "Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and benchmarks but does not specify any particular datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 54.8203821182251,
      "citing_paper_id": "224705337",
      "cited_paper_id": 147704286
    },
    {
      "context_text": "Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models and benchmarks but does not specify any particular datasets. The cited papers are about models and methods, not datasets.",
      "processing_time": 54.8203821182251,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195069387
    },
    {
      "context_text": "Each S i ∈ S passes through the stacked self-attention layers, and is ﬁ-nally represented as e i = CLS(BERT( S i )) where BERT( S i ) refers to the sequence of vectors from the last layer of the encoder and CLS( · ) is a function that returns the ﬁrst vector of the sequence (i.e., the vector corresponding to the [CLS] to-ken).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT, which is a model and not a dataset.",
      "processing_time": 54.053410053253174,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Speciﬁcally, we build the knowledge selection module on the basis of BERT, and formalize knowledge selection as a sequence prediction process, by which the model can take advantage of the pre-training techniques and dynamically determine the relevant knowledge for a given context.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for knowledge selection. BERT is a model, not a dataset.",
      "processing_time": 54.82344388961792,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "…on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss improvements in natural language understanding and generation. No verifiable datasets are named.",
      "processing_time": 55.2445170879364,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We choose BERT (Devlin et al., 2018) as the back-bone of the encoder.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. BERT is a model, not a dataset, and thus should not be included.",
      "processing_time": 55.238900899887085,
      "citing_paper_id": "224705337",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "In the context of dialogue generation, by ﬁne-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense question-answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'social media data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 54.744805574417114,
      "citing_paper_id": "224705337",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "In the context of dialogue generation, by ﬁne-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense question-answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'social media data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 54.744805574417114,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "In the context of dialogue generation, by ﬁne-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense question-answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'social media data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 54.744805574417114,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "In the context of dialogue generation, by ﬁne-tuning GPT-2 (Radford et al., 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense question-answering.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'social media data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 54.744805574417114,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": ", 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense questionanswering.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to social media data and recent work on conversation engagement and commonsense question answering.",
      "processing_time": 54.980128049850464,
      "citing_paper_id": "224705337",
      "cited_paper_id": 59222757
    },
    {
      "context_text": ", 2019) in different sizes on social media data, recent work has (Zhang et al., 2019c; Wolf et al., 2019) shown promising progress on conversation engagement and commonsense questionanswering.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general references to social media data and recent work on conversation engagement and commonsense question answering.",
      "processing_time": 54.980128049850464,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "…are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019; Sun et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers and their contributions to NLP and interdisciplinary applications. No verifiable resources are identified.",
      "processing_time": 55.507784605026245,
      "citing_paper_id": "224705337",
      "cited_paper_id": 85459677
    },
    {
      "context_text": "…2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in NLP and computer…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and their impacts on NLP applications. No verifiable resources are identified.",
      "processing_time": 55.26608729362488,
      "citing_paper_id": "224705337",
      "cited_paper_id": 85459677
    },
    {
      "context_text": ", 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only references to various research works and their impacts on NLP and interdisciplinary applications.",
      "processing_time": 54.15459084510803,
      "citing_paper_id": "224705337",
      "cited_paper_id": 85459677
    },
    {
      "context_text": ", 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only references to various research works and their impacts on NLP and interdisciplinary applications.",
      "processing_time": 54.15459084510803,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": ", 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only references to various research works and their impacts on NLP and interdisciplinary applications.",
      "processing_time": 54.15459084510803,
      "citing_paper_id": "224705337",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Each response receives 3 scores per aspect, and the agreement among the annotators is measured via Fleiss’ kappa (Fleiss, 1971).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for measuring agreement among raters.",
      "processing_time": 52.76391863822937,
      "citing_paper_id": "224705337",
      "cited_paper_id": 143544759
    },
    {
      "context_text": "…language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various benchmarks and models. The context is about the impact of these models on NLP applications, not the use of specific datasets.",
      "processing_time": 56.90709567070007,
      "citing_paper_id": "224705337",
      "cited_paper_id": 146808476
    },
    {
      "context_text": "…and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers and benchmarks. The context is too generic to identify specific, verifiable datasets.",
      "processing_time": 55.042930364608765,
      "citing_paper_id": "224705337",
      "cited_paper_id": 147704286
    },
    {
      "context_text": "While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about language models. No dataset names are present in the citation span.",
      "processing_time": 54.16101551055908,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about language models. No dataset names are present in the citation span.",
      "processing_time": 54.16101551055908,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "While the giant language models can memorize enough patterns in language during pre-training, they only capture “average” semantics of the data (Zhang et al., 2019c).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about language models. No dataset names are present in the citation span.",
      "processing_time": 54.16101551055908,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "…2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and their impacts on NLP applications. No verifiable resources are identified.",
      "processing_time": 55.045711040496826,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "…2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and their impacts on NLP applications. No verifiable resources are identified.",
      "processing_time": 55.045711040496826,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "…2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in NLP and computer vision (Lu et al., 2019; Su et al., 2019;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and their impacts on NLP applications. No verifiable resources are identified.",
      "processing_time": 55.045711040496826,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "The model is implemented based on the code shared at https://github.com/microsoft/DialoGPT .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a GitHub repository for code implementation, which is not a dataset. No specific datasets are mentioned in the citation context.",
      "processing_time": 53.94926738739014,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "The model is implemented based on the code shared at https://github.com/microsoft/DialoGPT .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a GitHub repository for code implementation, which is not a dataset. No specific datasets are mentioned in the citation context.",
      "processing_time": 53.94926738739014,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "The model is implemented based on the code shared at https://github.com/microsoft/DialoGPT .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a GitHub repository for code implementation, which is not a dataset. No specific datasets are mentioned in the citation context.",
      "processing_time": 53.94926738739014,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "We discard the associated knowledge and ﬁne-tune DialoGPT on the knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions fine-tuning DialoGPT on knowledge-grounded dialogues, but does not specify a named dataset. DialoGPT is a model, not a dataset.",
      "processing_time": 56.68261742591858,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "We discard the associated knowledge and ﬁne-tune DialoGPT on the knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions fine-tuning DialoGPT on knowledge-grounded dialogues, but does not specify a named dataset. DialoGPT is a model, not a dataset.",
      "processing_time": 56.68261742591858,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "We discard the associated knowledge and ﬁne-tune DialoGPT on the knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions fine-tuning DialoGPT on knowledge-grounded dialogues, but does not specify a named dataset. DialoGPT is a model, not a dataset.",
      "processing_time": 56.68261742591858,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "DialoGPT I think it’s worth it.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific information about datasets used. The mention 'DialoGPT' refers to a model, not a dataset.",
      "processing_time": 54.83368921279907,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "DialoGPT I think it’s worth it.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific information about datasets used. The mention 'DialoGPT' refers to a model, not a dataset.",
      "processing_time": 54.83368921279907,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "DialoGPT I think it’s worth it.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific information about datasets used. The mention 'DialoGPT' refers to a model, not a dataset.",
      "processing_time": 54.83368921279907,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "…al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in various papers. No verifiable resources are identified.",
      "processing_time": 54.35360622406006,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "…al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in various papers. No verifiable resources are identified.",
      "processing_time": 54.35360622406006,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "…al., 2017a; Zhao et al., 2017; Tao et al., 2018); to model the structure of conversation contexts (Ser-ban et al., 2016, 2017; Xing et al., 2017b; Zhang et al., 2019a); to control attributes of responses (Xu et al., 2019; Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in various papers. No verifiable resources are identified.",
      "processing_time": 54.35360622406006,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the conversation going.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-rich responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are about models, not datasets.",
      "processing_time": 62.19151306152344,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the conversation going.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-rich responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are about models, not datasets.",
      "processing_time": 62.19151306152344,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the conversation going.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-rich responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are about models, not datasets.",
      "processing_time": 62.19151306152344,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-specific responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are models (DialoGPT) and not datasets.",
      "processing_time": 63.08198022842407,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-specific responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are models (DialoGPT) and not datasets.",
      "processing_time": 63.08198022842407,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "Table 1: An example from the test set (Test Seen) of Wizard of Wikipedia (Dinan et al., 2019) . such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry speciﬁc content for keeping the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wizard of Wikipedia"
      ],
      "dataset_descriptions": {
        "Wizard of Wikipedia": "Used to evaluate conversational response generation, focusing on generating contextually relevant and content-specific responses in multi-turn dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Wizard of Wikipedia' as a test set, which is a specific dataset used for evaluating dialogue systems. The other mentions are models (DialoGPT) and not datasets.",
      "processing_time": 63.08198022842407,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "In CMU DoG the gap between DialoGPT and KnowledGPT is narrowed because about 35% of the conversation has a weak correlation with the document (e.g. BLEU < 0.1).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and metrics. The citation is discussing the performance of models rather than the use of a dataset.",
      "processing_time": 55.26371192932129,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "In CMU DoG the gap between DialoGPT and KnowledGPT is narrowed because about 35% of the conversation has a weak correlation with the document (e.g. BLEU < 0.1).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and metrics. The citation is discussing the performance of models rather than the use of a dataset.",
      "processing_time": 55.26371192932129,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "In CMU DoG the gap between DialoGPT and KnowledGPT is narrowed because about 35% of the conversation has a weak correlation with the document (e.g. BLEU < 0.1).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and metrics. The citation is discussing the performance of models rather than the use of a dataset.",
      "processing_time": 55.26371192932129,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "We compare KnowledGPT and with DialoGPT in order to learn if a pre-trained generation model with state-of-the-art performance on open domain dialogues is already good enough when it is ﬁne-tuned with knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (DialoGPT and KnowledGPT). The citation intent is to compare these models, not to reference a dataset.",
      "processing_time": 56.86876177787781,
      "citing_paper_id": "224705337",
      "cited_paper_id": 155100086
    },
    {
      "context_text": "We compare KnowledGPT and with DialoGPT in order to learn if a pre-trained generation model with state-of-the-art performance on open domain dialogues is already good enough when it is ﬁne-tuned with knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (DialoGPT and KnowledGPT). The citation intent is to compare these models, not to reference a dataset.",
      "processing_time": 56.86876177787781,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "We compare KnowledGPT and with DialoGPT in order to learn if a pre-trained generation model with state-of-the-art performance on open domain dialogues is already good enough when it is ﬁne-tuned with knowledge-grounded dialogues.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (DialoGPT and KnowledGPT). The citation intent is to compare these models, not to reference a dataset.",
      "processing_time": 56.86876177787781,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "…corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks (Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and benchmarks. No verifiable datasets are identified.",
      "processing_time": 53.69656181335449,
      "citing_paper_id": "224705337",
      "cited_paper_id": 195069387
    },
    {
      "context_text": "Although some benchmarks built upon crowd-sourcing have been released by recent papers (Zhou et al., 2018b; Dinan et al., 2019; Gopalakrishnan et al., 2019), the small training size makes the generation models generalize badly on unseen topics (Dinan et al., 2019) and the cost of building such data…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions benchmarks built upon crowd-sourcing but does not specify any particular dataset names. It discusses the limitations of these benchmarks, particularly their small training sizes.",
      "processing_time": 55.87670564651489,
      "citing_paper_id": "224705337",
      "cited_paper_id": 202717047
    },
    {
      "context_text": "…2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019), and therefore are revolutionizing almost the full spectrum of NLP applications (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lam-ple and Conneau, 2019) and some interdisciplinary applications in…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and their impact on NLP applications. There are no clear identifiers for datasets.",
      "processing_time": 55.66919422149658,
      "citing_paper_id": "224705337",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "such as DialoGPT (Zhang et al., 2019c) have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (DialoGPT). No verifiable resources are identified.",
      "processing_time": 54.17390060424805,
      "citing_paper_id": "224705337",
      "cited_paper_id": 207869708
    },
    {
      "context_text": ", 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al., 2020), building an open domain dialogue system with data-driven approaches has attracted increasing attention from the community of artificial intelligence and natural language processing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'huge amount of human conversations on social media' but does not specify any named datasets. The reference to Adiwardana et al., 2020, suggests the use of social media data, but no specific dataset is named.",
      "processing_time": 59.2996506690979,
      "citing_paper_id": "224705337",
      "cited_paper_id": 210920238
    },
    {
      "context_text": "…(Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) and availability of the huge amount of human conversations on social media (Adiwardana et al., 2020), building an open domain dialogue system with data-driven approaches has attracted increasing attention from the community…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'huge amount of human conversations on social media' but does not specify a named dataset. It refers to a general type of data rather than a specific, verifiable dataset.",
      "processing_time": 57.04026460647583,
      "citing_paper_id": "224705337",
      "cited_paper_id": 210920238
    },
    {
      "context_text": "As a result, a recently proposed paradigm, prompt learning[6], further unifies the use of PLMs on different tasks in a simple yet flexible manner.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'prompt learning'. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.618284940719604,
      "citing_paper_id": "265308990",
      "cited_paper_id": 236493269
    },
    {
      "context_text": "For example, BERT4Rec[10] models sequential user behavior with a bidirectional self-attention network through Cloze task, and Transformer4Rec[11]adopts a haggingface transformer-based architecture as the base model for next-item prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the architectures and approaches used in the cited papers.",
      "processing_time": 55.06450605392456,
      "citing_paper_id": "265308990",
      "cited_paper_id": 237494589
    },
    {
      "context_text": "UP5[13] intends to solve unfairness problem by designing a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, which shows an improvement in eliminating unfairness across gender, age, and occupation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UP5) designed to enhance fairness in recommendation systems.",
      "processing_time": 53.918233156204224,
      "citing_paper_id": "265308990",
      "cited_paper_id": 258832356
    },
    {
      "context_text": "UP5[13] is a P5-based model, it includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributues and a prompt mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (UP5) and its components. No verifiable resources are identified.",
      "processing_time": 54.6901068687439,
      "citing_paper_id": "265308990",
      "cited_paper_id": 258832356
    },
    {
      "context_text": "Fine-tuning process will adjust the whole model parameters, for example, Petrov and Macdonald proposed GPTRec[12], which is a generative sequential recommendation model based on GPT-2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (GPTRec) and its base model (GPT-2).",
      "processing_time": 54.882750034332275,
      "citing_paper_id": "265308990",
      "cited_paper_id": 259203027
    },
    {
      "context_text": ", 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of Tweets from a codeswitching detection workshop (Molina et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TweetLID",
        "Tweets from a codeswitching detection workshop"
      ],
      "dataset_descriptions": {
        "TweetLID": "Used for tweet language identification, focusing on distinguishing languages in multilingual tweets.",
        "Tweets from a codeswitching detection workshop": "Used to detect code-switching in monolingual tweets, analyzing language mixing patterns."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions three specific datasets: 'TweetLID', 'Tweets from a codeswitching detection workshop'. These are clearly identified as datasets used in the research.",
      "processing_time": 67.96957659721375,
      "citing_paper_id": "19096382",
      "cited_paper_id": 24631
    },
    {
      "context_text": "The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of Tweets from a code-switching detection workshop (Molina et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter70",
        "TweetLID",
        "Tweets from a code-switching detection workshop"
      ],
      "dataset_descriptions": {
        "Twitter70": "Used to create a corpus for language identification, focusing on multilingual tweets and their linguistic features.",
        "TweetLID": "Used to create a corpus for language identification, specifically for evaluating tweet language identification systems.",
        "Tweets from a code-switching detection workshop": "Used to create a corpus for language identification, focusing on monolingual portions of tweets from a code-switching detection workshop."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used to create a corpus for language identification. These datasets are clearly named and have been used in the research context.",
      "processing_time": 74.46465873718262,
      "citing_paper_id": "19096382",
      "cited_paper_id": 24631
    },
    {
      "context_text": "The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of Tweets from a code-switching detection workshop (Molina et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter70",
        "TweetLID",
        "Tweets from a code-switching detection workshop"
      ],
      "dataset_descriptions": {
        "Twitter70": "Used to create a corpus for language identification, focusing on multilingual tweets and their linguistic features.",
        "TweetLID": "Used to create a corpus for language identification, specifically for evaluating tweet language identification systems.",
        "Tweets from a code-switching detection workshop": "Used to create a corpus for language identification, focusing on monolingual portions of tweets from a code-switching detection workshop."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used to create a corpus for language identification. These datasets are clearly named and have been used in the research context.",
      "processing_time": 74.46465873718262,
      "citing_paper_id": "19096382",
      "cited_paper_id": 14584778
    },
    {
      "context_text": "The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of Tweets from a code-switching detection workshop (Molina et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter70",
        "TweetLID",
        "Tweets from a code-switching detection workshop"
      ],
      "dataset_descriptions": {
        "Twitter70": "Used to create a corpus for language identification, focusing on multilingual tweets and their linguistic features.",
        "TweetLID": "Used to create a corpus for language identification, specifically for evaluating tweet language identification systems.",
        "Tweets from a code-switching detection workshop": "Used to create a corpus for language identification, focusing on monolingual portions of tweets from a code-switching detection workshop."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used to create a corpus for language identification. These datasets are clearly named and have been used in the research context.",
      "processing_time": 74.46465873718262,
      "citing_paper_id": "19096382",
      "cited_paper_id": null
    },
    {
      "context_text": "The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work (Eisenstein et al., 2011b; Hutchinson et al., 2013; Parikh et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works that have investigated similar concepts. No verifiable resources are identified.",
      "processing_time": 54.66066074371338,
      "citing_paper_id": "19096382",
      "cited_paper_id": 45378
    },
    {
      "context_text": ", 2016) Modality Adapt a model trained on written text for use in conversational speech (Bulyko et al., 2003; Jaech and Ostendorf, 2015; Mendels et al., 2015) Language Share information between similar languages (Ragni et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to adapting models and sharing information between languages. No clear, verifiable datasets are identified.",
      "processing_time": 55.0926833152771,
      "citing_paper_id": "19096382",
      "cited_paper_id": 362196
    },
    {
      "context_text": "The ﬁrst three datasets (AGNews, DBPedia, and Yelp) have previously been used for text classiﬁcation (Zhang et al., 2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGNews",
        "DBPedia",
        "Yelp"
      ],
      "dataset_descriptions": {
        "AGNews": "Used for text classification, specifically categorizing news articles into predefined classes using character-level convolutional networks.",
        "DBPedia": "Used for text classification, specifically categorizing Wikipedia articles into predefined classes using character-level convolutional networks.",
        "Yelp": "Used for text classification, specifically categorizing Yelp reviews into predefined classes using character-level convolutional networks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three datasets by name, which are used for text classification. These datasets are specific and have clear identifiers.",
      "processing_time": 70.66037368774414,
      "citing_paper_id": "19096382",
      "cited_paper_id": 368182
    },
    {
      "context_text": "The first three datasets (AGNews, DBPedia, and Yelp) have previously been used for text classification (Zhang et al., 2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGNews",
        "DBPedia",
        "Yelp"
      ],
      "dataset_descriptions": {
        "AGNews": "Used for text classification, specifically to categorize news articles into predefined classes using character-level convolutional networks.",
        "DBPedia": "Used for text classification, specifically to classify Wikipedia articles into categories such as company, school, artist, etc., using character-level convolutional networks.",
        "Yelp": "Used for text classification, specifically to classify Yelp reviews into positive or negative sentiment categories using character-level convolutional networks."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three datasets by name, which are used for text classification. These datasets are specific and have clear identifiers.",
      "processing_time": 73.06942391395569,
      "citing_paper_id": "19096382",
      "cited_paper_id": 368182
    },
    {
      "context_text": "Models can be interesting for other reasons such as speed (Brants et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a model and its characteristic.",
      "processing_time": 52.991416692733765,
      "citing_paper_id": "19096382",
      "cited_paper_id": 633992
    },
    {
      "context_text": "From Yogatama et al. (2017), we see that for AG-News, much more so than for other datasets, the un-igram statistics capture the discriminating information, and it is the only dataset in that work where a naive Bayes classiﬁer is competitive with the generative LSTM for the full range of training…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AG-News"
      ],
      "dataset_descriptions": {
        "AG-News": "Used to compare unigram statistics and classifier performance, specifically evaluating the competitiveness of a naive Bayes classifier against a generative LSTM."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'AG-News' as a specific dataset used to compare unigram statistics and classifier performance. The dataset is clearly identified and used in a specific research context.",
      "processing_time": 62.454073667526245,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "This is the “independent component” case in (Yogatama et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a method or finding in the cited paper.",
      "processing_time": 53.488935708999634,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "…relate to our work are neural models that correspond to special cases of the more general FactorCell (Dieng et al., 2016; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et al., 2013; Chen et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.838300943374634,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "With less training data, it probably would be, based on the results in (Yogatama et al., 2017).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to training data in a general sense.",
      "processing_time": 52.892250537872314,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "For the DBPedia and Yelp datasets, the Factor-Cell model beats previously reported classiﬁcation accuracies for generative models (Yogatama et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBPedia",
        "Yelp"
      ],
      "dataset_descriptions": {
        "DBPedia": "Used to evaluate the classification accuracy of the Factor-Cell model, comparing it against previously reported generative models.",
        "Yelp": "Used to evaluate the classification accuracy of the Factor-Cell model, comparing it against previously reported generative models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DBPedia and Yelp datasets' which are specific, verifiable datasets used for classification tasks. The citation indicates these datasets were used to evaluate the performance of the Factor-Cell model.",
      "processing_time": 68.38453125953674,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "For the DBPedia and Yelp datasets, the FactorCell model beats previously reported classification accuracies for generative models (Yogatama et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DBPedia",
        "Yelp"
      ],
      "dataset_descriptions": {
        "DBPedia": "Used to evaluate the FactorCell model's classification accuracy, comparing it against previously reported generative models' performance.",
        "Yelp": "Used to evaluate the FactorCell model's classification accuracy, comparing it against previously reported generative models' performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'DBPedia and Yelp datasets' which are specific, verifiable datasets used for evaluating the FactorCell model's performance in text classification.",
      "processing_time": 66.48682808876038,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "The studies that most directly relate to our work are neural models that correspond to special cases of the more general FactorCell model, including those that leverage what we call the SoftmaxBias model (Dieng et al., 2017; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.66380333900452,
      "citing_paper_id": "19096382",
      "cited_paper_id": 668431
    },
    {
      "context_text": "The studies that most directly relate to our work are neural models that correspond to special cases of the more general FactorCell model, including those that leverage what we call the SoftmaxBias model (Dieng et al., 2017; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 54.66380333900452,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Research is proceeding into neural systems that can decode audio directly into text one character at a time (Graves and Jaitly, 2014; Chan et al., 2015; Bahdanau et al., 2016; Maas et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and methods. There are no clear identifiers for datasets.",
      "processing_time": 54.02150630950928,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1166498
    },
    {
      "context_text": "We used the fastText model because it is near state-of-the-art and it can be efficiently trained and evaluated (Grave et al., 2017b).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the fastText model but does not refer to any specific dataset. The model is discussed in terms of its efficiency and performance, which is not relevant to the topic of personalized text generation.",
      "processing_time": 56.65307259559631,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1210515
    },
    {
      "context_text": "For example, some work on other natural language processing tasks, such as spoken language understanding, have already made use of the ConcatCell style adaptation (Mesnil et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ConcatCell style adaptation) used in spoken language understanding.",
      "processing_time": 53.363643169403076,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1317136
    },
    {
      "context_text": "Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm (Bar-Yossef and Kraus, 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an algorithm. No verifiable resources are identified.",
      "processing_time": 52.86915731430054,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1623913
    },
    {
      "context_text": "ﬂow library. The model is trained with the standard negative log likelihood loss function, i.e. minimizing cross entropy. Dropout is used as a regularizer in the recurrent connections as described in Semeniuta et al. (2016). Training is done using the Adam optimizer with a learning rate of 0:001. For the models with word-based vocabularies, a sampled softmax loss is used with a unigram proposal distribution and sampling",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and training details.",
      "processing_time": 51.70357656478882,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1707814
    },
    {
      "context_text": ", 2006), time-series analysis, music generation (Goel et al., 2014), and more.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists application areas such as time-series analysis and music generation.",
      "processing_time": 54.021589517593384,
      "citing_paper_id": "19096382",
      "cited_paper_id": 1713711
    },
    {
      "context_text": ", 2016) Lectures, Talks, & Meetings Use text from slides, lecture titles, and other written materials to bias language model in speech recognition (Schwarm et al., 2004; Glass et al., 2007; Hsu and Glass, 2008; Hoang et al., 2016) Dialog State Generate an appropriate response given the dialog state (Riccardi and Gorin, 2000; Liu and Lane, 2017) Personalization Match the model predictions to the style of each individual from a large group (Tseng et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and approaches for personalizing language models but does not cite any named datasets.",
      "processing_time": 54.841444969177246,
      "citing_paper_id": "19096382",
      "cited_paper_id": 2688207
    },
    {
      "context_text": "We used an LSTM with coupled input and forget gates for a 20% reduction in computation time (Greff et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions an LSTM model but does not refer to any specific dataset. The citation is about the model architecture and its computational efficiency.",
      "processing_time": 54.4352605342865,
      "citing_paper_id": "19096382",
      "cited_paper_id": 3356463
    },
    {
      "context_text": "There are ways of producing context-specific text without generating it from a language model such as by retrieving examples with matching context from the training data or combining retrieval with a neural edit model (Li et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'training data' but does not specify a named dataset. The reference to Li et al. (2018) does not provide a specific dataset name either.",
      "processing_time": 56.3914270401001,
      "citing_paper_id": "19096382",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "In applications, such as machine translation, the generation is conditioned on a source sequence in an encoder-decoder model (also called sequence-to-sequence, or seq2seq) (Cho et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (encoder-decoder model).",
      "processing_time": 52.61060833930969,
      "citing_paper_id": "19096382",
      "cited_paper_id": 5590763
    },
    {
      "context_text": ", 2016) Dialog State Generate an appropriate response given the dialog state (Riccardi and Gorin, 2000; Liu and Lane, 2017) Personalization Match the model predictions to the style of each individual from a large group (Tseng et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and approaches in dialog state generation and personalization but does not cite any particular dataset.",
      "processing_time": 55.40130567550659,
      "citing_paper_id": "19096382",
      "cited_paper_id": 6106078
    },
    {
      "context_text": ", 2016) Dialog State Generate an appropriate response given the dialog state (Riccardi and Gorin, 2000; Liu and Lane, 2017) Personalization Match the model predictions to the style of each individual from a large group (Tseng et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to methods and approaches in dialog state generation and personalization but does not cite any particular dataset.",
      "processing_time": 55.40130567550659,
      "citing_paper_id": "19096382",
      "cited_paper_id": 17013309
    },
    {
      "context_text": "Prior work has already shown that a simple language model that incorporates contextual data can provide gains in machine translation (Drexler et al., 2014).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not mention any specific dataset names, only a general reference to a corpus used for contextualized machine translation.",
      "processing_time": 53.81587529182434,
      "citing_paper_id": "19096382",
      "cited_paper_id": 6322805
    },
    {
      "context_text": "The remedy is to borrow from the probability mass given to some of the observed n-grams and distribute it to unobserved ones (Katz, 1987).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating probabilities from sparse data.",
      "processing_time": 52.45089793205261,
      "citing_paper_id": "19096382",
      "cited_paper_id": 6555412
    },
    {
      "context_text": "The language model is first trained on general background data and then learning is briefly continued on smaller in-domain data to “fine-tune” the weights (Gangireddy et al., 2016; Zhang et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'general background data' and 'smaller in-domain data' but does not provide specific names for these datasets. The terms are too generic and do not meet the criteria for inclusion.",
      "processing_time": 57.06700921058655,
      "citing_paper_id": "19096382",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "The language model is first trained on general background data and then learning is briefly continued on smaller in-domain data to “fine-tune” the weights (Gangireddy et al., 2016; Zhang et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'general background data' and 'smaller in-domain data' but does not provide specific names for these datasets. The terms are too generic and do not meet the criteria for inclusion.",
      "processing_time": 57.06700921058655,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7287513
    },
    {
      "context_text": "The corpus was created by combining portions of multiple published datasets for language identiﬁcation including Twitter70 (Jaech et al., 2016), TweetLID (Zubiaga et al., 2014), and the monolingual portion of Tweets from a code-switching detection workshop (Molina et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter70",
        "TweetLID",
        "Tweets from a code-switching detection workshop"
      ],
      "dataset_descriptions": {
        "Twitter70": "Used to create a combined corpus for language identification, focusing on multilingual tweet classification.",
        "TweetLID": "Used to create a combined corpus for language identification, focusing on multilingual tweet classification.",
        "Tweets from a code-switching detection workshop": "Used to create a combined corpus for language identification, focusing on monolingual tweets from a code-switching detection workshop."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions specific datasets used to create a corpus for language identification, which are relevant to personalized text generation.",
      "processing_time": 72.2531726360321,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7306663
    },
    {
      "context_text": "The corpus was created by combining portions of multiple published datasets for language identification including Twitter70 (Jaech et al., 2016), Tweet-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Twitter70"
      ],
      "dataset_descriptions": {
        "Twitter70": "Used to create a combined corpus for language identification, focusing on integrating diverse social media data sources."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Twitter70' as a published dataset for language identification, which is a specific, verifiable dataset.",
      "processing_time": 59.38746118545532,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7306663
    },
    {
      "context_text": "We use Adadelta (Zeiler, 2012) and tune the online learning rate to give the best perplexity on a held-out set of 12,000 queries, having previously verified (See Figure 5.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a held-out set of 12,000 queries, which is too generic and lacks a clear identifier.",
      "processing_time": 56.75831437110901,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7365802
    },
    {
      "context_text": "We also tie the word embeddings in the input layer with the ones in the output layer (Press and Wolf, 2016; Inan et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods related to tying word embeddings in neural network layers.",
      "processing_time": 52.98507905006409,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7443908
    },
    {
      "context_text": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a data structure (Bloom filter) and its properties. No verifiable resources are identified.",
      "processing_time": 54.66015934944153,
      "citing_paper_id": "19096382",
      "cited_paper_id": 7931252
    },
    {
      "context_text": "The design of this data structure trades off a compact representation of set membership against a small probability of false positives (Bloom, 1970; Talbot and Brants, 2008; Xu et al., 2011).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a data structure (Bloom filter) and its properties. No verifiable resources are identified.",
      "processing_time": 54.66015934944153,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16825268
    },
    {
      "context_text": "These techniques have been extended to RNN language models by allowing the model to look back at the previous hidden states from the same sentence or document (Merity et al., 2017b; Grave et al., 2017a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods applied to RNN language models.",
      "processing_time": 53.0484344959259,
      "citing_paper_id": "19096382",
      "cited_paper_id": 8693672
    },
    {
      "context_text": ", 2016), or occasionally a non-linear transformation is used instead (Ma et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or approaches. No verifiable resources are identified.",
      "processing_time": 53.67276930809021,
      "citing_paper_id": "19096382",
      "cited_paper_id": 9470913
    },
    {
      "context_text": "Our focus was on language modeling, but RNNs are widely used in many other natural language processing tasks and in other domains that have little or nothing to do with language such as acoustic modeling (Graves et al., 2006), time-series analysis, music generation (Goel et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of RNNs. No verifiable resources are identified.",
      "processing_time": 53.87531876564026,
      "citing_paper_id": "19096382",
      "cited_paper_id": 9901844
    },
    {
      "context_text": "that correspond to special cases of the more general FactorCell model, including those that leverage what we call the SoftmaxBias model (Dieng et al., 2017; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et al., 2013; Chen et al., 2015; Ghosh et al., 2016). One study (Ji et al., 2016) compares the two approaches, which they ref",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is focused on comparing different models and approaches in neural language generation.",
      "processing_time": 55.02696633338928,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "ective of our work, including speaker characteristics (Luan et al., 2016; Li et al., 2016), dialog act (Wen et al., 2015), sentiment and other factors (Tang et al., 2016; Hu et al., 2017), and style (Ficler and Goldberg, 2017). As noted earlier, some of this work has used discriminative text classiﬁcation to evaluate generation. In preliminary experiments with the Yelp data set, we found that the generative classiﬁer accur",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp data set"
      ],
      "dataset_descriptions": {
        "Yelp data set": "Used for preliminary experiments to evaluate generative classifier accuracy, focusing on the effectiveness of discriminative text classification in neural language generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Yelp data set' which is a specific, verifiable dataset. It is used for preliminary experiments to evaluate generative classifier accuracy.",
      "processing_time": 61.758198738098145,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "…are neural models that correspond to special cases of the more general FactorCell (Dieng et al., 2016; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et al., 2013; Chen et al., 2015; Ghosh et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is focused on describing different neural models and cells used in language generation.",
      "processing_time": 55.204219818115234,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "est samples. This differs from the accuracy criterion used for evaluating context-sensitive language models for text generation based on a separate discriminative classiﬁer trained on generated text (Ficler and Goldberg, 2017; Hu et al., 2017). We discuss this further in Section 5. The experiments compare the FactorCell model (equations 4 and 6) to two popular alternatives, which we refer to as ConcatCell (equations 2 and",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and evaluation criteria. The context is focused on comparing different models and their evaluation methods.",
      "processing_time": 54.62809348106384,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "To evaluate the performance of the different models for text generation, we initially looked at clas-siﬁcation accuracy using a discriminative classiﬁer trained on generated text, as in (Ficler and Goldberg, 2017; Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of a discriminative classifier for evaluating model performance.",
      "processing_time": 53.44474959373474,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "els that can be dynamically adapted based on context. Methods have been referred to as contextdependent models (Mikolov and Zweig, 2012), context-aware models (Tang et al., 2016), conditioned models (Ficler and Goldberg, 2017), and controllable text generation (Hu et al., 2017). These models have been used in scoring word sequences (such as for speech recognition or machine translation), for text classiﬁcation, and for gen",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.98753809928894,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054023
    },
    {
      "context_text": "The hybrid LSTM and count based language model is an alternative way of correcting for a low-rank approximation (Neubig and Dyer, 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining LSTM and count-based language models.",
      "processing_time": 52.795310258865356,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11054466
    },
    {
      "context_text": "Subword models have found use before especially when working with highly inflected or low-resource languages (Tucker et al., 1994; Creutz et al., 2007; Saraçlar et al., 2010); however, there has been a recent surge of interest in character- and subword-based models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to subword models and their applications. No verifiable resources are identified.",
      "processing_time": 54.40031862258911,
      "citing_paper_id": "19096382",
      "cited_paper_id": 11235831
    },
    {
      "context_text": "h our ﬁnding that adapting at the recurrent layer can beneﬁt certain tasks while having only a minor impact on perplexity. They do not test any models that adapt both the recurrent and output layers. Hoang et al. (2016) also consider adapting at the hidden layer vs. at the softmax layer, but their architecture does not ﬁt cleanly into the framework of the SoftmaxBias model because they use an extra perceptron layer;",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model architectures and adaptation strategies.",
      "processing_time": 52.3420467376709,
      "citing_paper_id": "19096382",
      "cited_paper_id": 12672327
    },
    {
      "context_text": "Hoang et al. (2016) also consider adapting at the hidden layer vs. at the softmax layer, but their architecture does not ﬁt cleanly into the framework of the SoftmaxBias model because they use an extra perceptron layer; thus, it is difﬁcult to compare the experimental ﬁndings with ours.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses architectural differences in neural network models.",
      "processing_time": 52.59229063987732,
      "citing_paper_id": "19096382",
      "cited_paper_id": 12672327
    },
    {
      "context_text": "Fine-tuning suffers from the possibility of catastrophic forgetting, where the model loses access to the information it learned from training on the background data (Goodfellow et al., 2013).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'background data'. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 56.43554496765137,
      "citing_paper_id": "19096382",
      "cited_paper_id": 12730344
    },
    {
      "context_text": "Several sophisticated techniques and heuristics exist (such as adding random penalties to subsets of the vocabulary (Juuti et al., 2018) or having special models trained to enforce relevance and avoid contradiction and repetition (Holtzman et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only techniques and methods. No verifiable resources are identified.",
      "processing_time": 53.213579416275024,
      "citing_paper_id": "19096382",
      "cited_paper_id": 13679899
    },
    {
      "context_text": ", 2017) and the Transformer Network (Vaswani et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and methods, not datasets. No specific datasets are named or described.",
      "processing_time": 52.777804374694824,
      "citing_paper_id": "19096382",
      "cited_paper_id": 13756489
    },
    {
      "context_text": ", 2014), and the monolingual portion of Tweets from a codeswitching detection workshop (Molina et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Tweets from a codeswitching detection workshop"
      ],
      "dataset_descriptions": {
        "Tweets from a codeswitching detection workshop": "Used to train and evaluate models for codeswitching detection, focusing on monolingual portions of tweets to improve language identification accuracy."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Tweets from a codeswitching detection workshop', which appears to be a specific dataset used in the research. The title confirms it is a shared task dataset.",
      "processing_time": 64.8177170753479,
      "citing_paper_id": "19096382",
      "cited_paper_id": 14584778
    },
    {
      "context_text": "The basic RNN has a flaw known as the vanishing/exploding gradient problem that prevents it from learning to use information from far back in the history (Pascanu et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in RNNs.",
      "processing_time": 52.98419260978699,
      "citing_paper_id": "19096382",
      "cited_paper_id": 14650762
    },
    {
      "context_text": "Hash collisions are known to negatively effect the perplexity (Mikolov et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a methodological issue in neural network language models.",
      "processing_time": 53.17649602890015,
      "citing_paper_id": "19096382",
      "cited_paper_id": 15076873
    },
    {
      "context_text": "Our fourth dataset, from TripAdvisor, was previously used for language modeling and consists of two relevant context variables: an identifier for the hotel and a sentiment score from one to five stars (Tang et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TripAdvisor"
      ],
      "dataset_descriptions": {
        "TripAdvisor": "Used for language modeling, incorporating hotel identifiers and sentiment scores ranging from one to five stars to generate context-aware text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset from TripAdvisor, which is used for language modeling and includes hotel identifiers and sentiment scores.",
      "processing_time": 58.706820487976074,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "The embeddings can either be learned off-line using a topic model (Mikolov and Zweig, 2012) or end-to-end as part of the adapted LM (Tang et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for learning embeddings. No verifiable resources are identified.",
      "processing_time": 53.38557767868042,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Methods have been referred to as contextdependent models (Mikolov and Zweig, 2012), context-aware models (Tang et al., 2016), conditioned models (Ficler and Goldberg, 2017), and controllable text generation (Hu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various terms and methods related to context-dependent models and text generation.",
      "processing_time": 54.159573793411255,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Methods have been referred to as context-dependent models (Mikolov and Zweig, 2012), context-aware models (Tang et al., 2016), conditioned models (Ficler and Goldberg, 2017), and controllable text generation (Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various terms and methods related to personalized text generation. No verifiable resources are identified.",
      "processing_time": 54.945536851882935,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Methods have been referred to as context-dependent models (Mikolov and Zweig, 2012), context-aware models (Tang et al., 2016), conditioned models (Ficler and Goldberg, 2017), and controllable text generation (Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various terms and methods related to personalized text generation. No verifiable resources are identified.",
      "processing_time": 54.945536851882935,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": ", 2015), sentiment and other factors (Tang et al., 2016; Hu et al., 2017), and style (Ficler and Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors like sentiment and style. No clear, verifiable resources are identified.",
      "processing_time": 54.7097909450531,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": ", 2015), sentiment and other factors (Tang et al., 2016; Hu et al., 2017), and style (Ficler and Goldberg, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general factors like sentiment and style. No clear, verifiable resources are identified.",
      "processing_time": 54.7097909450531,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "…to special cases of the more general FactorCell model, including those that leverage what we call the SoftmaxBias model\n(Dieng et al., 2016; Tang et al., 2016; Yogatama et al., 2017; Ficler and Goldberg, 2017) and others that use the ConcatCell approach (Mikolov and Zweig, 2012; Wen et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers are referenced for their contributions to models and approaches, not for datasets.",
      "processing_time": 55.69578194618225,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Neural models were trained to predict only the words from a subset of the vocabulary known as a shortlist (Schwenk, 2004), and the predictions from the neural model were interpolated with an n-gram model that could handle a full-sized vocabulary.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training neural models. No verifiable resources are identified.",
      "processing_time": 53.69522047042847,
      "citing_paper_id": "19096382",
      "cited_paper_id": 16930073
    },
    {
      "context_text": "Recurrent neural networks (RNNs) have been shown to be very effective language models compared to previous approaches such as n-gram models or maximum entropy models (Mikolov et al., 2010), in part because they are able to make use of arbitrarily long word histories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the effectiveness of RNNs compared to other language models.",
      "processing_time": 55.10462546348572,
      "citing_paper_id": "19096382",
      "cited_paper_id": 17048224
    },
    {
      "context_text": "Recurrent neural network (RNN) language models (Mikolov et al., 2010) extend that advantage by permitting the incorporation of information from arbitrarily long word histories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RNN language model).",
      "processing_time": 52.9309241771698,
      "citing_paper_id": "19096382",
      "cited_paper_id": 17048224
    },
    {
      "context_text": "This differs from the accuracy criterion used for evaluating context-sensitive language models for text generation based on a separate discriminative classifier trained on generated text (Ficler and Goldberg, 2017; Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach for evaluating context-sensitive language models.",
      "processing_time": 53.156840085983276,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "…for logbilinear sequence models (Eisenstein et al., 2011; Hutchinson et al., 2015), but these are less powerful than the RNN.\nVariational auto-encoders have also been used for controlling text generation (Hu et al., 2017), and it may provide opportunities for extending the work presented here.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different models and their capabilities.",
      "processing_time": 54.0837779045105,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "…the context posterior probability using Bayes rule), and therefore differs from the accuracy criterion used for evaluating context-sensitive language models for text generation based on a separate discriminative classifier trained on generated text (Ficler and Goldberg, 2017; Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and evaluation criteria for text generation models.",
      "processing_time": 53.14932870864868,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "To evaluate the performance of the different models for text generation, we initially looked at classification accuracy using a discriminative classifier trained on generated text, as in (Ficler and Goldberg, 2017; Hu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of a discriminative classifier for evaluating text generation models.",
      "processing_time": 53.72781991958618,
      "citing_paper_id": "19096382",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "The RNN variant that we use is an LSTM with coupled input and forget gates (Melis et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM with coupled input and forget gates).",
      "processing_time": 53.50047421455383,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "Even the seed for the random weight initialization can have a “major impact” on the final performance of an LSTM (Reimers and Gurevych, 2017).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) and a finding about random weight initialization.",
      "processing_time": 54.08019971847534,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "The basic operation of the recurrent layer is to use a matrix W to transform the concatenation of a word embedding (wt) with the hidden state from the previous time step (ht−1) and produce a new hidden state (ht), where\nht = σ(W1wt +W2ht−1 + b)\n= σ(W[wt, ht−1] + b)\nFor simplicity, we write the equations for a recurrent neural network, but the same methods can be applied to RNN variants such as the LSTM, used here.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes the operation of a recurrent neural network and mentions LSTM as a variant. No verifiable resources are identified.",
      "processing_time": 55.57308053970337,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "on perplexity is greater than the sometimes small relative differences between models. Even the seed for the random weight initialization can have a “major impact” on the ﬁnal performance of an LSTM (Reimers and Gurevych, 2017). We use Figure 1 to show how the three classes of models perform across a range of hyperparameters. The ﬁgure compares perplexity on the x-axis with accuracy on the y-axis with both metrics computed",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the impact of random initialization on LSTM performance.",
      "processing_time": 53.081361055374146,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "The LSTM cell size is mainly responsible for this; it has a much bigger impact on perplexity than on accuracy.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the impact of LSTM cell size on performance metrics.",
      "processing_time": 53.281471729278564,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "For any fixed LSTM size the FactorCell has a higher count of learned parameters compared to the ConcatCell.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between FactorCell and ConcatCell in the context of LSTM networks.",
      "processing_time": 54.065255880355835,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "The hyperparameter with the strongest effect on perplexity is the size of the LSTM.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the impact of hyperparameters on LSTM performance.",
      "processing_time": 53.26314187049866,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "From Yogatama et al. (2017), we see that for AGNews, much more so than for other datasets, the unigram statistics capture the discriminating information, and it is the only dataset in that work where a naive Bayes classifier is competitive with the generative LSTM for the full range of training data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AGNews"
      ],
      "dataset_descriptions": {
        "AGNews": "Used to compare unigram statistics and classifier performance, specifically evaluating the competitiveness of a naive Bayes classifier against a generative LSTM across varying amounts of training data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'AGNews' as a specific dataset used to compare unigram statistics and classifier performance. It is clearly identified and used in a specific research context.",
      "processing_time": 62.570080280303955,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "This finding is consistent with previous work that showed that individual dimensions of LSTM hidden states can be strong indicators of concepts like sentiment (Karpathy et al., 2015; Radford et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous work on LSTM hidden states and sentiment. No verifiable resources are identified.",
      "processing_time": 54.85777950286865,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "To investigate this further, we trained a logistic regression classifier to predict the language using the state from the LSTM at the last time step on the unadapted model as a feature vector.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method for training a logistic regression classifier.",
      "processing_time": 54.288339376449585,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "The effect on classification accuracy of increasing the LSTM size was mixed.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about LSTM network performance.",
      "processing_time": 53.263593435287476,
      "citing_paper_id": "19096382",
      "cited_paper_id": 23678406
    },
    {
      "context_text": "Following (Park and Chiba, 2017), we remove queries seen less than three times from the MPC training data to avoid excessive memory usage.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'MPC training data' but does not provide enough information to determine if it is a specific, verifiable dataset. The title does not help in disambiguating.",
      "processing_time": 55.960697174072266,
      "citing_paper_id": "19096382",
      "cited_paper_id": 27562075
    },
    {
      "context_text": "Recently, (Park and Chiba, 2017) suggested a significantly different approach to QAC.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a different approach to query auto-completion.",
      "processing_time": 52.475173234939575,
      "citing_paper_id": "19096382",
      "cited_paper_id": 27562075
    },
    {
      "context_text": "The experiments make use of the AOL Query data collected over three months in 2006 (Pass et al., 2006).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AOL Query data"
      ],
      "dataset_descriptions": {
        "AOL Query data": "Used to analyze user search behavior, specifically focusing on queries over a three-month period in 2006 to understand search patterns and trends."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'AOL Query data' which is a specific dataset used for search query analysis. It is clearly identified and used in the research.",
      "processing_time": 62.51368975639343,
      "citing_paper_id": "19096382",
      "cited_paper_id": 28918994
    },
    {
      "context_text": "The RNN variant that we use is an LSTM with coupled input and forget gates (Melis et al., 2018). The different model variants are implemented2 using the Tensorflow library. The model is trained with the standard negative log likelihood loss function, i.e. minimizing cross entropy. Dropout is used as a regularizer in the recurrent connections as described in Semeniuta et al. (2016). Training is done using the Adam optimizer with a learning rate of 0.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the implementation details of the LSTM model and training process.",
      "processing_time": 55.28305721282959,
      "citing_paper_id": "19096382",
      "cited_paper_id": 33513311
    },
    {
      "context_text": "The RNN variant that we use is an LSTM with coupled input and forget gates (Melis et al., 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM with coupled input and forget gates).",
      "processing_time": 54.26996994018555,
      "citing_paper_id": "19096382",
      "cited_paper_id": 33513311
    },
    {
      "context_text": ", 2014) Geographic Variations in speaking style in different geographic regions (Eisenstein et al., 2010; Chelba et al., 2015; Halpern et al., 2016) Modality Adapt a model trained on written text for use in conversational speech (Bulyko et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It references geographic variations in speaking style and modality adaptation, but no dataset names are provided.",
      "processing_time": 55.71850824356079,
      "citing_paper_id": "19096382",
      "cited_paper_id": 35735426
    },
    {
      "context_text": "In the basic n-gram language model only the most recent n−1 words from the history are considered (Bahl et al., 1978).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or model. The context is about the n-gram language model, which is a method, not a dataset.",
      "processing_time": 56.04176354408264,
      "citing_paper_id": "19096382",
      "cited_paper_id": 37867788
    },
    {
      "context_text": "One improvement was to add skip-grams, which are like n-grams except they can skip over some words to look farther back in the history (Siu and Ostendorf, 2000; Shazeer et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (skip-grams).",
      "processing_time": 52.85054349899292,
      "citing_paper_id": "19096382",
      "cited_paper_id": 37868618
    },
    {
      "context_text": ", 2010) nor is it designed to be efficient at geolocation (Han et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing geolocation prediction. No verifiable resources are identified.",
      "processing_time": 54.59204840660095,
      "citing_paper_id": "19096382",
      "cited_paper_id": 42914721
    },
    {
      "context_text": "For augmentative communications, the method would also apply to icon-based communication, which relies on a language model with icons instead of words or characters (Dudy and Bedrick, 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method for icon-based communication but does not reference any dataset by name.",
      "processing_time": 55.647849559783936,
      "citing_paper_id": "19096382",
      "cited_paper_id": 51997874
    },
    {
      "context_text": "he parameter count in an LSTM LM (Kuchaiev and Ginsburg, 2017), ﬁnding that the reduced number of parameters leads to faster training. There is a long history of adapting n-gram language models. (See DeMori and Federico (1999) or Bellegarda (2004) for a survey.) One recent example is Chelba and Shazeer (2015) where a 34% relative improvement in perplexity was obtained when using geographic features for adaptation. We hypot",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and research works.",
      "processing_time": 53.22033977508545,
      "citing_paper_id": "19096382",
      "cited_paper_id": 186597544
    },
    {
      "context_text": "There has also been work on personalizing MPC (Shokouhi, 2013; Cai et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research works. The context is about personalizing query auto-completion, but no datasets are explicitly named.",
      "processing_time": 56.10999774932861,
      "citing_paper_id": "19096382",
      "cited_paper_id": 207204302
    },
    {
      "context_text": "The multiplicative rescaling of the recurrent layer weights is used in the Hypernetwork model (Ha et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Hypernetwork).",
      "processing_time": 53.42112588882446,
      "citing_paper_id": "19096382",
      "cited_paper_id": 208981547
    },
    {
      "context_text": "Our method of updating the model during evaluation resembles work on dynamic evaluation for language modeling (Krause et al., 2017), but differs in that only the user embeddings (latent demographic factors) are updated.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for dynamic evaluation in language modeling.",
      "processing_time": 53.527705669403076,
      "citing_paper_id": "19096382",
      "cited_paper_id": 215827131
    },
    {
      "context_text": "Recently, some language modeling papers have used “dynamic evaluation”, whereby the model is allowed to continue to train on the test data after making predictions for each segment (Krause et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called 'dynamic evaluation'. No dataset names are provided.",
      "processing_time": 54.58976411819458,
      "citing_paper_id": "19096382",
      "cited_paper_id": 215827131
    },
    {
      "context_text": "The Reddit and SCOTUS data are tokenized and lower-cased using the standard NLTK tokenizer (Bird et al., 2009).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit",
        "SCOTUS"
      ],
      "dataset_descriptions": {
        "Reddit": "Used to tokenize and lowercase text using the NLTK tokenizer, focusing on preprocessing for natural language processing tasks.",
        "SCOTUS": "Used to tokenize and lowercase text using the NLTK tokenizer, focusing on preprocessing for natural language processing tasks."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Reddit' and 'SCOTUS' data, which are specific datasets used in the research. However, they are not clearly identified as datasets in the citation context.",
      "processing_time": 68.21672582626343,
      "citing_paper_id": "19096382",
      "cited_paper_id": null
    },
    {
      "context_text": "The content of this chapter draws from our previously published work (Jaech and Ostendorf, 2017)",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a previously published work.",
      "processing_time": 54.755324602127075,
      "citing_paper_id": "19096382",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, since the sampling process is nondifferentiable, we use Gumbell-softmax relaxation trick to solve this problem [8, 19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Gumbell-softmax relaxation) used to address a technical challenge in the sampling process.",
      "processing_time": 55.77343416213989,
      "citing_paper_id": "235703107",
      "cited_paper_id": 2428314
    },
    {
      "context_text": "However, deep learning models can be vulnerable to adversarial attacks [21].",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general vulnerability of deep learning models to adversarial attacks.",
      "processing_time": 54.09175705909729,
      "citing_paper_id": "235703107",
      "cited_paper_id": 7004303
    },
    {
      "context_text": "While adversarial attacks on deep learning models have received a lot of attention in graph representation learning, natural language processing, and computer vision domains [21], the vulnerability of deep user sequence embedding-based classification models remains unknown.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general domains where adversarial attacks have been studied.",
      "processing_time": 53.914790868759155,
      "citing_paper_id": "235703107",
      "cited_paper_id": 7004303
    },
    {
      "context_text": "A non-parametric Maximum Mean Discrepancy (MMD) based on the Reproducing Kernel Hilbert Space (RKHS) is utilized to effectively estimate this kind of distance [25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating distance using MMD and RKHS.",
      "processing_time": 54.75093078613281,
      "citing_paper_id": "235703107",
      "cited_paper_id": 8308769
    },
    {
      "context_text": "Here we treat the text generation process as a conditional language model which can leverage additional information [14, 15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to a conditional language model. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.47307348251343,
      "citing_paper_id": "235703107",
      "cited_paper_id": 14994977
    },
    {
      "context_text": "Here we treat the text generation process as a conditional language model which can leverage additional information [14, 15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general reference to a conditional language model. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.47307348251343,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Similar to the training of conditional language model [14, 15], we train G by using Maximal Likelihood Estimation (MLE) with teacher-forcing and minimize the loss of negative log-likelihood for all posts based on the corresponding posts and contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.14633107185364,
      "citing_paper_id": "235703107",
      "cited_paper_id": 14994977
    },
    {
      "context_text": "Similar to the training of conditional language model [14, 15], we train G by using Maximal Likelihood Estimation (MLE) with teacher-forcing and minimize the loss of negative log-likelihood for all posts based on the corresponding posts and contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.14633107185364,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "However, these models can not be directly used to attack sequence classification models, as generated text is not personalized to the user and thus, can be identified by anomaly detection models [1].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general limitation of models in generating personalized text.",
      "processing_time": 54.747230529785156,
      "citing_paper_id": "235703107",
      "cited_paper_id": 57825713
    },
    {
      "context_text": "The proliferation of undesirable users, such as fake accounts [20], spammers [3, 23], fake news spreaders [15, 26], abnormal users [1], vandal editors [12], fraudsters [11], and sockpuppets [10], poses a threat to the safety and integrity of online communities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of undesirable users in online communities. No verifiable resources are identified.",
      "processing_time": 55.6046838760376,
      "citing_paper_id": "235703107",
      "cited_paper_id": 57825713
    },
    {
      "context_text": "The proliferation of undesirable users, such as fake accounts [20], spammers [3, 23], fake news spreaders [15, 26], abnormal users [1], vandal editors [12], fraudsters [11], and sockpuppets [10], poses a threat to the safety and integrity of online communities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only types of undesirable users in online communities. No verifiable resources are identified.",
      "processing_time": 55.6046838760376,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "However, these models have various shortcomings: they are incapable of fully leveraging a user’s rich history of posts, they can not generate original content, and their modifications can be easily detected by finding misspelled words and improperly manipulated sentences [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their limitations. The context focuses on the shortcomings of existing models rather than the use of datasets.",
      "processing_time": 56.20029902458191,
      "citing_paper_id": "235703107",
      "cited_paper_id": 166228669
    },
    {
      "context_text": "(1)HierarchicalRecurrentNeuralNetwork (HRNN) is amodel where the sequential pattern of the input text is captured by the hierarchical structure for accurate classification [30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (HRNN) and its application for text classification.",
      "processing_time": 55.21479940414429,
      "citing_paper_id": "235703107",
      "cited_paper_id": 199466300
    },
    {
      "context_text": "To determine whether a user is malicious or not, existing methods usually focus on building deep sequence embedding models to encode the sequential information and use the embedding for downstream applications [13, 16, 30].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 55.742713928222656,
      "citing_paper_id": "235703107",
      "cited_paper_id": 199466300
    },
    {
      "context_text": "(c)Universal adversarial Trigger (UniTrigger) [27]: UniTrigger generates an input-agnostic and fixed-length sequence of tokens to attack the classifier when concatenated to the end of an existing post.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (UniTrigger) for generating adversarial triggers. The context focuses on the method's functionality rather than a dataset.",
      "processing_time": 56.61747074127197,
      "citing_paper_id": "235703107",
      "cited_paper_id": 201698258
    },
    {
      "context_text": "Modifications include changing or adding characters, words, or phrases [4, 9, 17, 27].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only modifications to text. No verifiable resources are identified.",
      "processing_time": 54.86358880996704,
      "citing_paper_id": "235703107",
      "cited_paper_id": 201698258
    },
    {
      "context_text": "Specifically, we test whether posts generated by PETGEN are more realistic compared to those generated by Malcom (the SOTA end-to-end adversarial text generation method).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (PETGEN and Malcom).",
      "processing_time": 55.27166938781738,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Among the remaining posts, reviewers label 58.33% posts by PETGEN more realistic than Malcom.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to posts generated by PETGEN and Malcom, which are methods or tools, not datasets.",
      "processing_time": 56.4013295173645,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Following the similar vectorization method in the previous works [15], we use the Latent Dirichlet Allocation model trained on the whole text to compute the vector representation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Latent Dirichlet Allocation) used for vector representation.",
      "processing_time": 56.14358353614807,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "In each pair, one post is generated by PETGEN and the other by Malcom.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two systems, PETGEN and Malcom, but does not specify any datasets. The cited paper title suggests Malcom is a method, not a dataset.",
      "processing_time": 56.852538108825684,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Compared to the Malcom model, PETGEN deploys the context-aware text generator and the learning task of recent post relevance to leverage the historical post and recent post information for generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing PETGEN and Malcom models, without referencing any datasets.",
      "processing_time": 56.174071311950684,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Existing text generation methods suffer from three major shortcomings with respect to our attack setting: (a) recent work has adversarially attacked fake news detection classifiers by generating fake reply comments on the posts [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating malicious comments to attack fake news detection models.",
      "processing_time": 55.44388127326965,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "We turn to the topic modeling function in this specific application setting, similar to that adopted in prior work [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general application setting. No verifiable resources are identified.",
      "processing_time": 55.29672312736511,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "A recent attack model called Malcom [15] generates new fake reply comments to news articles to fool detectors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Malcom' as an attack model, which is not a dataset but a method for generating malicious comments. No specific dataset is mentioned.",
      "processing_time": 56.62968873977661,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Attacks that append short random text or phrases to the original text can also be detected by topic coherence checkers [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for detecting attacks on text. No verifiable resources are identified.",
      "processing_time": 55.742356300354004,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "From this result, we can see our method is able to outperform Malcom in generating realistic posts, and has great potential in real-world applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. The context focuses on the performance of the method compared to Malcom.",
      "processing_time": 55.90527129173279,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "We select Relational Memory Recurrent Network (RMRN) as the basic text generationmodelg ofG, following previous work [15, 19], as RMRN models have shown remarkable performance in generating long text posts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (RMRN) which is not a dataset. The citation is about using a model for text generation, not a dataset.",
      "processing_time": 57.842631101608276,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "(e)Malcom [15]: Malcom is the current state-of-the-art model in adversarial text generation to fool classifiers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Malcom as a model, not a dataset. There are no specific, verifiable datasets mentioned in the context.",
      "processing_time": 55.99298667907715,
      "citing_paper_id": "235703107",
      "cited_paper_id": 221446550
    },
    {
      "context_text": "Generating adversarial text to attack text classifiers is an important task due to its contribution tomodel robustness [29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task related to adversarial attacks on text classifiers.",
      "processing_time": 55.73566031455994,
      "citing_paper_id": "235703107",
      "cited_paper_id": 260428188
    },
    {
      "context_text": "Networks (GAN) [6] provides a novel way to generate text, which consists of a generator and a discriminator.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GAN).",
      "processing_time": 54.560227394104004,
      "citing_paper_id": "127953732",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "Lately, the Generative Adversarial Networks (GAN) [6] has been introduced, and several variants of the GAN model for generating text have been proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GAN).",
      "processing_time": 54.812100410461426,
      "citing_paper_id": "127953732",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "Long Short-Term Memory (LSTM [8]) is used as a sequential neural network model to generate sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the LSTM model. No verifiable resources are identified.",
      "processing_time": 55.67376136779785,
      "citing_paper_id": "127953732",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "To address this problem to generate text via GAN, some researchers employ Long Short-term Memory (LSTM [8]) and convolutional neural network (CNN [10]) for adversarial training to generate realistic text and optimize a new feature distance when training the generator [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of LSTM and CNN in adversarial training for text generation.",
      "processing_time": 57.00137138366699,
      "citing_paper_id": "127953732",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "With the rise of deep learning [22, 20, 7, 12], researchers have tried to introduce neural networks to generate sentences.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research. No verifiable resources are identified.",
      "processing_time": 55.98810410499573,
      "citing_paper_id": "127953732",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "A method is inspired by the Reinforcement Learning (RL)[17] reward signal come from the GAN discriminator judged on a complete sequence, which is passed back to the intermediate state-action steps using Monte Carlo search [20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of reinforcement learning and GANs, but does not reference any particular dataset.",
      "processing_time": 57.89337348937988,
      "citing_paper_id": "127953732",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "These GAN variants yield good performances in the context of generating short texts, such as SeqGAN [20], RankGAN [12], TextGAN [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several GAN variants but does not refer to any specific datasets. The focus is on methods rather than datasets.",
      "processing_time": 56.28551506996155,
      "citing_paper_id": "127953732",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "The Texygen tool [23] is a benchmark platform that integrates several GAN models for text generation, such as MaliGAN [3], RankGAN [12], LeakGAN [7], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Texygen as a benchmark platform but does not refer to any specific dataset. It lists models (MaliGAN, RankGAN, LeakGAN) which are excluded.",
      "processing_time": 57.95749092102051,
      "citing_paper_id": "127953732",
      "cited_paper_id": 3636178
    },
    {
      "context_text": "second system is LeakGAN-NER, which is trained using the text set of NER ltered sentences. Here we adopt two popular objective measurements to evaluate the sentence similarity. The rst one is SimHash [2]. SimHash is one widely used eective way to remove duplicate text. The feature vector of each word is obtained from a given sentence rst, and weights are set for each feature vector, which indicates",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (SimHash) for evaluating sentence similarity, but does not reference a dataset.",
      "processing_time": 56.914013147354126,
      "citing_paper_id": "127953732",
      "cited_paper_id": 4229473
    },
    {
      "context_text": "To evaluate topic similarity of the generated sentences by the comparing methods to the user-defined topics, the named entities of those topics are used to find similar words based on the cosine similarity obtained by Word2Vec algorithm.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Word2Vec) and a general concept (cosine similarity).",
      "processing_time": 57.29652714729309,
      "citing_paper_id": "127953732",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "Instead of using the word embedding directly from the GAN model, we trained a word embedding by Word2Vec [13] algorithm using the features extracted from the personalized related information extraction phase to determine word similarity.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions training a word embedding using Word2Vec, but does not specify a dataset. Word2Vec is a method, not a dataset.",
      "processing_time": 56.74527287483215,
      "citing_paper_id": "127953732",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "The Word2vec algorithm is proposed in [13], which includes two architectures: Continuous Bag-of-Words Model and Continuous Skip-gram Model, to calculate continuous vector representations of words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Word2vec but does not refer to any specific dataset. It describes the method and its architectures.",
      "processing_time": 55.73524475097656,
      "citing_paper_id": "127953732",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "(3) Word2Vec and POS2Vec: In order to generate sentences containing personalized word usage of the target author, the cosine similarity of the word vectors is applied.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (Word2Vec and POS2Vec). The citation is focused on the methodology rather than a specific dataset.",
      "processing_time": 57.1251745223999,
      "citing_paper_id": "127953732",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "In order to improve the accuracy of NER process, another work [5] is based on the conditional random field model, using Gibbs sampling to adopt the structure of long sentence instead of only local feature.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (conditional random field model) and an approach (Gibbs sampling).",
      "processing_time": 56.2900927066803,
      "citing_paper_id": "127953732",
      "cited_paper_id": 10977241
    },
    {
      "context_text": "The StanfordNERTagger [5] is adopted to extract the named entities that appear in each article.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a tool (StanfordNERTagger) but does not refer to any specific dataset. The citation is about using a method for information extraction.",
      "processing_time": 56.982682943344116,
      "citing_paper_id": "127953732",
      "cited_paper_id": 10977241
    },
    {
      "context_text": "ne the sentence similarity according to the Hamming distance of SimHash values of dierent sentences, which is the number of dierent digits of SimHash value. The second measurement is the Bleu score [14]. Bleu score measures the uency and translation quality of generated text by measuring the similarity between the generated text and the reference text. We calculated the unigram, bigram, and trigram",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (Hamming distance, Bleu score). The cited paper 'Bleu: a Method for Automatic Evaluation of Machine Translation' is a method, not a dataset.",
      "processing_time": 59.36559772491455,
      "citing_paper_id": "127953732",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Text generation, as a basic task of natural language processing, has many applications, such as dialogue robots [11], machine translation [9], paraphrasing [15] and so on.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation. No verifiable resources are identified.",
      "processing_time": 55.81767272949219,
      "citing_paper_id": "127953732",
      "cited_paper_id": 18825948
    },
    {
      "context_text": "(2) Bigram Extractor: In statistical machine translation, bigram language model [21] is a useful probability distribution, which preserves the phrase characteristics of the text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a bigram language model, which is a method, not a dataset.",
      "processing_time": 57.16496706008911,
      "citing_paper_id": "127953732",
      "cited_paper_id": 207670589
    },
    {
      "context_text": "According to the research in cognitive science, effective communication creates similar activation maps in the brains of both interlocutors (Hasson et al., 2012), suggesting that understanding be-tween interlocutors is an essential signal for a high-quality chit-chat conversation.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general finding from cognitive science research.",
      "processing_time": 54.92234683036804,
      "citing_paper_id": "215745354",
      "cited_paper_id": 696401
    },
    {
      "context_text": "In practice, REINFORCE algorithm (Williams, 1992) is leveraged to approximate Equation 5 by sampling a B n from policy p θ ( a B n | s B n ) .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reinforcement learning algorithm. No verifiable resources are identified.",
      "processing_time": 55.51633286476135,
      "citing_paper_id": "215745354",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "Methods like special reward shaping to reduce generic responses (Li et al., 2016b) and representing the speakers with latent variables (Li et al., 2016a) were introduced to improve the engagingness of chit-chat systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches to improve chit-chat systems.",
      "processing_time": 55.34880781173706,
      "citing_paper_id": "215745354",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Our implementation was based on PyTorch (Paszke et al., 2019), ParlAI (Miller et al., 2017), and HuggingFace’s transformers library (Wolf et al., 2019a).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions software libraries and frameworks, which are not considered datasets. No specific datasets are named or described.",
      "processing_time": 55.50965976715088,
      "citing_paper_id": "215745354",
      "cited_paper_id": 3677429
    },
    {
      "context_text": "Yang et al. (2018) proposed to generate dialogue responses by dual learning based domain adaptation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating dialogue responses.",
      "processing_time": 54.883734941482544,
      "citing_paper_id": "215745354",
      "cited_paper_id": 4945763
    },
    {
      "context_text": "Retrieval-based meth-ods retrieve response candidates and rank them based on the matching scores with the dialogue (Sordoni et al., 2015; Wu et al., 2017; Gu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional information about datasets.",
      "processing_time": 56.69166946411133,
      "citing_paper_id": "215745354",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "Retrieval-based meth-ods retrieve response candidates and rank them based on the matching scores with the dialogue (Sordoni et al., 2015; Wu et al., 2017; Gu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not provide additional information about datasets.",
      "processing_time": 56.69166946411133,
      "citing_paper_id": "215745354",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "C HAT , assessing P 2 B OT using both automatic metrics and human evaluations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (P2BOT) and evaluation approaches. The context is about personalizing dialogue agents, but no verifiable dataset is named.",
      "processing_time": 58.25138831138611,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide any specific dataset names or clear usage descriptions. The citation appears to be incomplete or lacks necessary details.",
      "processing_time": 55.62204623222351,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Following previous work (Li et al., 2016b; Zhang et al., 2018b), we treat dialogue generation as a sequence generation problem.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that dialogue generation is treated as a sequence generation problem. No verifiable resources are identified.",
      "processing_time": 56.22872877120972,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT unique is that personas of both interlocutors are explicitly described using several proﬁle sentences, facilitating the training of chatbots with conﬁgurable and persistent personalities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "C HAT unique"
      ],
      "dataset_descriptions": {
        "C HAT unique": "Used to train chatbots with configurable and persistent personalities, specifically by providing profile sentences for both interlocutors, enhancing personalized dialogue generation."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'C HAT unique' which appears to be a dataset or resource used for training chatbots with configurable and persistent personalities. However, the name is not clearly a multi-word proper noun, uppercase acronym, or hyphenated name with digits, making it less certain.",
      "processing_time": 69.53788352012634,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Language Model , Generative Pro-ﬁle Memory (Zhang et al., 2018b) and S EQ 2S EQ with attention mechanism (Bahdanau et al., 2015) were implemented as generative baselines for dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.081591844558716,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Language Model , Generative Pro-ﬁle Memory (Zhang et al., 2018b) and S EQ 2S EQ with attention mechanism (Bahdanau et al., 2015) were implemented as generative baselines for dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.081591844558716,
      "citing_paper_id": "215745354",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "C HAT (Zhang et al., 2018b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions C HAT, which is likely a method or model rather than a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 56.391037464141846,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT dataset contains 8,939 / 1,000 multi-turn dialogues conditioned on 1,155 / 100 per-sonas for train / dev.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "C HAT"
      ],
      "dataset_descriptions": {
        "C HAT": "Used to train and evaluate dialogue agents personalized with personas, focusing on multi-turn dialogues to enhance conversational coherence and personalization."
      },
      "confidence_score": 1.0,
      "reasoning": "The C HAT dataset is explicitly mentioned and described, fitting the criteria for a specific, verifiable dataset.",
      "processing_time": 62.2705442905426,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Among the retrieval-based base-lines, KV Proﬁle Memory (Zhang et al., 2018b) was the ofﬁcial baseline which employed the memory network along with proﬁle information, and Dually Interactive Matching Network (Gu et al., 2019) proposed a dual matching architecture to match between the responses and their corresponding contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 56.21388602256775,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Among the retrieval-based base-lines, KV Proﬁle Memory (Zhang et al., 2018b) was the ofﬁcial baseline which employed the memory network along with proﬁle information, and Dually Interactive Matching Network (Gu et al., 2019) proposed a dual matching architecture to match between the responses and their corresponding contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 56.21388602256775,
      "citing_paper_id": "215745354",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "C HAT dataset demonstrate the superiority of our approach over the baselines in both automatic metrics and human evaluations 1 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "C HAT dataset"
      ],
      "dataset_descriptions": {
        "C HAT dataset": "Used to evaluate personalized dialogue agents, demonstrating superiority in automatic metrics and human evaluations through comparative analysis."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'C HAT dataset' which appears to be a specific dataset used to demonstrate the effectiveness of the approach in personalized dialogue agents.",
      "processing_time": 63.48817467689514,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Among the retrieval-based base-lines, KV Proﬁle Memory (Zhang et al., 2018b) was the ofﬁcial baseline which employed the memory network along with proﬁle information, and Dually Interactive Matching Network (Gu et al., 2019) proposed a dual matching architecture to match between the responses and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 55.45478844642639,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Following Zhang et al. (2018b), we reported the ofﬁcial automatic metrics to evaluate the methods: Hits@1 , Perplexity (ppl) and F1 .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only evaluation metrics. The cited paper title suggests a focus on personalized dialogue agents but does not introduce a dataset.",
      "processing_time": 57.1509792804718,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Moreover, they are usually lack of coherent personality traits due to the fact that training dialogues actually come from a diverse set of speakers (Zhang et al., 2018b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about training dialogues coming from a diverse set of speakers.",
      "processing_time": 54.982523918151855,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT demonstrate the effectiveness of our approach.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only states the effectiveness of their approach without providing details on the resources used.",
      "processing_time": 56.304006576538086,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT also provides revised personas by rephrasing, generalizing or specializing the original ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for revising personas in dialogue agents.",
      "processing_time": 53.894572734832764,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "C HAT has fueled a growing interest in developing methods for personalized dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general interest in personalized dialogue generation.",
      "processing_time": 53.89106559753418,
      "citing_paper_id": "215745354",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Furthermore, subtracting a baseline (Weaver and Tao, 2001), here the mean reward of a mini-batch, is applied on R ( a B n ) to reduce variance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for reducing variance in reinforcement learning.",
      "processing_time": 54.44117712974548,
      "citing_paper_id": "215745354",
      "cited_paper_id": 7317294
    },
    {
      "context_text": "Generative-based methods typically use S EQ 2S EQ model as the backbone (Sutskever et al., 2014; Bahdanau et al., 2015; Serban et al., 2017; Wolf et al., 2019b), where the encoder extracts the information in an utterance and the decoder generates the response.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models and their usage in sequence-to-sequence tasks.",
      "processing_time": 55.278828144073486,
      "citing_paper_id": "215745354",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Wolf et al. (2019b) ﬁne-tuned pretrained language model (Radford et al., 2018) to improve the dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the fine-tuning of a pretrained language model. No verifiable resources are identified.",
      "processing_time": 55.66380262374878,
      "citing_paper_id": "215745354",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Concretely, we employ the pretraining transformer language model introduced in Radford et al. (2018) (i.e. GPT) to initialize Transmitter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (GPT) but does not refer to any specific dataset. The citation is used to describe the initialization method for a model, not a dataset.",
      "processing_time": 57.123873472213745,
      "citing_paper_id": "215745354",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "Initialized by BERT (Devlin et al., 2019), both encoders provide deep contextualized representations for each token.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for initializing encoders. BERT is a model, not a dataset.",
      "processing_time": 56.162575244903564,
      "citing_paper_id": "215745354",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "As mentioned in Dinan et al. (2019), no automatic metric is perfect for evaluating such an open-domain task.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about evaluation metrics.",
      "processing_time": 53.833593130111694,
      "citing_paper_id": "215745354",
      "cited_paper_id": 59553505
    },
    {
      "context_text": "Transfertransfo (Wolf et al., 2019b) 3 achieved the state-of-the-art performance on automatic metrics, while Lost In Conver-sation 4 topped the human evaluations (Dinan et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on performance comparisons using automatic metrics and human evaluations.",
      "processing_time": 55.101391553878784,
      "citing_paper_id": "215745354",
      "cited_paper_id": 59553505
    },
    {
      "context_text": "Madotto et al. (2019) applied meta-learning to quickly adapt to new speakers, and Tigunova et al. (2019) extracted user attributes from daily dialogues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and research works.",
      "processing_time": 53.388556241989136,
      "citing_paper_id": "215745354",
      "cited_paper_id": 86681432
    },
    {
      "context_text": "…KV Proﬁle Memory (Zhang et al., 2018b) was the ofﬁcial baseline which employed the memory network along with proﬁle information, and Dually Interactive Matching Network (Gu et al., 2019) proposed a dual matching architecture to match between the responses and their corresponding contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods and models but does not refer to any specific datasets. The context focuses on describing the methodologies used in the cited papers.",
      "processing_time": 55.823538303375244,
      "citing_paper_id": "215745354",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation (Li et al., 2017) and explanation generation (Li et al., 2020c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation strategies. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.528231143951416,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation (Li et al., 2017) and explanation generation (Li et al., 2020c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation strategies. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.528231143951416,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation (Dong et al., 2017), tip generation (Li et al., 2017) and explanation generation (Li et al., 2020c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation strategies. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.528231143951416,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "For recommendation, besides NRT and NETE, we include another two traditional methods: • PMF (Mnih and Salakhutdinov, 2007) is a standard probabilistic matrix factorization method that characterizes users and items by latent factors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context focuses on describing traditional recommendation methods, particularly PMF.",
      "processing_time": 54.95868706703186,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "For recommendation, besides NRT and NETE, we include another two traditional methods: • PMF (Mnih and Salakhutdinov, 2007) is a standard probabilistic matrix factorization method that characterizes users and items by latent factors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. The context focuses on describing traditional recommendation methods, particularly PMF.",
      "processing_time": 54.95868706703186,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": ", 2017), tip generation (Li et al., 2017) and explanation generation (Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of generation tasks. No clear, verifiable resource names are provided.",
      "processing_time": 54.845786809921265,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "• NRT (Li et al., 2017) can predict a rating and generate a tip simultaneously based on user and item IDs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'NRT' which is likely a method or model, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 55.634899854660034,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "• NRT (Li et al., 2017) can predict a rating and generate a tip simultaneously based on user and item IDs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'NRT' which is likely a method or model, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 55.634899854660034,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "For NRT, Att2Seq, NETE and our PE-TER and PETER+, we set the size of vocabulary to 20,000 by keeping the most frequent words.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses vocabulary size settings for various models.",
      "processing_time": 54.23857879638672,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "For NRT, Att2Seq, NETE and our PE-TER and PETER+, we set the size of vocabulary to 20,000 by keeping the most frequent words.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses vocabulary size settings for various models.",
      "processing_time": 54.23857879638672,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "On the largest dataset Yelp with approximately 1.3 million records, our model PETER performs as good as the three competitive baselines (i.e., SVD++, NRT and NETE), which shows the rationale of our recommendation module.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to evaluate the performance of the model PETER against three competitive baselines (SVD++, NRT, and NETE) on a large-scale recommendation task with 1.3 million records."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp' as a dataset with 1.3 million records, which is a specific, verifiable dataset. It is used to evaluate the performance of the model PETER against baselines.",
      "processing_time": 67.7512469291687,
      "citing_paper_id": "235187032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "On the largest dataset Yelp with approximately 1.3 million records, our model PETER performs as good as the three competitive baselines (i.e., SVD++, NRT and NETE), which shows the rationale of our recommendation module.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to evaluate the performance of the model PETER against three competitive baselines (SVD++, NRT, and NETE) on a large-scale recommendation task with 1.3 million records."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp' as a dataset with 1.3 million records, which is a specific, verifiable dataset. It is used to evaluate the performance of the model PETER against baselines.",
      "processing_time": 67.7512469291687,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "However, previous works mostly rely on recurrent neural networks (RNN), e.g., LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014), leaving the potentially more effective Transformer under-explored, which motivates this work.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of RNNs, LSTMs, and GRUs, and mentions the Transformer as under-explored.",
      "processing_time": 58.397653102874756,
      "citing_paper_id": "235187032",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Our PETER+ consistently and signiﬁcantly outperforms ACMLM and NETE on the three datasets in terms of text quality (BLEU and ROUGE).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'three datasets' but does not specify their names. The context does not provide enough information to identify specific datasets.",
      "processing_time": 55.27463626861572,
      "citing_paper_id": "235187032",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "For the former, we adopt BLEU (Papineni et al., 2002) in machine translation and ROUGE (Lin, 2004) in text summarization, and report BLEU-1 and BLEU-4, and Precision, Recall and F1 of ROUGE-1 and ROUGE-2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BLEU and ROUGE, which are evaluation metrics, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 55.27111840248108,
      "citing_paper_id": "235187032",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "• We evaluate the generated explanations on not only text quality metrics (such as BLEU and ROUGE), but also metrics that particularly focus on explainability from the angle of item features.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The context focuses on the evaluation of generated explanations using text quality metrics and explainability metrics.",
      "processing_time": 55.950053215026855,
      "citing_paper_id": "235187032",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al., 2013) with a threshold of",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only optimization methods and techniques.",
      "processing_time": 53.0580952167511,
      "citing_paper_id": "235187032",
      "cited_paper_id": 14650762
    },
    {
      "context_text": "We optimize the model via stochastic gradient descent (Robbins and Monro, 1951), and apply gradient clipping (Pascanu et al., 2013) with a threshold of Top-15ContextWords Explanation Ground-truth the rooms arespaciousandthebathroomhasalargetub PETER < eos >…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and optimization techniques. No verifiable resources are identified.",
      "processing_time": 54.03709626197815,
      "citing_paper_id": "235187032",
      "cited_paper_id": 14650762
    },
    {
      "context_text": "The former (Gedikli et al., 2014; Chen and Wang, 2017; Chen et al., 2019b) investigates how people perceive different styles of explanations, while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only research works and their contributions to the field of explainable recommendation systems.",
      "processing_time": 54.9538140296936,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17471203
    },
    {
      "context_text": "The former (Gedikli et al., 2014; Chen and Wang, 2017; Chen et al., 2019b) investigates how people perceive different styles of explanations, while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only research works and their contributions to the field of explainable recommendation systems.",
      "processing_time": 54.9538140296936,
      "citing_paper_id": "235187032",
      "cited_paper_id": 201894190
    },
    {
      "context_text": "• Att2Seq (Dong et al., 2017) is a review generation approach and we take the explanations as reviews.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Att2Seq' as a method for generating reviews, but does not refer to a specific dataset. The context is about using explanations as reviews, which is a methodological detail rather than a dataset.",
      "processing_time": 58.77557849884033,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "…in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 55.95858693122864,
      "citing_paper_id": "235187032",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "…in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 55.95858693122864,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52233682
    },
    {
      "context_text": "…in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018; Chen et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 55.95858693122864,
      "citing_paper_id": "235187032",
      "cited_paper_id": 69778590
    },
    {
      "context_text": "Later works (Liu et al., 2018; Devlin et al., 2019) show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of the parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the effectiveness of certain architectures, not on the use of datasets.",
      "processing_time": 56.21406674385071,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Under the paradigm of pre-training plus finetuning, Transformer’s effectiveness has been confirmed on a wide range of tasks, including both natural language understanding and generation (Radford et al., 2018; Devlin et al., 2019; Dong et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the effectiveness of Transformer models across various tasks.",
      "processing_time": 55.26832556724548,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "PETER is a small un-pretrained Transformer with only 2 layers, yet it outperforms a ﬁne-tuned BERT (Ni et al., 2019) on most metrics by a large margin, and takes less time to train, as shown in our experiments.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between PETER and BERT. BERT is a model, not a dataset.",
      "processing_time": 55.79540038108826,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": ", 2019) is a fine-tuned BERT (Devlin et al., 2019), where an attention layer is introduced to encode the features from both the user and the item.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BERT) which is excluded according to the rules.",
      "processing_time": 54.8068413734436,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "When features are used, we denote our model as PETER+, and compare it with two recent models: • ACMLM (Ni et al., 2019) is a ﬁne-tuned BERT (Devlin et al., 2019), where an attention layer is introduced to encode the features from both the user and the item.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing PETER+ with other models, particularly ACMLM, which uses BERT.",
      "processing_time": 57.472317695617676,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Again, in terms of text quality, the performance gap between PETER+ and ACMLM (a ﬁne-tuned BERT) is extremely large, because the latter’s generation is achieved by predicting masked tokens, which is quite different from word-by-word generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the performance of PETER+ and ACMLM, a fine-tuned BERT model.",
      "processing_time": 58.03282904624939,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": ", 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the language modeling abilities of certain models, which are not considered datasets.",
      "processing_time": 56.68800735473633,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Transformer (Vaswani et al., 2017), whose strong language modeling ability has been demonstrated on a variety of tasks (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020), however, is relatively under-explored for personalized natural language generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. The context focuses on the potential of the Transformer model for personalized natural language generation.",
      "processing_time": 56.223384857177734,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Under the paradigm of pre-training plus ﬁne-tuning, Transformer’s effectiveness has been con-ﬁrmed on a wide range of tasks, including both nat-ural language understanding and generation (Rad-ford et al., 2018; Devlin et al., 2019; Dong et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the effectiveness of Transformer models across various tasks.",
      "processing_time": 55.42881536483765,
      "citing_paper_id": "235187032",
      "cited_paper_id": 52967399
    },
    {
      "context_text": ", 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only research areas and papers. No verifiable resources are identified.",
      "processing_time": 54.54760718345642,
      "citing_paper_id": "235187032",
      "cited_paper_id": 69778590
    },
    {
      "context_text": "…such as pre-deﬁned templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al., 2019d; Li et al., 2021), image visualizations (Chen et al., 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian et al., 2020), reasoning rules (Shi et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 55.67110538482666,
      "citing_paper_id": "235187032",
      "cited_paper_id": 201894190
    },
    {
      "context_text": "In the case of explainable recommendation, users may value more an explanation that justi-ﬁes a recommendation’s advantages on certain features (Li et al., 2020c; Chen et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of explanations in recommendations. No verifiable resources are identified.",
      "processing_time": 55.043376207351685,
      "citing_paper_id": "235187032",
      "cited_paper_id": 201894190
    },
    {
      "context_text": "In the case of explainable recommendation, users may value more an explanation that justi-ﬁes a recommendation’s advantages on certain features (Li et al., 2020c; Chen et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of explanations in recommendations. No verifiable resources are identified.",
      "processing_time": 55.043376207351685,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "There exist various types of explanation styles, such as pre-deﬁned templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al., 2019d; Li et al., 2021), image visualizations (Chen et al., 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of explanation styles. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 56.35243821144104,
      "citing_paper_id": "235187032",
      "cited_paper_id": 201894190
    },
    {
      "context_text": "There exist various types of explanation styles, such as pre-deﬁned templates (Zhang et al., 2014; Li et al., 2020a), ranked sentences (Chen et al., 2019d; Li et al., 2021), image visualizations (Chen et al., 2019c), knowledge graph paths (Ai et al., 2018; Xian et al., 2019; Fu et al., 2020; Xian…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of explanation styles. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 56.35243821144104,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "• SVD++ (Koren, 2008) leverages a user’s interacted items to enhance the latent factors.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (SVD++). The context focuses on the method's functionality rather than a dataset.",
      "processing_time": 55.74598264694214,
      "citing_paper_id": "235187032",
      "cited_paper_id": 207168823
    },
    {
      "context_text": ", 2017) and explanation generation (Li et al., 2020c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers on neural template explanations and recommendation systems.",
      "processing_time": 54.41969299316406,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "To this end, we adopt the other three metrics proposed by (Li et al., 2020c): Feature Matching Ratio ( FMR ), Feature Coverage Ratio ( FCR ) and Feature Diversity ( DIV ).",
      "catation_intent": "research work",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics. The context is focused on the adoption of certain metrics for evaluation.",
      "processing_time": 54.78773522377014,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "To this end, we adopt the other three metrics proposed by (Li et al., 2020c): Feature Matching Ratio (FMR), Feature Coverage Ratio (FCR) and Feature Diversity (DIV).",
      "catation_intent": "research work",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics. The context is focused on evaluating the performance of a model using certain metrics.",
      "processing_time": 55.232696533203125,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "For experimentation, we adopt three publicly available explainable recommendation datasets, and their data splits (Li et al., 2020c).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'explainable recommendation datasets' but does not specify the names of the datasets. The title does not provide additional information to disambiguate.",
      "processing_time": 56.74482703208923,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "This can be very useful when, for instance, a user proactively asks the system to explain certain feature(s) of a recommendation (Li et al., 2020c), e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper about generating neural template explanations for recommendations.",
      "processing_time": 54.43902945518494,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "• NETE (Li et al., 2020c) is a tailored GRU (Cho et al., 2014) feature into the decoding process to generate template-like explanations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NETE) and a neural network architecture (GRU).",
      "processing_time": 55.27255916595459,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications and methods. No clear identifiers for datasets are present.",
      "processing_time": 54.616700172424316,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": ", among which, recently, generated natural language explanations (Ni et al., 2019; Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of textual data on recommendation platforms such as e-commerce.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the availability of textual data on recommendation platforms, which is too generic.",
      "processing_time": 56.111079931259155,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "• NETE (Li et al., 2020c) is a tailored GRU (Cho et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (NETE) and a neural network architecture (GRU).",
      "processing_time": 55.052146434783936,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "…rules (Shi et al., 2020; Chen et al., 2021; Zhu et al., 2021), etc., among which, recently, generated natural language explanations (Ni et al., 2019; Li et al., 2020c) have received much attention, mainly owing to the advancement of natural language generation technology and the availability of…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in natural language generation technology and the availability of generated natural language explanations.",
      "processing_time": 55.4121310710907,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "To quantitatively measure how severe the problem is, we adopt USR that computes the Unique Sentence Ratio of generated sentences (Li et al., 2020c).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (USR) for computing the Unique Sentence Ratio. The context is about measuring the severity of a problem using this method.",
      "processing_time": 57.27070951461792,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "Moreover, we found that the model’s problem of generating identical sentences (as reported in Li et al., 2020c) is caused by the L2 regularization in its original design.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue with a model.",
      "processing_time": 53.50848841667175,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "For experimentation, we adopt three publicly available explainable recommendation datasets, and their data splits (Li et al., 2020c (hotel), Amazon (movies & TV) and Yelp (restaurant).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "hotel",
        "Amazon (movies & TV)",
        "Yelp (restaurant)"
      ],
      "dataset_descriptions": {
        "hotel": "Used to evaluate explainable recommendation systems, focusing on user reviews and ratings for hotels.",
        "Amazon (movies & TV)": "Used to evaluate explainable recommendation systems, focusing on user reviews and ratings for movies and TV shows.",
        "Yelp (restaurant)": "Used to evaluate explainable recommendation systems, focusing on user reviews and ratings for restaurants."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions three specific datasets used for experimentation in explainable recommendation systems.",
      "processing_time": 70.24507403373718,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "In the case of explainable recommendation, users may value more an explanation that justifies a recommendation’s advantages on certain features (Li et al., 2020c; Chen et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing explainable recommendations.",
      "processing_time": 53.6582465171814,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "Many of the applications in fact require certain degree of personalization, such as explainable recommendation (Zhang et al., 2014; Li et al., 2020c; Zhang and Chen, 2020), review generation (Dong et al., 2017), review summarization (Li et al., 2019), and conversational systems (Zhang et al., 2018;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references various applications and papers that deal with personalization in different contexts.",
      "processing_time": 55.83824443817139,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "Notice that Li et al. (2020b) conducted a user survey and reported that NETE’s explanations were perceived useful by most participants.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a user survey conducted by Li et al. (2020b) but does not specify a dataset name. The survey is used to evaluate the usefulness of NETE’s explanations.",
      "processing_time": 58.36181354522705,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "This can be very useful when, for instance, a user proactively asks the system to explain certain feature(s) of a recommendation (Li et al., 2020c), e.g., price .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to recommendations and explanations. No clear, verifiable resource is identified.",
      "processing_time": 55.6261305809021,
      "citing_paper_id": "235187032",
      "cited_paper_id": 224270828
    },
    {
      "context_text": "Recently, Generative Adversarial Networks (GANs) [10] have been explored extensively for perceptually realistic image generation [10,17,20,21,31,36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.05277109146118,
      "citing_paper_id": "251040605",
      "cited_paper_id": 211227
    },
    {
      "context_text": "Recently, Generative Adversarial Networks (GANs) [10] have been explored extensively for perceptually realistic image generation [10,17,20,21,31,36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.05277109146118,
      "citing_paper_id": "251040605",
      "cited_paper_id": 980236
    },
    {
      "context_text": "Recently, Generative Adversarial Networks (GANs) [10] have been explored extensively for perceptually realistic image generation [10,17,20,21,31,36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models and their applications. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.05277109146118,
      "citing_paper_id": "251040605",
      "cited_paper_id": 32665336
    },
    {
      "context_text": "In [37], the authors propose a GAN-based architecture for synthesizing the images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GAN-based architecture).",
      "processing_time": 53.6453173160553,
      "citing_paper_id": "251040605",
      "cited_paper_id": 1563370
    },
    {
      "context_text": "Conditional generative models [16,31,41,54] have become popular in different fields of computer vision, such as inpainting [46], super-resolution [8,18] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of conditional generative models in computer vision. No verifiable resources are identified.",
      "processing_time": 55.79337954521179,
      "citing_paper_id": "251040605",
      "cited_paper_id": 1925189
    },
    {
      "context_text": "Conditional generative models [16,31,41,54] have become popular in different fields of computer vision, such as inpainting [46], super-resolution [8,18] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of conditional generative models in computer vision. No verifiable resources are identified.",
      "processing_time": 55.79337954521179,
      "citing_paper_id": "251040605",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "Conditional generative models [16,31,41,54] have become popular in different fields of computer vision, such as inpainting [46], super-resolution [8,18] etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of conditional generative models in computer vision. No verifiable resources are identified.",
      "processing_time": 55.79337954521179,
      "citing_paper_id": "251040605",
      "cited_paper_id": 24005817
    },
    {
      "context_text": "We also evaluate the Learned Perceptual Image Patch Similarity (LPIPS) [48] metric as a more modern replacement of the SSIM for perceptual image quality assessment.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LPIPS as a metric, which is excluded according to the instructions. No datasets are mentioned.",
      "processing_time": 54.346291303634644,
      "citing_paper_id": "251040605",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "In our evaluation, we calculate LPIPS using both VGG19 [43] and SqueezeNet [14] backbones.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LPIPS but does not refer to a specific dataset. It only mentions models (VGG19 and SqueezeNet) which are excluded.",
      "processing_time": 56.503679513931274,
      "citing_paper_id": "251040605",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "In our evaluation, we calculate LPIPS using both VGG19 [43] and SqueezeNet [14] backbones.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LPIPS but does not refer to a specific dataset. It only mentions models (VGG19 and SqueezeNet) which are excluded.",
      "processing_time": 56.503679513931274,
      "citing_paper_id": "251040605",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "However, the proposed text-guided approach performs satisfactorily, as reflected in SSIM, IS, DS, and LPIPS scores.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No verifiable resources are identified.",
      "processing_time": 53.621103048324585,
      "citing_paper_id": "251040605",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "[3] propose a method of pose transfer by segmenting and generating the foreground and background individually.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for pose transfer.",
      "processing_time": 52.9660747051239,
      "citing_paper_id": "251040605",
      "cited_paper_id": 5038229
    },
    {
      "context_text": "At each block, we perform a transposed convolution followed by batch normalization [15] and ReLU activation [32].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (batch normalization and ReLU activation).",
      "processing_time": 53.74351954460144,
      "citing_paper_id": "251040605",
      "cited_paper_id": 5808102
    },
    {
      "context_text": "At first, we encode T B into an embedded vector v B either by many-hot encoding or using a pre-trained NLP model such as BERT [7], FastText [2], or Word2Vec [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained NLP models but does not specify the use of any particular dataset. The models mentioned (BERT, FastText, Word2Vec) are excluded as they are methods, not datasets.",
      "processing_time": 58.1524453163147,
      "citing_paper_id": "251040605",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "At first, we encode T B into an embedded vector v B either by many-hot encoding or using a pre-trained NLP model such as BERT [7], FastText [2], or Word2Vec [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained NLP models but does not specify the use of any particular dataset. The models mentioned (BERT, FastText, Word2Vec) are excluded as they are methods, not datasets.",
      "processing_time": 58.1524453163147,
      "citing_paper_id": "251040605",
      "cited_paper_id": 47012788
    },
    {
      "context_text": "At first, we encode T B into an embedded vector v B either by many-hot encoding or using a pre-trained NLP model such as BERT [7], FastText [2], or Word2Vec [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained NLP models but does not specify the use of any particular dataset. The models mentioned (BERT, FastText, Word2Vec) are excluded as they are methods, not datasets.",
      "processing_time": 58.1524453163147,
      "citing_paper_id": "251040605",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "As shown in Table 4, the encoding methods like FastText [2] and Word2Vec [30] perform closely to BERT [7].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions FastText and Word2Vec as encoding methods, but does not indicate their use as datasets. BERT is also mentioned, but it is a model, not a dataset.",
      "processing_time": 57.52662968635559,
      "citing_paper_id": "251040605",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "As shown in Table 4, the encoding methods like FastText [2] and Word2Vec [30] perform closely to BERT [7].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions FastText and Word2Vec as encoding methods, but does not indicate their use as datasets. BERT is also mentioned, but it is a model, not a dataset.",
      "processing_time": 57.52662968635559,
      "citing_paper_id": "251040605",
      "cited_paper_id": 47012788
    },
    {
      "context_text": "As shown in Table 4, the encoding methods like FastText [2] and Word2Vec [30] perform closely to BERT [7].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions FastText and Word2Vec as encoding methods, but does not indicate their use as datasets. BERT is also mentioned, but it is a model, not a dataset.",
      "processing_time": 57.52662968635559,
      "citing_paper_id": "251040605",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "At first, we encode TB into an embedded vector vB either by many-hot encoding or using a pre-trained NLP model such as BERT [7], FastText [2], or Word2Vec [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained NLP models but does not specify the use of any particular dataset. The models mentioned (BERT, FastText, Word2Vec) are excluded as they are methods, not datasets.",
      "processing_time": 57.93647217750549,
      "citing_paper_id": "251040605",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "At first, we encode TB into an embedded vector vB either by many-hot encoding or using a pre-trained NLP model such as BERT [7], FastText [2], or Word2Vec [30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained NLP models but does not specify the use of any particular dataset. The models mentioned (BERT, FastText, Word2Vec) are excluded as they are methods, not datasets.",
      "processing_time": 57.93647217750549,
      "citing_paper_id": "251040605",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The discriminator network DS utilizes a PatchGAN [16] to evaluate the quality of the generated image by taking a channel-wise concatenation between IA and either IB or ĨB .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PatchGAN) and a general description of the discriminator network's operation.",
      "processing_time": 54.98169803619385,
      "citing_paper_id": "251040605",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "The discriminator network D S utilizes a PatchGAN [16] to evaluate the quality of the generated image by taking a channel-wise concatenation between I A and either I B or ˜ I B .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (PatchGAN) and a model (discriminator network).",
      "processing_time": 54.779152393341064,
      "citing_paper_id": "251040605",
      "cited_paper_id": 6200260
    },
    {
      "context_text": "Researchers [12] have shown that the WGAN training is more stable if DT is Lipschitz continuous, which mitigates the undesired behavior due to gradient clipping.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WGAN) and a condition for its stability (Lipschitz continuity).",
      "processing_time": 55.18396806716919,
      "citing_paper_id": "251040605",
      "cited_paper_id": 10894094
    },
    {
      "context_text": "Researchers have also investigated 3D appearance flow [23], pose flow [51], and surface-based modeling [11,33] for pose transfer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for pose transfer. No verifiable resources are identified.",
      "processing_time": 54.985267162323,
      "citing_paper_id": "251040605",
      "cited_paper_id": 13637778
    },
    {
      "context_text": "Researchers have also investigated 3D appearance flow [23], pose flow [51], and surface-based modeling [11,33] for pose transfer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for pose transfer. No verifiable resources are identified.",
      "processing_time": 54.985267162323,
      "citing_paper_id": "251040605",
      "cited_paper_id": 52169649
    },
    {
      "context_text": "Researchers have also investigated 3D appearance flow [23], pose flow [51], and surface-based modeling [11,33] for pose transfer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for pose transfer. No verifiable resources are identified.",
      "processing_time": 54.985267162323,
      "citing_paper_id": "251040605",
      "cited_paper_id": 85529164
    },
    {
      "context_text": "To calculate GCR, we remove the last layer of the VGG19 network and add a single neuron with sigmoid activation to design a binary classifier and train it with the image samples from the DeepFashion dataset with label 0 for males and label 1 for females.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DeepFashion"
      ],
      "dataset_descriptions": {
        "DeepFashion": "Used to train a binary classifier for gender classification, specifically employing images labeled for males and females to design a model with a single neuron and sigmoid activation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the DeepFashion dataset, which is used to train a binary classifier for gender classification using images.",
      "processing_time": 61.8813898563385,
      "citing_paper_id": "251040605",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "The optimization objective of GS consists of three loss components – a pixelwise l1 loss LS l1 , a discrimination loss L GS GAN by DS , and a perceptual loss L GS Pρ computed using a pre-trained VGG-19 network [43].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only loss functions and a pre-trained network. The cited paper title 'Very Deep Convolutional Networks for Large-Scale Image Recognition' suggests the use of a pre-trained model rather than a dataset.",
      "processing_time": 59.200257778167725,
      "citing_paper_id": "251040605",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "Wang et al. [44] introduce a characteristic preserving generative network with a geometric matching module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on introducing a network architecture.",
      "processing_time": 54.757115840911865,
      "citing_paper_id": "251040605",
      "cited_paper_id": 49901141
    },
    {
      "context_text": "In [47], the authors first approximate a 3D mesh from a single image, and then the 3D mesh is used to transfer the pose.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for approximating a 3D mesh from a single image and using it for pose transfer.",
      "processing_time": 55.28000831604004,
      "citing_paper_id": "251040605",
      "cited_paper_id": 52860522
    },
    {
      "context_text": "[35] have incorporated redescription of textual descriptions for image synthesis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for text-to-image generation.",
      "processing_time": 53.210752964019775,
      "citing_paper_id": "251040605",
      "cited_paper_id": 76661216
    },
    {
      "context_text": "Each encoder block features a sequence of convolution (kernel size = 4 × 4, stride = 2, padding = 1, bias = 0), batch normalization, ReLU activation, and a basic residual block [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or architecture. The context is about the structure of encoder blocks in a neural network.",
      "processing_time": 55.6072678565979,
      "citing_paper_id": "251040605",
      "cited_paper_id": 206594692
    },
    {
      "context_text": "These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of assistants. No verifiable resources are identified.",
      "processing_time": 53.18370032310486,
      "citing_paper_id": "31298398",
      "cited_paper_id": 539059
    },
    {
      "context_text": "(Wen et al., 2015) introduced a Dialog-Act component into the LSTM cell to guide generated content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological enhancement to LSTM cells.",
      "processing_time": 53.16977381706238,
      "citing_paper_id": "31298398",
      "cited_paper_id": 739696
    },
    {
      "context_text": "(Wen et al., 2015) introduced a Dialog-Act component into the LSTM cell to guide generated content.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological enhancement to LSTM cells.",
      "processing_time": 53.16977381706238,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "M’s gain over the LSTM-MMI baseline is signiﬁcant at the level of α = 0 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between models. No verifiable resources are identified.",
      "processing_time": 53.797794580459595,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Compared with the baseline model LSTM-MMI, we obtain a 34.9% decrease in perplexity for the MT ASK -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of model performance. No verifiable resources are identified.",
      "processing_time": 54.270705223083496,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Given a sequence of inputs X = { x 1 , x 2 , . . . , x n X } and the corresponding output Y = { y 1 , y 2 , . . . , y n Y } , Sequence-to-Sequence (S EQ 2S EQ ) models use a Long Short-Term Memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sequence, taking the last hidden state of encoder h n X to represent output sequence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM) and a model (SEQ2SEQ).",
      "processing_time": 54.47565817832947,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Like a S EQ 2S EQ model, it comprises encoding and decoding components built by an LSTM sequential model as in Section 3.2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture (LSTM).",
      "processing_time": 53.24361538887024,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "The LSTM cell includes an input gate, a memory gate and an output gate, respectively denoted as i t , f t and o t .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the structure of the LSTM cell.",
      "processing_time": 52.788634300231934,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Our baseline is our implementation of the LSTM-MMI of (Li et al., 2016a).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM-MMI).",
      "processing_time": 53.03323817253113,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "As in standard S EQ 2S EQ models, the persona-based conversational model presented in (Li et al., 2016b) ﬁrst encodes the source message into a vector representation using the source LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) and a model (persona-based conversational model).",
      "processing_time": 54.976001501083374,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": ", ynY }, Sequence-to-Sequence (SEQ2SEQ) models use a Long Short-Term Memories (LSTM) (Hochreiter and Schmidhuber, 1997) to encode the input sequence, taking the last hidden state of encoder hnX to represent output sequence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 54.963141202926636,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "1 The embedding of the unseen user is updated by A UTOENCODER training together with the decoder LSTM parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method involving autoencoder and LSTM. No verifiable resources are identified.",
      "processing_time": 54.44928526878357,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "M both perform better on average than LSTM-MMI, consistent with the BLEU results.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance metrics.",
      "processing_time": 52.75243353843689,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "We then use LSTM-MMI to rerank the N-best list and use the 1-best result of the re-ranked list in all evaluation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM-MMI) used for reranking. No verifiable resources are identified.",
      "processing_time": 55.41054058074951,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Then, for each element in the target side, hidden units are obtained by combining the representation produced by the target LSTM at the previous time step h t − 1 , the word representations e t at the current time step, and the embedding s i for user i . where W ∈ R 4 K × 3 K .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes a method for combining hidden units in a neural network.",
      "processing_time": 53.82882785797119,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "• 2 layer LSTM models with 500 hidden cells for each layer.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a model architecture. No verifiable resources are identified.",
      "processing_time": 53.61375164985657,
      "citing_paper_id": "31298398",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "We use MERT (Och, 2003) to optimize γ and λ on BLEU us-ing N-best lists of response candidates generated from the development set.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimizing parameters using N-best lists from the development set, but does not specify a named dataset. MERT is a method, not a dataset.",
      "processing_time": 55.625064849853516,
      "citing_paper_id": "31298398",
      "cited_paper_id": 5474833
    },
    {
      "context_text": "We use MERT (Och, 2003) to optimize γ and λ on BLEU using N-best lists of response candidates generated from the development set.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimizing parameters using N-best lists from a development set, but does not specify a named dataset. MERT is a method, not a dataset.",
      "processing_time": 55.61596345901489,
      "citing_paper_id": "31298398",
      "cited_paper_id": 5474833
    },
    {
      "context_text": "In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task learning. No verifiable resources are identified.",
      "processing_time": 54.29823565483093,
      "citing_paper_id": "31298398",
      "cited_paper_id": 6070356
    },
    {
      "context_text": "In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task learning. No verifiable resources are identified.",
      "processing_time": 54.29823565483093,
      "citing_paper_id": "31298398",
      "cited_paper_id": 16193002
    },
    {
      "context_text": "In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of multi-task learning. No verifiable resources are identified.",
      "processing_time": 54.29823565483093,
      "citing_paper_id": "31298398",
      "cited_paper_id": 45998148
    },
    {
      "context_text": "(Luan et al., 2016b) use a multiplicative matrix on word embeddings to bias the word distribution of different speaker roles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for biasing word distributions using word embeddings.",
      "processing_time": 54.06278467178345,
      "citing_paper_id": "31298398",
      "cited_paper_id": 6070356
    },
    {
      "context_text": "(Luan et al., 2016b) use a multiplicative matrix on word embeddings to bias the word distribution of different speaker roles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for biasing word distributions using word embeddings.",
      "processing_time": 54.06278467178345,
      "citing_paper_id": "31298398",
      "cited_paper_id": 16193002
    },
    {
      "context_text": "Autoencoders are widely used to initialize neural networks (Dai and Le, 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (autoencoders) used for initializing neural networks.",
      "processing_time": 54.05655813217163,
      "citing_paper_id": "31298398",
      "cited_paper_id": 7138078
    },
    {
      "context_text": "(Mei et al., 2017) introduce an attention mechanism into an encoder-decoder network for a conversation model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an attention mechanism in a model.",
      "processing_time": 52.14389753341675,
      "citing_paper_id": "31298398",
      "cited_paper_id": 8466284
    },
    {
      "context_text": "These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chit-chats (De Mori et al., 2008; Chen et al., 2015, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of assistants. No verifiable resources are identified.",
      "processing_time": 54.03533983230591,
      "citing_paper_id": "31298398",
      "cited_paper_id": 10232742
    },
    {
      "context_text": "End-to-end conversation models have been explored within the framework of neural networks (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,b; Luan et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neural network-based conversation models. No verifiable resources are identified.",
      "processing_time": 54.73986506462097,
      "citing_paper_id": "31298398",
      "cited_paper_id": 10822819
    },
    {
      "context_text": "End-to-end conversation models have been explored within the framework of neural networks (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a,b; Luan et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neural network-based conversation models. No verifiable resources are identified.",
      "processing_time": 54.73986506462097,
      "citing_paper_id": "31298398",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No dataset names are present in the text.",
      "processing_time": 54.531357526779175,
      "citing_paper_id": "31298398",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. No dataset names are present in the text.",
      "processing_time": 54.531357526779175,
      "citing_paper_id": "31298398",
      "cited_paper_id": 16853283
    },
    {
      "context_text": "This is a technique that has seen success in machine translation, where large monolingual data sets have been used to improve translation models (Sennrich et al., 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large monolingual data sets' but does not specify any particular dataset name. The reference is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 55.944042444229126,
      "citing_paper_id": "31298398",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "(Sennrich et al., 2016) report methods of exploiting mono-lingual data—usually available in much larger quantities—to improve the performance of machine translation, including multi-task learning of a language model for the decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions monolingual data but does not specify a named dataset. It focuses on methods and approaches rather than a specific reusable resource.",
      "processing_time": 54.53163957595825,
      "citing_paper_id": "31298398",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "(Sennrich et al., 2016) report methods of exploiting monolingual data—usually available in much larger quantities—to improve the performance of machine translation, including multi-task learning of a language model for the decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses methods for using monolingual data to improve machine translation, but does not mention any specific datasets.",
      "processing_time": 53.16739463806152,
      "citing_paper_id": "31298398",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Multi-task learning has been successfully used to improve performance in various tasks, including machine translation (Sennrich et al., 2016) and image captioning (Luong et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods. No verifiable resources are identified.",
      "processing_time": 53.58327865600586,
      "citing_paper_id": "31298398",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "Multi-task learning has been successfully used to improve performance in various tasks, including machine translation (Sennrich et al., 2016) and image captioning (Luong et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks and methods. There are no clear identifiers for datasets in the given context.",
      "processing_time": 54.89595174789429,
      "citing_paper_id": "31298398",
      "cited_paper_id": 15600925
    },
    {
      "context_text": "For instance, Imamura et al. [16] modeled the confusion matrices at a cluster level to capture the shared confusion patterns among annotators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modeling confusion matrices. No verifiable resources are identified.",
      "processing_time": 54.498135566711426,
      "citing_paper_id": "267547503",
      "cited_paper_id": 3644663
    },
    {
      "context_text": "Inspired by the crowdsourcing literature [16], we develop this clustering-based user model that assumes user embeddings (and hence preferences) span a common set of vectors given by V ; each user embedding is a weighted combination of these vectors (Figure 4b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for clustering user embeddings. The context focuses on the development of a user model inspired by crowdsourcing literature.",
      "processing_time": 56.481746196746826,
      "citing_paper_id": "267547503",
      "cited_paper_id": 3644663
    },
    {
      "context_text": "The observed annotations are often modeled as the confused outputs for the hidden ground-truth labels and the confusion of each annotator is characterized by an individual confusion matrix [7, 29, 30].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modeling annotator confusion using confusion matrices.",
      "processing_time": 53.55868625640869,
      "citing_paper_id": "267547503",
      "cited_paper_id": 45813168
    },
    {
      "context_text": "When determining a more preferable response, different people may share certain common criteria such as helpfulness, but disagree on other aspects such as length or style.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It discusses human preferences in responses but does not reference any verifiable resource.",
      "processing_time": 55.33993363380432,
      "citing_paper_id": "267547503",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "In vanilla RLHF, there are three steps [42, 34, 25]: (1) obtain a supervised fine-tuned policy (denoted as π SFT ) using a demonstration dataset; (2) learn a Reward Model (RM) using a preference dataset; and (3) optimize the LM against the learned reward model using reinforcement learning (RL),…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'demonstration dataset' and 'preference dataset', which are domain-qualified data phrases. However, no specific names are provided.",
      "processing_time": 55.72358012199402,
      "citing_paper_id": "267547503",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "In vanilla RLHF, there are three steps [42, 34, 25]: (1) obtain a supervised fine-tuned policy (denoted as π SFT ) using a demonstration dataset; (2) learn a Reward Model (RM) using a preference dataset; and (3) optimize the LM against the learned reward model using reinforcement learning (RL),…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'demonstration dataset' and 'preference dataset', which are domain-qualified data phrases. However, no specific names are provided.",
      "processing_time": 55.72358012199402,
      "citing_paper_id": "267547503",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 245329531
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 247594830
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 248118878
    },
    {
      "context_text": "RLHF has been utilized to improve the LM performances on a variety of NLP tasks, including summarization [42, 34], question answering [24, 23], instruction following [25] and improving helpfulness and harmlessness [1, 11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.12717604637146,
      "citing_paper_id": "267547503",
      "cited_paper_id": 252596089
    },
    {
      "context_text": "…for democratizing LM utilization across various domains, such as fostering more engaging learning experiences by customizing the content for students in online education and reducing the administrative burden of medical professionals by tailoring summary reports for their respective priorities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.459038972854614,
      "citing_paper_id": "267547503",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "A personalized human feedback (or preference) dataset D p = { ( x i , y i, 1 , y i, 2 , u i ) } ni =1 consists of n samples where u i ∈ U is the information of the user who annotates the data or provides the preferences, x i is the prompt, y i, 1 and y i, 2 are two generated texts such that y i, 1…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context describes a personalized human feedback dataset but does not provide a specific name for the dataset. The description is too generic and lacks a clear identifier.",
      "processing_time": 55.86476469039917,
      "citing_paper_id": "267547503",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "As we do not have access to the SFT model used by Stiennon et al. [34], we initialize the personalized LM in P-DPO using an open-source SFT 7 .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to an SFT model and an open-source SFT, which are not datasets.",
      "processing_time": 55.71064472198486,
      "citing_paper_id": "267547503",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Current dominating RLHF approaches implicitly assume that all crowdsourced human feedback (and preference) comes from the same distribution [42, 34, 25, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to crowdsourced human feedback. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 57.56209373474121,
      "citing_paper_id": "267547503",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Current dominating RLHF approaches implicitly assume that all crowdsourced human feedback (and preference) comes from the same distribution [42, 34, 25, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to crowdsourced human feedback. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 57.56209373474121,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "…text generation tasks (1) Generation with Synthetic Preferences: we use the Reddit TL;DR summarization dataset 4 curated by Stiennon et al. [34], where the pairwise preferences over Reddit post summaries were crowdsourced from multiple workers, and a GPT-J 6B model [37] supervised…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit TL;DR summarization dataset"
      ],
      "dataset_descriptions": {
        "Reddit TL;DR summarization dataset": "Used to generate summaries with synthetic preferences, focusing on crowdsourced pairwise preferences over Reddit post summaries to train a GPT-J 6B model."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Reddit TL;DR summarization dataset' which is a specific, verifiable dataset used for generating summaries with synthetic preferences.",
      "processing_time": 64.36100745201111,
      "citing_paper_id": "267547503",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "The methods most closely related to our work include prefix-tuning [20] and soft-prompt learning [19], which prepend task-specific continuous embeddings to the transformer layers or the embedded inputs to adapt the pre-trained LMs to specific downstream tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.04499840736389,
      "citing_paper_id": "267547503",
      "cited_paper_id": 230433941
    },
    {
      "context_text": "The methods most closely related to our work include prefix-tuning [20] and soft-prompt learning [19], which prepend task-specific continuous embeddings to the transformer layers or the embedded inputs to adapt the pre-trained LMs to specific downstream tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.04499840736389,
      "citing_paper_id": "267547503",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "We integrate it into the personalized language model through soft prompting [19].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for integrating personalized language models through soft prompting.",
      "processing_time": 53.781094789505005,
      "citing_paper_id": "267547503",
      "cited_paper_id": 233296808
    },
    {
      "context_text": "We utilized LoRA [14] for training, with LoRA α = 32 , LoRA r = 8 and LoRA dropout 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LoRA but does not refer to it as a dataset. It is used as a method for training models.",
      "processing_time": 54.65415620803833,
      "citing_paper_id": "267547503",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "We utilized LoRA [14] for training, with LoRA α = 16 , LoRA r = 8 and LoRA dropout 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LoRA but does not refer to it as a dataset. LoRA is a method for adapting large language models, not a dataset.",
      "processing_time": 55.582111835479736,
      "citing_paper_id": "267547503",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "…model, recent studies have imposed more sophisticated structures on reward learning, e.g., training separate reward models for different targets [11], assigning fine-grained rewards to text segments [39], or merging LMs fine-tuned with separate reward models on pre-defined preference dimensions…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on reward learning and fine-tuning models, which are methodological aspects.",
      "processing_time": 56.163896322250366,
      "citing_paper_id": "267547503",
      "cited_paper_id": 252596089
    },
    {
      "context_text": "Reinforcement Learning from Human Feedback (RLHF) is a widely adopted framework to align pre-trained large Language Models (LMs) [3, 6, 35] with human values [42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Reinforcement Learning from Human Feedback) and a type of model (pre-trained large Language Models).",
      "processing_time": 56.57542967796326,
      "citing_paper_id": "267547503",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The Personalized-Soups (P-SOUPS) dataset [17] includes pairwise feedback for responses to instructions in GPT-4 Alpaca [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Personalized-Soups (P-SOUPS)"
      ],
      "dataset_descriptions": {
        "Personalized-Soups (P-SOUPS)": "Used to collect and analyze pairwise feedback for responses to instructions in GPT-4 Alpaca, focusing on improving personalized text generation through user preferences."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Personalized-Soups (P-SOUPS)', which is used for instruction tuning with GPT-4 Alpaca. The dataset is clearly identified and its usage is described.",
      "processing_time": 68.81228685379028,
      "citing_paper_id": "267547503",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "(2) Instruction Following under Different Preference Profiles: we use the Personalized-Soups dataset [17], which includes pairwise feedback for responses to GPT4-Alpaca instructions [26] under various user preference profiles, and Tulu-7B [38], an instruction fine-tuned LLaMA-7B model, as the SFT.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Personalized-Soups"
      ],
      "dataset_descriptions": {
        "Personalized-Soups": "Used to evaluate instruction following under different user preference profiles, specifically analyzing pairwise feedback for responses to GPT4-Alpaca instructions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Personalized-Soups' dataset, which is used for evaluating instruction following under different user preference profiles. The dataset includes pairwise feedback for responses to GPT4-Alpaca instructions.",
      "processing_time": 65.07563900947571,
      "citing_paper_id": "267547503",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "The Personalized-Soups (P-SOUPS) dataset [17] includes pairwise comparisons for responses to GPT-4 Alpaca instructions [26].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Personalized-Soups (P-SOUPS)"
      ],
      "dataset_descriptions": {
        "Personalized-Soups (P-SOUPS)": "Used to conduct pairwise comparisons of responses to GPT-4 Alpaca instructions, focusing on personalized text generation and evaluating model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Personalized-Soups (P-SOUPS)', which is used for pairwise comparisons of responses to GPT-4 Alpaca instructions. The dataset is clearly identified and relevant to personalized text generation.",
      "processing_time": 68.6702606678009,
      "citing_paper_id": "267547503",
      "cited_paper_id": 257985497
    },
    {
      "context_text": "To personalize language generations without re-training the LM, prompts with relevant historical data are used to align the LM outputs with user intents [21] or opinions [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 'historical data' in a generic sense, which does not meet the criteria for inclusion.",
      "processing_time": 56.44058060646057,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258865429
    },
    {
      "context_text": "Following [17], we evaluate the performance by the pairwise win-rate between P-DPO and the baseline methods on the generations for 50 instructions from the Koala evaluation [10], using the same AlpacaFarm-based framework 6 [9] for simulated evaluation by GPT-4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Koala evaluation' as a source of instructions for evaluating the performance of P-DPO against baseline methods. However, 'Koala evaluation' does not meet the criteria for a specific, verifiable dataset.",
      "processing_time": 58.80068373680115,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "Uncovering a reparametrization of the optimal LM under the learned RM and the RL objective, DPO directly learns the LM using a preference dataset [28].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'preference dataset' but does not provide a specific name or identifier. The term 'preference dataset' is too generic and lacks the necessary specificity to be included.",
      "processing_time": 57.03199553489685,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "As noted in Zhu et al. [41], Rafailov et al. [28], the underlying assumption for using (1) to learn the reward model r vanilla is that the user preferences follow the Bradley-Terry (BT) model [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Bradley-Terry model) and a general approach to learning a reward model. No verifiable datasets are referenced.",
      "processing_time": 56.51919722557068,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Vanilla DPO As noted in Rafailov et al. [28], given any RM r , its corresponding optimal policy under the RL objective (2) can be written as where Z ( x ) is a generated-text-independent (or y -independent) normalizing factor.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a method or theoretical framework.",
      "processing_time": 52.9178352355957,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Direct Preference Optimization (DPO) has emerged as an RL-free algorithm which directly fine-tunes the language model using the preference data, significantly improving the training efficiency of RLHF [28].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DPO) and a general reference to 'preference data'. No verifiable dataset names are provided.",
      "processing_time": 56.50768995285034,
      "citing_paper_id": "267547503",
      "cited_paper_id": 258959321
    },
    {
      "context_text": "Conditional Natural Language Generation With the advent of autoregressive pre-trained LMs such as GPT-3 [3] and PaLM [6], natural language generation tasks are often performed via prompting or in-context learning approaches [22, 32, 8, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.034931898117065,
      "citing_paper_id": "267547503",
      "cited_paper_id": 259287175
    },
    {
      "context_text": "Conditional Natural Language Generation With the advent of autoregressive pre-trained LMs such as GPT-3 [3] and PaLM [6], natural language generation tasks are often performed via prompting or in-context learning approaches [22, 32, 8, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.034931898117065,
      "citing_paper_id": "267547503",
      "cited_paper_id": null
    },
    {
      "context_text": "Crowdsourcing When collecting large sets of labeled data (like in the preference data collection phase of RLHF), crowdsourcing is often adopted by first dispatching the unlabeled samples to multiple annotators and then estimating the ground-truth labels by aggregating the noisy annotations [33, 12].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses a general method for collecting and labeling data through crowdsourcing.",
      "processing_time": 54.42355728149414,
      "citing_paper_id": "267547503",
      "cited_paper_id": 263888978
    },
    {
      "context_text": "Personalization is a well studied topic in machine learning and recommendation systems [5], which focuses on generating tailored recommendations or responses based on user profile (e.g. demographic or interests) or historical data of individual users.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and areas of study. No verifiable resources are identified.",
      "processing_time": 54.40548849105835,
      "citing_paper_id": "267547503",
      "cited_paper_id": 265308990
    },
    {
      "context_text": "…Preferences: we use the Reddit TL;DR summarization dataset 4 curated by Stiennon et al. [34], where the pairwise preferences over Reddit post summaries were crowdsourced from multiple workers, and a GPT-J 6B model [37] supervised fine-tuned using the TRLX library 5 [13] is used as the SFT.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit TL;DR summarization dataset"
      ],
      "dataset_descriptions": {
        "Reddit TL;DR summarization dataset": "Used to train and evaluate models on summarizing Reddit posts, focusing on crowdsourced pairwise preferences over summaries to improve personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Reddit TL;DR summarization dataset, which is a specific, verifiable dataset used for training and evaluating models on summarization tasks.",
      "processing_time": 64.12199568748474,
      "citing_paper_id": "267547503",
      "cited_paper_id": 266163891
    },
    {
      "context_text": "…Preferences: we use the Reddit TL;DR summarization dataset 4 curated by Stiennon et al. [34], where the pairwise preferences over Reddit post summaries were crowdsourced from multiple workers, and a GPT-J 6B model [37] supervised fine-tuned using the TRLX library 5 [13] is used as the SFT.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit TL;DR summarization dataset"
      ],
      "dataset_descriptions": {
        "Reddit TL;DR summarization dataset": "Used to train and evaluate models on summarizing Reddit posts, focusing on crowdsourced pairwise preferences over summaries to improve personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Reddit TL;DR summarization dataset, which is a specific, verifiable dataset used for training and evaluating models on summarization tasks.",
      "processing_time": 64.12199568748474,
      "citing_paper_id": "267547503",
      "cited_paper_id": null
    },
    {
      "context_text": "…that perform consistently across different groups in the data, including enhancing the performance on challenging samples via group invariant learning [40] and mitigating the performance discrepancies between different preference groups via distributionally robust optimization (DRO) methods [4].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on techniques for improving model performance across different groups.",
      "processing_time": 55.01208281517029,
      "citing_paper_id": "267547503",
      "cited_paper_id": 267657954
    },
    {
      "context_text": "For example, empathetic agents received more positive user ratings, including greater likeability and trustworthiness than controls (Brave et al., 2005).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the results of a study. The context focuses on the outcomes of using empathetic agents rather than the data used.",
      "processing_time": 56.212231159210205,
      "citing_paper_id": "271403894",
      "cited_paper_id": 429769
    },
    {
      "context_text": "Li et al. (2016) ally have three modules: encoding, matching and aggregation (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhang et al., 2018b; Chen and Wang, 2019; Feng et al., 2019; Yuan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or models. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 56.01252198219299,
      "citing_paper_id": "271403894",
      "cited_paper_id": 2867243
    },
    {
      "context_text": "Li et al. (2016) ally have three modules: encoding, matching and aggregation (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2017; Zhou et al., 2018b; Zhang et al., 2018b; Chen and Wang, 2019; Feng et al., 2019; Yuan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods or models. The cited papers' titles do not provide additional context to identify a dataset.",
      "processing_time": 56.01252198219299,
      "citing_paper_id": "271403894",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "In NLP, empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains (Klein, 1998; Liu and Picard, 2005; Wright and McCarthy, 2008; Fitzpatrick et al., 2017; Zhou et al., 2018a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research findings and studies. No verifiable resources are identified.",
      "processing_time": 54.566587924957275,
      "citing_paper_id": "271403894",
      "cited_paper_id": 3772810
    },
    {
      "context_text": "In NLP, empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains (Klein, 1998; Liu and Picard, 2005; Wright and McCarthy, 2008; Fitzpatrick et al., 2017; Zhou et al., 2018a).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research findings and studies. No verifiable resources are identified.",
      "processing_time": 54.566587924957275,
      "citing_paper_id": "271403894",
      "cited_paper_id": 13508942
    },
    {
      "context_text": "One possible psychological root of this link is that persona is highly correlated to personality (Leary and Allen, 2011), which in turn inﬂuences empathy and em-pathetic responding (Costa et al., 2014).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies and concepts. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.390883445739746,
      "citing_paper_id": "271403894",
      "cited_paper_id": 4585434
    },
    {
      "context_text": "Persona has been shown to be highly correlated with personality (Leary and Allen, 2011), which in turn inﬂuences empathy (Richen-doller and Weaver III, 1994; Costa et al., 2014).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to studies that explore the relationship between personality and empathy.",
      "processing_time": 54.141597270965576,
      "citing_paper_id": "271403894",
      "cited_paper_id": 4585434
    },
    {
      "context_text": "Model Settings We use fastText (Paszke et al., 2019) embeddings of size 300 to initialize BoW and HLSTM.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (fastText) and a model architecture (BoW and HLSTM).",
      "processing_time": 55.150489807128906,
      "citing_paper_id": "271403894",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "Note that the BoW, HLSTM (Lowe et al., 2015) and Bi-encoder (Humeau et al., 2020) baselines share the same Tri-encoder architecture, where the ﬁnal matching score is the dot product between the average of context and persona representations and the response representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not introduce any new datasets either.",
      "processing_time": 55.15698599815369,
      "citing_paper_id": "271403894",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "Note that the BoW, HLSTM (Lowe et al., 2015) and Bi-encoder (Humeau et al., 2020) baselines share the same Tri-encoder architecture, where the ﬁnal matching score is the dot product between the average of context and persona representations and the response representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not introduce any new datasets either.",
      "processing_time": 55.15698599815369,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "HLSTM (Lowe et al., 2015): The context encoder has an utterance-level BiLSTM and a context-level BiLSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only describes a method (HLSTM). The title of the cited paper suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 57.201720237731934,
      "citing_paper_id": "271403894",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "Among the non-pretrained models, DIM outperforms BoW and HLSTM by large margins on all datasets, demonstrating the importance of ﬁner-grained matching and hierarchical aggregation for response selection.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only referring to 'all datasets' without providing explicit names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.56828832626343,
      "citing_paper_id": "271403894",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "Among the non-pretrained models, DIM outperforms BoW and HLSTM by large margins on all datasets, demonstrating the importance of ﬁner-grained matching and hierarchical aggregation for response selection.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only referring to 'all datasets' without providing explicit names. The cited papers do not provide additional context to identify specific datasets.",
      "processing_time": 56.56828832626343,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "…Despite the growing number of studies in neural conversational models, less attention has been paid to make conversations empathetic until recently (Siddique et al., 2017; Morris et al., 2018; Shi and Yu, 2018; Lin et al., 2019b; Shin et al., 2019; Rashkin et al., 2019; Li et al., 2019; Lin et…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies and papers. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 56.25193738937378,
      "citing_paper_id": "271403894",
      "cited_paper_id": 30676033
    },
    {
      "context_text": "…of studies in neural conversational models, less attention has been paid to make conversations empathetic until recently (Siddique et al., 2017; Morris et al., 2018; Shi and Yu, 2018; Lin et al., 2019b; Shin et al., 2019; Rashkin et al., 2019; Li et al., 2019; Lin et al., 2019a; Zandie and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. There are no clear identifiers for datasets, models, or other resources.",
      "processing_time": 55.5554895401001,
      "citing_paper_id": "271403894",
      "cited_paper_id": 49427985
    },
    {
      "context_text": "We use BERT (Devlin et al., 2019) as our sentence encoders.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT as a sentence encoder, which is a model, not a dataset. No datasets are mentioned.",
      "processing_time": 54.55450963973999,
      "citing_paper_id": "271403894",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Recently, Rashkin et al. (2019) presented a new dataset and benchmark towards empathetic conversations and found that both Transformer-based generative models (Vaswani et al., 2017) and BERT-based retrieval models (Devlin et al., 2019) relying on this dataset exhibit stronger empathy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "empathetic conversations"
      ],
      "dataset_descriptions": {
        "empathetic conversations": "Used to train and evaluate Transformer-based generative and BERT-based retrieval models for empathy in conversations, focusing on improving empathetic responses in dialogue systems."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a new dataset and benchmark for empathetic conversations, which is directly relevant to personalized text generation. The dataset is used to train and evaluate models for empathy in conversations.",
      "processing_time": 64.69247794151306,
      "citing_paper_id": "271403894",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Persona-Based Conversational Models In recent years, personalized conversational models are emerging (Li et al., 2016; Zhang et al., 2018a; Wolf et al., 2019; Chan et al., 2019; Madotto et al., 2019; Zheng et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.69099950790405,
      "citing_paper_id": "271403894",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "Persona-Based Conversational Models In recent years, personalized conversational models are emerging (Li et al., 2016; Zhang et al., 2018a; Wolf et al., 2019; Chan et al., 2019; Madotto et al., 2019; Zheng et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.69099950790405,
      "citing_paper_id": "271403894",
      "cited_paper_id": 202788651
    },
    {
      "context_text": "Persona-Based Conversational Models In recent years, personalized conversational models are emerging (Li et al., 2016; Zhang et al., 2018a; Wolf et al., 2019; Chan et al., 2019; Madotto et al., 2019; Zheng et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 55.69099950790405,
      "citing_paper_id": "271403894",
      "cited_paper_id": 207863734
    },
    {
      "context_text": "…last layer, respectively, where d denotes the embedding size of BERT, and m , q 9 Reusing the same positional information for all persona sentences (Wolf et al., 2019) to model position invariance produces worse performance in our preliminary experiments. and n denote the sequence lengths of…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) and a research finding about positional information in persona sentences.",
      "processing_time": 54.970781087875366,
      "citing_paper_id": "271403894",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "…d from the last layer, respectively, where d denotes the embedding size of BERT, and m , q 9 Reusing the same positional information for all persona sentences (Wolf et al., 2019) to model position invariance produces worse performance in our preliminary experiments. and n denote the sequence…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BERT) and a paper discussing neural network-based conversational agents. No verifiable datasets are referenced.",
      "processing_time": 56.35683631896973,
      "citing_paper_id": "271403894",
      "cited_paper_id": 59222757
    },
    {
      "context_text": "In addition, our empirical analysis of empathetic con-1 versations in (Rashkin et al., 2019) also shows that for most emotions, the empathetic responses from two different persons 2 have more differences than that between two disjoint sets of random responses, as shown in Figure 1.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'empathetic conversations'. No clear, verifiable dataset names are provided.",
      "processing_time": 55.68127179145813,
      "citing_paper_id": "271403894",
      "cited_paper_id": 143152561
    },
    {
      "context_text": "In addition, our CoBERT is substantially faster (9.18x) than BERT+DIM in inference, thus more practical in real-world applications.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between CoBERT and BERT+DIM in terms of inference speed.",
      "processing_time": 54.983688831329346,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "Evaluation Metrics Following (Zhou et al., 2018b; Gu et al., 2019; Humeau et al., 2020), we evaluate models using Recall@ k where each test example has C possible candidates to select from, abbreviated to R@ k , as well as mean reciprocal rank (MRR).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.66382336616516,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "Evaluation Metrics Following (Zhou et al., 2018b; Gu et al., 2019; Humeau et al., 2020), we evaluate models using Recall@ k where each test example has C possible candidates to select from, abbreviated to R@ k , as well as mean reciprocal rank (MRR).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 55.66382336616516,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "DIM (Gu et al., 2019): A state-of-the-art non-pretraiend model for persona-based response selection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DIM, which is a model, not a dataset. No datasets are explicitly mentioned or used in the given citation.",
      "processing_time": 55.02085542678833,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "BERT+DIM (Gu et al., 2019): The BERT+DIM model combines the beneﬁts from both the strong sentence representation of BERT and the rich ﬁner-0 grained matching of DIM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT+DIM). The citation is about combining BERT and DIM for better sentence representation and matching, which is not related to dataset usage.",
      "processing_time": 57.431182622909546,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "DIM adopts ﬁner-grained matching and hierarchical aggregation to learn rich matching representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on the technique used in the paper rather than a dataset.",
      "processing_time": 55.89035391807556,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "%) are comparable to the in-domain performance of DIM (40.6% on offmychest and 31.3% on happy), suggesting that our CoBERT can generalize well across em-pathetic conversations in contrasting sentiments.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses performance metrics on unnamed datasets.",
      "processing_time": 54.0696656703949,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "We follow the released code 10 to implement DIM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to code implementation. The cited paper title suggests a focus on personalized response selection but does not introduce a dataset.",
      "processing_time": 55.87396192550659,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "However, BERT+DIM performs slightly worse than CoBERT, suggesting that the more complex matching and aggregation methods in DIM do not lead to performance improvement over our multi-hop co-attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance comparisons.",
      "processing_time": 53.062978982925415,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "The simple Bi-encoder performs noticeably better than DIM, suggesting that sentence representation is another critical factor in response selection and that BERT can provide much richer representation than the BiLSTM used in DIM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between models (Bi-encoder and DIM). No verifiable resources are identified.",
      "processing_time": 55.363365650177,
      "citing_paper_id": "271403894",
      "cited_paper_id": 201058480
    },
    {
      "context_text": "Poly-encoder (Humeau et al., 2020): A state-of-the-art BERT-based model for response selection.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (Poly-encoder) and does not reference any specific dataset. The citation is focused on describing the model rather than a dataset.",
      "processing_time": 55.89534664154053,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "A recent work Humeau et al. (2020) proposed Poly-encoder, an efﬁcient BERT-based response selection model that obtained the state-of-the-art performance on multiple conversation datasets. discuss any topics on their corresponding sub-forums/subreddits.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'conversation datasets' but does not specify any particular dataset names. The citation is focused on the method (Poly-encoder) rather than a specific dataset.",
      "processing_time": 56.58675456047058,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "Bi-encoder (Humeau et al., 2020): A state-of-the-art BERT-based model for empathetic response selection (Rashkin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its application. The context is about a method (Bi-encoder) and its use in empathetic response selection.",
      "processing_time": 56.8782639503479,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "Note that we do not consider Cross-encoder (Humeau et al., 2020) as an appropriate baseline because it performs two orders of magnitude slower than Poly-encoder in inference, rendering it intractable for real-time applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models. No verifiable resources are identified.",
      "processing_time": 53.83960962295532,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "CoBERT outperforms several competitive base-lines on PEC, including Poly-encoder (Humeau et al., 2020), the state-of-the-art BERT-based response selection model, by large margins.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing CoBERT's performance against other models.",
      "processing_time": 55.49859690666199,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "Similar to the Bi-encoder (Humeau et al., 2020), we concatenate context utterances as a single context sentence before passing it into BERT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BERT) and a model (Bi-encoder). The citation is about a methodological approach, not a dataset.",
      "processing_time": 56.79455232620239,
      "citing_paper_id": "271403894",
      "cited_paper_id": 210063976
    },
    {
      "context_text": "By sharing the user and item embeddings between the rating prediction and review text generation components, we seek shared representations that will be beneficial to both components [2, 23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving shared embeddings.",
      "processing_time": 53.812317848205566,
      "citing_paper_id": "86471548",
      "cited_paper_id": 352650
    },
    {
      "context_text": "We report the F-measure, which is the geometric mean of the precision and recall of ROUGE-1 (covering 1-grams) and ROUGE-L (covering the longest subsequences) respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (ROUGE-1 and ROUGE-L). These are excluded as per the instructions.",
      "processing_time": 55.85808706283569,
      "citing_paper_id": "86471548",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Another metric is ROUGE [17] which has been extensively used for text summarization.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "ROUGE is a metric, not a dataset, and thus does not meet the criteria for inclusion.",
      "processing_time": 54.060137033462524,
      "citing_paper_id": "86471548",
      "cited_paper_id": 964287
    },
    {
      "context_text": "For all the datasets, MRG consistently has higher BLEU and ROUGE scores than the baselines, indicating that it synthesizes review texts that are closer to the ground-truth references.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU and ROUGE). The cited papers are about the metrics themselves, not datasets.",
      "processing_time": 56.30299925804138,
      "citing_paper_id": "86471548",
      "cited_paper_id": 964287
    },
    {
      "context_text": "For all the datasets, MRG consistently has higher BLEU and ROUGE scores than the baselines, indicating that it synthesizes review texts that are closer to the ground-truth references.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU and ROUGE). The cited papers are about the metrics themselves, not datasets.",
      "processing_time": 56.30299925804138,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Long Short-Term Memory (LSTM) [9], a gated version of recurrent neural networks, is capable of capturing long-term dependencies and is demonstrably an effective approach [3, 6, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 54.73703861236572,
      "citing_paper_id": "86471548",
      "cited_paper_id": 1805048
    },
    {
      "context_text": "Long Short-Term Memory (LSTM) [9], a gated version of recurrent neural networks, is capable of capturing long-term dependencies and is demonstrably an effective approach [3, 6, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 54.73703861236572,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Long Short-Term Memory (LSTM) [9], a gated version of recurrent neural networks, is capable of capturing long-term dependencies and is demonstrably an effective approach [3, 6, 31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 54.73703861236572,
      "citing_paper_id": "86471548",
      "cited_paper_id": 206741496
    },
    {
      "context_text": "The vast majority of recommendation works are based on modeling ratings [31], and most rely onmatrix factorization [13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches in recommendation systems.",
      "processing_time": 54.051573514938354,
      "citing_paper_id": "86471548",
      "cited_paper_id": 1805048
    },
    {
      "context_text": "All models usingword embeddings are initialized from Glove [27] pre-trained word embeddings of 200 dimensions.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'GloVe' but it is used as pre-trained word embeddings, which are not considered datasets according to the instructions.",
      "processing_time": 55.51945352554321,
      "citing_paper_id": "86471548",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "where E ∈ RmxV is the learned word embedding matrix initialized randomly or from pre-trained word embeddings [21, 27], W∗,b∗ are learned projection matrices and biases initialized randomly, and σ ,φ are sigmoid and tanh activation functions respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained word embeddings which are likely referring to methods or models rather than datasets.",
      "processing_time": 55.30790877342224,
      "citing_paper_id": "86471548",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "where E ∈ RmxV is the learned word embedding matrix initialized randomly or from pre-trained word embeddings [21, 27], W∗,b∗ are learned projection matrices and biases initialized randomly, and σ ,φ are sigmoid and tanh activation functions respectively.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained word embeddings which are likely referring to methods or models rather than datasets.",
      "processing_time": 55.30790877342224,
      "citing_paper_id": "86471548",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "For an additional benefit, the generated text could also potentially serve as explanations [7, 37] to the rating-based recommendation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential benefit of generated text serving as explanations for rating-based recommendations.",
      "processing_time": 55.3035683631897,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2023519
    },
    {
      "context_text": "• NMF: Non-negative Matrix Factorization [15] ensures that the factorized user and item matrices are non-negative.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Non-negative Matrix Factorization (NMF) but does not refer to any specific dataset. It is discussing a method, not a dataset.",
      "processing_time": 56.44057846069336,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "For PMF, NMF, and SVD++, we use grid search to find the number of latent factors from {10, . . . , 100} where\nwe end up with 10 for PMF, 30 for NMF, and 10 for SVD++.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only algorithms and their parameters. No verifiable resources are identified.",
      "processing_time": 55.054484844207764,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Among the three baselines, SVD++ with the advantage of modeling neighborhood information tends to perform better than PMF and NMF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms (SVD++, PMF, NMF).",
      "processing_time": 55.294864892959595,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "These include the popularmethods such as Probabilistic Matrix Factorization or PMF[22], Non-negative Matrix Factorization or NMF [15], and Singular Value Decomposition with neighborhood information or SVD++ [12], which we will use as baselines to validate the benefits of content modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions methods (PMF, NMF, SVD++) but does not refer to any specific datasets. The context is about using these methods as baselines for validation.",
      "processing_time": 57.44189238548279,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "In turn, PMF performs slightly better than NMF with 80% data in training, but deteriorates quickly with more sparsity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the performance of PMF and NMF algorithms under varying data sparsity conditions.",
      "processing_time": 56.57263708114624,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "In turn, [24] is based on pairwise ranking, which is usually seen as a different formulation from rating prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No verifiable resources are identified.",
      "processing_time": 55.08919548988342,
      "citing_paper_id": "86471548",
      "cited_paper_id": 2765857
    },
    {
      "context_text": "By incorporating visual features in the text generation, our problem is related to image captioning [35, 36].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general relation to image captioning. No verifiable resources are identified.",
      "processing_time": 55.90319609642029,
      "citing_paper_id": "86471548",
      "cited_paper_id": 3120635
    },
    {
      "context_text": "MRG 1 is implemented using Tensorflow [1] and trained with batch size of 64.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Tensorflow but does not refer to it as a dataset. It is used as a software toolkit for training models, which is excluded according to the instructions.",
      "processing_time": 57.22958779335022,
      "citing_paper_id": "86471548",
      "cited_paper_id": 6287870
    },
    {
      "context_text": "Even in those cases that relied on review texts, the approach was usually to aggregate all the reviews belonging to an item into a document [19, 20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to using review texts. No clear, verifiable resource names are provided.",
      "processing_time": 56.20666742324829,
      "citing_paper_id": "86471548",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Such approaches [19, 20, 32] outperform rating-only methods.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of approaches. No verifiable resources are identified.",
      "processing_time": 55.41828107833862,
      "citing_paper_id": "86471548",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "For instance, some expect extraneous inputs not applicable to our setting, such as generating tips based on a review [16], generating review based on pre-defined phrases [25], or considering the neighborhood reviews of a user [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks or methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.834415674209595,
      "citing_paper_id": "86471548",
      "cited_paper_id": 10154792
    },
    {
      "context_text": "For instance, some expect extraneous inputs not applicable to our setting, such as generating tips based on a review [16], generating review based on pre-defined phrases [25], or considering the neighborhood reviews of a user [34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general tasks or methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.834415674209595,
      "citing_paper_id": "86471548",
      "cited_paper_id": 29161455
    },
    {
      "context_text": "We use smoothed BLEU [18] and report the results of BLEU scores from 1 to 4.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BLEU scores but does not refer to a specific dataset. BLEU is a metric, not a dataset, and thus should not be included.",
      "processing_time": 57.21926236152649,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "BLEU [26] is a well-known metric for evaluating the quality of generated text, which has beenwidely used for machine translation and image captioning.",
      "catation_intent": "none",
      "resource_type": "metric",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BLEU as a metric, which is excluded according to the instructions. No datasets are mentioned.",
      "processing_time": 55.440430879592896,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Due\nto space limitation, we omit BLEU-2 and BLEU-3 measurements as both of them show the same trend as discussed.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only evaluation metrics (BLEU-2 and BLEU-3). These are excluded as per the instructions.",
      "processing_time": 57.12618660926819,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Therefore, we apply soft attention mechanism [3, 35] to allow the model to weigh on the more relevant annotations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (soft attention mechanism).",
      "processing_time": 55.30559539794922,
      "citing_paper_id": "86471548",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Note that we focus on models that fit rating values, rather than pairwise rankings [8, 29] that are generally seen as a different formulation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. No dataset names are present in the citation span.",
      "processing_time": 56.241051197052,
      "citing_paper_id": "86471548",
      "cited_paper_id": 13907106
    },
    {
      "context_text": "In a different formulation, MLP was combined with matrix factorization to learn pairwise ranking scores [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining MLP with matrix factorization.",
      "processing_time": 55.12717795372009,
      "citing_paper_id": "86471548",
      "cited_paper_id": 13907106
    },
    {
      "context_text": "Non-linear transformations promise to learn better representations as shown for various tasks [6, 14, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only non-linear transformations and their effectiveness in learning representations.",
      "processing_time": 55.59169244766235,
      "citing_paper_id": "86471548",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Non-linear transformations promise to learn better representations as shown for various tasks [6, 14, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only non-linear transformations and their effectiveness in learning representations.",
      "processing_time": 55.59169244766235,
      "citing_paper_id": "86471548",
      "cited_paper_id": 206741496
    },
    {
      "context_text": "Att2Seq andMRG use the same number of dimensions for user and item embeddings as well as rating embeddings for the former.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.203847885131836,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "We compare with two versions of Att2Seq which are: without observing rating (- Rating) that is similar to our setting and with observed rating (+Rating) that is the authors’ original setting (which confers it an advantage since the ground-truth rating is given).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only variations of a method (Att2Seq). The context focuses on comparing different settings of the method rather than using a particular dataset.",
      "processing_time": 57.43819308280945,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "On the other hand, product review generation [5] seeks to generate the review text based on user, item, and the observed rating.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general task of product review generation. No verifiable resources are identified.",
      "processing_time": 56.24328947067261,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "For LSTM-basedmodels Att2Seq, SAT, andMRG, we use the same number of dimensions of the LSTM cells (256).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and their configurations.",
      "processing_time": 54.70516395568848,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "In some cases, SAT that uses image information is slightly better than Att2Seq (no image).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (SAT and Att2Seq).",
      "processing_time": 55.937010049819946,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "• Att2Seq: Attribute-to-Sequence [5] treats user, item, and rating as three attributes and generates review text with the attention-based multilayer LSTMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (Att2Seq) for generating product reviews from attributes, which is not a dataset.",
      "processing_time": 57.741734981536865,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "SAT and MRG use one-layer LSTM whereas Att2Seq uses two layers of LSTMs as we respect the original design of the authors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.99598717689514,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "The baselines are text generation methods Att2Seq that relies on user, item, rating, and SAT that relies on image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to methods (Att2Seq, SAT) and their reliance on certain types of data (user, item, rating, image).",
      "processing_time": 59.415074586868286,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "However, this is an imperfect comparison as Att2Seq factors in user and item, whereas SAT does not.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a comparison between two methods (Att2Seq and SAT).",
      "processing_time": 55.51723289489746,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "In Section 4, we will compare to Att2Seq as one of the review generation baselines, assessing their performance in both scenarios when the test rating is unknown (our setting) and when it is given (their original setting).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Att2Seq) used for comparison. The context focuses on the experimental setup and performance assessment.",
      "processing_time": 57.386378049850464,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "Examining the baselines, we see that for Chicago, Los Angeles, and San Francisco, Att2Seq with rating (+Rating) performs slightly better than without rating (-Rating).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance comparisons of a method (Att2Seq) with and without ratings. No clear, verifiable dataset names are provided.",
      "processing_time": 58.19938540458679,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "The review text generation technique closest to ours is Att2Seq [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Att2Seq). The context is about comparing techniques, not using a dataset.",
      "processing_time": 56.88251972198486,
      "citing_paper_id": "86471548",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "We use the pre-trained model of VGGNet on ImageNet [4] without finetuning, and treat the 14 × 14 × 512 feature map output from the fifth convolutional layer (conv5_3) as our image annotations.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ImageNet but only in the context of pre-training a model, not as a dataset used directly for the research. Therefore, it does not meet the criteria for inclusion.",
      "processing_time": 58.79557418823242,
      "citing_paper_id": "86471548",
      "cited_paper_id": 57246310
    },
    {
      "context_text": "Respondents were asked to rate the images based on their (1) similarity to the concept’s training images and (2) similarity to the text prompt.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes a method for rating images, which is not a dataset.",
      "processing_time": 56.571412563323975,
      "citing_paper_id": "258866047",
      "cited_paper_id": 8236317
    },
    {
      "context_text": "Specifically, each input ( 𝑡, ℓ ) is encoded with Random Fourier Features [Rahimi and Recht 2007; Tancik et al. 2020] into a 2048-dimensional vector, 𝑓 ( 𝑡, ℓ ) ∈ R 2048 , modulated by 1024 random frequencies.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers are about methods, not datasets.",
      "processing_time": 56.06922507286072,
      "citing_paper_id": "258866047",
      "cited_paper_id": 219791950
    },
    {
      "context_text": "Specifically, each input (t, l) is encoded with Random Fourier Features [25,35]",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving Random Fourier Features.",
      "processing_time": 54.89855623245239,
      "citing_paper_id": "258866047",
      "cited_paper_id": 219791950
    },
    {
      "context_text": "Image inversion is the process of finding a latent code that can be passed to a generator to reconstruct a given image [43, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of image inversion in GANs.",
      "processing_time": 55.4061918258667,
      "citing_paper_id": "258866047",
      "cited_paper_id": 231603119
    },
    {
      "context_text": "Image inversion is the process of finding a latent code that can be passed to a generator to reconstruct a given image [Xia et al. 2022; Zhu et al. 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the process of image inversion using GANs. No verifiable resources are identified.",
      "processing_time": 56.582934617996216,
      "citing_paper_id": "258866047",
      "cited_paper_id": 231603119
    },
    {
      "context_text": "Recent advancements in largescale autoregressive models [28, 44] and diffusion models [8, 12, 21] have resulted in unprecedented diversity and fidelity in visual content creation guided by a free-form text prompt [4,20,27,31,33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 56.76796293258667,
      "citing_paper_id": "258866047",
      "cited_paper_id": 231979499
    },
    {
      "context_text": "Recent advancements in largescale autoregressive models [28, 44] and diffusion models [8, 12, 21] have resulted in unprecedented diversity and fidelity in visual content creation guided by a free-form text prompt [4,20,27,31,33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 56.76796293258667,
      "citing_paper_id": "258866047",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "Recent advancements in largescale autoregressive models [28, 44] and diffusion models [8, 12, 21] have resulted in unprecedented diversity and fidelity in visual content creation guided by a free-form text prompt [4,20,27,31,33].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 56.76796293258667,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recent advancements in large-scale autoregressive models [Ramesh et al. 2021; Yu et al. 2022] and diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Nichol and Dhari-wal 2021] have resulted in unprecedented diversity and fidelity in visual content creation guided by a free-form text prompt…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.933249711990356,
      "citing_paper_id": "258866047",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "Recent large-scale text-to-image models [27,31,33] have quickly revolutionized the world of artistic creation, demonstrating an unprecedented ability to generate incredible and diverse visual content.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.32056188583374,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recent large-scale text-to-image models [Ramesh et al. 2022; Rom-bach et al. 2022; Saharia et al. 2022] have quickly revolutionized the world of artistic creation, demonstrating an unprecedented ability to generate incredible and diverse visual content.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.38430953025818,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "In the context of diffusion models, inversion often refers to the process of finding an initial noise latent that can be iteratively denoised into the target image [8, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of diffusion models and the process of inversion. No verifiable resources are identified.",
      "processing_time": 57.19882798194885,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "In the context of diffusion models, inversion often refers to the process of finding an initial noise latent that can be iteratively denoised into the target image [8, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of diffusion models and the process of inversion. No verifiable resources are identified.",
      "processing_time": 57.19882798194885,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253581838
    },
    {
      "context_text": "…models [Dhariwal and Nichol 2021; Ho et al. 2020; Nichol and Dhari-wal 2021] have resulted in unprecedented diversity and fidelity in visual content creation guided by a free-form text prompt [Balaji et al. 2023; Nichol et al. 2021; Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.473114252090454,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "In the context of diffusion models, inversion often refers to the process of finding an initial noise latent that can be iteratively denoised into the target image [Dhariwal and Nichol 2021; Mokady et al. 2022; Ramesh et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and processes in diffusion models.",
      "processing_time": 55.69549489021301,
      "citing_paper_id": "258866047",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "In the context of diffusion models, inversion often refers to the process of finding an initial noise latent that can be iteratively denoised into the target image [Dhariwal and Nichol 2021; Mokady et al. 2022; Ramesh et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and processes in diffusion models.",
      "processing_time": 55.69549489021301,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253581838
    },
    {
      "context_text": "To train the neural mapper, we follow a similar optimization scheme to that of Textual Inversion but directly optimize the parameters of the mapper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Textual Inversion) and a general approach to training a neural mapper.",
      "processing_time": 56.860774517059326,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We evaluate NeTI with respect to state-of-the-art inversion methods (Textual Inversion (TI) [Gal et al. 2023a], Extended Textual Inversion (XTI) [Voynov et al. 2023]) and fine-tuning approaches (DreamBooth [Ruiz et al. 2023a], CustomDiffusion [Ku-mari et al. 2023]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for evaluation. No verifiable resources are identified.",
      "processing_time": 56.031057596206665,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We evaluate NeTI with respect to state-of-the-art inversion methods (Textual Inversion (TI) [Gal et al. 2023a], Extended Textual Inversion (XTI) [Voynov et al. 2023]) and fine-tuning approaches (DreamBooth [Ruiz et al. 2023a], CustomDiffusion [Ku-mari et al. 2023]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for evaluation. No verifiable resources are identified.",
      "processing_time": 56.031057596206665,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Inversion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called 'Textual Inversion'. No verifiable datasets are identified.",
      "processing_time": 56.43744134902954,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Extended Textual Inversion 𝒫 + Space Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Textual Inversion'. No verifiable datasets are referenced.",
      "processing_time": 56.32521462440491,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Textual Inversion.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called 'Textual Inversion'. No verifiable datasets are identified.",
      "processing_time": 56.09081482887268,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "An interesting avenue could be exploring pairing our approach with faster encoder-based approaches [Arar et al. 2023; Gal et al. 2023b; Ruiz et al. 2023b; Wei et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about exploring the combination of methods, not the use of datasets.",
      "processing_time": 57.624587297439575,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "An interesting avenue could be exploring pairing our approach with faster encoder-based approaches [Arar et al. 2023; Gal et al. 2023b; Ruiz et al. 2023b; Wei et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. The context is about exploring the combination of methods, not the use of datasets.",
      "processing_time": 57.624587297439575,
      "citing_paper_id": "258866047",
      "cited_paper_id": 259847716
    },
    {
      "context_text": "In Textual Inversion, Gal et al .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Textual Inversion. No verifiable resources are identified.",
      "processing_time": 56.160656213760376,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "A general problem with personalization methods, including NeTI, is their inherent tradeoff between reconstruction quality and editability [9, 37, 46].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem with personalization methods. No verifiable resources are identified.",
      "processing_time": 55.93445563316345,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "A general problem with personalization methods, including NeTI, is their inherent tradeoff between reconstruction quality and ed-itability [Gal et al. 2023a; Tov et al. 2021; Zhu et al. 2020].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a general problem with personalization methods.",
      "processing_time": 55.08987092971802,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "[9] introduce a new token S∗ and a corresponding embedding vector v∗ ∈ P representing the concept.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method involving a new token and embedding vector.",
      "processing_time": 54.95336365699768,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "However, in the context of inversion and personalization spaces, this property is not commonly used [1, 2, 9, 41].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general property in the context of inversion and personalization spaces.",
      "processing_time": 55.62258195877075,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "However, in the context of inversion and personalization spaces, this property is not commonly used [1, 2, 9, 41].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general property in the context of inversion and personalization spaces.",
      "processing_time": 55.62258195877075,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "In a sense, the entire network represents a concept in P ∗ defined by its learned parameters, resulting in a neural representation for Textual Inversion, which we dub NeTI .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Textual Inversion. No verifiable resources are identified.",
      "processing_time": 56.125956296920776,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Spaces for Inversion and Personalization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a concept related to personalizing text-to-image generation. No clear, verifiable resource names are present.",
      "processing_time": 57.30409502983093,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "A new line of work has recently explored encoder-based approaches for mapping a given concept to its textual representation [Gal et al. 2023b; Shi et al. 2023; Wei et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent work on encoder-based approaches. No verifiable resources are identified.",
      "processing_time": 56.521435499191284,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "With Neural Textual Inversion (NeTI), we are interested in finding a personalized representation for a user-provided concept depicted using a small set of images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a method (Neural Textual Inversion) and a general approach to personalizing text-to-image generation.",
      "processing_time": 57.997448444366455,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Textual Inversion [9] (left) invert into the P space where a single token embedding is learned for all timesteps and U-Net layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called 'Textual Inversion'. The citation is focused on describing a technique rather than a dataset.",
      "processing_time": 56.95387387275696,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "However, in the context of inversion and personalization spaces, this property is not commonly used [Abdal et al. 2019, 2020; Gal et al. 2023a; Voynov et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to properties in the context of inversion and personalization spaces.",
      "processing_time": 55.61608552932739,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "However, in the context of inversion and personalization spaces, this property is not commonly used [Abdal et al. 2019, 2020; Gal et al. 2023a; Voynov et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to properties in the context of inversion and personalization spaces.",
      "processing_time": 55.61608552932739,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "[9], freezes the generative model and optimizes an input vector to represent the desired subject or artistic style.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses optimizing an input vector for personalizing text-to-image generation, which is a methodological approach rather than a dataset.",
      "processing_time": 57.61005425453186,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Textual Inversion [Gal et al. 2023a] (left) invert into the P space where a single token embedding is learned for all timesteps and U-Net layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called 'Textual Inversion'. No verifiable datasets are referenced.",
      "processing_time": 55.78967761993408,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We show that a new concept can be learned by optimizing the parameters of our neural representation, similar to the standard optimization mechanism in Textual Inversion.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Textual Inversion). No verifiable resources are identified.",
      "processing_time": 55.67056393623352,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We evaluate NeTI with respect to state-of-the-art inversion methods (Textual Inversion (TI) [9], Extended Textual Inversion (XTI) [41]) and fine-tuning approaches (DreamBooth [32], CustomDiffusion [14]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for evaluation. No verifiable resources are identified.",
      "processing_time": 55.857839584350586,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We evaluate NeTI with respect to state-of-the-art inversion methods (Textual Inversion (TI) [9], Extended Textual Inversion (XTI) [41]) and fine-tuning approaches (DreamBooth [32], CustomDiffusion [14]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models used for evaluation. No verifiable resources are identified.",
      "processing_time": 55.857839584350586,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Finally, as with Textual Inversion, NeTI still struggles to generate images depicting multiple concepts, as shown in the third row of Figure 14.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Textual Inversion) and a limitation in generating images with multiple concepts.",
      "processing_time": 56.126851081848145,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "In [6, 9] it was first observed that personalization can be approached as an inversion problem where text embeddings are optimized to describe the target concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to personalization using textual inversion.",
      "processing_time": 55.35562181472778,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "[9] invert a given concept into a single vector representation residing in the input space of the text encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for personalizing text-to-image generation.",
      "processing_time": 54.81059455871582,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "In [Cohen et al. 2022; Gal et al. 2023a] it was first observed that personalization can be approached as an inversion problem where text embeddings are optimized to describe the target concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to personalization using textual inversion.",
      "processing_time": 55.477333307266235,
      "citing_paper_id": "258866047",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Numerous works have analyzed the latent spaces of pretrained text-to-image diffusion models [Haas et al. 2023; Kwon et al. 2023; Park et al. 2023; Zhu et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to works analyzing latent spaces of pretrained text-to-image diffusion models.",
      "processing_time": 56.24804925918579,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253018703
    },
    {
      "context_text": "Numerous works have analyzed the latent spaces of pretrained text-to-image diffusion models [Haas et al. 2023; Kwon et al. 2023; Park et al. 2023; Zhu et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to works analyzing latent spaces of pretrained text-to-image diffusion models.",
      "processing_time": 56.24804925918579,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257631803
    },
    {
      "context_text": "Numerous works have already analyzed the latent spaces of pretrained text-to-image diffusion models [11, 15, 22, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to pretrained models and their latent spaces. No verifiable resources are identified.",
      "processing_time": 56.19671607017517,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253018703
    },
    {
      "context_text": "Numerous works have already analyzed the latent spaces of pretrained text-to-image diffusion models [11, 15, 22, 47].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to pretrained models and their latent spaces. No verifiable resources are identified.",
      "processing_time": 56.19671607017517,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257631803
    },
    {
      "context_text": "[2023a] proposed the personalization-by-fine-tuning approach, where one directly fine-tunes the generative model to represent the user-specified concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for personalizing generative models.",
      "processing_time": 55.023085594177246,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253018768
    },
    {
      "context_text": "[2023a] proposed the personalization-by-fine-tuning approach, where one directly fine-tunes the generative model to represent the user-specified concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for personalizing generative models.",
      "processing_time": 55.023085594177246,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253801961
    },
    {
      "context_text": "This initial noise latent can then be used for editing the given input image using text prompts [7, 13, 16, 38], but is less suited for representing new personalized concepts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for editing images using text prompts. No verifiable resources are identified.",
      "processing_time": 56.18205809593201,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253224280
    },
    {
      "context_text": "This initial latent can then be used for editing the image using text prompts [Coua-iron et al. 2022; Kawar et al. 2023; Liew et al. 2022; Tumanyan et al. 2022], but is less suited for representing new concepts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about using latent representations for image editing with text prompts, which does not involve a specific dataset.",
      "processing_time": 57.79103469848633,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253224280
    },
    {
      "context_text": "In the context of time -dependent representation, previous works have demonstrated that using different inputs for different timesteps of a diffusion model has intriguing properties [Liew et al. 2022; Patashnik et al. 2023] with Gal et al .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 55.95316505432129,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253224280
    },
    {
      "context_text": "In the context of time -dependent representation, previous works have demonstrated that using different inputs for different timesteps of a diffusion model has intriguing properties [Liew et al. 2022; Patashnik et al. 2023] with Gal et al .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 55.95316505432129,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257632209
    },
    {
      "context_text": "In the context of time-dependent representation, previous works have demonstrated that using different inputs for different timesteps of a diffusion model has intriguing properties [16, 23] with Gal et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous works and their findings on time-dependent representations in diffusion models.",
      "processing_time": 56.1288537979126,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253224280
    },
    {
      "context_text": "In the context of time-dependent representation, previous works have demonstrated that using different inputs for different timesteps of a diffusion model has intriguing properties [16, 23] with Gal et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous works and their findings on time-dependent representations in diffusion models.",
      "processing_time": 56.1288537979126,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257632209
    },
    {
      "context_text": "The personalization of text-to-image models has given rise to various downstream applications such as image editing [13,39] and personalized 3D generation [17,18,26,29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of personalized text-to-image models. No verifiable resources are identified.",
      "processing_time": 56.05609965324402,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253510536
    },
    {
      "context_text": "The personalization of text-to-image models has given rise to various downstream applications such as image editing [Kawar et al. 2023; Valevski et al. 2022] and personalized 3D generation [Lin et al. 2023; Metzer et al. 2022; Raj et al. 2023; Richardson et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing applications of personalized text-to-image models.",
      "processing_time": 55.47973585128784,
      "citing_paper_id": "258866047",
      "cited_paper_id": 253510536
    },
    {
      "context_text": "DreamBooth XTI NeTI Average Rating ( ↑ ) 2.77 ± 1.20 3.71 ± 1.13 3.15 ± 1.09 3.97 ± 1.12 Reconstruction-Editability.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only numerical ratings and a comparison metric. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.503355264663696,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "[41] demonstrated that one can improve inversion approaches by inverting into an extended input space, P+, where a different vector p ∈ P is learned for each attention layer in the denoising U-Net network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on improving inversion approaches in text-to-image generation.",
      "processing_time": 56.21248197555542,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "By operating over P+ , XTI achieves improved reconstructions and editability but still fails to capture concept-specific details.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (XTI) and a modified version (P+). No verifiable resources are identified.",
      "processing_time": 56.539631366729736,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "[41] (middle) introduce the P+ space where different embeddings are optimized for each attention layer but are shared across all timesteps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model related to text-to-image generation.",
      "processing_time": 54.89870262145996,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "This vector resides in the recently dubbed P space [Voynov et al. 2023] containing all possible input",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept called 'P space'. No verifiable resources are identified.",
      "processing_time": 55.26725625991821,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Notably, these results are obtained after 500 training steps, the same number used for DreamBooth and XTI.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training steps. No verifiable resources are identified.",
      "processing_time": 54.542686223983765,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "space [41] containing all possible input embeddings to the text encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to input embeddings for a text encoder.",
      "processing_time": 54.72124195098877,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "[41] propose an extended P+ latent space composed of a set of vectors p ∈ P , one for each layer of the UNet denoising network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (UNet denoising network).",
      "processing_time": 55.255126953125,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Finally, when compared to XTI trained for the same number of steps, NeTI achieves both improved reconstruction and editability across this dropout curve.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two models (NeTI and XTI). There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.9261417388916,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "In the supplementary, we additionally compare NeTI and XTI after 250 and 500 training steps.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training steps and model comparisons.",
      "processing_time": 54.045599937438965,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Yet, our approach borrows from the same intuition regarding the roles of keys and values in cross-attention layers, as also discussed in [Patashnik et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 56.05587911605835,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257632209
    },
    {
      "context_text": "Yet, our approach borrows from the same intuition regarding the roles of keys and values in crossattention layers, as also discussed in [23].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 54.905190229415894,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257632209
    },
    {
      "context_text": ", style) also aligns with previous observations of the denoising process [5, 23].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous observations about the denoising process.",
      "processing_time": 55.0852165222168,
      "citing_paper_id": "258866047",
      "cited_paper_id": 257632209
    },
    {
      "context_text": "Personalization arises in applications where different clients need models specifically customized to their environment and user profiles (Yang and Eisenstein, 2017; Mazaré et al., 2018; Flek, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts of personalization in NLP applications. No verifiable resources are identified.",
      "processing_time": 56.23381781578064,
      "citing_paper_id": "238252929",
      "cited_paper_id": 15295411
    },
    {
      "context_text": "Personalization arises in applications where different clients need models specifically customized to their environment and user profiles (Yang and Eisenstein, 2017; Mazaré et al., 2018; Flek, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general concepts of personalization in NLP applications. No verifiable resources are identified.",
      "processing_time": 56.23381781578064,
      "citing_paper_id": "238252929",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "A.1 Federated Learning as an Application Federated learning is a form of distributed learning where data never leaves each user’s device (Wang et al., 2021; Konečnỳ et al., 2018; Mireshghallah et al., 2020; Basu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches related to federated learning.",
      "processing_time": 54.90738892555237,
      "citing_paper_id": "238252929",
      "cited_paper_id": 40441734
    },
    {
      "context_text": "where data never leaves each user’s device (Wang et al., 2021; Konečnỳ et al., 2018; Mireshghallah et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to methods or approaches in federated learning.",
      "processing_time": 55.47907495498657,
      "citing_paper_id": "238252929",
      "cited_paper_id": 40441734
    },
    {
      "context_text": "In addition to IMDB and Yelp, we also report the performance of the proposed method on the Sentiment140 dataset (Go et al.; Caldas et al., 2018), which is a set of Tweets collected from Twitter and labeled positive or negative based on the emojis in each Tweet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Sentiment140"
      ],
      "dataset_descriptions": {
        "Sentiment140": "Used to evaluate the proposed method's performance on sentiment classification, focusing on tweets labeled positive or negative based on emojis."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Sentiment140 dataset, which is a specific, verifiable dataset used for sentiment analysis. The dataset is described as a set of Tweets labeled based on emojis.",
      "processing_time": 65.09673857688904,
      "citing_paper_id": "238252929",
      "cited_paper_id": 53701546
    },
    {
      "context_text": "For this dataset, We use the methodology provided by Li et al. (2019) to preprocess and partition this dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not provide a specific name for the dataset being used. It only mentions 'this dataset' without any clear identifier.",
      "processing_time": 55.98238515853882,
      "citing_paper_id": "238252929",
      "cited_paper_id": 166227978
    },
    {
      "context_text": "We use the methodology provided by (Li et al., 2019) to preprocess and partition this dataset.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset name, only a generic reference to 'this dataset'. No clear, verifiable resource is identified.",
      "processing_time": 55.81205606460571,
      "citing_paper_id": "238252929",
      "cited_paper_id": 166227978
    },
    {
      "context_text": "For parameterizations of the user identifiers, we use parameter tying (He et al., 2019), where the user identifiers use the same set of parameters for their embeddings as the rest of the user utterance.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (parameter tying) for handling user identifiers in embeddings.",
      "processing_time": 55.16280794143677,
      "citing_paper_id": "238252929",
      "cited_paper_id": 211069439
    },
    {
      "context_text": "This parameter tying couples the learning problems for both domains (user identifier and text) and allows us to jointly learn from the full data, as\nin (He et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on parameter tying and joint learning, which are methodological aspects.",
      "processing_time": 56.74131512641907,
      "citing_paper_id": "238252929",
      "cited_paper_id": 211069439
    },
    {
      "context_text": "This parameter tying couples the learning problems for both domains (user identifier and text) and allows us to jointly learn from the full data, as in (He et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for unsupervised text style transfer. No verifiable resources are identified.",
      "processing_time": 55.85437893867493,
      "citing_paper_id": "238252929",
      "cited_paper_id": 211069439
    },
    {
      "context_text": "We target this setup since it is a good candidate for personalization, given how a conventionally trained global model often fails to accommodate all users (Kulkarni et al., 2020; Mansour et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of personalization in federated learning. No verifiable resources are named.",
      "processing_time": 56.20429301261902,
      "citing_paper_id": "238252929",
      "cited_paper_id": 211296702
    },
    {
      "context_text": "We target this setup since it is a good candidate for personalization, given how a conventionally trained global model often fails to accommodate all users (Kulkarni et al., 2020; Mansour et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the concept of personalization in federated learning. No verifiable resources are named.",
      "processing_time": 56.20429301261902,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "1), which is a real-world application of such personalization (Kulkarni et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a real-world application of personalization in federated learning.",
      "processing_time": 55.49927997589111,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "built between all users, and then, it is personalized for each client using their data (Kulkarni et al., 2020; Schneider and Vlachos, 2019; Lee et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to personalization techniques in federated learning.",
      "processing_time": 54.697221517562866,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "This need for customization stems from the inherent heterogeneity existing in the data and the labels, especially when the task is classification (Kulkarni et al., 2020; Wang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general need for customization in data and labels for classification tasks.",
      "processing_time": 55.114818811416626,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "We also show that UserIdentifier is effective in a federated learning setup (Appendix A.1), which is a real-world application of such personalization (Kulkarni et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method or setup (federated learning) and a potential application (personalization), but no dataset names are provided.",
      "processing_time": 58.362576484680176,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "Corresponding author email: fatemeh@ucsd.edu\nbuilt between all users, and then, it is personalized for each client using their data (Kulkarni et al., 2020; Schneider and Vlachos, 2019; Lee et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to personalization techniques in federated learning.",
      "processing_time": 54.62718486785889,
      "citing_paper_id": "238252929",
      "cited_paper_id": 213004721
    },
    {
      "context_text": "This shows that the proposed method performs better in setups where personalization is actually needed (Deng et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to personalization setups. No clear, verifiable resource is identified.",
      "processing_time": 55.80154252052307,
      "citing_paper_id": "238252929",
      "cited_paper_id": 214713948
    },
    {
      "context_text": "We use the IMDB (Diao et al., 2014) and Yelp (Tang et al., 2015) datasets for comparison with the UserAdapter method (Zhong et al., 2021) and for the ablation studies.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMDB",
        "Yelp"
      ],
      "dataset_descriptions": {
        "IMDB": "Used for comparison and ablation studies in sentiment analysis, evaluating the effectiveness of the UserAdapter method in few-shot user learning scenarios.",
        "Yelp": "Used for comparison and ablation studies in sentiment analysis, assessing the performance of the UserAdapter method in adapting to user-specific sentiments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of IMDB and Yelp datasets for comparison and ablation studies in sentiment analysis, which is relevant to personalized text generation.",
      "processing_time": 69.17705583572388,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "5%−13% classification accuracy improvement on average, over the prefix-tuning based method UserAdapter (Zhong et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called UserAdapter. The context focuses on the performance improvement of a method over another.",
      "processing_time": 56.15154433250427,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "p 1 , p bee 1 denote the trainable prefix vector for users kat and bee, in the prefix tuning method (Zhong et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (prefix tuning) and a paper title. No verifiable resources are identified.",
      "processing_time": 56.14682197570801,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "UserAdapter (Zhong et al., 2021), the state-ofthe-art in personalized sentiment analysis, takes a prefix-tuning based approach (Li and Liang, 2021), as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UserAdapter) and a technique (prefix-tuning).",
      "processing_time": 55.29346776008606,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": ", 2015) datasets for comparison with the UserAdapter method (Zhong et al., 2021) and for the ablation studies.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets' but does not specify any particular dataset names. It only indicates that datasets were used for comparison and ablation studies.",
      "processing_time": 56.54274582862854,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "…tokens, and random tokens (all types), and observe that, surprisingly, random identifiers, sampled from all possible tokens in the vocabulary perform best, providing 1.5%−13% classification accuracy improvement on average, over the prefix-tuning based method UserAdapter (Zhong et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (UserAdapter) and general tokens. No verifiable resources are identified.",
      "processing_time": 55.794384717941284,
      "citing_paper_id": "238252929",
      "cited_paper_id": 236478014
    },
    {
      "context_text": "The approach is similar in essence to those of Daumé III (2009); Kocoń et al. (2021); Kocoń et al. (2021), which augments each individual feature with domain annotations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on augmenting features with domain annotations, which is a methodological approach.",
      "processing_time": 56.75555396080017,
      "citing_paper_id": "238252929",
      "cited_paper_id": 246288645
    },
    {
      "context_text": "…social media platforms such as Reddit and Twitter provide us with good opportunities to build a large scale of collections of naturally occurring conversations (Xifra and Grau, 2010; De Choudhury and De, 2014; Schrading et al., 2015) and also make it possible to provide consistent personalities.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions social media platforms like Reddit and Twitter as sources of naturally occurring conversations, but does not specify any named datasets. The reference to 'collections' is too generic.",
      "processing_time": 57.6506769657135,
      "citing_paper_id": "234757004",
      "cited_paper_id": 1578178
    },
    {
      "context_text": "There are also several dialog datasets that focus on chit-chat scenarios, such as PERSONA-CHAT dataset (Zhang et al., 2018), Reddit dataset (Al-Rfou et al., 2016), Twitter dataset (Li et al., 2016b) and PersonalDialog dataset (Zheng et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONA-CHAT",
        "Reddit dataset",
        "Twitter dataset",
        "PersonalDialog"
      ],
      "dataset_descriptions": {
        "PERSONA-CHAT": "Used to train dialogue systems for generating personalized responses, focusing on incorporating persona information into conversations.",
        "Reddit dataset": "Utilized to study casual and diverse conversational patterns, enhancing the ability of models to generate natural and engaging dialogues.",
        "Twitter dataset": "Applied to analyze short-form, informal conversations, improving the generation of concise and contextually relevant responses.",
        "PersonalDialog": "Employed to develop dialogue systems that can handle personal and emotional topics, enhancing the empathy and engagement of generated text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific dialog datasets that are used for chit-chat scenarios, which are directly relevant to personalized text generation.",
      "processing_time": 78.60146021842957,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "There are also several dialog datasets that focus on chit-chat scenarios, such as PERSONA-CHAT dataset (Zhang et al., 2018), Reddit dataset (Al-Rfou et al., 2016), Twitter dataset (Li et al., 2016b) and PersonalDialog dataset (Zheng et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONA-CHAT",
        "Reddit dataset",
        "Twitter dataset",
        "PersonalDialog"
      ],
      "dataset_descriptions": {
        "PERSONA-CHAT": "Used to train dialogue systems for generating personalized responses, focusing on incorporating persona information into conversations.",
        "Reddit dataset": "Utilized to study casual and diverse conversational patterns, enhancing the ability of models to generate natural and engaging dialogues.",
        "Twitter dataset": "Applied to analyze short-form, informal conversations, improving the generation of concise and contextually relevant responses.",
        "PersonalDialog": "Employed to develop dialogue systems that can handle personal and emotional topics, enhancing the empathy and engagement of generated text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific dialog datasets that are used for chit-chat scenarios, which are directly relevant to personalized text generation.",
      "processing_time": 78.60146021842957,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "…pairs with respondents’ proﬁle and posting histories, with related information from proﬁle and histories in blue and red respectively. models are often trained over conversations spoken by different people, ignoring their personality (Li et al., 2016b; Wei et al., 2019; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and training methods but does not reference any named datasets.",
      "processing_time": 55.57477021217346,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "…pairs with respondents’ proﬁle and posting histories, with related information from proﬁle and histories in blue and red respectively. models are often trained over conversations spoken by different people, ignoring their personality (Li et al., 2016b; Wei et al., 2019; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and training methods but does not reference any named datasets.",
      "processing_time": 55.57477021217346,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Li et al. (2016b) introduced the Speaker Model and the Speaker-Addressee Model that encoded user-id information into an additional vector and fed it into the decoder to capture the identity of the speakers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the models used for encoding user-id information.",
      "processing_time": 56.298842668533325,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "Li et al. (2016b) introduced the Speaker Model and the Speaker-Addressee Model that encoded user-id information into an additional vector and fed it into the decoder to capture the identity of the speakers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on describing the models used for encoding user-id information.",
      "processing_time": 56.298842668533325,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "14% increase in PC-Score, similar to Li et al. (2016b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PC-Score) and a reference to another paper. No verifiable resources are identified.",
      "processing_time": 57.08281970024109,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "14% increase in PC-Score, similar to Li et al. (2016b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (PC-Score) and a reference to another paper. No verifiable resources are identified.",
      "processing_time": 57.08281970024109,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "• Speaker Models: Similar to (Li et al., 2016b), we employed an additional vector to model the respondent A .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method or model for dialogue generation.",
      "processing_time": 55.174514293670654,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "• Speaker Models: Similar to (Li et al., 2016b), we employed an additional vector to model the respondent A .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method or model for dialogue generation.",
      "processing_time": 55.174514293670654,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "We also used BLEU (Papineni et al., 2002; Li et al., 2016a; Gal-ley et al., 2015) with n -grams (n= 1 ) to measure how many n-grams in generated responses overlap with those in reference responses.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the metric BLEU which is excluded as per instructions.",
      "processing_time": 54.6601984500885,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "We also used BLEU (Papineni et al., 2002; Li et al., 2016a; Gal-ley et al., 2015) with n -grams (n= 1 ) to measure how many n-grams in generated responses overlap with those in reference responses.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the metric BLEU which is excluded as per instructions.",
      "processing_time": 54.6601984500885,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Despite the recent successes on building chit-chat agents using data-driven approaches (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016; Li et al., 2016c; Parthasarathi and Pineau, 2018), lack of a consistent personality is still one of the common issues.",
      "catation_intent": "findings",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The context discusses the issue of lack of consistent personality in chit-chat agents, which is a finding or gap in the field.",
      "processing_time": 58.72917318344116,
      "citing_paper_id": "234757004",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "Despite the recent successes on building chit-chat agents using data-driven approaches (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016; Li et al., 2016c; Parthasarathi and Pineau, 2018), lack of a consistent personality is still one of the common issues.",
      "catation_intent": "findings",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The context discusses the issue of lack of consistent personality in chit-chat agents, which is a finding or gap in the field.",
      "processing_time": 58.72917318344116,
      "citing_paper_id": "234757004",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "Despite the recent successes on building chit-chat agents using data-driven approaches (Ritter et al., 2011; Banchs and Li, 2012; Serban et al., 2016; Li et al., 2016c; Parthasarathi and Pineau, 2018), lack of a consistent personality is still one of the common issues.",
      "catation_intent": "findings",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The context discusses the issue of lack of consistent personality in chit-chat agents, which is a finding or gap in the field.",
      "processing_time": 58.72917318344116,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "Unlike task-oriented dialog agents that focus on completing speciﬁc tasks (Wen et al., 2017; Eric et al., 2017; Lei et al., 2018; Lowe et al., 2015), chit-chat agents need to dynamically interact with people, understand the meaning of human conversations (Hovy and Yang, 2021), and thereby make…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to task-oriented dialog agents and chit-chat agents. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.57036566734314,
      "citing_paper_id": "234757004",
      "cited_paper_id": 5932528
    },
    {
      "context_text": "Unlike task-oriented dialog agents that focus on completing speciﬁc tasks (Wen et al., 2017; Eric et al., 2017; Lei et al., 2018; Lowe et al., 2015), chit-chat agents need to dynamically interact with people, understand the meaning of human conversations (Hovy and Yang, 2021), and thereby make…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to task-oriented dialog agents and chit-chat agents. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.57036566734314,
      "citing_paper_id": "234757004",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "Unlike task-oriented dialog agents that focus on completing speciﬁc tasks (Wen et al., 2017; Eric et al., 2017; Lei et al., 2018; Lowe et al., 2015), chit-chat agents need to dynamically interact with people, understand the meaning of human conversations (Hovy and Yang, 2021), and thereby make…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to task-oriented dialog agents and chit-chat agents. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.57036566734314,
      "citing_paper_id": "234757004",
      "cited_paper_id": 10565222
    },
    {
      "context_text": "With the development of recent large scale social media data and the success of sequence to sequence framework (Serban et al., 2016; Shang et al., 2015; Sutskever et al., 2014), several personalized response generation models have been proposed, and we can only mention a few here due to space…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and frameworks. No verifiable resources are identified.",
      "processing_time": 55.21036219596863,
      "citing_paper_id": "234757004",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "With the development of recent large scale social media data and the success of sequence to sequence framework (Serban et al., 2016; Shang et al., 2015; Sutskever et al., 2014), several personalized response generation models have been proposed, and we can only mention a few here due to space…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and frameworks. No verifiable resources are identified.",
      "processing_time": 55.21036219596863,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Evaluation Metrics Most response generation models utilize perplexity, BLEU (Papineni et al., 2002) and recently BERTScore (Zhang et al., 2019) and Moverscore (Zhao et al., 2019) for evaluation (Serban et al., 2016; Xing et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and models. No verifiable resources are identified.",
      "processing_time": 54.84617233276367,
      "citing_paper_id": "234757004",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "Perplexity is used to measure how the outputs ﬁt test data (Vinyals and Le, 2015; Serban et al., 2016).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of perplexity as a metric.",
      "processing_time": 54.138654708862305,
      "citing_paper_id": "234757004",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "14% increase in PC-Score, similar to Li et al. (2016b). However, because the user set is quite large, the performance",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric and a reference to another paper. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.99200749397278,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "With the development of recent large scale social media data and the success of sequence to sequence framework (Serban et al., 2016; Shang et al., 2015; Sutskever et al., 2014), several personalized response generation models have been proposed, and we can only mention a few here due to space limits.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and frameworks. No verifiable resources are identified.",
      "processing_time": 55.162872076034546,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "…the development of recent large scale social media data and the success of sequence to sequence framework (Serban et al., 2016; Shang et al., 2015; Sutskever et al., 2014), several personalized response generation models have been proposed, and we can only mention a few here due to space limits.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and frameworks. The context is about the development of personalized response generation models using social media data and sequence-to-sequence frameworks.",
      "processing_time": 57.70882821083069,
      "citing_paper_id": "234757004",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Unlike task-oriented dialog agents that focus on completing specific tasks (Wen et al., 2017; Eric et al., 2017; Lei et al., 2018; Lowe et al., 2015),",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to task-oriented dialog agents. The cited paper title suggests a potential dataset, but it is not mentioned in the citation context.",
      "processing_time": 56.92542791366577,
      "citing_paper_id": "234757004",
      "cited_paper_id": 8379583
    },
    {
      "context_text": ", 2019) for evaluation (Serban et al., 2016; Xing et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 54.639642000198364,
      "citing_paper_id": "234757004",
      "cited_paper_id": 14247119
    },
    {
      "context_text": "Kottur et al. (2017) further extended these speaker models into multi-turn conversations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only an extension of speaker models into multi-turn conversations.",
      "processing_time": 54.28485631942749,
      "citing_paper_id": "234757004",
      "cited_paper_id": 20956365
    },
    {
      "context_text": "To evaluate persona consistency between user comments and generated sentences, Madotto et al. proposed consistency C score using sequence classiﬁcation model trained on Dialog NLI dataset (Welleck et al., 2019), a corpus based on Persona dataset, with NLI annotation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dialog NLI dataset",
        "Persona dataset"
      ],
      "dataset_descriptions": {
        "Dialog NLI dataset": "Used to train a sequence classification model for evaluating persona consistency between user comments and generated sentences, focusing on natural language inference annotations.",
        "Persona dataset": "Provided the basis for the Dialog NLI dataset, used to create a corpus with NLI annotations for evaluating persona consistency in dialogue systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Dialog NLI dataset' and the 'Persona dataset'. Both are specific datasets used in the research for training a sequence classification model to evaluate persona consistency.",
      "processing_time": 71.2695255279541,
      "citing_paper_id": "234757004",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "…between generated sentences and given user comments, Madotto et al. proposed consistency score using NLI models pre-trained on Dialog NLI dataset (Welleck et al., 2019), which is a corpus based on Persona dataset, with NLI annotation be-tween persona description sentences and dialogues utterance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Dialog NLI dataset",
        "Persona dataset"
      ],
      "dataset_descriptions": {
        "Dialog NLI dataset": "Used to train NLI models for evaluating consistency between generated sentences and user comments, focusing on natural language inference in dialogues.",
        "Persona dataset": "Provided the basis for the Dialog NLI dataset, with NLI annotations between persona descriptions and dialogue utterances, enhancing personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Dialog NLI dataset' and the 'Persona dataset'. Both are specific datasets used in the research.",
      "processing_time": 69.29440236091614,
      "citing_paper_id": "234757004",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "We used AdamW (Loshchilov and Hutter, 2018) as our optimizer with an initial learning rate of 5e-5 and a linear decay learning rate schedule.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions AdamW as an optimizer, which is a method, not a dataset. No datasets are mentioned in the citation.",
      "processing_time": 55.34579515457153,
      "citing_paper_id": "234757004",
      "cited_paper_id": 53592270
    },
    {
      "context_text": ", 2016b) and PersonalDialog dataset (Zheng et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonalDialog dataset"
      ],
      "dataset_descriptions": {
        "PersonalDialog dataset": "Used to generate personalized dialogues with diversified traits, focusing on enhancing conversational diversity and personalization in chatbot interactions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PersonalDialog dataset' which is a specific dataset used in personalized dialogue generation research.",
      "processing_time": 61.24749493598938,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "The PersonalDialog dataset (PD) (Zheng et al., 2020), collected from a Chinese social media Weibo, con-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonalDialog dataset (PD)"
      ],
      "dataset_descriptions": {
        "PersonalDialog dataset (PD)": "Used to train and evaluate personalized dialogue systems, focusing on generating diverse and contextually appropriate responses in a Chinese social media setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PersonalDialog dataset (PD)', which is a specific dataset used for personalized dialogue generation, fitting the research topic.",
      "processing_time": 63.20109820365906,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "Models with lower perplexity scores are found to demonstrate better performance to generate grammatical and fluent responses (Xie et al., 2019; Zheng et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context is about evaluating model performance using perplexity scores.",
      "processing_time": 55.91541600227356,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "ating personalization, Zheng et al. (2020) proposed to measure the accuracy of predicting personality traits by firstly training classifiers for different personality traits such as gender and age.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of training classifiers for personality traits. No verifiable resource names are provided.",
      "processing_time": 55.65293312072754,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "• Generative Memory Network w/ Profile: We designed a memory network model to incorporate user profiles P by doing attention over user attributes (Zheng et al., 2020) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for incorporating user profiles into a generative memory network.",
      "processing_time": 55.11994528770447,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "PD denotes the PersonalDialog dataset (Zheng et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonalDialog"
      ],
      "dataset_descriptions": {
        "PersonalDialog": "Used to generate personalized dialogues, focusing on incorporating diversified traits into conversational responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly identifies 'PersonalDialog' as a dataset, which is used for personalized dialogue generation.",
      "processing_time": 59.591476917266846,
      "citing_paper_id": "234757004",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "Recently, there are a few works using meta-learning and re-inforcement learning to enhance mutual persona perception Madotto et al. (2019); Kim et al. (2020); Majumder et al. (2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of meta-learning and reinforcement learning for enhancing persona perception in dialogue systems.",
      "processing_time": 56.951948404312134,
      "citing_paper_id": "234757004",
      "cited_paper_id": 165163819
    },
    {
      "context_text": "We only released raw data from pushshift.io( Baumgartner et al. (2020)), and open-sourced our scripts for preprocessing user attributes and models for reproducibility.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Pushshift Reddit Dataset"
      ],
      "dataset_descriptions": {
        "Pushshift Reddit Dataset": "Used to provide raw data for preprocessing user attributes, focusing on reproducibility of models and scripts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'raw data from pushshift.io', which is a specific dataset. The cited paper title confirms it is a dataset.",
      "processing_time": 61.498672008514404,
      "citing_paper_id": "234757004",
      "cited_paper_id": 210868223
    },
    {
      "context_text": "This concept is akin to the property for mixture models, which suggests that a mixture model can approximate any number of diverse user sub-populations to an arbitrary level of accuracy, given a sufficient number of components and appropriate parameterization (Goodfellow et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept from mixture models. No verifiable resources are referenced.",
      "processing_time": 55.328131675720215,
      "citing_paper_id": "275118993",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "As highlighted by Aroyo & Welty (2015); Aroyo et al. (2023a,b), “the notion of ‘one truth’ in crowdsourcing responses is a myth” and we need to account for the diversity in opinions and preferences.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about crowdsourcing and human annotation. No verifiable resources are identified.",
      "processing_time": 55.47721552848816,
      "citing_paper_id": "275118993",
      "cited_paper_id": 6134326
    },
    {
      "context_text": "As highlighted by Aroyo & Welty (2015); Aroyo et al. (2023a,b), “the notion of ‘one truth’ in crowdsourcing responses is a myth” and we need to account for the diversity in opinions and preferences.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about crowdsourcing and human annotation. No verifiable resources are identified.",
      "processing_time": 55.47721552848816,
      "citing_paper_id": "275118993",
      "cited_paper_id": 256105556
    },
    {
      "context_text": "Here, low evaluation accuracy is an indication that the eventual alignment with the minority user group will be poor after the PPO step since the reward model itself is not accurate.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the evaluation accuracy of a reward model in the context of PPO algorithms.",
      "processing_time": 55.38384246826172,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "Note that for the implementation in practice, we leverage the PPO algorithm (Schulman et al., 2017) as commonly used in existing RLHF pipelines (Stiennon et al., 2022b; Christian, 2020; Ouyang et al., 2022b).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the PPO algorithm but does not refer to any specific dataset. The cited papers do not introduce any datasets either.",
      "processing_time": 55.21049761772156,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "Note that for the implementation in practice, we leverage the PPO algorithm (Schulman et al., 2017) as commonly used in existing RLHF pipelines (Stiennon et al., 2022b; Christian, 2020; Ouyang et al., 2022b).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the PPO algorithm but does not refer to any specific dataset. The cited papers do not introduce any datasets either.",
      "processing_time": 55.21049761772156,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "Note that for the implementation in practice, we leverage the PPO algorithm (Schulman et al., 2017) as commonly used in existing RLHF pipelines (Stiennon et al., 2022b; Christian, 2020; Ouyang et al., 2022b).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the PPO algorithm but does not refer to any specific dataset. The cited papers do not introduce any datasets either.",
      "processing_time": 55.21049761772156,
      "citing_paper_id": "275118993",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "The current RLHF approaches (Stiennon et al., 2022b; Ziegler et al., 2020; Zhu et al., 2023) involve training a reward model based on human preference feedback and then fine-tuning the language model using proximal policy optimization (PPO) (Schulman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers are about reinforcement learning and human feedback, which do not introduce specific datasets.",
      "processing_time": 56.67971873283386,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "The current RLHF approaches (Stiennon et al., 2022b; Ziegler et al., 2020; Zhu et al., 2023) involve training a reward model based on human preference feedback and then fine-tuning the language model using proximal policy optimization (PPO) (Schulman et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and algorithms. The cited papers are about reinforcement learning and human feedback, which do not introduce specific datasets.",
      "processing_time": 56.67971873283386,
      "citing_paper_id": "275118993",
      "cited_paper_id": 256274676
    },
    {
      "context_text": "Our results in Table 2 and Table 3 show that MaxMin alignment keeps a high win-rate in comparison with conventional RLHF methods trained by PPO with a single reward model.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of methods. The context focuses on the performance of MaxMin alignment against conventional RLHF methods using PPO.",
      "processing_time": 56.83150100708008,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "Besides PPO, DPO (Direct Preference Optimization, Rafailov et al. (2023)) directly trains the large language model using human preferences without training the reward model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on comparing PPO and DPO methods.",
      "processing_time": 55.113765001297,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "We use the same dataset as Jang et al. (2023) and 10k data points from GPT4-Alpaca (Peng et al., 2023) are used as the instruction dataset to generate rollouts, collect pairwise feedback data, and PPO training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GPT4-Alpaca"
      ],
      "dataset_descriptions": {
        "GPT4-Alpaca": "Used as the instruction dataset to generate rollouts, collect pairwise feedback data, and for PPO training, focusing on personalized text generation."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions using 'GPT4-Alpaca' as an instruction dataset, which is a specific, identifiable dataset. The other dataset is not named but referenced as the same dataset used by Jang et al. (2023).",
      "processing_time": 68.135662317276,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "We use the same dataset as Jang et al. (2023) and 10k data points from GPT4-Alpaca (Peng et al., 2023) are used as the instruction dataset to generate rollouts, collect pairwise feedback data, and PPO training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GPT4-Alpaca"
      ],
      "dataset_descriptions": {
        "GPT4-Alpaca": "Used as the instruction dataset to generate rollouts, collect pairwise feedback data, and for PPO training, focusing on personalized text generation."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions using 'GPT4-Alpaca' as an instruction dataset, which is a specific, identifiable dataset. The other dataset is not named but referenced as the same dataset used by Jang et al. (2023).",
      "processing_time": 68.135662317276,
      "citing_paper_id": "275118993",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "For SFT, we fine-tune GPT-2 until convergence on reviews from the train split of the IMDB dataset and use this GPT-2 backbone for both the reward model and PPO training.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMDB dataset"
      ],
      "dataset_descriptions": {
        "IMDB dataset": "Used to fine-tune GPT-2 on movie reviews for supervised fine-tuning, focusing on sentiment analysis and text generation tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the IMDB dataset, which is a well-known dataset used for sentiment analysis and text classification tasks. It is clearly used for fine-tuning GPT-2 on review data.",
      "processing_time": 64.53135919570923,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "The PPO algorithm helps to learn a model that produces responses that maximize the reward (Ouyang et al., 2022b; Bai et al., 2022a).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the PPO algorithm and its application in learning a model. The cited papers do not introduce any datasets either.",
      "processing_time": 56.18922233581543,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "The PPO algorithm helps to learn a model that produces responses that maximize the reward (Ouyang et al., 2022b; Bai et al., 2022a).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the PPO algorithm and its application in learning a model. The cited papers do not introduce any datasets either.",
      "processing_time": 56.18922233581543,
      "citing_paper_id": "275118993",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "2: Reward Learning with EM : Utilize Algorithm 2 for learning rewards with EM to learn r uϕ for all user subpopulation u 3: Max-Min Policy Iteration : 4: for t = 0 to T − 1 do 5: Choosing Minimum Utility Subpopulation : Perform the PPO Update : Update policy π towards maximizing the objective: ) 10: end for 11: Output : Policy π T aligned with socially fair preference dataset where η denotes the weights distribution across human sub-population groups, D denotes the upper bound on the feature representation ∥ ψ ( y , x ) ∥≤ D for all ( x , y ) , λ ψ denotes the minimum eigenvalue of the feature matrix, β is the regularization parameter of RLHF framework, and diversity as defined in Definition 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method for reward learning and policy optimization but does not reference any named datasets.",
      "processing_time": 55.92958331108093,
      "citing_paper_id": "275118993",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "From the definition of the value function as an inner product of the state-action occupancy and the reward (Bertsekas, 2019; Sutton & Barto, 2018) E-step (hard cluster assignment) : assign h to the u -th cluster s.t. where exp ( r ϕu ( y 1 , x ) ) +exp ( r ϕu ( y 2 , x ) ) 5: end for 6: M-step :…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and algorithms. The context is about reinforcement learning and clustering, which are methods, not datasets.",
      "processing_time": 56.18411350250244,
      "citing_paper_id": "275118993",
      "cited_paper_id": 60035920
    },
    {
      "context_text": "The preference distribution under the Bradley-Terry (BT) preference model (Bradley & Terry, 1952) is written as where r ∗ ( y , x ) is the latent reward model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a statistical model. No dataset names are present in the citation span.",
      "processing_time": 54.91356945037842,
      "citing_paper_id": "275118993",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "The parametrized preference probability distribution p ϕ ( y 1 ≻ y 2 | x ) = exp( r ϕ ( y 1 , x )) exp( r ϕ ( y 1 , x )) + exp( r ϕ ( y 2 , x )) under the Bradley -Terry model (Bradley & Terry, 1952) is Lipschitz with respect to parameter ϕ .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a statistical model. No verifiable resources are identified.",
      "processing_time": 54.35896873474121,
      "citing_paper_id": "275118993",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "Hence, recent literature (Sagawa et al., 2020) has shown leveraging the prior knowledge of user-group information, can mitigate this by constructing the uncertainty set Q ′ in terms of these groups H u ∈ U , also referred to as the group distributionally robust optimization (Group-DRO).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Group-DRO) and a concept (user-group information).",
      "processing_time": 55.3241753578186,
      "citing_paper_id": "275118993",
      "cited_paper_id": 208176471
    },
    {
      "context_text": "With this, the general utility formulation boils down to Hence we can utilize the analysis developed in (Zhang et al., 2020, 2021) for the general utility RL to derive the theoretical convergence results for our proposed algorithm in this work.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to theoretical work and algorithms.",
      "processing_time": 52.74708127975464,
      "citing_paper_id": "275118993",
      "cited_paper_id": 220363593
    },
    {
      "context_text": "Zhang et al. (2020) first proposed the notion of general utility reinforcement learning that goes beyond the standard setting of cumulative reward maximization to more general utilities where standard notions of Bellman’s equation, the value function, etc fail.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach in reinforcement learning.",
      "processing_time": 53.95788788795471,
      "citing_paper_id": "275118993",
      "cited_paper_id": 220363593
    },
    {
      "context_text": "General Utility Reinforcement Learning: Next, we highlight the connection of our proposed formulation to general utility reinforcement learning as proposed in (Zhang et al., 2020, 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to reinforcement learning methods and formulations.",
      "processing_time": 53.62809443473816,
      "citing_paper_id": "275118993",
      "cited_paper_id": 220363593
    },
    {
      "context_text": "In this subsection, we highlight the generality of our proposed formulation beyond social utility design to broader formulations of general utility reinforcement learning (Zhang et al., 2020) and distributionally robust optimization (Duchi et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and concepts in reinforcement learning.",
      "processing_time": 54.15635418891907,
      "citing_paper_id": "275118993",
      "cited_paper_id": 220363593
    },
    {
      "context_text": "The intuition behind learning a mixture of preference models is rooted in the principles of statistical learning theory (Lindsay, 1995; Seidel, 2011) which states that a combination of multiple distributions can approximate a wide range of complex, real-world distributions more effectively than a…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical principles from statistical learning theory.",
      "processing_time": 53.62347769737244,
      "citing_paper_id": "275118993",
      "cited_paper_id": 240309946
    },
    {
      "context_text": "…backgrounds (Vogels, 2021), ( ii ) personal bias and context subjectivity (Denton et al., 2021b; Sandri et al., 2023)), ( iii ) Imperfect preferences, (Sandri et al., 2023), and ( iv ) Linguistic ambiguity & missing context (Sandri et al., 2023; Denton et al., 2021b; Sap et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to papers discussing various challenges in annotation and subjective tasks.",
      "processing_time": 54.695274353027344,
      "citing_paper_id": "275118993",
      "cited_paper_id": 244117167
    },
    {
      "context_text": "…backgrounds (Vogels, 2021), ( ii ) personal bias and context subjectivity (Denton et al., 2021b; Sandri et al., 2023)), ( iii ) Imperfect preferences, (Sandri et al., 2023), and ( iv ) Linguistic ambiguity & missing context (Sandri et al., 2023; Denton et al., 2021b; Sap et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to papers discussing various challenges in annotation and subjective tasks.",
      "processing_time": 54.695274353027344,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258378247
    },
    {
      "context_text": "(iii) Linguistic ambiguity & missing context , could lead to diversity because of words or phrases with multiple possible interpretations and without clear context (Sandri et al., 2023; Denton et al., 2021b; Sap et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general issues related to linguistic ambiguity and missing context. No verifiable resources are identified.",
      "processing_time": 55.53083157539368,
      "citing_paper_id": "275118993",
      "cited_paper_id": 244117167
    },
    {
      "context_text": "(iii) Linguistic ambiguity & missing context , could lead to diversity because of words or phrases with multiple possible interpretations and without clear context (Sandri et al., 2023; Denton et al., 2021b; Sap et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general issues related to linguistic ambiguity and missing context. No verifiable resources are identified.",
      "processing_time": 55.53083157539368,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258378247
    },
    {
      "context_text": "To deal with the diversity in preferences, we first propose learning a mixture distribution for preferences rather than a single preference distribution, which is prevalent in the current RLHF pipelines (Christian, 2020; Ouyang et al., 2022b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach in reinforcement learning.",
      "processing_time": 53.95624279975891,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "The alignment problem, central to developing and fine-tuning current large language models (LLMs), represents a crucial challenge in artificial intelligence, especially in ensuring these models operate in harmony with human values, preferences and social welfare (Wang et al., 2023; Christian, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the alignment problem in the context of large language models and human values.",
      "processing_time": 55.25603246688843,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "Lemma 1 indicates that the current RLHF-based reward learning paradigm (Christian, 2020; Bai et al., 2022a; Rafailov et al., 2023) will suffer sub-optimality due to diversity amongst the humans, which is highly likely in practice (Aroyo et al., 2023b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers and concepts. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 56.98884057998657,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": ") (Christian, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It appears to be a reference to a book discussing broader issues in machine learning and human values.",
      "processing_time": 56.633535623550415,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "Despite the criticality, most of the latest RLHF approaches ignore the consideration of the diversity in human preference feedback by aligning the language model with a single reward function (Wang et al., 2023; Christian, 2020; Stiennon et al., 2022a; Ouyang et al., 2022a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing reinforcement learning and human feedback. No verifiable resources are identified.",
      "processing_time": 55.859529972076416,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "We remark that distributions q and η are crucial to rigorously characterize the alignment performance of different approaches, which is not considered in the existing literature (Christian, 2020; Bai et al., 2022a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only distributions q and η which are not datasets. The context focuses on the alignment performance of different approaches.",
      "processing_time": 56.50304079055786,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "To demonstrate our impossibility results as stated in Theorem 1, we perform the three steps of RLHF (described in (Christian, 2020; Ouyang et al., 2022b)) as prevalent currently with a single utility reward function on the combined preference dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "combined preference dataset"
      ],
      "dataset_descriptions": {
        "combined preference dataset": "Used to train and evaluate reinforcement learning with human feedback (RLHF) models, focusing on the alignment of model outputs with human preferences using a single utility reward function."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions a 'combined preference dataset' which is domain-qualified and relevant to the research on personalized text generation. However, the exact name of the dataset is not provided, only a general description.",
      "processing_time": 66.3513650894165,
      "citing_paper_id": "275118993",
      "cited_paper_id": 245937117
    },
    {
      "context_text": "To demonstrate our impossibility results as stated in Theorem 1, we perform the three steps of RLHF (described in (Christian, 2020; Ouyang et al., 2022b)) as prevalent currently with a single utility reward function on the combined preference dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "combined preference dataset"
      ],
      "dataset_descriptions": {
        "combined preference dataset": "Used to train and evaluate reinforcement learning with human feedback (RLHF) models, focusing on the alignment of model outputs with human preferences using a single utility reward function."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions a 'combined preference dataset' which is domain-qualified and relevant to the research on personalized text generation. However, the exact name of the dataset is not provided, only a general description.",
      "processing_time": 66.3513650894165,
      "citing_paper_id": "275118993",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Reinforcement learning from human feedback (RLHF) has emerged as a pivotal approach to alignment problems, specifically aligning LLM (Wang et al., 2023; Ouyang et al., 2022b; Stiennon et al., 2022a; Ouyang et al., 2022a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only reinforcement learning approaches and papers. No verifiable resources are identified.",
      "processing_time": 55.09308385848999,
      "citing_paper_id": "275118993",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Some recent literature has brought attention to the challenge of aligning single utility RLHF with diverse preferences (Bakker et al., 2022; Ram´e et al., 2023; Jang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature discussing the challenge of aligning reinforcement learning with human feedback (RLHF) to diverse preferences.",
      "processing_time": 57.046666622161865,
      "citing_paper_id": "275118993",
      "cited_paper_id": 254043997
    },
    {
      "context_text": "Some recent literature has brought attention to the challenge of aligning single utility RLHF with diverse preferences (Bakker et al., 2022; Ram´e et al., 2023; Jang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature discussing the challenge of aligning reinforcement learning with human feedback (RLHF) to diverse preferences.",
      "processing_time": 57.046666622161865,
      "citing_paper_id": "275118993",
      "cited_paper_id": 259096117
    },
    {
      "context_text": "Some recent literature has brought attention to the challenge of aligning single utility RLHF with diverse preferences (Bakker et al., 2022; Ram´e et al., 2023; Jang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to literature discussing the challenge of aligning reinforcement learning with human feedback (RLHF) to diverse preferences.",
      "processing_time": 57.046666622161865,
      "citing_paper_id": "275118993",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "To mitigate this issue, some of the recent research proposes to learn multiple reward functions, which can then be aggregated in arbitrary manners (Bakker et al., 2022; Jang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.30889940261841,
      "citing_paper_id": "275118993",
      "cited_paper_id": 254043997
    },
    {
      "context_text": "To mitigate this issue, some of the recent research proposes to learn multiple reward functions, which can then be aggregated in arbitrary manners (Bakker et al., 2022; Jang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.30889940261841,
      "citing_paper_id": "275118993",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Recently (Santurkar et al., 2023) created a dataset for evaluating the alignment of language models with 60 US demographic groups over a wide range of topics and found substantial misalignment between a selanguage models and those groups.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "dataset for evaluating the alignment of language models with 60 US demographic groups"
      ],
      "dataset_descriptions": {
        "dataset for evaluating the alignment of language models with 60 US demographic groups": "Used to assess the alignment of language models with 60 US demographic groups across various topics, identifying significant misalignment issues."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions a specific dataset created by Santurkar et al. (2023) for evaluating language model alignment with US demographic groups.",
      "processing_time": 66.70441484451294,
      "citing_paper_id": "275118993",
      "cited_paper_id": 257834040
    },
    {
      "context_text": "(ii) Personal bias and context subjectivity , which affects the human preferences for controversial topics in interpreting language and divisive themes (Denton et al., 2021b; Sandri et al., 2023)).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts of personal bias and context subjectivity. No verifiable resources are identified.",
      "processing_time": 55.87699627876282,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258378247
    },
    {
      "context_text": "(iii) Imperfect preferences , which arises due to variations in expertise, training, or quality control leading to diverse preferences, with certain content inaccurately considered offensive by some groups (Sandri et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses issues related to annotator disagreement in subjective tasks.",
      "processing_time": 54.88487005233765,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258378247
    },
    {
      "context_text": "…to this diversity include ( i ) socio-demographic backgrounds (Vogels, 2021), ( ii ) personal bias and context subjectivity (Denton et al., 2021b; Sandri et al., 2023)), ( iii ) Imperfect preferences, (Sandri et al., 2023), and ( iv ) Linguistic ambiguity & missing context (Sandri et al., 2023;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing various factors affecting subjective tasks. No clear, verifiable datasets are identified.",
      "processing_time": 56.667160511016846,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258378247
    },
    {
      "context_text": "We run pairwise evaluations by GPT-4 using AlpacaFarm codebase(Dubois et al., 2023) and use the win rate to the base Tulu2-7B model as the metric.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of the AlpacaFarm codebase but does not refer to it as a dataset. It is used as a tool or method for running evaluations.",
      "processing_time": 57.60249471664429,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "A number of theoretical studies have analyzed the efficiency and benefits for reinforcement learning using preference data Ji et al. (2023b); Zhang et al. (2023); Li et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical studies. No verifiable resources are identified.",
      "processing_time": 54.876503467559814,
      "citing_paper_id": "275118993",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "A number of theoretical studies have analyzed the efficiency and benefits for reinforcement learning using preference data Ji et al. (2023b); Zhang et al. (2023); Li et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only theoretical studies. No verifiable resources are identified.",
      "processing_time": 54.876503467559814,
      "citing_paper_id": "275118993",
      "cited_paper_id": 260357869
    },
    {
      "context_text": "On the other hand, (Ovadya, 2023) adopts a consensus-based method for aggregating human representations by emphasizing specific principles (Bai et al., 2022b; Kovaˇc et al., 2023), which might result in the under-representation of marginalized groups (Ram´e et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and potential issues with representation. No clear, verifiable resources are identified.",
      "processing_time": 55.8524534702301,
      "citing_paper_id": "275118993",
      "cited_paper_id": 259096117
    },
    {
      "context_text": "Another line of research focuses on the aspect of designing multi-policy strategies by fine-tuning personalized language models towards individual rewards (Jang et al., 2023; Ram´e et al., 2023; Ji et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research papers that discuss methods for personalizing language models.",
      "processing_time": 54.94448256492615,
      "citing_paper_id": "275118993",
      "cited_paper_id": 259096117
    },
    {
      "context_text": "Another line of research focuses on the aspect of designing multi-policy strategies by fine-tuning personalized language models towards individual rewards (Jang et al., 2023; Ram´e et al., 2023; Ji et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research papers that discuss methods for personalizing language models.",
      "processing_time": 54.94448256492615,
      "citing_paper_id": "275118993",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Following Jang et al. (2023), we use the same 50 instances from Koala evaluation(Geng et al., 2023) and test the model’s ability to generate answers in different groups of users’ preferences.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Koala evaluation"
      ],
      "dataset_descriptions": {
        "Koala evaluation": "Used to test the model's ability to generate answers aligned with different groups of users' preferences, focusing on personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Koala evaluation' as a specific dataset used for testing the model's ability to generate personalized answers. The dataset is referenced as being used in a similar manner as in Jang et al. (2023).",
      "processing_time": 67.04904198646545,
      "citing_paper_id": "275118993",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Chakraborty et al. (2024) proposed a bilevel reinforcement learning framework for policy alignment.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological framework.",
      "processing_time": 53.851988554000854,
      "citing_paper_id": "275118993",
      "cited_paper_id": null
    },
    {
      "context_text": "The key factors contributing to this diversity include ( i ) socio-demographic backgrounds (Vogels, 2021), ( ii ) personal bias and context subjectivity (Denton et al., 2021b; Sandri et al., 2023)), ( iii ) Imperfect preferences, (Sandri et al., 2023), and ( iv ) Linguistic ambiguity & missing…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references factors contributing to diversity, which are not datasets.",
      "processing_time": 55.89669942855835,
      "citing_paper_id": "275118993",
      "cited_paper_id": null
    },
    {
      "context_text": "Gender differences, for example, influence sensitivity to online content, with women facing more online harassment Vogels (2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about gender differences and online harassment.",
      "processing_time": 54.26417088508606,
      "citing_paper_id": "275118993",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, researchers found that social data such as Twitter/Weibo posts and replies [Ritter et al. , 2011; Shang et al. , 2015], and movie dialogues can be used to learn and generate spoken language.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions social data from Twitter/Weibo and movie dialogues, which are plausible datasets but not explicitly named. No specific dataset names are provided.",
      "processing_time": 56.24699687957764,
      "citing_paper_id": "51608471",
      "cited_paper_id": 780171
    },
    {
      "context_text": "Large-scale conversation generation with social media data was ﬁrstly proposed in [Ritter et al. , 2011] and has been greatly advanced by applying sequence-to-sequence models [Sutskever et al. , 2014; Shang et al. , 2015; Serban et al. , 2016 ] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.82127642631531,
      "citing_paper_id": "51608471",
      "cited_paper_id": 780171
    },
    {
      "context_text": "Large-scale conversation generation with social media data was ﬁrstly proposed in [Ritter et al. , 2011] and has been greatly advanced by applying sequence-to-sequence models [Sutskever et al. , 2014; Shang et al. , 2015; Serban et al. , 2016 ] .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.82127642631531,
      "citing_paper_id": "51608471",
      "cited_paper_id": 6126582
    },
    {
      "context_text": ", 2016] and [Al-Rfou et al., 2016] can handle implicit personality using user embeddings in response generation (projecting each user into a vector).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches for handling implicit personality using user embeddings.",
      "processing_time": 54.741419315338135,
      "citing_paper_id": "51608471",
      "cited_paper_id": 1473130
    },
    {
      "context_text": "Recent works proposed in [ Li et al. , 2016 ] and [ Al-Rfou et al. , 2016 ] can handle implicit personality using user embeddings in response generation (projecting each user into a vector).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for handling implicit personality using user embeddings.",
      "processing_time": 53.86791801452637,
      "citing_paper_id": "51608471",
      "cited_paper_id": 1473130
    },
    {
      "context_text": "A chatbot needs to present a coherent personality to gain confidence and trust from the user [Yu et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept about chatbots and conversational systems.",
      "processing_time": 55.05284667015076,
      "citing_paper_id": "51608471",
      "cited_paper_id": 2268489
    },
    {
      "context_text": "In reverse, spoken language can be generated in accordance to particular personality [Mairesse and Walker, 2007].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or approach for generating spoken language according to personality.",
      "processing_time": 55.04932761192322,
      "citing_paper_id": "51608471",
      "cited_paper_id": 2817528
    },
    {
      "context_text": "In this scenario, personality settings include age, gender, language, speaking style [Walker et al. , 1997], level of knowledge, areas of expertise, and other explicit and implicit cues that may portray character [Shum et al. , 2018] 1 .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only aspects of personality settings in chatbots. No verifiable resources are identified.",
      "processing_time": 55.88526654243469,
      "citing_paper_id": "51608471",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "Person-∗ ality can make the chatbot easier to communicate with, more predictable and trustable, and therefore helps to establish an emotional connection with the user [ Shum et al. , 2018 ] .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the impact of personality in chatbots.",
      "processing_time": 54.51424169540405,
      "citing_paper_id": "51608471",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "Though personality is a well-defined concept in psychology [Norman, 1963; Gosling et al., 2003], while in this paper, the personality of a chatbot refers to the character that the bot plays or performs during conversational interactions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to personality concepts in psychology. No verifiable resources are identified.",
      "processing_time": 55.467052936553955,
      "citing_paper_id": "51608471",
      "cited_paper_id": 7147133
    },
    {
      "context_text": ", 2016], avoiding universal responses [Jiwei Li, 2016], generating more diverse and meaningful responses [Mou et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods for improving diversity in neural conversation models.",
      "processing_time": 55.23660087585449,
      "citing_paper_id": "51608471",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "DC-Font [Jiang et al. 2017] achieves a more realistic style by introducing a style classifier.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'DC-Font' but does not refer to it as a dataset. It is described as a method or system for generating Chinese fonts.",
      "processing_time": 56.787647008895874,
      "citing_paper_id": "266177436",
      "cited_paper_id": 23763057
    },
    {
      "context_text": "In previous studies [Chen et al. 2021; Gao et al. 2019; Jiang et al. 2017; Li et al. 2022, 2023b; Su et al. 2023; Xie et al. 2021; Yang et al. 2019a], generating novel collocations by combining various existing glyphs and textures has been the main research focus, and previous works have achieved…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research focuses and achievements. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 56.18615007400513,
      "citing_paper_id": "266177436",
      "cited_paper_id": 23763057
    },
    {
      "context_text": "In previous studies [Chen et al. 2021; Gao et al. 2019; Jiang et al. 2017; Li et al. 2022, 2023b; Su et al. 2023; Xie et al. 2021; Yang et al. 2019a], generating novel collocations by combining various existing glyphs and textures has been the main research focus, and previous works have achieved…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research focuses and achievements. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 56.18615007400513,
      "citing_paper_id": "266177436",
      "cited_paper_id": 233168962
    },
    {
      "context_text": "In previous studies [Chen et al. 2021; Gao et al. 2019; Jiang et al. 2017; Li et al. 2022, 2023b; Su et al. 2023; Xie et al. 2021; Yang et al. 2019a], generating novel collocations by combining various existing glyphs and textures has been the main research focus, and previous works have achieved…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research focuses and achievements. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 56.18615007400513,
      "citing_paper_id": "266177436",
      "cited_paper_id": 237375666
    },
    {
      "context_text": "DG-Font [Xie et al. 2021] proposed Deformable Generative Networks realize un-supervised word art generation, which uses the spatial relationship of fonts to effectively ensure that the generated characters have a complete structure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for unsupervised font generation.",
      "processing_time": 54.645341634750366,
      "citing_paper_id": "266177436",
      "cited_paper_id": 233168962
    },
    {
      "context_text": "Denosing Diffusion Model [Choi et al. 2021; Ho et al. 2020; Lu et al. 2022a,b; Meng et al. 2022; Nichol and Dhariwal 2021; Saharia et al. 2022; Song et al. 2021a,b; Wang et al. 2022] has attracted attention since it was proposed because of its powerful generative ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.188090562820435,
      "citing_paper_id": "266177436",
      "cited_paper_id": 236950721
    },
    {
      "context_text": "Denosing Diffusion Model [Choi et al. 2021; Ho et al. 2020; Lu et al. 2022a,b; Meng et al. 2022; Nichol and Dhariwal 2021; Saharia et al. 2022; Song et al. 2021a,b; Wang et al. 2022] has attracted attention since it was proposed because of its powerful generative ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.188090562820435,
      "citing_paper_id": "266177436",
      "cited_paper_id": 243938678
    },
    {
      "context_text": "Blended Diffusion [Avrahami et al. 2022] enables editing of the original image by using a mask to limit the position of the object.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'Blended Diffusion'. The context focuses on the method's functionality rather than a dataset.",
      "processing_time": 56.722994804382324,
      "citing_paper_id": "266177436",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "…our method with other text-to-image diffusion model constrained meth-ods, namely: Stable Diffusion Inpaint [Rombach et al. 2022], Blended Diffusion [Avrahami et al. 2022], T2IAdapter [Mou et al. 2023], Con-trolNet [Zhang and Agrawala 2023], DALL·E 2 [Ramesh et al. 2022 [Podell et al. 2023] and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.1730170249939,
      "citing_paper_id": "266177436",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "Since previous methods for generating artistic fonts cannot generate glyphs composed of objects, we compare our method with other text-to-image diffusion model constrained meth-ods, namely: Stable Diffusion Inpaint [Rombach et al. 2022], Blended Diffusion [Avrahami et al. 2022], T2IAdapter [Mou et al. 2023], Con-trolNet [Zhang and Agrawala 2023], DALL·E 2 [Ramesh et al. 2022 [Podell et al. 2023] and Latent Guidance [Voynov et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.186665296554565,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "For Latent Guidance with no published code, we build its LGP for testing after training for the same amount of time on our dataset.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset names. 'Our dataset' is a generic placeholder and does not meet the criteria for inclusion.",
      "processing_time": 56.35755133628845,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "And Latent Guidance has some deficiencies both in position constraints and generation effects.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only discusses deficiencies in Latent Guidance.",
      "processing_time": 55.22033619880676,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "Latent Guidance performs poorly in both position constraints and generation effects.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses the performance of Latent Guidance in certain tasks.",
      "processing_time": 56.17711138725281,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "The classifier guidance approach, such as Voynov et al. [Voynov et al. 2023], which proposes a sketch constraint method based on the classifier guidance approach, by training a lightweight MLP to complete the sketch constraints in the generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for sketch constraint in text-to-image generation.",
      "processing_time": 55.20568561553955,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "…diffusion model constrained meth-ods, namely: Stable Diffusion Inpaint [Rombach et al. 2022], Blended Diffusion [Avrahami et al. 2022], T2IAdapter [Mou et al. 2023], Con-trolNet [Zhang and Agrawala 2023], DALL·E 2 [Ramesh et al. 2022 [Podell et al. 2023] and Latent Guidance [Voynov et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various diffusion models and their applications.",
      "processing_time": 56.53171467781067,
      "citing_paper_id": "266177436",
      "cited_paper_id": 254018130
    },
    {
      "context_text": "…diffusion model constrained meth-ods, namely: Stable Diffusion Inpaint [Rombach et al. 2022], Blended Diffusion [Avrahami et al. 2022], T2IAdapter [Mou et al. 2023], Con-trolNet [Zhang and Agrawala 2023], DALL·E 2 [Ramesh et al. 2022 [Podell et al. 2023] and Latent Guidance [Voynov et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not reference any specific datasets. The context is focused on describing various diffusion models and their applications.",
      "processing_time": 56.53171467781067,
      "citing_paper_id": "266177436",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "Some work [Mou et al. 2023; Zhang and Agrawala 2023] is controlled by introducing semantic maps, layouts, sketches, etc. as conditions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the use of semantic maps, layouts, and sketches as conditions in controllable text-to-image generation.",
      "processing_time": 57.75389122962952,
      "citing_paper_id": "266177436",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "The classifier-free guidance approach, such as [Zhang and Agrawala 2023] and [Mou et al. 2023], adds positional constraints to the training process, requiring a vast amount of paired data and considerable training time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach and the requirement for paired data.",
      "processing_time": 55.214390993118286,
      "citing_paper_id": "266177436",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "[Iluz et al. 2023] innovatively combines the shape of the object with the shape of the text, so that the text can show the intuitive pattern of the represented object under the premise of being readable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for combining object shapes with text shapes.",
      "processing_time": 55.37509489059448,
      "citing_paper_id": "266177436",
      "cited_paper_id": 257353586
    },
    {
      "context_text": "Existing methods can be summarized as: (1) Learning with user ID information (Li et al., 2016b; Al-Rfou et al., 2016; Bak and Oh, 2019; Chan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.48471808433533,
      "citing_paper_id": "252918698",
      "cited_paper_id": 1473130
    },
    {
      "context_text": "Existing methods can be summarized as: (1) Learning with user ID information (Li et al., 2016b; Al-Rfou et al., 2016; Bak and Oh, 2019; Chan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.48471808433533,
      "citing_paper_id": "252918698",
      "cited_paper_id": 202775604
    },
    {
      "context_text": "MMI (Li et al., 2016a) optimizes the maximum mutual information loss to improve the diversity of generated responses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MMI, which is a method for optimizing diversity in generated responses, not a dataset.",
      "processing_time": 54.98033928871155,
      "citing_paper_id": "252918698",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "This one-size-fits-all strategy cannot satisfy the varying needs of users and is lean to generate safe but meaningless responses, such as “I don’t know” (Li et al., 2016a; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with neural conversation models.",
      "processing_time": 54.364108085632324,
      "citing_paper_id": "252918698",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "This one-size-fits-all strategy cannot satisfy the varying needs of users and is lean to generate safe but meaningless responses, such as “I don’t know” (Li et al., 2016a; Zhu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general issues with neural conversation models.",
      "processing_time": 54.364108085632324,
      "citing_paper_id": "252918698",
      "cited_paper_id": 202733477
    },
    {
      "context_text": "…al., 2018b); (2) tracking the speakers’ emotions to make a more suitable reply (Zhou et al., 2018a); (3) responding with more diverse sentences to avoid a boring user experience (Li et al., 2016a); and (4) generating responses with personas to make the dialogue more consistent (Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research approaches and methods. The cited papers are referenced for their methodologies, not for providing datasets.",
      "processing_time": 56.981088399887085,
      "citing_paper_id": "252918698",
      "cited_paper_id": 7287895
    },
    {
      "context_text": "We apply an encoder-decoder architecture based on Transformer (Vaswani et al., 2017) to generate personalized responses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer) which is excluded. No verifiable resources are identified.",
      "processing_time": 55.87363004684448,
      "citing_paper_id": "252918698",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The number of Transformer layers is 6.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the number of Transformer layers. The cited paper title 'Attention is All you Need' is about a method, not a dataset.",
      "processing_time": 57.63586735725403,
      "citing_paper_id": "252918698",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The obtained history representation (containing personal information) and the input query are fed into a Transformer-based Seq2Seq structure to generate a personalized re-⋯ sponse.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer-based Seq2Seq structure). The cited paper 'Attention is All you Need' introduces the Transformer model, not a dataset.",
      "processing_time": 58.35172891616821,
      "citing_paper_id": "252918698",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "First, the Transformer encoder encodes both user profile U and current query p combined together.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the process of encoding user profiles and queries using a Transformer encoder.",
      "processing_time": 56.61116337776184,
      "citing_paper_id": "252918698",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The hidden size and number of heads of the Transformer are set respectively to 768 and 8.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model parameters. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.5036563873291,
      "citing_paper_id": "252918698",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Following previous studies (Ma et al., 2021b), we employ Persona-F1 (Lian et al., 2019) to measure the unigram F1 between the generated responses and historical responses, and Per-sona Coverage (Song et al., 2019a) to calculate IDF-weighted word overlapping score between the generated responses and dialogue response history.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (Persona-F1 and Persona Coverage) but does not refer to any specific datasets. The focus is on evaluating the performance of generated responses using these metrics.",
      "processing_time": 57.632972240448,
      "citing_paper_id": "252918698",
      "cited_paper_id": 53100214
    },
    {
      "context_text": "( 2) Learning with explicit user profiles (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods or models. No verifiable resources are identified.",
      "processing_time": 56.21883964538574,
      "citing_paper_id": "252918698",
      "cited_paper_id": 53100214
    },
    {
      "context_text": "( 2) Learning with explicit user profiles (Qian et al., 2018; Zhang et al., 2018; Olabiyi et al., 2019; Song et al., 2019a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods or models. No verifiable resources are identified.",
      "processing_time": 56.21883964538574,
      "citing_paper_id": "252918698",
      "cited_paper_id": 67956318
    },
    {
      "context_text": "It has been found that such an optimization way is easy to suffer from the data sparsity scenarios, where users may have limited dialogue history in real-world applications (Song et al., 2019b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It discusses a general issue related to data sparsity in dialogue systems.",
      "processing_time": 56.002742528915405,
      "citing_paper_id": "252918698",
      "cited_paper_id": 53100214
    },
    {
      "context_text": "…(Ma et al., 2021b), we employ Persona-F1 (Lian et al., 2019) to measure the unigram F1 between the generated responses and historical responses, and Per-sona Coverage (Song et al., 2019a) to calculate IDF-weighted word overlapping score between the generated responses and dialogue response history.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (Persona-F1 and Persona Coverage) but does not refer to any specific datasets. The focus is on evaluating the performance of generated responses using these metrics.",
      "processing_time": 57.62135672569275,
      "citing_paper_id": "252918698",
      "cited_paper_id": 53100214
    },
    {
      "context_text": "ReCoSa-P (Zhang et al., 2019) is also a multi-turn response generation model that measures the context with an attention mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions ReCoSa-P as a model, not a dataset. No specific dataset is referenced in the citation.",
      "processing_time": 55.98779821395874,
      "citing_paper_id": "252918698",
      "cited_paper_id": 195886246
    },
    {
      "context_text": "Following previous study (Ma et al., 2021b), we conduct experiments on two datasets from Chinese Weibo (Qian et al., 2021) and English Red-dit (Zhang et al., 2020a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Chinese Weibo",
        "English Reddit"
      ],
      "dataset_descriptions": {
        "Chinese Weibo": "Used to conduct experiments on personalized text generation, focusing on social media posts to analyze user-specific language patterns and interactions.",
        "English Reddit": "Used to conduct experiments on personalized text generation, focusing on social media posts to analyze user-specific language patterns and interactions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions two datasets, one from Chinese Weibo and another from English Reddit, which are used for conducting experiments. These datasets are specific and relevant to the topic of personalized text generation.",
      "processing_time": 71.21464323997498,
      "citing_paper_id": "252918698",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Inspired by research on session-based social recommendation (Zhang et al., 2020b), a pair of consecutive responses with tight time intervals imply more user interests.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach to understanding user interests in a conversational context.",
      "processing_time": 55.79837250709534,
      "citing_paper_id": "252918698",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "(2) We have noticed that pre-trained language models have been applied to dialogue generation ( e.g. , DialoGPT (Zhang et al., 2020a)).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions DialoGPT but does not refer to it as a dataset. It is a model used for dialogue generation.",
      "processing_time": 56.37225890159607,
      "citing_paper_id": "252918698",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "In recent years, open-domain chatbots have achieved impressive progress in the natural language processing (NLP) field (Li et al., 2016b; Zhou et al., 2018a; Chan et al., 2019; Liu et al., 2020; Zhu et al., 2021b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing progress in open-domain chatbots.",
      "processing_time": 55.59792637825012,
      "citing_paper_id": "252918698",
      "cited_paper_id": 231662221
    },
    {
      "context_text": "Self-supervised learning has achieved great success in various NLP (Devlin et al., 2019; Bansal et al., 2021) and Information Retrieval (Guo et al., 2022; Ma et al., 2021a; Zhu et al., 2021a; Zhou et al., 2021) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to self-supervised learning in NLP and Information Retrieval. No verifiable resources are identified.",
      "processing_time": 57.38839030265808,
      "citing_paper_id": "252918698",
      "cited_paper_id": 240230721
    },
    {
      "context_text": "Self-supervised learning has achieved great success in various NLP (Devlin et al., 2019; Bansal et al., 2021) and Information Retrieval (Guo et al., 2022; Ma et al., 2021a; Zhu et al., 2021a; Zhou et al., 2021) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to self-supervised learning in NLP and Information Retrieval. No verifiable resources are identified.",
      "processing_time": 57.38839030265808,
      "citing_paper_id": "252918698",
      "cited_paper_id": 240419611
    },
    {
      "context_text": "Self-supervised learning has achieved great success in various NLP (Devlin et al., 2019; Bansal et al., 2021) and Information Retrieval (Guo et al., 2022; Ma et al., 2021a; Zhu et al., 2021a; Zhou et al., 2021) tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to self-supervised learning in NLP and Information Retrieval. No verifiable resources are identified.",
      "processing_time": 57.38839030265808,
      "citing_paper_id": "252918698",
      "cited_paper_id": 250340272
    },
    {
      "context_text": "( 3) Learning with implicit user profiles (Ma et al., 2021b; Zhong et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods for personalized dialogue generation.",
      "processing_time": 55.22279453277588,
      "citing_paper_id": "252918698",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "To solve this problem, some researchers have begun to endow chatbots with personality and develop personalized chatbots (Zhang et al., 2018; Ma et al., 2021b; Zhong et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 57.207961082458496,
      "citing_paper_id": "252918698",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "Therefore, this module will probably be replaced by a probabilistic module as is seen in Ferreira et al. (2016) in future versions of PASS.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a method or model. The context is about replacing a module with a probabilistic one, which is not related to a dataset.",
      "processing_time": 58.56182289123535,
      "citing_paper_id": "2364329",
      "cited_paper_id": 2925986
    },
    {
      "context_text": "At the same time, early NLG systems on sports-reporting (André et al., 1988; Robin, 1994; Theune et al., 2001, among others) are also inaccessible because the code for these systems has become obsolete or abandoned.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only early NLG systems on sports-reporting. No verifiable resources are identified.",
      "processing_time": 56.21149396896362,
      "citing_paper_id": "2364329",
      "cited_paper_id": 7543469
    },
    {
      "context_text": "Some of the data-to-text language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span mentions various data-to-text language generation tasks but does not specify any particular datasets. The context is too generic and lacks specific dataset names.",
      "processing_time": 56.82693791389465,
      "citing_paper_id": "2364329",
      "cited_paper_id": 8841050
    },
    {
      "context_text": "…been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and ﬁnancial reports (Nesterenko, 2016, among others).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 58.06382489204407,
      "citing_paper_id": "2364329",
      "cited_paper_id": 10422921
    },
    {
      "context_text": "To achieve this, the templates were derived from sentences in the MeMo FC corpus (Braun et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MeMo FC corpus"
      ],
      "dataset_descriptions": {
        "MeMo FC corpus": "Used to derive sentence templates for personalized text generation, focusing on the structure and content of soccer reportage in multiple languages."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'MeMo FC corpus', which is a specific dataset used for deriving templates. The cited paper title does not contradict this being a dataset.",
      "processing_time": 64.13998460769653,
      "citing_paper_id": "2364329",
      "cited_paper_id": 12383522
    },
    {
      "context_text": "Personalization was achieved through the use of more ’biased’ emotional language as was found in the MeMo FC corpus (Braun et al., 2016).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MeMo FC corpus"
      ],
      "dataset_descriptions": {
        "MeMo FC corpus": "Used to achieve personalization through the use of more 'biased' emotional language, focusing on the impact of emotional content in text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'MeMo FC corpus', which is a specific dataset used for studying emotional language in personalization. The cited paper title confirms it is a corpus.",
      "processing_time": 65.34509515762329,
      "citing_paper_id": "2364329",
      "cited_paper_id": 12383522
    },
    {
      "context_text": "The rare study (Kondadadi et al., 2013) that does try to execute all these steps, attempts to produce reports where the topics are always ﬁxed.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to generating reports. No clear, verifiable resource names are present.",
      "processing_time": 56.48459029197693,
      "citing_paper_id": "2364329",
      "cited_paper_id": 13550936
    },
    {
      "context_text": "…been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as weather forecast generation, medical reports, and financial reports.",
      "processing_time": 56.48217797279358,
      "citing_paper_id": "2364329",
      "cited_paper_id": 15641339
    },
    {
      "context_text": "…been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial reports (Nesterenko, 2016, among others).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of research such as weather forecast generation, medical reports, and financial reports.",
      "processing_time": 56.48217797279358,
      "citing_paper_id": "2364329",
      "cited_paper_id": 21201747
    },
    {
      "context_text": "Examples of data-to-text systems that fall into this category are Tanaka-Ishii et al. (1998), Chen and Mooney (2008), and Konstas and Lapata (2012), which all produce soccer reports.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that produce soccer reports. No clear, verifiable datasets are identified.",
      "processing_time": 56.48668885231018,
      "citing_paper_id": "2364329",
      "cited_paper_id": 15747255
    },
    {
      "context_text": "In the last few years, people have become increasingly interested in replicating published research (Ioannidis, 2005; Nosek et al., 2015; Mieskes, 2017, among others).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general trend in research replication. No verifiable resources are identified.",
      "processing_time": 56.31752395629883,
      "citing_paper_id": "2364329",
      "cited_paper_id": 16399410
    },
    {
      "context_text": "One of the strengths of data-to-text generation is that texts can easily be tailored towards speciﬁc audiences (Gatt and Krahmer, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general capability of data-to-text generation systems.",
      "processing_time": 55.60170865058899,
      "citing_paper_id": "2364329",
      "cited_paper_id": 16946362
    },
    {
      "context_text": "…language generation tasks that have been investigated recently include weather forecast generation (Belz and Kow, 2010; Angeli et al., 2010; Gkatzia et al., 2016a, among others), medical reports (Gatt et al., 2009; Gkatzia et al., 2016b; Schneider et al., 2013, among others), and financial…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span mentions various applications of language generation but does not specify any particular datasets. The context is too generic and lacks specific dataset names.",
      "processing_time": 56.59383010864258,
      "citing_paper_id": "2364329",
      "cited_paper_id": 21201747
    },
    {
      "context_text": "Examples of systems in this category are Robin (1994), and McKeown et al. (1995), which produced basketball reports, and The-une et al. (2001), and Barzilay and Lapata (2005), which produced reports on soccer matches.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only systems and their applications. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.09363794326782,
      "citing_paper_id": "2364329",
      "cited_paper_id": 27336344
    },
    {
      "context_text": "Examples of systems in this category are Robin (1994), and McKeown et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only references systems, which are not considered datasets.",
      "processing_time": 56.43255686759949,
      "citing_paper_id": "2364329",
      "cited_paper_id": 218065162
    },
    {
      "context_text": "…with gaps that can be ﬁlled with information. while this approach is sometimes contrasted with “real” NLG, research has shown that template-based approaches generally result in texts of relatively high quality (van Deemter et al., 2005), that are generated relatively quickly (Sanby et al., 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research findings about template-based approaches in NLG.",
      "processing_time": 55.805543661117554,
      "citing_paper_id": "2364329",
      "cited_paper_id": 250217793
    },
    {
      "context_text": "We demonstrate superior rating prediction performance for ourMT learner, but what about explanation quality? We plan to conduct a live-user trial as the ultimate test of recommendation effectiveness and explanation quality, but for now we continue with our off-line evaluation approach, estimating the quality of explanations using the common perplexity metric [1] for evaluating the goodness of language models and the tf-idf similarity metric for evaluating the relevance of the generated reviews compared with the ground-truth ones.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses evaluation metrics and future plans for a live-user trial, but no dataset names are provided.",
      "processing_time": 57.68900990486145,
      "citing_paper_id": "52902832",
      "cited_paper_id": 2249
    },
    {
      "context_text": "Inspired by the success of Generative Adversarial Nets [16] in the computer vision literature, in recent years, the idea of adversarial training for natural language processing has been extensively explored.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Generative Adversarial Nets).",
      "processing_time": 55.63730978965759,
      "citing_paper_id": "52902832",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "In order to address the non-differentiability problem associated with sequence generation frameworks, both models utilize the REINFORCE algorithm [46] together with Monte Carlo search for estimating the gradients.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods. The context is about addressing a technical issue in sequence generation using reinforcement learning.",
      "processing_time": 57.115697145462036,
      "citing_paper_id": "52902832",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "In order to bypass the indifferentiability problem, we employ the policy gradient algorithm [46] to estimate the gradient of the objective function J (θ ) with respect to the generator’s parameters θ :",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (policy gradient algorithm).",
      "processing_time": 55.04293179512024,
      "citing_paper_id": "52902832",
      "cited_paper_id": 2332513
    },
    {
      "context_text": "In [50], the authors propose the Explicit Factor Model, which utilizes phrase-level sentiment analysis for identifying “topics” that are correlated with user ratings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of phrase-level sentiment analysis but does not specify a dataset. The focus is on the method rather than a specific dataset.",
      "processing_time": 57.310407400131226,
      "citing_paper_id": "52902832",
      "cited_paper_id": 3331952
    },
    {
      "context_text": "els and natural language processing [23, 31, 33, 39, 50], for review mining in a recommendation setting, by combining them into a multi-task learning framework.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.3168044090271,
      "citing_paper_id": "52902832",
      "cited_paper_id": 3331952
    },
    {
      "context_text": "For example, the SeqGAN model [48] introduced a policy gradient based approach to effectively training generative adversarial nets for sequences generation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SeqGAN but does not refer to a dataset. It describes a method for training generative adversarial networks for sequence generation.",
      "processing_time": 57.147074460983276,
      "citing_paper_id": "52902832",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "Deep Cooperative Neural Networks (DeepCoNN) [51] model user-item interactions based on review texts by utilizing a factorization machine model [38] on top of two convolutional neural networks [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the architecture and methodology of DeepCoNN.",
      "processing_time": 56.75883150100708,
      "citing_paper_id": "52902832",
      "cited_paper_id": 5180076
    },
    {
      "context_text": "To improve the accuracy and explainability of recommender systems, the recommender systems community has recently tried to harness user-generated reviews as a new source of recommendation data [4, 9, 10, 12, 13, 18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to user-generated reviews as a source of recommendation data.",
      "processing_time": 56.53301954269409,
      "citing_paper_id": "52902832",
      "cited_paper_id": 7847519
    },
    {
      "context_text": "To improve the accuracy and explainability of recommender systems, the recommender systems community has recently tried to harness user-generated reviews as a new source of recommendation data [4, 9, 10, 12, 13, 18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to user-generated reviews as a source of recommendation data.",
      "processing_time": 56.53301954269409,
      "citing_paper_id": "52902832",
      "cited_paper_id": 17262299
    },
    {
      "context_text": "3240365 To improve the accuracy and explainability of recommender systems, the recommender systems community has recently tried to harness user-generated reviews as a new source of recommendation data [4, 9, 10, 12, 13, 18].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to user-generated reviews as a source of recommendation data.",
      "processing_time": 56.049800395965576,
      "citing_paper_id": "52902832",
      "cited_paper_id": 7988420
    },
    {
      "context_text": "Recently, sequence-to-sequence (seq2seq) deep neural network models [42] have been used in various tasks, ranging from text summarization [34] to question answering [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of seq2seq models in text summarization and question answering.",
      "processing_time": 56.75529456138611,
      "citing_paper_id": "52902832",
      "cited_paper_id": 8928715
    },
    {
      "context_text": "Recently, sequence-to-sequence (seq2seq) deep neural network models [42] have been used in various tasks, ranging from text summarization [34] to question answering [52].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the application of seq2seq models in text summarization and question answering.",
      "processing_time": 56.75529456138611,
      "citing_paper_id": "52902832",
      "cited_paper_id": 18415191
    },
    {
      "context_text": "(i) N-gram: The N-gram languagemodel [22] is a classic model in the literature of statistical natural language processing, which models the probability of occurrence of N consecutive words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a classic model in NLP.",
      "processing_time": 56.04403877258301,
      "citing_paper_id": "52902832",
      "cited_paper_id": 9685476
    },
    {
      "context_text": "The TransNets model [3] extends the DeepCoNN model by introducing an additional layer that is regularized by the latent representation of review texts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 56.95877385139465,
      "citing_paper_id": "52902832",
      "cited_paper_id": 9932413
    },
    {
      "context_text": "Therefore we adopt the Alternating Least Squares (ALS) technique [19] which repeatedly optimizes one ofU and V while temporarily fixing the other to be constant:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ALS) used for optimization in collaborative filtering.",
      "processing_time": 55.070629358291626,
      "citing_paper_id": "52902832",
      "cited_paper_id": 10537313
    },
    {
      "context_text": "can often reveal hidden factors which influence customer choices [11, 14, 37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to general factors influencing customer choices.",
      "processing_time": 56.58922052383423,
      "citing_paper_id": "52902832",
      "cited_paper_id": 12187582
    },
    {
      "context_text": "can often reveal hidden factors which influence customer choices [11, 14, 37].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to general factors influencing customer choices.",
      "processing_time": 56.58922052383423,
      "citing_paper_id": "52902832",
      "cited_paper_id": 34993100
    },
    {
      "context_text": "(v) ConvMF+ :ConvolutionalMatrixFactorization(ConvMF)[20] is a ConvNet [24] based model that addresses sparseness by mining attributes from item documents.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (ConvMF) and a technique (ConvNet). There are no clear identifiers for datasets.",
      "processing_time": 57.671215534210205,
      "citing_paper_id": "52902832",
      "cited_paper_id": 207239982
    },
    {
      "context_text": "However, existing human datasets either do not annotate ID information [31, 37, 61, 78], or the richness of the scenes they contain is very limited [38, 46, 68] ( i.e ., they only focus on the face area).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human datasets' but does not provide specific names. It only describes limitations of existing datasets without naming them.",
      "processing_time": 56.570923805236816,
      "citing_paper_id": "266052536",
      "cited_paper_id": 459456
    },
    {
      "context_text": "However, existing human datasets either do not annotate ID information [31, 37, 61, 78], or the richness of the scenes they contain is very limited [38, 46, 68] ( i.e ., they only focus on the face area).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human datasets' but does not provide specific names. It only describes limitations of existing datasets without naming them.",
      "processing_time": 56.570923805236816,
      "citing_paper_id": "266052536",
      "cited_paper_id": 221727985
    },
    {
      "context_text": "Therefore, to simplify and accelerate the customized generation process, recent works, driven by existing human-centric datasets [31, 38], have trained visual encoders [9, 72] or hyper-networks [2, 56] to represent the input ID images as embeddings or LoRA [25] weights of the model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'human-centric datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.37207865715027,
      "citing_paper_id": "266052536",
      "cited_paper_id": 459456
    },
    {
      "context_text": "Therefore, to simplify and accelerate the customized generation process, recent works, driven by existing human-centric datasets [31, 38], have trained visual encoders [9, 72] or hyper-networks [2, 56] to represent the input ID images as embeddings or LoRA [25] weights of the model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'human-centric datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.37207865715027,
      "citing_paper_id": "266052536",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Therefore, to simplify and accelerate the customized generation process, recent works, driven by existing human-centric datasets [31, 38], have trained visual encoders [9, 72] or hyper-networks [2, 56] to represent the input ID images as embeddings or LoRA [25] weights of the model.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'human-centric datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.37207865715027,
      "citing_paper_id": "266052536",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "However, existing datasets either do not classify by IDs [31, 37, 61, 78] or only focus on faces without including other contextual information [38, 46, 68].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions existing datasets but does not provide specific names. It only describes their limitations, which are not sufficient to identify them as verifiable resources.",
      "processing_time": 57.66228461265564,
      "citing_paper_id": "266052536",
      "cited_paper_id": 459456
    },
    {
      "context_text": "However, existing datasets either do not classify by IDs [31, 37, 61, 78] or only focus on faces without including other contextual information [38, 46, 68].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions existing datasets but does not provide specific names. It only describes their limitations, which are not sufficient to identify them as verifiable resources.",
      "processing_time": 57.66228461265564,
      "citing_paper_id": "266052536",
      "cited_paper_id": 221727985
    },
    {
      "context_text": "Firstly, since we aim to generate portrait images, we used the publicly available FFHQ-wild [31] dataset as training data instead of FFHQ [31] and CelebA-HQ [38], which focus on face area generation only.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FFHQ-wild"
      ],
      "dataset_descriptions": {
        "FFHQ-wild": "Used to train a model for generating full portrait images, focusing on broader facial and background elements rather than just the face area."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of the FFHQ-wild dataset for training a model to generate portrait images, distinguishing it from other datasets that focus only on face areas.",
      "processing_time": 65.51634311676025,
      "citing_paper_id": "266052536",
      "cited_paper_id": 459456
    },
    {
      "context_text": "Next, we calculate the LPIPS [76] scores between each pair of facial areas for all generated images and take the average.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (LPIPS) which is not a dataset.",
      "processing_time": 55.15695238113403,
      "citing_paper_id": "266052536",
      "cited_paper_id": 4766599
    },
    {
      "context_text": "The remarkable performance of these models can be attributable to high-quality large-scale text-image datasets [7, 60, 61], the continuous upgrades of foundational models [8, 48], conditioning encoders [27, 50, 51], and the improvement of controllability [36, 45, 73, 75].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets by name, only referring to 'high-quality large-scale text-image datasets' in a generic manner.",
      "processing_time": 57.46206545829773,
      "citing_paper_id": "266052536",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "The remarkable performance of these models can be attributable to high-quality large-scale text-image datasets [7, 60, 61], the continuous upgrades of foundational models [8, 48], conditioning encoders [27, 50, 51], and the improvement of controllability [36, 45, 73, 75].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets by name, only referring to 'high-quality large-scale text-image datasets' in a generic manner.",
      "processing_time": 57.46206545829773,
      "citing_paper_id": "266052536",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "The remarkable performance of these models can be attributable to high-quality large-scale text-image datasets [7, 60, 61], the continuous upgrades of foundational models [8, 48], conditioning encoders [27, 50, 51], and the improvement of controllability [36, 45, 73, 75].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span does not mention any specific datasets by name, only referring to 'high-quality large-scale text-image datasets' in a generic manner.",
      "processing_time": 57.46206545829773,
      "citing_paper_id": "266052536",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "Thanks to larger-scale text-image pair training datasets [61], larger generation models [49, 58], and text/visual encoders [50, 51] that can provide stronger semantic embeddings, diffusion-based text-to-image generation models have been continuously evolving recently.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'larger-scale text-image pair training datasets' but does not provide specific names. It focuses on the evolution of diffusion-based text-to-image generation models.",
      "processing_time": 58.27427434921265,
      "citing_paper_id": "266052536",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Thanks to larger-scale text-image pair training datasets [61], larger generation models [49, 58], and text/visual encoders [50, 51] that can provide stronger semantic embeddings, diffusion-based text-to-image generation models have been continuously evolving recently.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'larger-scale text-image pair training datasets' but does not provide specific names. It focuses on the evolution of diffusion-based text-to-image generation models.",
      "processing_time": 58.27427434921265,
      "citing_paper_id": "266052536",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "Customized image generation related to humans [15, 30, 37, 56] has received considerable attention, giving rise to numerous applications, such as personalized portrait photos [41], image animation [77], and virtual try-on [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of customized image generation. No verifiable resources are identified.",
      "processing_time": 56.55034589767456,
      "citing_paper_id": "266052536",
      "cited_paper_id": 216590578
    },
    {
      "context_text": "Customized image generation related to humans [15, 30, 37, 56] has received considerable attention, giving rise to numerous applications, such as personalized portrait photos [41], image animation [77], and virtual try-on [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of customized image generation. No verifiable resources are identified.",
      "processing_time": 56.55034589767456,
      "citing_paper_id": "266052536",
      "cited_paper_id": 248987735
    },
    {
      "context_text": "Customized image generation related to humans [15, 30, 37, 56] has received considerable attention, giving rise to numerous applications, such as personalized portrait photos [41], image animation [77], and virtual try-on [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of customized image generation. No verifiable resources are identified.",
      "processing_time": 56.55034589767456,
      "citing_paper_id": "266052536",
      "cited_paper_id": 253761522
    },
    {
      "context_text": "Customized image generation related to humans [15, 30, 37, 56] has received considerable attention, giving rise to numerous applications, such as personalized portrait photos [41], image animation [77], and virtual try-on [67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of customized image generation. No verifiable resources are identified.",
      "processing_time": 56.55034589767456,
      "citing_paper_id": "266052536",
      "cited_paper_id": 258048904
    },
    {
      "context_text": "We use RetinaFace [14] as the detection model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RetinaFace as a detection model, which is a method, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 57.38464093208313,
      "citing_paper_id": "266052536",
      "cited_paper_id": 232139129
    },
    {
      "context_text": "If an image does not contain any bounding boxes that meet the requirements, the image will be filtered out.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a filtering process for images. No verifiable resources are identified.",
      "processing_time": 55.35310912132263,
      "citing_paper_id": "266052536",
      "cited_paper_id": 232139129
    },
    {
      "context_text": "To make the diffusion model better perceive the ID information contained in the stacked ID embedding, we additionally train the LoRA [25, 57] residuals of the matrices in the attention layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of LoRA for training residuals in attention layers. LoRA is a method, not a dataset.",
      "processing_time": 57.50783848762512,
      "citing_paper_id": "266052536",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "To make the diffusion model better perceive the ID information contained in the stacked ID embedding, we additionally train the LoRA [25, 57] residuals of the matrices in the attention layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of LoRA for training residuals in attention layers. LoRA is a method, not a dataset.",
      "processing_time": 57.50783848762512,
      "citing_paper_id": "266052536",
      "cited_paper_id": null
    },
    {
      "context_text": "Diffusion models [23, 64] have made remarkable progress in text-conditioned image and video generation [4, 32, 52, 54, 58, 69, 71], attracting widespread attention in recent years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in text-conditioned image and video generation using diffusion models.",
      "processing_time": 56.86803460121155,
      "citing_paper_id": "266052536",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Diffusion models [23, 64] have made remarkable progress in text-conditioned image and video generation [4, 32, 52, 54, 58, 69, 71], attracting widespread attention in recent years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in text-conditioned image and video generation using diffusion models.",
      "processing_time": 56.86803460121155,
      "citing_paper_id": "266052536",
      "cited_paper_id": 252918469
    },
    {
      "context_text": "Diffusion models [23, 64] have made remarkable progress in text-conditioned image and video generation [4, 32, 52, 54, 58, 69, 71], attracting widespread attention in recent years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in text-conditioned image and video generation using diffusion models.",
      "processing_time": 56.86803460121155,
      "citing_paper_id": "266052536",
      "cited_paper_id": 265696111
    },
    {
      "context_text": "Diffusion models [23, 64] have made remarkable progress in text-conditioned image and video generation [4, 32, 52, 54, 58, 69, 71], attracting widespread attention in recent years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general progress in text-conditioned image and video generation using diffusion models.",
      "processing_time": 56.86803460121155,
      "citing_paper_id": "266052536",
      "cited_paper_id": 272723369
    },
    {
      "context_text": "Besides, we can adjust the degree of participation of one input ID image in generating the new customized ID through prompt weighting [21, 26], demonstrating the flexibility of our Pho-toMaker.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adjusting input image participation in generating new images.",
      "processing_time": 56.426737546920776,
      "citing_paper_id": "266052536",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "At the same time, the merging ratio can be simply adjusted by prompt weighting [21, 26] or by changing the proportion of different ID images in the input image pool, demonstrating the flexibility of our framework.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or techniques for adjusting the merging ratio in image editing.",
      "processing_time": 56.27616310119629,
      "citing_paper_id": "266052536",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "The controllability has also greatly improved due to the existence of text prompts and structural guidance [45, 75] Meanwhile, under the nurturing of powerful diffusion text-to-image models, many diffusion-based customized generation algorithms [16, 55] have emerged to meet users’ demand for…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the improvement of controllability in text-to-image generation.",
      "processing_time": 57.686561822891235,
      "citing_paper_id": "266052536",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "Recent studies [19, 39, 40] also showed how to make different concepts appear in the generated image by training multiple LoRAs, which is different from our approach of semantically integrating multiple IDs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the use of LoRAs and concept neurons in diffusion models, which are not datasets.",
      "processing_time": 58.6646671295166,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257427549
    },
    {
      "context_text": "These methods either utilize personalization datasets [10, 63] for training or encode the images to be customized in the semantic space [9, 29, 43, 62, 70, 72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'personalization datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.67386722564697,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257557302
    },
    {
      "context_text": "These methods either utilize personalization datasets [10, 63] for training or encode the images to be customized in the semantic space [9, 29, 43, 62, 70, 72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'personalization datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.67386722564697,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257952647
    },
    {
      "context_text": "These methods either utilize personalization datasets [10, 63] for training or encode the images to be customized in the semantic space [9, 29, 43, 62, 70, 72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'personalization datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.67386722564697,
      "citing_paper_id": "266052536",
      "cited_paper_id": 258999204
    },
    {
      "context_text": "These methods either utilize personalization datasets [10, 63] for training or encode the images to be customized in the semantic space [9, 29, 43, 62, 70, 72].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'personalization datasets' but does not specify any particular dataset names. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.67386722564697,
      "citing_paper_id": "266052536",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "Recently, some studies [10, 11, 29, 42, 43, 62, 70] attempt to perform personalized generation using a single image with a single forward pass, significantly accelerating the personalization process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to personalized generation. No dataset names are provided in the context.",
      "processing_time": 57.359938859939575,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257557302
    },
    {
      "context_text": "Recently, some studies [10, 11, 29, 42, 43, 62, 70] attempt to perform personalized generation using a single image with a single forward pass, significantly accelerating the personalization process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to personalized generation. No dataset names are provided in the context.",
      "processing_time": 57.359938859939575,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257952647
    },
    {
      "context_text": "Recently, some studies [10, 11, 29, 42, 43, 62, 70] attempt to perform personalized generation using a single image with a single forward pass, significantly accelerating the personalization process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to personalized generation. No dataset names are provided in the context.",
      "processing_time": 57.359938859939575,
      "citing_paper_id": "266052536",
      "cited_paper_id": 259951373
    },
    {
      "context_text": "Recently, some studies [10, 11, 29, 42, 43, 62, 70] attempt to perform personalized generation using a single image with a single forward pass, significantly accelerating the personalization process.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general approach to personalized generation. No dataset names are provided in the context.",
      "processing_time": 57.359938859939575,
      "citing_paper_id": "266052536",
      "cited_paper_id": 260091569
    },
    {
      "context_text": "Following recent works [28, 29, 62, 70], we use the CLIP [50] image encoder E img to extract image embed-dings for its alignment with the output space of the CLIP text encoder in diffusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of the CLIP image encoder for extracting image embeddings. CLIP is a model, not a dataset.",
      "processing_time": 58.26871180534363,
      "citing_paper_id": "266052536",
      "cited_paper_id": 257952647
    },
    {
      "context_text": "To generate higher quality portrait images [49], we filtered out images with the shortest side of the resolution less than 512 during the download process.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It only describes a filtering process applied to images during download.",
      "processing_time": 56.717196226119995,
      "citing_paper_id": "266052536",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "[49] developed the currently most powerful open-source generative model, SDXL.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions SDXL, which is a model, not a dataset. No datasets are mentioned in the citation context.",
      "processing_time": 56.868561029434204,
      "citing_paper_id": "266052536",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "To generate more photo-realistic human portraits, we employ SDXL model [49] stable-diffusion-xl-base-1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the SDXL model but does not refer to any specific dataset. The model is used for generating high-resolution images, but no dataset is explicitly mentioned.",
      "processing_time": 58.21229910850525,
      "citing_paper_id": "266052536",
      "cited_paper_id": 259341735
    },
    {
      "context_text": "Besides, we can control the proportion of this ID in the new generated ID by controlling the Inversion [16], FastComposer [72], and IPAdapter [73] for five different identities and corresponding prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on controlling the proportion of identity in generated IDs using various adapters and models.",
      "processing_time": 57.743242025375366,
      "citing_paper_id": "266052536",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "The comparison method will be chosen from DreamBooth [55], Textual Inversion [16], Fast-Composer [72], and IPAdapter [73].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. The context is about comparing different methods for text-to-image generation, which is related to personalized text generation but does not mention datasets.",
      "processing_time": 59.684704065322876,
      "citing_paper_id": "266052536",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "The most widely used in both commercial and community applications are DreamBooth-based methods [55, 57].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.56493616104126,
      "citing_paper_id": "266052536",
      "cited_paper_id": null
    },
    {
      "context_text": "Given that both pioneer works require substantial time for fine-tuning, some studies have attempted to expedite the process of personalized customization by reducing the number of parameters needed for tuning [20, 34, 57, 74] or by pre-training with large datasets [17, 56, 65].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to large datasets in general. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 58.66067695617676,
      "citing_paper_id": "266052536",
      "cited_paper_id": null
    },
    {
      "context_text": "The topic has changed after Turn-6. date response (Lowe et al. 2015; Kadlec, Schmid, and Klein-dienst 2015; Yan, Song, and Wu 2016; Tan et al. 2015; Wan et al. 2016; Wang and Jiang 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. There are no clear identifiers for datasets, and the context does not describe the use of any particular dataset.",
      "processing_time": 58.66806650161743,
      "citing_paper_id": "221971391",
      "cited_paper_id": 1405154
    },
    {
      "context_text": "…in multi-turn dialogues, early studies conduct single-turn match, which directly concatenates all context utterances and then match with the candidate response (Lowe et al. 2015; Kadlec, Schmid, and Kleindienst 2015; Yan, Song, and Wu 2016; Tan et al. 2015; Wan et al. 2016; Wang and Jiang 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in multi-turn dialogues.",
      "processing_time": 56.164445638656616,
      "citing_paper_id": "221971391",
      "cited_paper_id": 1405154
    },
    {
      "context_text": "The former views conversation as a generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for dialogue response generation. No verifiable resources are identified.",
      "processing_time": 56.81892800331116,
      "citing_paper_id": "221971391",
      "cited_paper_id": 1415790
    },
    {
      "context_text": "The former views conversation as a generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for dialogue response generation. No verifiable resources are identified.",
      "processing_time": 56.81892800331116,
      "citing_paper_id": "221971391",
      "cited_paper_id": 7562717
    },
    {
      "context_text": "The former views conversation as a generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for dialogue response generation. No verifiable resources are identified.",
      "processing_time": 56.81892800331116,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8994314
    },
    {
      "context_text": "The former views conversation as a generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for dialogue response generation. No verifiable resources are identified.",
      "processing_time": 56.81892800331116,
      "citing_paper_id": "221971391",
      "cited_paper_id": 19253447
    },
    {
      "context_text": "…Both datasets will be publicly released after the anonymous review period ends. sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Both datasets' but does not specify their names. The cited papers do not provide additional information to disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 59.288065910339355,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "…Both datasets will be publicly released after the anonymous review period ends. sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Both datasets' but does not specify their names. The cited papers do not provide additional information to disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 59.288065910339355,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2867243
    },
    {
      "context_text": "…Both datasets will be publicly released after the anonymous review period ends. sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Both datasets' but does not specify their names. The cited papers do not provide additional information to disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 59.288065910339355,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "…Both datasets will be publicly released after the anonymous review period ends. sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Both datasets' but does not specify their names. The cited papers do not provide additional information to disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 59.288065910339355,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "…Both datasets will be publicly released after the anonymous review period ends. sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Both datasets' but does not specify their names. The cited papers do not provide additional information to disambiguate the datasets. Therefore, no specific datasets can be identified.",
      "processing_time": 59.288065910339355,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "DAM (Zhou et al. 2018b) proposes to match a response with its multi-turn context-based entirely on attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAM' which is a method, not a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 57.02065134048462,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "DAM (Zhou et al. 2018b) proposes to match a response with its multi-turn context-based entirely on attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DAM' which is a method, not a dataset. No specific dataset is mentioned or used in the context provided.",
      "processing_time": 57.02065134048462,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "The latter matching modules vary in previous works, for example, DAM (Zhou et al. 2018b) introduces self-attention-match and cross-attention-match at each level of granularity, MSN (Yuan et al. 2019) introduces a multi-hop selector which selects relevant utterances to reduce matching noise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe previous research work and methodologies.",
      "processing_time": 56.772327184677124,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "The latter matching modules vary in previous works, for example, DAM (Zhou et al. 2018b) introduces self-attention-match and cross-attention-match at each level of granularity, MSN (Yuan et al. 2019) introduces a multi-hop selector which selects relevant utterances to reduce matching noise.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe previous research work and methodologies.",
      "processing_time": 56.772327184677124,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "We also use the Attentive Module in DAM (Zhou et al. 2018b) which is a unit of transformer (Vaswani et al. 2017) to encode the interaction between two sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.60307240486145,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "We also use the Attentive Module in DAM (Zhou et al. 2018b) which is a unit of transformer (Vaswani et al. 2017) to encode the interaction between two sequences.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.60307240486145,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2867243
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "…generation problem (Xing et al. 2017; Serban et al. 2017a,b; Zhou et al. 2017; Wu et al. 2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in personalized text generation. No verifiable resources are identified.",
      "processing_time": 56.92042064666748,
      "citing_paper_id": "221971391",
      "cited_paper_id": 263792952
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a ﬁnal matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in conversation generation and response selection.",
      "processing_time": 56.14694142341614,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2024574
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a ﬁnal matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in conversation generation and response selection.",
      "processing_time": 56.14694142341614,
      "citing_paper_id": "221971391",
      "cited_paper_id": 2867243
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a ﬁnal matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in conversation generation and response selection.",
      "processing_time": 56.14694142341614,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a ﬁnal matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in conversation generation and response selection.",
      "processing_time": 56.14694142341614,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a ﬁnal matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches in conversation generation and response selection.",
      "processing_time": 56.14694142341614,
      "citing_paper_id": "221971391",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "We use GRU to model the relation of segments as SMN (Wu et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SMN' which is disambiguated by the cited paper title to refer to 'Sequential Match Network'. However, SMN is a method, not a dataset.",
      "processing_time": 58.76252222061157,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "We use GRU to model the relation of segments as SMN (Wu et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'SMN' which is disambiguated by the cited paper title to refer to 'Sequential Match Network'. However, SMN is a method, not a dataset.",
      "processing_time": 58.76252222061157,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "(2) Douban Corpus (Wu et al. 2017): consists of multi-turn conversations from the Douban group 3 , which is a popular social networking service in China.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Douban Corpus"
      ],
      "dataset_descriptions": {
        "Douban Corpus": "Used to train and evaluate multi-turn response selection in retrieval-based chatbots, focusing on conversational context and relevance in a Chinese social networking environment."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Douban Corpus', which is used for multi-turn conversations from a popular Chinese social networking service. This dataset is directly relevant to personalized text generation.",
      "processing_time": 66.17811298370361,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "In the aggregation process, recent studies use GRU (Cho et al. 2014) with ut-terance vectors as input and obtain a matching score based on the last hidden state.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GRU) and a general process. No verifiable resources are identified.",
      "processing_time": 56.43735861778259,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Besides, adding extra last segment match does make sense as the traditional multi-turn matching method with GRU.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a methodological approach.",
      "processing_time": 55.88631319999695,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Those matching vectors are fed into GRU for a single one.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GRU) which is not a dataset.",
      "processing_time": 55.5448215007782,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The former either encodes each context utterance independently with traditional RNNs (Cho et al. 2014), or models the whole context with a pre-trained contextualized language model (De-vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), and then splits out utterances for further matching (Zhu,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about encoding context utterances using RNNs and pre-trained language models.",
      "processing_time": 57.38664627075195,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Furthermore, we add a linear layer to C crossT : ˆ H = GRU( C cross ) , (5) 5 Experiments",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a method involving a linear layer and GRU. No verifiable resources are identified.",
      "processing_time": 56.54200100898743,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The former either encodes each context utterance independently with traditional RNNs (Cho et al. 2014), or models the whole context with a pre-trained contextualized language model (De-vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), and then splits out utterances for further matching (Zhu, Zhao, and Li 2020; Zhang et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.549259424209595,
      "citing_paper_id": "221971391",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Then layer normalization (Ba, Kiros, and Hinton 2016) takes V att as input and we denote the output as V att ∈ R n q × d , which is then fed into a feed-forward network FFN with RELU (LeCun, Bengio, and Hin-ton 2015) activation: where W 1 , b 1 , W 2 , b 2 are learnable parameters and FFN ( V att )…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.16130256652832,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8236317
    },
    {
      "context_text": "Besides, TextTiling+Embedding (Song et al. 2016) applies word embedding to compute similarity between texts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (TextTiling+Embedding) and its application.",
      "processing_time": 55.531514406204224,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "We can ﬁnd that, just except for GloVe in the Chinese dataset, our proposed segmentation algorithm for multi-turn dialogues surpasses TextTiling in all metrics.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Chinese dataset' but does not provide a specific name or identifier. GloVe and TextTiling are excluded as they are methods/models.",
      "processing_time": 57.06399703025818,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Topic-aware segmentation: To evaluate the performance of our method, we compare it with a typical text segmentation algorithm, TextTiling (Hearst 1997), which is a classic text segmentation algorithm using term frequency vectors to represent text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (TextTiling) for text segmentation. The citation is used to describe a comparison method, not a dataset.",
      "processing_time": 57.56088161468506,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Window size and block size for TextTiling are all set to 6.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses parameters for a method called TextTiling.",
      "processing_time": 55.389862298965454,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Therefore, we also compare TextTiling with our segmentation algorithm in three representation methods: BERT CLS , which uses ” [ CLS ] ” embedding to directly encode the entire text, and BERT mean , GloVe: just use mean vector of all words in the text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe the representation methods used for text segmentation.",
      "processing_time": 56.42657160758972,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Therefore, we also compare TextTiling with our segmentation algorithm in three representation methods: BERT CLS , which uses ” [ CLS ] ” embedding to directly encode the entire text, and BERT mean , GloVe: just use mean vector of all words in the text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to describe the representation methods used for text segmentation.",
      "processing_time": 56.42657160758972,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "TextTiling (Hearst 1997) proposes pseudo-sentences and applies cosine-based lexical similarity on term frequency.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It describes a method (TextTiling) and its application, but no dataset is referenced.",
      "processing_time": 56.23604393005371,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "TextTiling tends to have larger MAE , because it ignores the number of turns in a topic round.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method (TextTiling) and its performance metric (MAE), but does not reference any dataset.",
      "processing_time": 57.96356558799744,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "To alleviate the data sparsity from term frequency vector representation, Choi, Wiemer-Hastings, and Moore (2001) employ Latent Semantic Analysis (LSA) for representation and Song et al. (2016) further use word embed-dings to enhance TextTiling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (LSA and word embeddings) and a technique (TextTiling).",
      "processing_time": 56.09997797012329,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "For our baseline TextTiling, the length of a pseudo sentence is set to 20 in the Chinese dataset and 10 in the English dataset, which is close to the mean length of utterances in both datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Chinese dataset' and 'English dataset' but does not provide specific names or identifiers. These are generic references and do not meet the criteria for inclusion.",
      "processing_time": 57.69016695022583,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Based on TextTiling, LCSeg (Galley et al. 2003) introduces lexical chains (Morris and Hirst 1991) to build vectors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The citation discusses the introduction of lexical chains in a method called LCSeg, which builds on TextTiling.",
      "processing_time": 57.90644145011902,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8574660
    },
    {
      "context_text": "Based on TextTiling, LCSeg (Galley et al. 2003) introduces lexical chains (Morris and Hirst 1991) to build vectors.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and tools. The citation discusses the introduction of lexical chains in a method called LCSeg, which builds on TextTiling.",
      "processing_time": 57.90644145011902,
      "citing_paper_id": "221971391",
      "cited_paper_id": 10970495
    },
    {
      "context_text": "Wu et al. (2018a) introduce two topic vectors according to topic words of the context and response respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of topic vectors. The context is about methodological aspects rather than dataset usage.",
      "processing_time": 56.58932304382324,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8994314
    },
    {
      "context_text": "Wu et al. (2018a) introduce two topic vectors according to topic words of the context and response respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the introduction of topic vectors. The context is about methodological aspects rather than dataset usage.",
      "processing_time": 56.58932304382324,
      "citing_paper_id": "221971391",
      "cited_paper_id": 19253447
    },
    {
      "context_text": "Even though existing work like Yuan et al. (2019) select semantic-relevant information or like Wu et al. (2018a) use topic information at word-level for better response matching, all known systems keep using topic-agnostic or topic-mixed n -gram utterances as a whole for matching context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 55.34216356277466,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8994314
    },
    {
      "context_text": "Even though existing work like Yuan et al. (2019) select semantic-relevant information or like Wu et al. (2018a) use topic information at word-level for better response matching, all known systems keep using topic-agnostic or topic-mixed n -gram utterances as a whole for matching context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and methods. No verifiable resources are identified.",
      "processing_time": 55.34216356277466,
      "citing_paper_id": "221971391",
      "cited_paper_id": 19253447
    },
    {
      "context_text": "Wu et al. (2018a) introduce two topic vectors in the matching step, which are linear combinations of topic words of the context and the response respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method involving topic vectors. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 56.574657678604126,
      "citing_paper_id": "221971391",
      "cited_paper_id": 8994314
    },
    {
      "context_text": "Wu et al. (2018a) introduce two topic vectors in the matching step, which are linear combinations of topic words of the context and the response respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method involving topic vectors. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 56.574657678604126,
      "citing_paper_id": "221971391",
      "cited_paper_id": 19253447
    },
    {
      "context_text": "Recently, most works turn to explore the interaction between the response and each utterance, which is then fused for a final matching score (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the interaction between responses and utterances in multi-turn conversations.",
      "processing_time": 55.961071491241455,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "The E-commerce Corpus has an obvious topic shift, including commodity consultation, logistics express, recommendation, and chitchat.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "E-commerce Corpus"
      ],
      "dataset_descriptions": {
        "E-commerce Corpus": "Used to demonstrate topic shifts in multi-turn conversations, focusing on commodity consultation, logistics express, recommendation, and chitchat."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'E-commerce Corpus', which is used to illustrate topic shifts in multi-turn conversations.",
      "processing_time": 63.07117295265198,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "2018b), while the latter usually consists of a retrieval and matching process (Zhou et al. 2016; Wu et al. 2017; Zhou et al. 2018b; Zhu et al. 2018; Zhang et al. 2018; Tao et al. 2019a; Gu, Ling, and Liu 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works.",
      "processing_time": 54.47768235206604,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "For example, DUA (Zhang et al. 2018) presents a self-attention based deep utterance aggregation model to form a finegrained context representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DUA) for modeling multi-turn conversations.",
      "processing_time": 55.17334866523743,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "(3) E-commerce Corpus (Zhang et al. 2018): includes conversations between customers and shopkeepers from the largest e-commerce platform Taobao 4 in China.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "E-commerce Corpus"
      ],
      "dataset_descriptions": {
        "E-commerce Corpus": "Used to model multi-turn conversations between customers and shopkeepers on Taobao, focusing on deep utterance aggregation techniques to improve dialogue systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, 'E-commerce Corpus', which is used for modeling multi-turn conversations in an e-commerce setting.",
      "processing_time": 63.720473289489746,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "For example, DUA (Zhang et al. 2018) presents a self-attention based deep utterance aggregation model to form a ﬁne-grained context representation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (DUA) rather than a dataset. There are no specific, verifiable datasets mentioned in the citation context.",
      "processing_time": 56.52264857292175,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "sequentially among all utterances, which generally follow the representation-matching-aggregation framework (Zhou et al. 2016; Wu et al. 2017; Zhang et al. 2018; Zhou et al. 2018a,b; Tao et al. 2019a; Yuan et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and frameworks. No verifiable resources are identified.",
      "processing_time": 55.71242570877075,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "Table 1: A case in Yuan et al. (2019) from E-commerce Corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "E-commerce Corpus"
      ],
      "dataset_descriptions": {
        "E-commerce Corpus": "Used to model multi-turn conversations in e-commerce settings, focusing on deep utterance aggregation techniques to improve dialogue systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'E-commerce Corpus' which appears to be a specific dataset used in the research. The context suggests it is used for modeling multi-turn conversations.",
      "processing_time": 63.97871208190918,
      "citing_paper_id": "221971391",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "For both tasks, we use pre-trained BERT 5 (Devlin et al. 2018) (bert-base-uncased & bert-base-chinese) as encoder.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BERT but does not refer to it as a dataset. It is used as a pre-trained model for encoding, which is outside the scope of the extraction rules.",
      "processing_time": 57.65147018432617,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Other fusion methods do not perform so well as TADAM, but still surpass the BERT baseline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between TADAM and BERT. BERT is a model, not a dataset.",
      "processing_time": 56.539870262145996,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "As for the baseline BERT, we have ﬁnetuned it.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the fine-tuning of BERT. No verifiable resources are identified.",
      "processing_time": 55.516199350357056,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Following the encoding manner of pre-trained language models such as BERT and ALBERT, we concatenate all segments { S i } ni =1 and response R with special tokens: which is fed into the encoder.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained language models. No verifiable resources are identified.",
      "processing_time": 54.93585205078125,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "One is to encode each utterance separately while the other is to encode the whole context using pre-trained contextualized language models such as BERT (Devlin et al. 2018), ALBERT (Lan et al. 2020), RoBERTa (Liu et al. 2019), and then split out each ut-terance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models (BERT, ALBERT, RoBERTa) but does not refer to any specific datasets. The focus is on the methodology of encoding utterances.",
      "processing_time": 57.73020553588867,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "One is to encode each utterance separately while the other is to encode the whole context using pre-trained contextualized language models such as BERT (Devlin et al. 2018), ALBERT (Lan et al. 2020), RoBERTa (Liu et al. 2019), and then split out each ut-terance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions pre-trained models (BERT, ALBERT, RoBERTa) but does not refer to any specific datasets. The focus is on the methodology of encoding utterances.",
      "processing_time": 57.73020553588867,
      "citing_paper_id": "221971391",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "Besides, we concatenate the context and candidate response as input for BERT as a basic sequence classiﬁ-cation baseline.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT as a method for sequence classification.",
      "processing_time": 54.95165276527405,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "In our following response selection part, we use our algorithm with BERT CLS for topic-aware segmentation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of BERT for topic-aware segmentation. BERT is a model, not a dataset.",
      "processing_time": 56.671488523483276,
      "citing_paper_id": "221971391",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "…utterance independently with traditional RNNs (Cho et al. 2014), or models the whole context with a pre-trained contextualized language model (De-vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), and then splits out utterances for further matching (Zhu, Zhao, and Li 2020; Zhang et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.17500066757202,
      "citing_paper_id": "221971391",
      "cited_paper_id": 59316891
    },
    {
      "context_text": "…utterance independently with traditional RNNs (Cho et al. 2014), or models the whole context with a pre-trained contextualized language model (De-vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), and then splits out utterances for further matching (Zhu, Zhao, and Li 2020; Zhang et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.17500066757202,
      "citing_paper_id": "221971391",
      "cited_paper_id": 198953378
    },
    {
      "context_text": "…utterance independently with traditional RNNs (Cho et al. 2014), or models the whole context with a pre-trained contextualized language model (De-vlin et al. 2018; Liu et al. 2019; Lan et al. 2020), and then splits out utterances for further matching (Zhu, Zhao, and Li 2020; Zhang et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.17500066757202,
      "citing_paper_id": "221971391",
      "cited_paper_id": 210919918
    },
    {
      "context_text": "In the aggregation process, recent studies use GRU (Cho et al. 2014) with utterance vectors as input and obtain a matching score based on the last hidden state. Recently, many works such as SA-BERT (Gu et al. 2020), BERT-VFT (Whang et al. 2020), DCM (Li, Li, and Ji 2020) have applied pre-training on the domain dataset, and get much improvement with their settings like speaker embeddings. As to incorporate topic",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'domain dataset' but does not specify a particular dataset name. It focuses on methods and models rather than a specific dataset.",
      "processing_time": 56.362008810043335,
      "citing_paper_id": "221971391",
      "cited_paper_id": 215415812
    },
    {
      "context_text": "The adaptation of the robot’s timbre is realized with an (1 + λ)-ES [22], where λ denotes the offspring size.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Evolution Strategies).",
      "processing_time": 54.42234826087952,
      "citing_paper_id": "209322955",
      "cited_paper_id": 271331
    },
    {
      "context_text": "the variance of the Gaussian distribution) rely on much more iterations [24].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to the variance of a Gaussian distribution and the number of iterations, which are general concepts.",
      "processing_time": 57.89638090133667,
      "citing_paper_id": "209322955",
      "cited_paper_id": 1961097
    },
    {
      "context_text": "Technically speaking, there are two basic options for sonic design of a robot’s non-verbal sounds: either by using prerecorded or preprocessed audio samples [14]–[16], [18]–[20]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for sonic design in robots.",
      "processing_time": 54.773115158081055,
      "citing_paper_id": "209322955",
      "cited_paper_id": 3017616
    },
    {
      "context_text": "[16] explore eight different expressional designs for anger, sadness, fear and joy with the NAO robot.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'NAO robot' but does not specify a dataset. The context focuses on the creation and evaluation of emotion expressions, which is more about method or findings rather than a reusable dataset.",
      "processing_time": 58.90783405303955,
      "citing_paper_id": "209322955",
      "cited_paper_id": 3017616
    },
    {
      "context_text": "We also believe that there is a possibility to further optimize the sonic interaction design considering implicit human reactions to a robot’s expressions in real-time, such as with reinforcement learning and human social signals [6], [7], [25]–[28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts and methods. No verifiable resources are identified.",
      "processing_time": 55.40426421165466,
      "citing_paper_id": "209322955",
      "cited_paper_id": 9272651
    },
    {
      "context_text": ", male and female) or prefabricated samples, similarly to spoken language, where Natural Language Generation can be used to adapt the robot’s linguistic style accordingly [6]–[12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to Natural Language Generation. No verifiable resources are identified.",
      "processing_time": 55.866883754730225,
      "citing_paper_id": "209322955",
      "cited_paper_id": 9272651
    },
    {
      "context_text": "However, ranking the products and display them to users can no longer meet the requirements of customers (Zhang et al. 2019; Gong et al. 2019; Chen et al. 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about customer requirements and product display. No verifiable resources are identified.",
      "processing_time": 56.066168546676636,
      "citing_paper_id": "232092620",
      "cited_paper_id": 4533209
    },
    {
      "context_text": "However, ranking the products and display them to users can no longer meet the requirements of customers (Zhang et al. 2019; Gong et al. 2019; Chen et al. 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general statements about customer requirements and product display. No verifiable resources are identified.",
      "processing_time": 56.066168546676636,
      "citing_paper_id": "232092620",
      "cited_paper_id": 146120595
    },
    {
      "context_text": "Reichelt et al. (2014) showed that personalized information of learning materials can increase motivation and learning outcomes.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about personalized information in learning materials.",
      "processing_time": 54.385255336761475,
      "citing_paper_id": "232092620",
      "cited_paper_id": 12708444
    },
    {
      "context_text": "We also apply warm-up trick over the ﬁrst 8, 000 steps, and decay as in Vaswani et al. (2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (warm-up trick) and a reference to a paper for decay strategy.",
      "processing_time": 55.94539928436279,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "P vocab ( y t | H ) is the output probability from a stack of Transformer decoder layers (Vaswani et al. 2017). λ 1 , λ 2 and λ 3 are the coordination probability, which are estimated as follows:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer decoder layers) and parameters (λ 1, λ 2, λ 3).",
      "processing_time": 56.37285614013672,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "We refer the readers to Vaswani et al. (2017) for more details.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a reference to a paper for more details.",
      "processing_time": 54.44981646537781,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "…short product title generation (Sun et al. 2018), (iii) Trans-former : an encoder-decoder architecture relying solely on self-attention mechanisms (Vaswani et al. 2017), (iv) Hi-erTrans : a hierarchical transformer for abstractive multi-document summarization tasks (Liu and Lapata 2019), (v) EMA…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 55.04454207420349,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The encoding layer employs a stacked Transformer encoder module (Vaswani et al. 2017) to encode context including attributes, titles and customer reviews.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer encoder module).",
      "processing_time": 54.20417785644531,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The sub-layer FFN( · ) is a position-wise fully connected feed-forward network, and MHA( Q , K , V ) (Vaswani et al. 2017) is a multi-head attention function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and architectures.",
      "processing_time": 53.3208646774292,
      "citing_paper_id": "232092620",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "As for product description generation task, early work focuses on template-based generation approaches that incorporates statistical methods (Wang et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'template-based generation approaches that incorporates statistical methods' but does not specify any dataset names. The context is about methodological approaches rather than specific datasets.",
      "processing_time": 57.17447280883789,
      "citing_paper_id": "232092620",
      "cited_paper_id": 36574384
    },
    {
      "context_text": "High-quality customer reviews are an ideal source to mine user-cared aspects (Pecar 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to customer reviews as a source but does not name a particular dataset.",
      "processing_time": 56.088422775268555,
      "citing_paper_id": "232092620",
      "cited_paper_id": 51878959
    },
    {
      "context_text": "Recently, Krishna et al. (2018) presented a framework for the summary generation that takes into consideration the linguistic preferences of the speciﬁc audience.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework for summary generation. No verifiable resources are identified.",
      "processing_time": 55.22718048095703,
      "citing_paper_id": "232092620",
      "cited_paper_id": 52011109
    },
    {
      "context_text": "…with pointer generator mechanism (See, Liu, and Man-ning 2017), (ii) MS-Ptr : a multi-source pointer network for short product title generation (Sun et al. 2018), (iii) Trans-former : an encoder-decoder architecture relying solely on self-attention mechanisms (Vaswani et al. 2017), (iv)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context focuses on describing different models used for text generation, which are not considered datasets.",
      "processing_time": 57.01949691772461,
      "citing_paper_id": "232092620",
      "cited_paper_id": 52055965
    },
    {
      "context_text": "E ′ R = FFN(MHA(H̃ s item, H̃ s item,E (L) R )), where W1, b1 are the parameters, and Gelu (Gaussian Error Linear Unit) (Hendrycks and Gimpel 2016) is the non-linear projection function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GELU).",
      "processing_time": 53.954482316970825,
      "citing_paper_id": "232092620",
      "cited_paper_id": 125617073
    },
    {
      "context_text": "We also employ Fleiss’ kappa scores (Fleiss 1971) to measure the reliability between different annotators.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Fleiss' kappa scores but does not refer to a specific dataset. It is used to measure inter-annotator reliability, which is a methodological consideration.",
      "processing_time": 57.38120365142822,
      "citing_paper_id": "232092620",
      "cited_paper_id": 143544759
    },
    {
      "context_text": "…both structured and unstructured data with exponential moving average (EMA) technique (Shahidi, Li, and Lin 2020), (vi) KOBE : the state-of-the-art product description generation model with incorporated personalized knowledge attributes from external Wikipedia knowledge base (Chen et al. 2019a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'KOBE', which is described as a model, not a dataset. No other specific datasets are mentioned.",
      "processing_time": 55.449418783187866,
      "citing_paper_id": "232092620",
      "cited_paper_id": 146120595
    },
    {
      "context_text": "Chen et al. (2019b) built a bridge between personalized outﬁt generation and recommendation by considering both user preferences and individual items.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for personalized outfit generation. The context focuses on the approach rather than a particular dataset.",
      "processing_time": 56.56518197059631,
      "citing_paper_id": "232092620",
      "cited_paper_id": 146120595
    },
    {
      "context_text": "To enhance the effectiveness of user-cared aspects, other researchers (Chen et al. 2019a,b) propose to incorporate customer’s personalized proﬁles and/or external product knowledge from Wikipedia to generate product descriptions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to incorporate external product knowledge into personalized product description generation, enhancing the effectiveness of user-cared aspects."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions incorporating customer’s personalized profiles and external product knowledge from Wikipedia, which suggests the use of specific datasets or sources of information.",
      "processing_time": 61.72828221321106,
      "citing_paper_id": "232092620",
      "cited_paper_id": 146120595
    },
    {
      "context_text": "…on self-attention mechanisms (Vaswani et al. 2017), (iv) Hi-erTrans : a hierarchical transformer for abstractive multi-document summarization tasks (Liu and Lapata 2019), (v) EMA : a uniﬁed text generation model for both structured and unstructured data with exponential moving average (EMA)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.85115909576416,
      "citing_paper_id": "232092620",
      "cited_paper_id": 170079112
    },
    {
      "context_text": "…and Lapata 2019), (v) EMA : a uniﬁed text generation model for both structured and unstructured data with exponential moving average (EMA) technique (Shahidi, Li, and Lin 2020), (vi) KOBE : the state-of-the-art product description generation model with incorporated personalized knowledge…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several models and methods but does not specify any datasets. No clear identifiers for datasets are present.",
      "processing_time": 54.53047847747803,
      "citing_paper_id": "232092620",
      "cited_paper_id": 218470266
    },
    {
      "context_text": "…persuasive message (Ding and Pan 2016; Zhang et al. 2018), poetry generation (Shen, Guo, and Chen 2020), ar-gument generation (Carenini and Moore 2006) and dialogue generation (Shen and Feng 2020; Feng et al. 2020a; Shen, Feng, and Zhan 2019; Shen et al. 2021; Cai et al. 2020; Liu et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of text generation. No verifiable resources are identified.",
      "processing_time": 54.80748915672302,
      "citing_paper_id": "232092620",
      "cited_paper_id": 222212879
    },
    {
      "context_text": "…persuasive message (Ding and Pan 2016; Zhang et al. 2018), poetry generation (Shen, Guo, and Chen 2020), ar-gument generation (Carenini and Moore 2006) and dialogue generation (Shen and Feng 2020; Feng et al. 2020a; Shen, Feng, and Zhan 2019; Shen et al. 2021; Cai et al. 2020; Liu et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of text generation. No verifiable resources are identified.",
      "processing_time": 54.80748915672302,
      "citing_paper_id": "232092620",
      "cited_paper_id": 231951585
    },
    {
      "context_text": "…persuasive message (Ding and Pan 2016; Zhang et al. 2018), poetry generation (Shen, Guo, and Chen 2020), ar-gument generation (Carenini and Moore 2006) and dialogue generation (Shen and Feng 2020; Feng et al. 2020a; Shen, Feng, and Zhan 2019; Shen et al. 2021; Cai et al. 2020; Liu et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of text generation. No verifiable resources are identified.",
      "processing_time": 54.80748915672302,
      "citing_paper_id": "232092620",
      "cited_paper_id": null
    },
    {
      "context_text": "2018), poetry generation (Shen, Guo, and Chen 2020), argument generation (Carenini and Moore 2006) and dialogue generation (Shen and Feng 2020; Feng et al. 2020a; Shen, Feng, and Zhan 2019; Shen et al. 2021; Cai et al. 2020; Liu et al. 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of text generation tasks. No clear, verifiable resources are identified.",
      "processing_time": 55.32070183753967,
      "citing_paper_id": "232092620",
      "cited_paper_id": 222212879
    },
    {
      "context_text": "Inspired by the knowledge distillation’s success (Hinton, Vinyals, and Dean 2015; Feng et al. 2020b) on model compression and knowledge transfer, we propose an adaptive posterior distillation layer to transfer user-cared aspects in review information ( teacher ) to item representation ( student ),…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (knowledge distillation) and its application in transferring user-cared aspects in review information.",
      "processing_time": 55.91812014579773,
      "citing_paper_id": "232092620",
      "cited_paper_id": null
    },
    {
      "context_text": "2020), persuasive message (Ding and Pan 2016), poetry generation (Shen, Guo, and Chen 2020), argument generation (Carenini and Moore 2006) and dialogue generation (Shen and Feng 2020; Feng et al. 2020a; Shen, Feng, and Zhan 2019; Shen et al. 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various types of text generation tasks. No verifiable resources are identified.",
      "processing_time": 54.978259325027466,
      "citing_paper_id": "232092620",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Sutton & Barto, 2018), which possesses many limitations such as (1) suboptimal solutions due to lack of representation (Hayes et al., 2022), (2) lack of explainability of distinct objectives, and (3) ensuring fair outcomes for multiple participants (Vamplew et al., 2018; Siddique et al., 2020).",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of reinforcement learning approaches. No verifiable resources are identified.",
      "processing_time": 54.963160037994385,
      "citing_paper_id": "264289231",
      "cited_paper_id": 3696067
    },
    {
      "context_text": "…(Sutton & Barto, 2018), which possesses many limitations such as (1) suboptimal solutions due to lack of representation (Hayes et al., 2022), (2) lack of explainability of distinct objectives, and (3) ensuring fair outcomes for multiple participants (Vamplew et al., 2018; Siddique et al., 2020).",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only limitations of reinforcement learning approaches. No verifiable resources are identified.",
      "processing_time": 54.963160037994385,
      "citing_paper_id": "264289231",
      "cited_paper_id": 221081510
    },
    {
      "context_text": "…Natural Language Processing Personalization in Natural Language Processing (NLP) has mainly been focused on creating personalized dialogue agents (Zhang et al., 2018; Mazar´e et al., 2018; Zheng et al., 2019; Wu et al., 2021b; Xu et al., 2022), where the task is to create chitchat agents that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research on personalized dialogue agents. No verifiable resources are identified.",
      "processing_time": 54.97117066383362,
      "citing_paper_id": "264289231",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Vanilla Baseline (VB) As the most simple baseline, we simply utilize the base Tulu-7B model to generate responses without providing it any notion of preferences.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Tulu-7B) which is not a dataset. No verifiable resources are identified.",
      "processing_time": 56.17950916290283,
      "citing_paper_id": "264289231",
      "cited_paper_id": 15364622
    },
    {
      "context_text": "Previous work has aimed to alleviate these problems through novel MORL methods (Van Moffaert et al., 2013; Van Moffaert & Now´e, 2014; Yang et al., 2019; Xu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works.",
      "processing_time": 53.532630443573,
      "citing_paper_id": "264289231",
      "cited_paper_id": 15364622
    },
    {
      "context_text": "…in Natural Language Processing (NLP) has mainly been focused on creating personalized dialogue agents (Zhang et al., 2018; Mazar´e et al., 2018; Zheng et al., 2019; Wu et al., 2021b; Xu et al., 2022), where the task is to create chitchat agents that are engaging with distinct personas based…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. The focus is on the creation of personalized dialogue agents, but no datasets are explicitly named.",
      "processing_time": 56.634055852890015,
      "citing_paper_id": "264289231",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "…in Natural Language Processing (NLP) has mainly been focused on creating personalized dialogue agents (Zhang et al., 2018; Mazar´e et al., 2018; Zheng et al., 2019; Wu et al., 2021b; Xu et al., 2022), where the task is to create chitchat agents that are engaging with distinct personas based on…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. The focus is on the creation of personalized dialogue agents, which is a research topic rather than a specific dataset.",
      "processing_time": 57.39514350891113,
      "citing_paper_id": "264289231",
      "cited_paper_id": 59316441
    },
    {
      "context_text": "Another line of work (Salemi et al., 2023) leverages personalized information to boost performance on specific tasks such as review generation (Li & Tuzhilin, 2019), recipe generation (Majumder et al., 2019), and headline generation (Ao et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific tasks such as review generation, recipe generation, and headline generation, but does not explicitly mention any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.216954708099365,
      "citing_paper_id": "264289231",
      "cited_paper_id": 202120896
    },
    {
      "context_text": "Another line of work (Salemi et al., 2023) leverages personalized information to boost performance on specific tasks such as review generation (Li & Tuzhilin, 2019), recipe generation (Majumder et al., 2019), and headline generation (Ao et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific tasks such as review generation, recipe generation, and headline generation, but does not explicitly mention any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.216954708099365,
      "citing_paper_id": "264289231",
      "cited_paper_id": 202767450
    },
    {
      "context_text": "Another line of work (Salemi et al., 2023) leverages personalized information to boost performance on specific tasks such as review generation (Li & Tuzhilin, 2019), recipe generation (Majumder et al., 2019), and headline generation (Ao et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific tasks such as review generation, recipe generation, and headline generation, but does not explicitly mention any datasets. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.216954708099365,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "…Processing (NLP) has mainly been focused on creating personalized dialogue agents (Zhang et al., 2018; Mazar´e et al., 2018; Zheng et al., 2019; Wu et al., 2021b; Xu et al., 2022), where the task is to create chitchat agents that are engaging with distinct personas based on user profile (e.g.…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the task of creating personalized dialogue agents. No verifiable resources are identified.",
      "processing_time": 54.8174614906311,
      "citing_paper_id": "264289231",
      "cited_paper_id": 234757004
    },
    {
      "context_text": "This process has been applied to summarization (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021a), answering questions with long-form answers using text retrieved from the web (Nakano et al., 2021b; Menick et al., 2022), generating engaging responses in a dialogue settings (Thoppilan…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that apply certain processes to various tasks. No clear, verifiable datasets are named.",
      "processing_time": 56.06217813491821,
      "citing_paper_id": "264289231",
      "cited_paper_id": 237593001
    },
    {
      "context_text": "…with long-form answers using text retrieved from the web (Nakano et al., 2021b; Menick et al., 2022), generating engaging responses in a dialogue settings (Thoppilan et al., 2022; Cohen et al., 2022) and following human instructions (Kojima et al., 2021; Suhr & Artzi, 2022; Kim et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research works and methods. The context is about generating personalized text in dialogues and following instructions, but no datasets are explicitly named.",
      "processing_time": 57.18023681640625,
      "citing_paper_id": "264289231",
      "cited_paper_id": 246063428
    },
    {
      "context_text": "Reinforcement Learning from Human Feedback (RLHF) (Nakano et al., 2021a; Ouyang et al., 2022a; Bai et al., 2022a; Dubois et al., 2023; Bai et al., 2022b) typically optimizes a policy model that receives training signals from a single reward model that aims to capture the general preferences of a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on reinforcement learning from human feedback, which is a method rather than a dataset.",
      "processing_time": 56.61809825897217,
      "citing_paper_id": "264289231",
      "cited_paper_id": 246426909
    },
    {
      "context_text": "Reinforcement Learning from Human Feedback (RLHF) (Nakano et al., 2021a; Ouyang et al., 2022a; Bai et al., 2022a; Dubois et al., 2023; Bai et al., 2022b) typically optimizes a policy model that receives training signals from a single reward model that aims to capture the general preferences of a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on reinforcement learning from human feedback, which is a method rather than a dataset.",
      "processing_time": 56.61809825897217,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "In this work, we instead propose Reinforcement Learning from Personalized Human Feedback (RL P HF), a new, multi-objective formulation of the human preference alignment problem, where Large Language Models (LLMs) are trained to be efficiently aligned with a range of different, potentially personalized combinations of human preferences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training language models using reinforcement learning with personalized human feedback.",
      "processing_time": 54.80472707748413,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "Previous work has shown that adapting LLMs with RLHF helps them generate outputs that are preferred by humans over the supervised fine-tuned counterpart.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general technique (RLHF) for improving LLMs. No verifiable resources are identified.",
      "processing_time": 56.06062364578247,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "While w i are given as inputs directly to the policy model in traditional RL settings using PPO with MORL, there is no straightforward way of integrating different w i as an input to LLMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the integration of different weights in LLMs and RL settings.",
      "processing_time": 54.79866862297058,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "Kirk et al. (2023) defines a taxonomy and policy framework for the alignment of LLMs with personalized feedback.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a taxonomy and policy framework. The context is about defining a framework for aligning LLMs with personalized feedback.",
      "processing_time": 56.352190017700195,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "Kirk et al. (2023) defines a taxonomy and policy framework for the alignment of LLMs with personalized feedback.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a taxonomy and policy framework. The context is about defining a framework for aligning LLMs with personalized feedback.",
      "processing_time": 56.352190017700195,
      "citing_paper_id": "264289231",
      "cited_paper_id": 257427629
    },
    {
      "context_text": "However, recent work has also pointed out that simply training LLMs to abide by the preference of the general may result in ignoring individual preferences and values.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concern about training large language models.",
      "processing_time": 53.03295087814331,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "…has led to many interesting applications of model merging such as composing the abilities of expert models that perform different tasks (Ilharco et al., 2022; Jang et al., 2023) and introducing language-specific modules for growing the total capacity of multilingual LMs (Pfeiffer et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to model merging and language-specific modules. No verifiable resources are identified.",
      "processing_time": 55.108556509017944,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "This line of work has led to many interesting applications of model merging such as composing the abilities of expert models that perform different tasks (Ilharco et al., 2022; Jang et al., 2023) and introducing language-specific modules for growing the total capacity of multilingual LMs (Pfeiffer et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and research works. The context focuses on model merging and language-specific modules for multilingual models.",
      "processing_time": 56.13248252868652,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "Next, we can see that supervised fine-tuning (RS) underperforms MORL-based methods, which is consistent with prior work that also showed the advantage of RL-based approaches when aligning LLMs with human feedback compared to its supervised-finetuning counterpart.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between supervised fine-tuning and reinforcement learning methods.",
      "processing_time": 54.51415991783142,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "For example, recent work has shown that LLMs aligned with RLHF prefer verbose output generations (Zheng et al., 2023; Dubois et al., 2023; Wang et al., 2023; Singhal et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research findings about LLMs and RLHF. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.47264099121094,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "For example, recent work has shown that LLMs aligned with RLHF prefer verbose output generations (Zheng et al., 2023; Dubois et al., 2023; Wang et al., 2023; Singhal et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research findings about LLMs and RLHF. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.47264099121094,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "For example, recent work has shown that LLMs aligned with RLHF prefer verbose output generations (Zheng et al., 2023; Dubois et al., 2023; Wang et al., 2023; Singhal et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research findings about LLMs and RLHF. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.47264099121094,
      "citing_paper_id": "264289231",
      "cited_paper_id": 263672200
    },
    {
      "context_text": "In this work, we provide the first steps to tackle this issue by proposing Reinforcement Learning from Personalized Human Feedback as a multi-objective problem so that LLMs can be aligned to follow conflicting preferences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.11761498451233,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "This modular approach significantly We empirically show that by transforming the problem of aligning LLMs to human preferences into a MORL problem, we are able to have personalized alignment that provides a deeper level of adaptation to individual users that supervised fine-tuning, RLHF, and prompting cannot attain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no clear identifiers for datasets in the text.",
      "processing_time": 55.5442430973053,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "While Dubois et al. (2023) mostly simulates GPT-4 and other LLMs to choose which is generally a better response be-tween two candidate responses, we provide GPT-4 with a single preference (full list shown in Table 1) to decide which is a better response.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of GPT-4 and simulations. No verifiable resources are identified.",
      "processing_time": 55.770267486572266,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "While Dubois et al. (2023) mostly simulates GPT-4 and other LLMs to choose which is generally a better response be-tween two candidate responses, we provide GPT-4 with a single preference (full list shown in Table 1) to decide which is a better response.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of GPT-4 and simulations. No verifiable resources are identified.",
      "processing_time": 55.770267486572266,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "In this work, we convert the problem of aligning LLMs to human preferences into a MORL problem to (1) provide a more optimal solution for each individual, (2) allow users to dynamically choose the distinct objectives they want to optimize, and (3) ensure fairness by allowing preferences that may be in the long-tail to be integrated.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It focuses on the approach and goals of the research.",
      "processing_time": 55.546252489089966,
      "citing_paper_id": "264289231",
      "cited_paper_id": 248721770
    },
    {
      "context_text": "Parameter Merging Recent work has shown that performing weighted linear interpolation of model parameters leads to the composition of each model ability (Li et al., 2022; Wortsman et al., 2022b;a; Don-Yehiya et al., 2022; Huang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent work on parameter merging in models.",
      "processing_time": 54.3414204120636,
      "citing_paper_id": "264289231",
      "cited_paper_id": 251371375
    },
    {
      "context_text": "Following previous work, we simulate human annotators with GPT-4 for collecting large-scale pair-wise feedback data (Bai et al., 2022b; Dubois et al., 2023)—but note that our evaluations are validated with (smaller-scale) human preference data collected from crowdworkers.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'pair-wise feedback data' and 'human preference data', but does not specify a named dataset. The reference to GPT-4 is about simulating human annotators, not a dataset.",
      "processing_time": 58.35680389404297,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "Simulated pairwise evaluation We use a modified version of the GPT4 annotation prompt used by Dubois et al. (2023).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a modified version of an annotation prompt, which is not a dataset.",
      "processing_time": 55.9268913269043,
      "citing_paper_id": "264289231",
      "cited_paper_id": 258865545
    },
    {
      "context_text": "Most recently, Rame et al. (2023) proposed to merge policy models that were trained to perform specific tasks such as question answering and summarization using proxy reward models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and tasks. There are no clear identifiers for datasets in the text.",
      "processing_time": 54.915831565856934,
      "citing_paper_id": "264289231",
      "cited_paper_id": 259096117
    },
    {
      "context_text": "Instead, we train our reward model on multiple comparisons (Song et al., 2023; Kim et al., 2023) by including a neutral response and a negative response as shown in Figure 2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the method of training a reward model using multiple comparisons.",
      "processing_time": 54.27474665641785,
      "citing_paper_id": "264289231",
      "cited_paper_id": 259308873
    },
    {
      "context_text": "However, the standard RLHF setup commonly addressed in prior work assumes a reward model that accounts only for average annotator preference, i.e., the fact that different users may desire different outputs, even for the same prompt, is ignored Casper et al. (2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general setup in reinforcement learning from human feedback.",
      "processing_time": 54.09037899971008,
      "citing_paper_id": "264289231",
      "cited_paper_id": 260316010
    },
    {
      "context_text": "Evaluation For evaluation, we manually filter out 50 instances from the Koala evaluation (Geng et al., 2023) that require open-ended generations.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Koala evaluation"
      ],
      "dataset_descriptions": {
        "Koala evaluation": "Used to filter out 50 instances requiring open-ended generations for evaluation, focusing on the quality and appropriateness of generated responses."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'Koala evaluation' which is used for filtering out specific instances for evaluation. It appears to be a specific dataset or evaluation set.",
      "processing_time": 63.28809070587158,
      "citing_paper_id": "264289231",
      "cited_paper_id": null
    },
    {
      "context_text": "Based on those criteria, we select two famous face datasets CelebA-HQ [28] and VGGFace2 [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebA-HQ",
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "CelebA-HQ": "Used to recognize faces across pose and age, focusing on high-quality images and diverse attributes for personalized text generation.",
        "VGGFace2": "Used to recognize faces across pose and age, providing a large-scale dataset with diverse identities and poses for robust facial recognition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, CelebA-HQ and VGGFace2, which are used for recognizing faces across pose and age. These datasets are clearly identified and relevant to the research topic.",
      "processing_time": 72.2032117843628,
      "citing_paper_id": "257766375",
      "cited_paper_id": 216009
    },
    {
      "context_text": "VGGFace2 [11] contains around 3 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used for recognizing faces across pose and age, but the specific research context and methodology are not detailed in the provided citation."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions VGGFace2, which is a known dataset for face recognition. However, the context does not provide enough information about how it is used in the current research.",
      "processing_time": 64.44629073143005,
      "citing_paper_id": "257766375",
      "cited_paper_id": 216009
    },
    {
      "context_text": "1 and on the VGGFace2 dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to train and evaluate face recognition models, focusing on recognizing faces across different poses and ages."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VGGFace2 dataset', which is a specific, verifiable dataset used for face recognition research. The title confirms it is a dataset.",
      "processing_time": 62.517210483551025,
      "citing_paper_id": "257766375",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Based on those criteria, we select two famous face datasets CelebA-HQ [29] and VGGFace2 [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebA-HQ",
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "CelebA-HQ": "Used to recognize faces across pose and age, focusing on high-quality images and diverse attributes for training and evaluation.",
        "VGGFace2": "Used to recognize faces across pose and age, emphasizing a large-scale dataset with varied poses and ages for robust facial recognition."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, CelebA-HQ and VGGFace2, which are used for recognizing faces across pose and age. These datasets are clearly identified and relevant to the research.",
      "processing_time": 72.03440356254578,
      "citing_paper_id": "257766375",
      "cited_paper_id": 216009
    },
    {
      "context_text": "For each of 50 identities in VGGFace2, we train two DreamBooth models on (1) origi-2122 Users are asked to input the numbers of good-quality and correct-identity images for each set.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to train DreamBooth models for recognizing faces across pose and age, focusing on identity preservation in personalized text generation."
      },
      "confidence_score": 0.9,
      "reasoning": "VGGFace2 is mentioned as a dataset used for training models, which aligns with the topic of personalized text generation, particularly in the context of identity recognition.",
      "processing_time": 63.906468868255615,
      "citing_paper_id": "257766375",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Combining them, [16] is an ensemble of various attacks that are commonly used as a benchmark metric, being able to break through gradient obfuscation [7] with the expectation-over-transformation technique [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.91936779022217,
      "citing_paper_id": "257766375",
      "cited_paper_id": 2645819
    },
    {
      "context_text": "Combining them, [16] is an ensemble of various attacks that are commonly used as a benchmark metric, being able to break through gradient obfuscation [7] with the expectation-over-transformation technique [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.91936779022217,
      "citing_paper_id": "257766375",
      "cited_paper_id": 3310672
    },
    {
      "context_text": "Combining them, [16] is an ensemble of various attacks that are commonly used as a benchmark metric, being able to break through gradient obfuscation [7] with the expectation-over-transformation technique [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.91936779022217,
      "citing_paper_id": "257766375",
      "cited_paper_id": 211818320
    },
    {
      "context_text": "For black-box attacks, where the adversary does not have full access to the model weights and gradients, [53] estimates the gradient using sampling methods, while [10, 14, 6] aim to synthesize a close-by example by searching for the classification boundary, then finding the direction to traverse towards a good adversarial example.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses methods for black-box attacks in machine learning, focusing on gradient estimation and adversarial example synthesis. No specific datasets are mentioned.",
      "processing_time": 55.16104245185852,
      "citing_paper_id": "257766375",
      "cited_paper_id": 3639844
    },
    {
      "context_text": "…with different approaches started to emerge, with more notable ones including: [31, 35] being FGSM’s iterative versions, [12] limiting the adversarial perturbation’s magnitude implicitly using regularization instead of projection, [37] searching for a close-by decision boundary to cross, etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 55.54653072357178,
      "citing_paper_id": "257766375",
      "cited_paper_id": 12387176
    },
    {
      "context_text": "Inspired by DeepFakes’s prevention studies [57, 43, 56, 25, 55], we propose to pro-actively defend each user from the DreamBooth threat by injecting subtle adversarial noise into their images before publishing.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other studies on DeepFakes prevention. No verifiable resources are identified.",
      "processing_time": 55.70296812057495,
      "citing_paper_id": "257766375",
      "cited_paper_id": 211818224
    },
    {
      "context_text": "Within a few years, denoising diffusion models [23, 54, 43] have revolutionized image generation studies, allowing producing images with realistic quality and diverse content [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their impact on image generation. No verifiable resources are identified.",
      "processing_time": 55.358423948287964,
      "citing_paper_id": "257766375",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Within a few years, denoising diffusion models [23, 54, 43] have revolutionized image generation studies, allowing producing images with realistic quality and diverse content [19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their impact on image generation. No verifiable resources are identified.",
      "processing_time": 55.358423948287964,
      "citing_paper_id": "257766375",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Diffusion models are a type of generative models [53, 23] that decouple the role of generation into two opposing procedures: a forward process and a backward process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models and their processes.",
      "processing_time": 53.55815100669861,
      "citing_paper_id": "257766375",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Images generated from successfully disrupted DreamBooth models may have no detectable face, and we measure that rate, called Face Detection Failure Rate (FDFR), using RetinaFace detector [17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the RetinaFace detector but does not refer to a specific dataset. The context is about using RetinaFace to measure the Face Detection Failure Rate (FDFR).",
      "processing_time": 57.38162541389465,
      "citing_paper_id": "257766375",
      "cited_paper_id": 219964874
    },
    {
      "context_text": "These models can be grouped into four main categories: auto-regressive [61], mask-prediction [13], GAN-based [48] and diffusion-based approaches, all of which show astounding qualitative and quantitative results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of models. No verifiable resources are identified.",
      "processing_time": 54.59117078781128,
      "citing_paper_id": "257766375",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "Large models [41, 3, 43, 47, 9] can produce photo-realistic or artistic images just from simple text description inputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the capability of large models to generate images from text. No verifiable resources are identified.",
      "processing_time": 55.88456630706787,
      "citing_paper_id": "257766375",
      "cited_paper_id": 253254800
    },
    {
      "context_text": "Large models [41, 3, 43, 47, 9] can produce photo-realistic or artistic images just from simple text description inputs.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the capability of large models to generate images from text. No verifiable resources are identified.",
      "processing_time": 55.88456630706787,
      "citing_paper_id": "257766375",
      "cited_paper_id": null
    },
    {
      "context_text": "However, the implementation of most prominent methods [47, 9] are not publicly available.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only that implementations of methods are not publicly available.",
      "processing_time": 53.740922927856445,
      "citing_paper_id": "257766375",
      "cited_paper_id": 253254800
    },
    {
      "context_text": "Among these methods, diffusion-based models [43, 47, 9, 38, 42] have exhibited an exceptional capacity for generating high-quality and easily modifiable images, leading to their widespread adoption in text-to-image synthesis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the capabilities of diffusion-based models in generating images.",
      "processing_time": 55.877283811569214,
      "citing_paper_id": "257766375",
      "cited_paper_id": 253254800
    },
    {
      "context_text": "For better trade-off be-tween efficiency and fidelity, following-up works either introduce coarse-to-fine generation process like Imagen [47] and eDiff-I [9] or work on latent space like LDM [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodologies used for text-to-image generation.",
      "processing_time": 55.87492370605469,
      "citing_paper_id": "257766375",
      "cited_paper_id": 253254800
    },
    {
      "context_text": "Similar to our goals, two concurrent works, GLAZE [51] and AdvDM [33], aim to protect against personalized text-to-image diffusion models exploited without consent using image cloaking.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions GLAZE and AdvDM as concurrent works with similar goals, but does not indicate that they are datasets. They are likely methods or tools.",
      "processing_time": 56.47598958015442,
      "citing_paper_id": "257766375",
      "cited_paper_id": 256662278
    },
    {
      "context_text": "Besides, ControlNet [62] offers extra options to control the generation outputs, further boosting the power of the text-to-image models and bringing them closer to mass users.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ControlNet) for controlling text-to-image generation outputs.",
      "processing_time": 54.94213628768921,
      "citing_paper_id": "257766375",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Besides, ControlNet [59] offers extra options to control the DSLR portrait Perturbed images in front of Eiffel Tower Clean images Personalized text-to-image",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method (ControlNet) and a general concept (personalized text-to-image) without naming any datasets.",
      "processing_time": 57.52804946899414,
      "citing_paper_id": "257766375",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Within the context of diffusion models, previous research has focused on adapting a pre-2117 trained model to create fresh images based on a particular target idea using natural language cues.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the general concept of using diffusion models with natural language cues.",
      "processing_time": 56.01932430267334,
      "citing_paper_id": "257766375",
      "cited_paper_id": null
    },
    {
      "context_text": "Alternatively, Stable Diffusion has released the pre-trained weights based on Hugging Face implementation [57] to facilitate research in the community.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained weights and an implementation. No verifiable resources are identified.",
      "processing_time": 54.904706954956055,
      "citing_paper_id": "257766375",
      "cited_paper_id": null
    },
    {
      "context_text": "StableDiffusion [4], primarily based on LDM, is the first open-source large model of this type, further boosting the widespread applications of text-to-image synthesis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their applications.",
      "processing_time": 53.476564168930054,
      "citing_paper_id": "257766375",
      "cited_paper_id": null
    },
    {
      "context_text": "Gulcehre et al. (2016) proposed a pointing method to handle the OOV words for summarization and MT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (pointing method) rather than a dataset. No specific dataset is referenced in the citation context.",
      "processing_time": 54.67335534095764,
      "citing_paper_id": "8174613",
      "cited_paper_id": 969555
    },
    {
      "context_text": "…tends to cover consecutive words, including UNK s. Unlike the explicit de-sign for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), C OPY N ET is more subtle: it provides the architecture that can facilitate some particular location-based addressing and lets…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the design aspects of COPYNET and its comparison with Neural Turing Machine.",
      "processing_time": 56.507893323898315,
      "citing_paper_id": "8174613",
      "cited_paper_id": 1174466
    },
    {
      "context_text": "As the result, the selective read often takes rigid move and tends to cover consecutive words, including UNK s. Unlike the explicit de-sign for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), C OPY N ET is more subtle: it provides the architecture that can facilitate some particular location-based addressing and lets the model ﬁgure out the details from the training data for speciﬁc tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is discussing architectural design choices in neural network models.",
      "processing_time": 54.69359874725342,
      "citing_paper_id": "8174613",
      "cited_paper_id": 1174466
    },
    {
      "context_text": "Unlike the explicit design for hybrid addressing in Neural Turing Machine (Graves et al., 2014; Kurach et al., 2015), COPYNET is more subtle: it provides the archi-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 55.80278301239014,
      "citing_paper_id": "8174613",
      "cited_paper_id": 1174466
    },
    {
      "context_text": "Recently, neural network-based sequence-to-sequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various NLP tasks and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.36582827568054,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Recently, neural network-based sequence-to-sequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various NLP tasks and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.36582827568054,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "Recently, neural network-based sequence-to-sequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various NLP tasks and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.36582827568054,
      "citing_paper_id": "8174613",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, ﬁrst by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.79714393615723,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, ﬁrst by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.79714393615723,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, ﬁrst by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.79714393615723,
      "citing_paper_id": "8174613",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "RNN-based Encoder-Decoder is successfully applied to real world Seq2Seq tasks, ﬁrst by Cho et al. (2014) and Sutskever et al. (2014), and then by (Vinyals and Le, 2015; Vinyals et al., 2015a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 55.79714393615723,
      "citing_paper_id": "8174613",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…(Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only NLP tasks and models. No verifiable resources are identified.",
      "processing_time": 55.39839196205139,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "For a fair comparison, we use bi-directional GRU for encoder and another GRU for decoder for all Seq2Seq models, with hidden layer size = 300 and word embedding dimension = 150.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model architectures and parameters. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.60503554344177,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "In practice it is found that gated RNN alternatives such as LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) often perform much better than vanilla ones.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (LSTM, GRU). No verifiable resources are referenced.",
      "processing_time": 55.391817569732666,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "In addition to the difference with ours in application, (Vinyals et al., 2015a) cannot predict outside of the set of input sequence, while C OPY N ET can naturally combine generating and copying.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between two methods (Pointer Networks and COPY NET).",
      "processing_time": 54.96588373184204,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "…achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only NLP tasks and models. No verifiable resources are identified.",
      "processing_time": 55.336684703826904,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "…achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only NLP tasks and models. No verifiable resources are identified.",
      "processing_time": 55.336684703826904,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "…achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al., 2015b), Text Summarization (Rush et al., 2015) and Dialogue Systems (Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only NLP tasks and models. No verifiable resources are identified.",
      "processing_time": 55.336684703826904,
      "citing_paper_id": "8174613",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In addition to the difference with ours in application, (Vinyals et al., 2015a) cannot predict outside of the set of input sequence, while COPYNET can naturally combine generating and copying.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (Pointer Networks and COPYNET).",
      "processing_time": 54.988539695739746,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "Our work is partially inspired by the recent work of Pointer Networks (Vinyals et al., 2015a), in which a pointer mechanism (quite similar with the proposed copying mechanism) is used to predict the output sequence directly from the input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Pointer Networks). The context focuses on the methodology and its similarity to the copying mechanism.",
      "processing_time": 56.32037425041199,
      "citing_paper_id": "8174613",
      "cited_paper_id": 5692837
    },
    {
      "context_text": "The attention mechanism was ﬁrst introduced to Seq2Seq (Bahdanau et al., 2014) to release the burden of summarizing the entire source into a ﬁxed-length vector as context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the introduction of the attention mechanism in Seq2Seq models. No verifiable resources are identified.",
      "processing_time": 56.08574318885803,
      "citing_paper_id": "8174613",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "The attention mechanism was ﬁrst introduced to Seq2Seq (Bahdanau et al., 2014) to release the burden of summarizing the entire source into a ﬁxed-length vector as context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the introduction of the attention mechanism in Seq2Seq models. No verifiable resources are identified.",
      "processing_time": 56.08574318885803,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "It is similar with the canonical RNN-decoder in (Bahdanau et al., 2014), with however the following important differences • Prediction: C OPY N ET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copy-mode , where the latter picks words from the…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (COPY NET) and its prediction mechanism. No verifiable resources are identified.",
      "processing_time": 55.75118398666382,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Note that in (Bahdanau et al., 2014) the source sentence is encoded with a Bi-directional RNN, making each hidden state h τ aware of the contextual information from both ends.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Bi-directional RNN) used in the encoding process.",
      "processing_time": 55.20376896858215,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Adding the attention mechanism (Bahdanau et al., 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (attention mechanism) and its application in machine translation.",
      "processing_time": 55.01380968093872,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Note that in (Bahdanau et al., 2014) the source sentence is encoded with a Bi-directional RNN, making each hidden state hτ aware of the contextual information from both ends.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Bi-directional RNN) used in the encoding process.",
      "processing_time": 55.4074432849884,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The score of each mode is calculated: Generate-Mode: The same scoring function as in the generic RNN encoder-decoder (Bahdanau et al., 2014) is used, i.e. where W o ∈ R ( N +1) × d s and v i is the one-hot indicator vector for v i .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a scoring function used in a generic RNN encoder-decoder model. The cited paper is about a method, not a dataset.",
      "processing_time": 57.003026485443115,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Recently, neural network-based sequence-tosequence learning (Seq2Seq) has achieved remarkable success in various natural language processing (NLP) tasks, including but not limited to Machine Translation (Cho et al., 2014; Bahdanau et al., 2014), Syntactic Parsing (Vinyals et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general NLP tasks and models. No verifiable resources are identified.",
      "processing_time": 55.3878276348114,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The attention mechanism was first introduced to Seq2Seq (Bahdanau et al., 2014) to release the burden of summarizing the entire source into a fixed-length vector as context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (attention mechanism) and its application in Seq2Seq models.",
      "processing_time": 55.391995906829834,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Encoder: Same as in (Bahdanau et al., 2014), a bi-directional RNN is used to transform the source sequence into a series of hidden states with equal length, with each hidden state ht corresponding to word xt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (bi-directional RNN) used in the encoder. The cited paper title confirms this is about a method, not a dataset.",
      "processing_time": 57.70169448852539,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Encoder: Same as in (Bahdanau et al., 2014), a bi-directional RNN is used to transform the source sequence into a series of hidden states with equal length, with each hidden state h t corresponding to word x t .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (bi-directional RNN) used in the encoder. The cited paper title confirms this is about a method, not a dataset.",
      "processing_time": 57.89223074913025,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "It is similar with the canonical RNN-decoder in (Bahdanau et al., 2014), with however the following important differences • Prediction: COPYNET predicts words based on a mixed probabilistic model of two modes, namely the generate-mode and the copymode, where the latter picks words from the source sequence (see Section 3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (COPYNET) and its differences from a canonical RNN-decoder.",
      "processing_time": 56.15473508834839,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Generate-Mode: The same scoring function as in the generic RNN encoder-decoder (Bahdanau et al., 2014) is used, i.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RNN encoder-decoder) which is not a dataset.",
      "processing_time": 55.561214447021484,
      "citing_paper_id": "8174613",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test COPYNET on single-turn dialogue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to neural dialogue models. The context focuses on the methodology and models rather than datasets.",
      "processing_time": 56.354332447052,
      "citing_paper_id": "8174613",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In this experiment we follow the work on neural dialogue model proposed in (Shang et al., 2015; Vinyals and Le, 2015; Sordoni et al., 2015), and test C OPY N ET on single-turn dialogue.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to neural dialogue models. The term 'single-turn dialogue' is too generic and lacks a specific identifier.",
      "processing_time": 57.02276420593262,
      "citing_paper_id": "8174613",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "8917 STSB The Semantic Text Similarity Benchmark (Cer et al., 2017) consists of English sentence pairs and a semantic similarity score for each pair.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "The Semantic Text Similarity Benchmark"
      ],
      "dataset_descriptions": {
        "The Semantic Text Similarity Benchmark": "Used to evaluate semantic textual similarity, specifically focusing on English sentence pairs with assigned similarity scores."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'The Semantic Text Similarity Benchmark' which is a specific dataset used for evaluating semantic textual similarity.",
      "processing_time": 62.06280493736267,
      "citing_paper_id": "238582870",
      "cited_paper_id": 4421747
    },
    {
      "context_text": "STSB The Semantic Text Similarity Benchmark (Cer et al., 2017) consists of English sentence pairs and a semantic similarity score for each pair.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Semantic Text Similarity Benchmark"
      ],
      "dataset_descriptions": {
        "Semantic Text Similarity Benchmark": "Used to evaluate semantic textual similarity, providing English sentence pairs with associated similarity scores for benchmarking models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'The Semantic Text Similarity Benchmark' which is a specific dataset used for evaluating semantic textual similarity. It is clearly identified and used in the research context.",
      "processing_time": 63.26308512687683,
      "citing_paper_id": "238582870",
      "cited_paper_id": 4421747
    },
    {
      "context_text": "Concerning (global) interpretability of text representations, previous works (Adi et al., 2017; Conneau et al., 2018) introspect the properties encoded in vector representations through probing classifiers—trained on external data to perform a certain linguistic task, such as inducing the…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'external data' used for training probing classifiers but does not specify any named datasets. The focus is on the method of using probing classifiers to introspect text representations.",
      "processing_time": 57.63039803504944,
      "citing_paper_id": "238582870",
      "cited_paper_id": 6771196
    },
    {
      "context_text": "resentations, previous works (Adi et al., 2017; Conneau et al., 2018) introspect the properties encoded in vector representations through probing classifiers—trained on external data to perform a certain linguistic task, such as inducing the depen-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of probing classifiers trained on external data. No clear, verifiable dataset names are provided.",
      "processing_time": 56.75719165802002,
      "citing_paper_id": "238582870",
      "cited_paper_id": 6771196
    },
    {
      "context_text": "The second observation shows that these metrics may be prone to adversarial fooling, just like BLEU and ROUGE, which we confirmed in an additional experiment in which we queried metric preferences over paraphrases with little lexical overlap and non-paraphrases with high lexical overlap.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only metrics (BLEU and ROUGE) which are excluded according to the instructions.",
      "processing_time": 55.8853976726532,
      "citing_paper_id": "238582870",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Lexical overlap score (LEX) We measure the lexical overlap between x and y by the BLEU score (Papineni et al., 2002b): BLEUn calculates the precision based on how many n-grams of one sentence can be found in the other sentence.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (BLEU) which is not a dataset. The citation is about a method for evaluating machine translation.",
      "processing_time": 56.920568227767944,
      "citing_paper_id": "238582870",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Traditional evaluation metrics in machine translation and summarization, BLEU and ROUGE (Papineni et al., 2002a; Lin, 2004), have measured lexical n-gram overlap between system prediction and a human reference.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BLEU and ROUGE but does not refer to them as datasets. They are evaluation metrics, which are excluded according to the instructions.",
      "processing_time": 56.74523448944092,
      "citing_paper_id": "238582870",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "This data is collected over multiple language pairs which have English as target language (so both the human reference and the hypothesis are in English) of WMT15-WMT17.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WMT15-WMT17"
      ],
      "dataset_descriptions": {
        "WMT15-WMT17": "Used to collect multilingual data for evaluating machine translation systems, focusing on English as the target language to study translation quality and personalization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'WMT15-WMT17' as a source of data, which is a specific and identifiable dataset used in machine translation evaluation.",
      "processing_time": 65.98281025886536,
      "citing_paper_id": "238582870",
      "cited_paper_id": 28232901
    },
    {
      "context_text": "Chinese is only available in WMT17.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WMT17"
      ],
      "dataset_descriptions": {
        "WMT17": "Mentioned as a source of Chinese data, but the specific usage, research context, and methodology are not detailed in the given context."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'WMT17' which is a known conference and dataset. However, the context does not specify how the dataset is used or for what purpose.",
      "processing_time": 65.2739109992981,
      "citing_paper_id": "238582870",
      "cited_paper_id": 28232901
    },
    {
      "context_text": "For German, we take the data from WMT15 (Bojar et al., 2015), WMT16 (Bojar et al., 2016) and WMT17 (Bojar et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "WMT15",
        "WMT16",
        "WMT17"
      ],
      "dataset_descriptions": {
        "WMT15": "Used for training and evaluating German translation models, focusing on improving translation quality and fluency.",
        "WMT16": "Used for training and evaluating German translation models, focusing on improving translation quality and fluency.",
        "WMT17": "Used for training and evaluating German translation models, focusing on improving translation quality and fluency."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for German translation data from WMT15, WMT16, and WMT17. These are clearly identified and relevant to the topic of personalized text generation.",
      "processing_time": 76.2320077419281,
      "citing_paper_id": "238582870",
      "cited_paper_id": 28232901
    },
    {
      "context_text": "PAWS We complement the analysis with the native English PAWS dataset (Zhang et al., 2019)",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PAWS"
      ],
      "dataset_descriptions": {
        "PAWS": "Used to complement the analysis of paraphrase adversaries, focusing on identifying and evaluating word scrambling techniques in native English text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'PAWS dataset' which is a specific, verifiable dataset used in the research. The cited paper title confirms it is a dataset.",
      "processing_time": 62.997756004333496,
      "citing_paper_id": "238582870",
      "cited_paper_id": 91184042
    },
    {
      "context_text": "PAWS We complement the analysis with the native English PAWS dataset (Zhang et al., 2019) which consists of paraphrase and non-paraphrase pairs that have high lexical overlap.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PAWS"
      ],
      "dataset_descriptions": {
        "PAWS": "Used to analyze paraphrase and non-paraphrase pairs with high lexical overlap, focusing on identifying adversarial examples in native English text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'PAWS dataset' which is a specific, verifiable dataset used for paraphrase identification. The dataset is clearly identified and its usage is described.",
      "processing_time": 63.28375267982483,
      "citing_paper_id": "238582870",
      "cited_paper_id": 91184042
    },
    {
      "context_text": "Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.538264989852905,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions metrics but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.538264989852905,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "• BERTScore (Zhang et al., 2020) aggregates and compares BERT embeddings by determining a greedy alignment between words in two sentences and summing up the cosine similarities of representations of aligned words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BERTScore) for evaluating text generation. No datasets are referenced for training or evaluation.",
      "processing_time": 56.39125037193298,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": ", 2019) or greedy alignment (Zhang et al., 2020), on top of BERT.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. BERT is a model, not a dataset, and there are no other specific resources mentioned.",
      "processing_time": 56.84428572654724,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "More importantly, the modern evaluation metrics sketched above rely on at least two factors: BERT (or its variants) and different aggregation schemes, such as Earth Mover Distance (Kusner et al., 2015; Zhao et al., 2019) or greedy alignment (Zhang et al., 2020), on top of BERT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not introduce new datasets.",
      "processing_time": 55.52812337875366,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "More importantly, the modern evaluation metrics sketched above rely on at least two factors: BERT (or its variants) and different aggregation schemes, such as Earth Mover Distance (Kusner et al., 2015; Zhao et al., 2019) or greedy alignment (Zhang et al., 2020), on top of BERT.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers also do not introduce new datasets.",
      "processing_time": 55.52812337875366,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "For example, BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), BLEURT (Sellam et al., 2020), XMoverScore (Zhao et al., 2020), and COMET (Rei et al., 2020) all use large-scale pretrained language models, but differ in whether they compare hypotheses to references, to source texts, to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several evaluation metrics but does not refer to any specific datasets. The cited papers are about evaluation methods, not datasets.",
      "processing_time": 55.808292388916016,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "For example, BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019), BLEURT (Sellam et al., 2020), XMoverScore (Zhao et al., 2020), and COMET (Rei et al., 2020) all use large-scale pretrained language models, but differ in whether they compare hypotheses to references, to source texts, to…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several evaluation metrics but does not refer to any specific datasets. The cited papers are about evaluation methods, not datasets.",
      "processing_time": 55.808292388916016,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "Popular metrics are BERTScore (Zhang et al., 2020), MoverScore (Zhao et al.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions BERTScore and MoverScore, which are evaluation metrics, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 56.84860897064209,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": "For example, BERTScore (Zhang et al., 2020), MoverScore (Zhao et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (BERTScore, MoverScore). These are excluded as per the instructions.",
      "processing_time": 56.19926047325134,
      "citing_paper_id": "238582870",
      "cited_paper_id": 127986044
    },
    {
      "context_text": ", 2020), MoverScore (Zhao et al., 2019), BLEURT (Sellam et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MoverScore, which is a method for evaluating text generation using contextualized embeddings and Earth Mover Distance. No specific dataset is mentioned.",
      "processing_time": 56.85733199119568,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "• MoverScore (Zhao et al., 2019) computes an optimal alignment between words in the two sentences using word mover distance (Kusner et al., 2015) on top of BERT representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MoverScore, which is a method for evaluating text generation, not a dataset. No datasets are explicitly mentioned or used in the context provided.",
      "processing_time": 56.66092109680176,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": ", 2020), MoverScore (Zhao et al., 2019), and BLEURT (Sellam et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions MoverScore, which is a method for evaluating text generation using contextualized embeddings and Earth Mover Distance. No specific dataset is mentioned.",
      "processing_time": 57.2904212474823,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "More importantly, the modern evaluation metrics sketched above rely on at least two factors: BERT (or its variants) and different aggregation schemes, such as Earth Mover Distance (Kusner et al., 2015; Zhao et al., 2019) or greedy alignment (Zhang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and metrics. The context focuses on evaluation metrics for text generation, mentioning BERT and Earth Mover Distance, but no datasets are explicitly named.",
      "processing_time": 58.326377391815186,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "• MoverScore (Zhao et al., 2019) computes an optimal alignment between words in the two sentences using word mover distance (Kusner et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (MoverScore) and a technique (word mover distance).",
      "processing_time": 55.79257035255432,
      "citing_paper_id": "238582870",
      "cited_paper_id": 202540033
    },
    {
      "context_text": "Both sentences are parsed by the Stanford dependency parser (Chen and Manning, 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a tool (Stanford dependency parser).",
      "processing_time": 55.08646535873413,
      "citing_paper_id": "238582870",
      "cited_paper_id": 211069110
    },
    {
      "context_text": "…work on introspecting and understanding BERT (see Rogers et al. (2020) for an overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always trustworthy.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and findings related to probing BERT. The cited papers also do not introduce any specific datasets.",
      "processing_time": 57.04699230194092,
      "citing_paper_id": "238582870",
      "cited_paper_id": 218486796
    },
    {
      "context_text": "…work on introspecting and understanding BERT (see Rogers et al. (2020) for an overview), e.g., via probing (Tenney et al., 2019), analyzes by Hewitt and Liang (2019); Eger et al. (2020); Ravichander et al. (2021) indicate that probing results (based on supervision) are not always trustworthy.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and findings related to probing BERT. The cited papers also do not introduce any specific datasets.",
      "processing_time": 57.04699230194092,
      "citing_paper_id": "238582870",
      "cited_paper_id": 263793125
    },
    {
      "context_text": "However, probing results are not always trustworthy due to the sensitivity to probing design choices, e.g., data size and classifier choices (Eger et al., 2020), and data artefacts (Ravichander et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'data size' and 'data artefacts' but does not specify any particular dataset. The context is about methodological issues in probing sentence embeddings.",
      "processing_time": 57.74558901786804,
      "citing_paper_id": "238582870",
      "cited_paper_id": 218486796
    },
    {
      "context_text": "However, probing results are not always trustworthy due to the sensitivity to probing design choices, e.g., data size and classifier choices (Eger et al., 2020), and data artefacts (Ravichander et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'data size' and 'data artefacts' but does not specify any particular dataset. The context is about methodological issues in probing sentence embeddings.",
      "processing_time": 57.74558901786804,
      "citing_paper_id": "238582870",
      "cited_paper_id": 263793125
    },
    {
      "context_text": "There are also reference-free metrics outside the field of machine translation; for example, SUPERT (Gao et al., 2020) for summarization.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions SUPERT, which is a metric, not a dataset. Therefore, no datasets are identified.",
      "processing_time": 55.58103108406067,
      "citing_paper_id": "238582870",
      "cited_paper_id": 218571152
    },
    {
      "context_text": "• LaBSE (Feng et al., 2020) is a dual-encoder framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LaBSE, which is a method, not a dataset. No datasets are mentioned in the given citation span.",
      "processing_time": 56.68833565711975,
      "citing_paper_id": "238582870",
      "cited_paper_id": 220347683
    },
    {
      "context_text": "The techniques for explainability differ in whether they provide justification or information for model outputs on individual instances (local explainability) or focus on a model as a whole and disclose its internal structure (global explainability) (Danilevsky et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses different types of explainability techniques.",
      "processing_time": 55.15271830558777,
      "citing_paper_id": "238582870",
      "cited_paper_id": 222125099
    },
    {
      "context_text": "1In work independent from us, Song et al. (2021) also ensemble BERT based reference-free evaluation metrics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of BERT-based metrics. No verifiable resources are identified.",
      "processing_time": 56.428065061569214,
      "citing_paper_id": "238582870",
      "cited_paper_id": 235097390
    },
    {
      "context_text": "Unsupervised metrics require no such supervisory signal (e.g., MoverScore, BERTScore, XMoverScore, SentSim).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions unsupervised metrics but does not refer to any specific datasets. The names mentioned are metrics, which are excluded according to the instructions.",
      "processing_time": 57.219764947891235,
      "citing_paper_id": "238582870",
      "cited_paper_id": 235097390
    },
    {
      "context_text": "Popular examples are XMoverScore (Zhao et al., 2020), Yisi-2 (Lo, 2019), KoBE (Gekhman et al., 2020), and SentSim (Song et al., 2021).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several methods/models but does not refer to any specific datasets. No dataset names are present in the context.",
      "processing_time": 56.05875873565674,
      "citing_paper_id": "238582870",
      "cited_paper_id": 235097390
    },
    {
      "context_text": ", data size and classifier choices (Eger et al., 2020), and data artefacts (Ravichander et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects like data size and classifier choices. No verifiable resources are identified.",
      "processing_time": 56.46630811691284,
      "citing_paper_id": "238582870",
      "cited_paper_id": 263793125
    },
    {
      "context_text": "(b) Then, we finetune/retrofit the embeddings on the morphological lexicons using the method described in Faruqui et al. (2015), so that words with the same morphological tags have more similar representations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'morphological lexicons' but does not specify a named dataset. The reference is to a method for finetuning/retrofitting embeddings, not a specific dataset.",
      "processing_time": 58.58305239677429,
      "citing_paper_id": "238582870",
      "cited_paper_id": null
    },
    {
      "context_text": "…metrics, including distribution-based Log-Likelihood and Perplexity, coherence-based Word Mover Distance (WMD) (Kusner et al., 2015), ngram-based BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), contextual label accuracy and human evaluation to measure the performance of review generation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 56.45083284378052,
      "citing_paper_id": "202767450",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "…metrics, including distribution-based Log-Likelihood and Perplexity, coherence-based Word Mover Distance (WMD) (Kusner et al., 2015), ngram-based BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), contextual label accuracy and human evaluation to measure the performance of review generation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 56.45083284378052,
      "citing_paper_id": "202767450",
      "cited_paper_id": 964287
    },
    {
      "context_text": "On average, we could witness a 5% increase in Word Mover Distance (WMD), 80% improvement in BLEU and 10% rising in ROUGE.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics. The cited papers are about evaluation methods, not datasets.",
      "processing_time": 56.31599402427673,
      "citing_paper_id": "202767450",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "On average, we could witness a 5% increase in Word Mover Distance (WMD), 80% improvement in BLEU and 10% rising in ROUGE.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics. The cited papers are about evaluation methods, not datasets.",
      "processing_time": 56.31599402427673,
      "citing_paper_id": "202767450",
      "cited_paper_id": 964287
    },
    {
      "context_text": "To demonstrate that our purposed model indeed achieves the state-of-the-art review generation performance, we implement various evaluation metrics, including distribution-based Log-Likelihood and Perplexity, coherence-based Word Mover Distance (WMD) (Kusner et al., 2015), ngram-based BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), contextual label accuracy and human evaluation to measure the performance of review generation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics and methods. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 56.74246311187744,
      "citing_paper_id": "202767450",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "To demonstrate that our purposed model indeed achieves the state-of-the-art review generation performance, we implement various evaluation metrics, including distribution-based Log-Likelihood and Perplexity, coherence-based Word Mover Distance (WMD) (Kusner et al., 2015), ngram-based BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), contextual label accuracy and human evaluation to measure the performance of review generation.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only evaluation metrics and methods. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 56.74246311187744,
      "citing_paper_id": "202767450",
      "cited_paper_id": 964287
    },
    {
      "context_text": "To validate the emotion label for each review, we implemented the state-of-the-art sentiment classifier VADER(Gilbert, 2014)3 to label the senti-",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions VADER, which is a sentiment analysis tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 55.738831520080566,
      "citing_paper_id": "202767450",
      "cited_paper_id": 12233345
    },
    {
      "context_text": "To validate the emotion label for each review, we implemented the state-of-the-art sentiment classi-ﬁer VADER(Gilbert, 2014) 3 to label the senti-1 ment score for each review.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using VADER for sentiment scoring, but VADER is a method/model, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 56.7369601726532,
      "citing_paper_id": "202767450",
      "cited_paper_id": 12233345
    },
    {
      "context_text": "To demonstrate that our purposed model indeed achieves the state-of-the-art review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance rather than using a particular dataset.",
      "processing_time": 57.42768955230713,
      "citing_paper_id": "202767450",
      "cited_paper_id": 14789841
    },
    {
      "context_text": "To demonstrate that our purposed model indeed achieves the state-of-the-art review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance rather than using a particular dataset.",
      "processing_time": 57.42768955230713,
      "citing_paper_id": "202767450",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "To demonstrate that our purposed model indeed achieves the state-of-the-art review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance rather than using a particular dataset.",
      "processing_time": 57.42768955230713,
      "citing_paper_id": "202767450",
      "cited_paper_id": 4857922
    },
    {
      "context_text": "…review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance using various evaluation metrics.",
      "processing_time": 57.08692264556885,
      "citing_paper_id": "202767450",
      "cited_paper_id": 14789841
    },
    {
      "context_text": "…review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance using various evaluation metrics.",
      "processing_time": 57.08692264556885,
      "citing_paper_id": "202767450",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "…review generation performance, we compare our model across various evaluation metrics with several important benchmarks, including charRNN (Yao et al., 2017), MLE (Bahl et al., 1990), SeqGAN (Yu et al., 2017), LeakGAN (Guo et al., 2017), RankGAN (Lin et al., 2017) and Attr2Seq (Dong et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several models and methods but does not refer to any specific datasets. The context is focused on comparing model performance using various evaluation metrics.",
      "processing_time": 57.08692264556885,
      "citing_paper_id": "202767450",
      "cited_paper_id": 4857922
    },
    {
      "context_text": "…reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.92939782142639,
      "citing_paper_id": "202767450",
      "cited_paper_id": 14838925
    },
    {
      "context_text": "…reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.92939782142639,
      "citing_paper_id": "202767450",
      "cited_paper_id": 9464307
    },
    {
      "context_text": "Researchers have been utilizing multiple versions of Seq2Seq(Sutskever et al., 2014) framework to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for generating online product reviews. No verifiable resources are identified.",
      "processing_time": 57.07406735420227,
      "citing_paper_id": "202767450",
      "cited_paper_id": 14838925
    },
    {
      "context_text": "Researchers have been utilizing multiple versions of Seq2Seq(Sutskever et al., 2014) framework to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for generating online product reviews. No verifiable resources are identified.",
      "processing_time": 57.07406735420227,
      "citing_paper_id": "202767450",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Researchers have been utilizing multiple versions of Seq2Seq(Sutskever et al., 2014) framework to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for generating online product reviews. No verifiable resources are identified.",
      "processing_time": 57.07406735420227,
      "citing_paper_id": "202767450",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Researchers have been utilizing multiple versions of Seq2Seq(Sutskever et al., 2014) framework to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and Sentiment Units(Radford et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models used for generating online product reviews. No verifiable resources are identified.",
      "processing_time": 57.07406735420227,
      "citing_paper_id": "202767450",
      "cited_paper_id": 9464307
    },
    {
      "context_text": "Typically, following(Koehn, 2004), we use bootstrap re-sampling methods to get the asymptotic standard error of the estimated value of the evaluation metrics.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for evaluating metrics. The cited paper is about statistical significance tests, which does not introduce a dataset.",
      "processing_time": 57.25307106971741,
      "citing_paper_id": "202767450",
      "cited_paper_id": 15119437
    },
    {
      "context_text": "…to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment Score(Zang and Wan, 2017), Generative Concatenative Nets(Lipton et al., 2015) and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about generating online product reviews using various techniques.",
      "processing_time": 56.59587025642395,
      "citing_paper_id": "202767450",
      "cited_paper_id": 16282767
    },
    {
      "context_text": "Besides, we check if reviews generated by RevGAN would have the same linguistic features by testing two major statistical laws of linguistics(Altmann and Gerlach, 2016): Zipf Law(Zipf, 1935) and Heap Law(Herdan, 1964).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions statistical laws of linguistics but does not refer to any specific datasets. The laws mentioned (Zipf Law and Heap Law) are theoretical concepts rather than datasets.",
      "processing_time": 57.97445344924927,
      "citing_paper_id": "202767450",
      "cited_paper_id": 17300076
    },
    {
      "context_text": "Besides, we check if reviews generated by RevGAN would have the same linguistic features by testing two major statistical laws of linguistics(Altmann and Gerlach, 2016): Zipf Law(Zipf, 1935) and Heap Law(Herdan, 1964).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions statistical laws of linguistics but does not refer to any specific datasets. The laws mentioned (Zipf Law and Heap Law) are theoretical concepts rather than datasets.",
      "processing_time": 57.97445344924927,
      "citing_paper_id": "202767450",
      "cited_paper_id": 141091906
    },
    {
      "context_text": "Besides, we check if reviews generated by RevGAN would have the same linguistic features by testing two major statistical laws of linguistics(Altmann and Gerlach, 2016): Zipf Law(Zipf, 1935) and Heap Law(Herdan, 1964).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions statistical laws of linguistics but does not refer to any specific datasets. The laws mentioned (Zipf Law and Heap Law) are theoretical concepts rather than datasets.",
      "processing_time": 57.97445344924927,
      "citing_paper_id": "202767450",
      "cited_paper_id": 158302286
    },
    {
      "context_text": "Also, they are limited in length and lack of coherence for neglecting the hierarchical connections within sentences, which are very important elements towards the helpfulness of a speciﬁc review(Mudambi and Schuff, 2010).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of reviews in terms of length and coherence.",
      "processing_time": 55.457549810409546,
      "citing_paper_id": "202767450",
      "cited_paper_id": 267842214
    },
    {
      "context_text": "The beam size for beam searching(Wiseman and Rush, 2016) would be ﬁxed as 3.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (beam searching).",
      "processing_time": 54.44897961616516,
      "citing_paper_id": "202767450",
      "cited_paper_id": 2783746
    },
    {
      "context_text": "Various methods have been proposed to get over the major problem of the discontinuity of textual information, including Se-qGAN(Yu et al., 2017), TextGAN(Zhang et al., 2017), RankGAN(Lin et al., 2017) and Leak-GAN(Guo et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several generative adversarial network (GAN) methods but does not reference any specific datasets. The focus is on methods rather than datasets.",
      "processing_time": 57.02060580253601,
      "citing_paper_id": "202767450",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "Various methods have been proposed to get over the major problem of the discontinuity of textual information, including Se-qGAN(Yu et al., 2017), TextGAN(Zhang et al., 2017), RankGAN(Lin et al., 2017) and Leak-GAN(Guo et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several generative adversarial network (GAN) methods but does not reference any specific datasets. The focus is on methods rather than datasets.",
      "processing_time": 57.02060580253601,
      "citing_paper_id": "202767450",
      "cited_paper_id": 4857922
    },
    {
      "context_text": "The baseline Seq-GAN, RankGAN and LeakGAN models are implemented through the Texygen(Zhu et al., 2018) 4 toolkit.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Texygen as a toolkit, not a dataset. No datasets are explicitly named or described in the citation.",
      "processing_time": 55.98609781265259,
      "citing_paper_id": "202767450",
      "cited_paper_id": 3636178
    },
    {
      "context_text": "The baseline Seq-GAN, RankGAN and LeakGAN models are implemented through the Texygen(Zhu et al., 2018) 4 toolkit.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Texygen as a toolkit, not a dataset. No datasets are explicitly named or described in the citation.",
      "processing_time": 55.98609781265259,
      "citing_paper_id": "202767450",
      "cited_paper_id": 4857922
    },
    {
      "context_text": "Besides, to capture the relative position representations. we incorporate self-attentive mechanism (Shaw et al., 2018) during encoding process.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (self-attentive mechanism with relative position representations).",
      "processing_time": 55.983182430267334,
      "citing_paper_id": "202767450",
      "cited_paper_id": 3725815
    },
    {
      "context_text": "Researchers have been utilizing multiple versions of Seq2Seq(Sutskever et al., 2014) framework to generate online product reviews of good quality, including Aspect-Aware Representations (Ni and McAuley, 2018), Gated Contexts to Sequences(Tang et al., 2016), RNN(Yao et al., 2017), Aspect-Sentiment…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The focus is on the Seq2Seq framework and its variations for generating product reviews.",
      "processing_time": 57.022356033325195,
      "citing_paper_id": "202767450",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Researchers have proposed various types of product review generation methods (Yao et al., 2017; Dong et al., 2017; Lipton et al., 2015; Rad-ford et al., 2017) and achieved great performance.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. There are no clear identifiers for datasets.",
      "processing_time": 56.24261713027954,
      "citing_paper_id": "202767450",
      "cited_paper_id": 9464307
    },
    {
      "context_text": "GAN(Goodfellow et al., 2014) has become a powerful method for reconstruction and generation in real data space, which leaves great potential to be used for natural language generation purposes.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GAN) and its potential application to natural language generation.",
      "processing_time": 55.61815118789673,
      "citing_paper_id": "202767450",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Gradient clipping(Gulrajani et al., 2017) is adopted by scaling the gradients when the norm exceeds the threshold 1.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gradient clipping) used in training GANs.",
      "processing_time": 55.44108819961548,
      "citing_paper_id": "202767450",
      "cited_paper_id": 10894094
    },
    {
      "context_text": "(3) Implicitly modeling persona profiles from historical dialogue query (Li et al., 2016b; Ma et al., 2021; Zhong et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss methods for personalized dialogue generation.",
      "processing_time": 55.25189018249512,
      "citing_paper_id": "258823272",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "(3) Implicitly modeling persona profiles from historical dialogue query (Li et al., 2016b; Ma et al., 2021; Zhong et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss methods for personalized dialogue generation.",
      "processing_time": 55.25189018249512,
      "citing_paper_id": "258823272",
      "cited_paper_id": 207863734
    },
    {
      "context_text": "(3) Implicitly modeling persona profiles from historical dialogue query (Li et al., 2016b; Ma et al., 2021; Zhong et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss methods for personalized dialogue generation.",
      "processing_time": 55.25189018249512,
      "citing_paper_id": "258823272",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "(1) Diversity Distinct-1/2 (Li et al., 2016a) considers the number of single or double frames in the generated responses and is usually used to evaluate diversity.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (Diversity Distinct-1/2) which is not a dataset.",
      "processing_time": 56.218257427215576,
      "citing_paper_id": "258823272",
      "cited_paper_id": 2955580
    },
    {
      "context_text": ", 2021); (3) Historical queries of current dialogue (Li et al., 2016b; Ma et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references historical queries and dialogue, which are not specific datasets.",
      "processing_time": 56.20570087432861,
      "citing_paper_id": "258823272",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "…sparse persona attributes (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Song et al., 2021); (2) Dense persona description texts (Qian et al., 2018; Zheng et al., 2020; Song et al., 2021); (3) Historical queries of current dialogue (Li et al., 2016b; Ma et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references methods and approaches for incorporating persona information in conversation models.",
      "processing_time": 56.376526832580566,
      "citing_paper_id": "258823272",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue has been studied in depth for a long time (Koehn et al., 2003; Ni et al., 2021),",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to research in open-domain dialogue. No verifiable resources are identified.",
      "processing_time": 56.4964816570282,
      "citing_paper_id": "258823272",
      "cited_paper_id": 8884845
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue has been studied in depth for a long time (Koehn et al., 2003; Ni et al., 2021),",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to research in open-domain dialogue. No verifiable resources are identified.",
      "processing_time": 56.4964816570282,
      "citing_paper_id": "258823272",
      "cited_paper_id": 234342691
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue has been studied in depth for a long time (Koehn et al., 2003; Ni et al., 2021), and under the influence of the psychological theory, personality has been incorporated into the requirements for dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to research in open-domain dialogue and personality incorporation. No verifiable resources are identified.",
      "processing_time": 56.659523248672485,
      "citing_paper_id": "258823272",
      "cited_paper_id": 8884845
    },
    {
      "context_text": "Personalized Dialogue Generation Opendomain dialogue has been studied in depth for a long time (Koehn et al., 2003; Ni et al., 2021), and under the influence of the psychological theory, personality has been incorporated into the requirements for dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to research in open-domain dialogue and personality incorporation. No verifiable resources are identified.",
      "processing_time": 56.659523248672485,
      "citing_paper_id": "258823272",
      "cited_paper_id": 234342691
    },
    {
      "context_text": "(16)\nwhere the NLI model is a triple classification model and can be found in Appendix A.\n(3) Coherence BLEU-1 (Papineni et al., 2002) and ROUGE-L (Lin and Och, 2004) are classical words overlap-based metrics for measuring the similarity between generated responses and factual responses, which we believe can indirectly measure the coherence of dialogues.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and a model. Metrics and models are excluded according to the instructions.",
      "processing_time": 55.887633323669434,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "… 2,if P is consistent with R and Q is coherent with R , 1,if P is consistent with R but Q is not coherent with R\n0,otherwise,\n(18)\nHere NLI (Welleck et al., 2019) is a pre-trained RoBERTa model (Liu et al., 2019), fine-tuned using a dataset constructed based on ConvAI2 and Baidu…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConvAI2"
      ],
      "dataset_descriptions": {
        "ConvAI2": "Used to construct a dataset for fine-tuning a pre-trained RoBERTa model, focusing on dialogue natural language inference tasks."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'ConvAI2' and 'Baidu', which are likely datasets used for constructing a dataset for fine-tuning a pre-trained RoBERTa model. However, 'Baidu' is not a specific dataset name but a company. 'ConvAI2' is a known dataset in the dialogue systems domain.",
      "processing_time": 70.16396999359131,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "A.2 NLI Model NLI model is a triple classification model and can be design as:\nNLI(P,Q,R)\n=  2,if P is consistent with R and Q is coherent with R , 1,if P is consistent with R but Q is not coherent with R\n0,otherwise,\n(18)\nHere NLI (Welleck et al., 2019) is a pre-trained RoBERTa model (Liu et al., 2019), fine-tuned using a dataset constructed based on ConvAI2 and Baidu PersonaChat, and the test set accuracy of NLI model on Chinese and English is 83.2% and 83.1%, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConvAI2",
        "Baidu PersonaChat"
      ],
      "dataset_descriptions": {
        "ConvAI2": "Used to construct a dataset for fine-tuning a pre-trained RoBERTa model for dialogue natural language inference, focusing on consistency and coherence in dialogues.",
        "Baidu PersonaChat": "Used to construct a dataset for fine-tuning a pre-trained RoBERTa model for dialogue natural language inference, focusing on consistency and coherence in dialogues."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the use of a dataset constructed from ConvAI2 and Baidu PersonaChat for fine-tuning a pre-trained RoBERTa model. These datasets are relevant to personalized text generation.",
      "processing_time": 75.0433406829834,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "Score(P,Q,R) = { 0, if NLI(P,Q,R) = 0, 1, if NLI(P,Q,R) = 2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a formula and a reference to NLI (Natural Language Inference). No verifiable resources are identified.",
      "processing_time": 57.34480261802673,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "Score(P,Q,R) = { 1, if NLI(P,Q,R) = 1 or 2, 0, if NLI(P,Q,R) = 0.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a function and a condition related to NLI (Natural Language Inference). No verifiable resources are identified.",
      "processing_time": 57.60829949378967,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "Score (Coherence-Consistency Score), which is also obtained based on the NLI model:\nCoh-Con.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a metric (Score) and a model (NLI model). No verifiable resources are identified.",
      "processing_time": 56.85869002342224,
      "citing_paper_id": "258823272",
      "cited_paper_id": 53298765
    },
    {
      "context_text": "ConvAI2 (Dinan et al., 2019) is an English dataset containing rich personal information, and the dialogues in this dataset are based on the personal facts corresponding to the characters.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ConvAI2"
      ],
      "dataset_descriptions": {
        "ConvAI2": "Used to generate personalized dialogues based on rich personal information, focusing on character-specific facts to enhance conversational intelligence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions ConvAI2 as an English dataset containing rich personal information, which is directly relevant to personalized text generation.",
      "processing_time": 62.87885808944702,
      "citing_paper_id": "258823272",
      "cited_paper_id": 59553505
    },
    {
      "context_text": "…persona attributes (e.g., gender, age), the model can utilize different attributes efficiently and interpretably, and knowledge-enhanced dialogue generation approaches can be borrowed (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Bao et al., 2020; Song et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.05812454223633,
      "citing_paper_id": "258823272",
      "cited_paper_id": 204744108
    },
    {
      "context_text": "…persona attributes (e.g., gender, age), the model can utilize different attributes efficiently and interpretably, and knowledge-enhanced dialogue generation approaches can be borrowed (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Bao et al., 2020; Song et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.05812454223633,
      "citing_paper_id": "258823272",
      "cited_paper_id": 215745354
    },
    {
      "context_text": ", gender, age), the model can utilize different attributes efficiently and interpretably, and knowledge-enhanced dialogue generation approaches can be borrowed (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Bao et al., 2020; Song et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. The context focuses on the use of attributes in dialogue generation models.",
      "processing_time": 57.05444002151489,
      "citing_paper_id": "258823272",
      "cited_paper_id": 204744108
    },
    {
      "context_text": "MSP (Zhong et al., 2022) enhances personalized dialogue generation by retrieving similar conversations from similar users via User Refiner and Topic Refiner and uses a Token Refiner to find the relevant tokens to be used during training, which is the best overall performance model for persona-free information personalized dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'similar conversations' and 'similar users' but does not specify a named dataset. The focus is on the method (MSP) and its components rather than a specific dataset.",
      "processing_time": 58.707117319107056,
      "citing_paper_id": "258823272",
      "cited_paper_id": 207863734
    },
    {
      "context_text": "MSP (Zhong et al., 2022) enhances personalized dialogue generation by retrieving similar conversations from similar users via User Refiner and Topic Refiner and uses a Token Refiner to find the relevant tokens to be used during training, which is the best overall performance model for persona-free information personalized dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'similar conversations' and 'similar users' but does not specify a named dataset. The focus is on the method (MSP) and its components rather than a specific dataset.",
      "processing_time": 58.707117319107056,
      "citing_paper_id": "258823272",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "BoB and MSP can also generate good answers due to the help of context in reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (BoB and MSP). The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.16958999633789,
      "citing_paper_id": "258823272",
      "cited_paper_id": 207863734
    },
    {
      "context_text": "BoB and MSP can also generate good answers due to the help of context in reasoning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (BoB and MSP). The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.16958999633789,
      "citing_paper_id": "258823272",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "MSP (Zhong et al., 2022) enhances personalized dialogue generation by retrieving similar conversations from similar users via User Refiner and Topic Refiner and uses a Token Refiner to find the relevant tokens to be used during training, which is the best overall performance model for persona-free…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'similar conversations' and 'similar users' but does not specify a named dataset. The focus is on the method (MSP) and its components rather than a specific dataset.",
      "processing_time": 58.629233837127686,
      "citing_paper_id": "258823272",
      "cited_paper_id": 207863734
    },
    {
      "context_text": "MSP (Zhong et al., 2022) enhances personalized dialogue generation by retrieving similar conversations from similar users via User Refiner and Topic Refiner and uses a Token Refiner to find the relevant tokens to be used during training, which is the best overall performance model for persona-free…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'similar conversations' and 'similar users' but does not specify a named dataset. The focus is on the method (MSP) and its components rather than a specific dataset.",
      "processing_time": 58.629233837127686,
      "citing_paper_id": "258823272",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "…mainly utilizing three kinds of resources: (1) Defined sparse persona attributes (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Song et al., 2021); (2) Dense persona description texts (Qian et al., 2018; Zheng et al., 2020; Song et al., 2021); (3) Historical…",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions types of resources used in personalized text generation but does not specify any named datasets. The resources are described generically, which does not meet the criteria for inclusion.",
      "processing_time": 58.18534803390503,
      "citing_paper_id": "258823272",
      "cited_paper_id": 215745354
    },
    {
      "context_text": "In order to develop personalized dialogue agents, current approaches enhance the personality of generated responses mainly utilizing three kinds of resources: (1) Defined sparse persona attributes (Zhang et al., 2018a; Song et al., 2019; Wolf et al., 2019; Liu et al., 2020; Song et al., 2021); (2) Dense persona description texts (Qian et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'Defined sparse persona attributes' as a resource but does not specify a dataset name. No other specific datasets are mentioned.",
      "processing_time": 56.80322790145874,
      "citing_paper_id": "258823272",
      "cited_paper_id": 215745354
    },
    {
      "context_text": "Therefore, referring to the research of Sun et al. (2021), we propose a self-separation method of persona information, which implicitly divides dense text persona information into N categories:\nPg = P-Sepa(p), (4)\nwhere Pg = {p1, p2, · · · , pN}, and Pg represents the persona information after…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for separating persona information. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.01896595954895,
      "citing_paper_id": "258823272",
      "cited_paper_id": 235358901
    },
    {
      "context_text": "MSP (Zhong et al., 2022) enhances personalized dialogue generation by retrieving similar conversations from similar users via User Refiner and Topic Refiner and uses a Token Refiner",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MSP' which is a method for enhancing personalized dialogue generation. No specific dataset is mentioned by name.",
      "processing_time": 56.20846700668335,
      "citing_paper_id": "258823272",
      "cited_paper_id": 247693734
    },
    {
      "context_text": "technique (Kingma and Welling, 2013), we have: [ μp σ2 p ] =W recog q [ q",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. It appears to be a fragment of a technical explanation or equation.",
      "processing_time": 56.557109117507935,
      "citing_paper_id": "258823272",
      "cited_paper_id": null
    },
    {
      "context_text": "The model is adapted from Merity et al. (2018b) with an added user embedding layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model adaptation. No verifiable resources are identified.",
      "processing_time": 55.564841508865356,
      "citing_paper_id": "248779887",
      "cited_paper_id": 212756
    },
    {
      "context_text": "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to LSTM architectures and their performance. No verifiable resources are identified.",
      "processing_time": 56.05872964859009,
      "citing_paper_id": "248779887",
      "cited_paper_id": 212756
    },
    {
      "context_text": "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to LSTM architectures and their performance. No verifiable resources are identified.",
      "processing_time": 56.05872964859009,
      "citing_paper_id": "248779887",
      "cited_paper_id": 208291331
    },
    {
      "context_text": "Variations of the LSTM have consistently achieved state-of-the-art performance without massive compute resources and thus we chose this architecture for our experiments (Merity et al., 2018a; Melis et al., 2019; Merity, 2019; Li et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to LSTM architectures and their performance. No verifiable resources are identified.",
      "processing_time": 56.05872964859009,
      "citing_paper_id": "248779887",
      "cited_paper_id": null
    },
    {
      "context_text": "Embeddings are initialized with 200d GloVe vectors pretrained on 6 billion tokens from randomly sampled Reddit posts (Pennington et al., 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GloVe vectors but does not refer to them as a dataset. It is a method for initializing embeddings, not a dataset.",
      "processing_time": 56.53990578651428,
      "citing_paper_id": "248779887",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Our optimizer starts with SGD and will switch to ASGD if there is no improvement in validation loss in the past 5 epochs (Polyak and Juditsky, 1992).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 55.712162494659424,
      "citing_paper_id": "248779887",
      "cited_paper_id": 3548228
    },
    {
      "context_text": "Similarly, “wicked” may mean “evil” or may function as an intensifier depending on where you live (Bamman et al., 2014).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a linguistic phenomenon discussed in the cited paper, but does not indicate the use of a particular dataset.",
      "processing_time": 57.869669914245605,
      "citing_paper_id": "248779887",
      "cited_paper_id": 4975368
    },
    {
      "context_text": "We removed continuous cache pointers (Grave et al., 2016) to speed up training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (continuous cache pointers) rather than a dataset. No specific dataset is referenced.",
      "processing_time": 55.31338548660278,
      "citing_paper_id": "248779887",
      "cited_paper_id": 8693672
    },
    {
      "context_text": "Another direction of research has shown impressive results using extremely large models (Radford et al., 2019; Devlin et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only large models. No verifiable resources are identified.",
      "processing_time": 55.17298245429993,
      "citing_paper_id": "248779887",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We recommend against such applications, as they threaten intellectual freedom and risk discrimination (Richards, 2013).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is a reference to a paper discussing the dangers of surveillance, which is not directly related to personalized text generation.",
      "processing_time": 58.16723299026489,
      "citing_paper_id": "248779887",
      "cited_paper_id": 151839409
    },
    {
      "context_text": "No ablation is performed, and though it would be interesting to compare to a trans-former method that conditions on authors alone, we opted for a model that is faster and cheaper to train (Grover-Mega from Zellers et al. (2019) was trained for two weeks and cost around 25k USD).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Grover-Mega) and its training cost. No verifiable datasets are referenced.",
      "processing_time": 57.283483028411865,
      "citing_paper_id": "248779887",
      "cited_paper_id": 168169824
    },
    {
      "context_text": "Zellers et al. (2019) developed a model for news generation that conditioned on meta-data including domain, date, authors, and headline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model for news generation. The context does not provide enough information to identify a verifiable dataset.",
      "processing_time": 57.273972511291504,
      "citing_paper_id": "248779887",
      "cited_paper_id": 168169824
    },
    {
      "context_text": "LMs are increasingly used as the backbone of models for a range of tasks in NLP, increasing the potential impact of personalization even further (Brown et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of language models for various NLP tasks. No verifiable resources are identified.",
      "processing_time": 56.80317687988281,
      "citing_paper_id": "248779887",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "…that there are several benefits to personalized models in natural language processing (NLP) over one-size-fits-all solutions: they are more accurate for individual users; they help us understand communities better; and they focus the attention of our evaluations on the end-user (Flek, 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general benefits of personalized models in NLP.",
      "processing_time": 54.89369034767151,
      "citing_paper_id": "248779887",
      "cited_paper_id": 220048583
    },
    {
      "context_text": "For example, Hofmann et al. (2021) find that in some contexts “testing” refers to seeing if a device works and “sanitation” refers to a pest control issue, while in another context both refer to conditions of the COVID-19 pandemic.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only findings about word meanings in different contexts.",
      "processing_time": 54.88964772224426,
      "citing_paper_id": "248779887",
      "cited_paper_id": 225067080
    },
    {
      "context_text": "Generation tasks in particular benefit from a personalized approach, for example, Dudy et al. (2021) argue that user intention is more often difficult to recover from the context alone.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general argument about personalization in NLG.",
      "processing_time": 55.34582543373108,
      "citing_paper_id": "248779887",
      "cited_paper_id": 237490430
    },
    {
      "context_text": "…dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neural architectures and their training methods. No verifiable resources are identified.",
      "processing_time": 56.25924062728882,
      "citing_paper_id": "52167799",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "…dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neural architectures and their training methods. No verifiable resources are identified.",
      "processing_time": 56.25924062728882,
      "citing_paper_id": "52167799",
      "cited_paper_id": 6126582
    },
    {
      "context_text": "…dialogue systems, based on neural architectures like bidirectional LSTMs or Memory Networks (Sukhbaatar et al., 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only neural architectures and their training methods. No verifiable resources are identified.",
      "processing_time": 56.25924062728882,
      "citing_paper_id": "52167799",
      "cited_paper_id": 10565222
    },
    {
      "context_text": ", 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 55.919161796569824,
      "citing_paper_id": "52167799",
      "cited_paper_id": 2129889
    },
    {
      "context_text": ", 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 55.919161796569824,
      "citing_paper_id": "52167799",
      "cited_paper_id": 6126582
    },
    {
      "context_text": ", 2015) trained directly by gradient descent on dialogue logs, have been showing promising performance in multiple contexts (Wen et al., 2016; Serban et al., 2016; Bordes et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 55.919161796569824,
      "citing_paper_id": "52167799",
      "cited_paper_id": 10565222
    },
    {
      "context_text": "As in (Dodge et al., 2015), we use a preexisting dump of REDDIT that consists of 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "REDDIT"
      ],
      "dataset_descriptions": {
        "REDDIT": "Used to train and evaluate dialog systems, focusing on end-to-end learning from user interactions. The dataset provides a large corpus of conversational data for developing and testing dialog models."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions using a 'preexisting dump of REDDIT', which is a specific, identifiable dataset. However, the sentence is incomplete, making it difficult to determine the full scope of its use.",
      "processing_time": 67.41446137428284,
      "citing_paper_id": "52167799",
      "cited_paper_id": 2239496
    },
    {
      "context_text": "Dodge et al. (2015) used it to assess chit-chat qualities of generic dialogue models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a general reference to assessing dialogue models.",
      "processing_time": 54.957555532455444,
      "citing_paper_id": "52167799",
      "cited_paper_id": 2239496
    },
    {
      "context_text": "As in (Dodge et al., 2015), we use a preexisting dump of R EDDIT that consists of 1.7 billion comments.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "R EDDIT"
      ],
      "dataset_descriptions": {
        "R EDDIT": "Used to train and evaluate dialog systems, focusing on the quality of generated responses using a large-scale comment dataset."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a 'preexisting dump of R EDDIT' which is a specific, identifiable dataset. It is used for training and evaluation in the context of dialog systems.",
      "processing_time": 65.0510036945343,
      "citing_paper_id": "52167799",
      "cited_paper_id": 2239496
    },
    {
      "context_text": "Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in dialogues (Yang et al., 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (next utterance retrieval tasks in dialogues).",
      "processing_time": 55.62680125236511,
      "citing_paper_id": "52167799",
      "cited_paper_id": 5033484
    },
    {
      "context_text": "Based solely on attention mechanisms, it exhibited state-of-the-art performance on next utterance retrieval tasks in di-alogues (Yang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task (next utterance retrieval) and a method (attention mechanisms).",
      "processing_time": 56.37453556060791,
      "citing_paper_id": "52167799",
      "cited_paper_id": 5033484
    },
    {
      "context_text": "Yang et al. (2018) used response prediction on R EDDIT as an auxiliary task in order to improve prediction performance on natural language inference problems.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'R EDDIT' as a data source for an auxiliary task in improving natural language inference. However, it does not specify a particular dataset or corpus from Reddit.",
      "processing_time": 58.62132787704468,
      "citing_paper_id": "52167799",
      "cited_paper_id": 5033484
    },
    {
      "context_text": "Wang and Blei [35] combine probabilistic matrix factorization with topic modeling for article recommendation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method combining probabilistic matrix factorization with topic modeling.",
      "processing_time": 55.46751308441162,
      "citing_paper_id": "47019137",
      "cited_paper_id": 96163
    },
    {
      "context_text": "NMF: Nonnegative Matrix Factorization [8], which is a widely applied latent factor model for recommendation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method (Nonnegative Matrix Factorization). The citation is used to reference the method, not a dataset.",
      "processing_time": 57.27178764343262,
      "citing_paper_id": "47019137",
      "cited_paper_id": 165018
    },
    {
      "context_text": "Various algorithms such as collaborative filtering [4, 18, 28, 29, 37] and factorization based methods [18, 22, 27] have been proposed to optimize those metrics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 55.4994592666626,
      "citing_paper_id": "47019137",
      "cited_paper_id": 467086
    },
    {
      "context_text": "Various algorithms such as collaborative filtering [4, 18, 28, 29, 37] and factorization based methods [18, 22, 27] have been proposed to optimize those metrics.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 55.4994592666626,
      "citing_paper_id": "47019137",
      "cited_paper_id": 2928997
    },
    {
      "context_text": "from each tensor and pairwise constraint set per iteration to calculate a averaged gradient, i.e., a mini-batch SGD. And to avoid manually specifying a step size, we employ adaptive gradient descent [9], which dynamically incorporates the updating trace in SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA Nan Wang, Hongning Wang, Yiling Jia, Yue Yin earlier iterations to perform more informative and fa",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for adaptive gradient descent. The citation is used to reference a method, not a dataset.",
      "processing_time": 56.91888236999512,
      "citing_paper_id": "47019137",
      "cited_paper_id": 538820
    },
    {
      "context_text": "We develop a joint tensor factorization solution to integrate two complementary tasks of user preference modeling for recommendation and opinionated content modeling for explanation , i.e., a multi-task learning approach [3, 10, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach involving multi-task learning. The cited papers also do not provide specific dataset names.",
      "processing_time": 57.163936138153076,
      "citing_paper_id": "47019137",
      "cited_paper_id": 719551
    },
    {
      "context_text": "We develop a joint tensor factorization solution to integrate two complementary tasks of user preference modeling for recommendation and opinionated content modeling for explanation , i.e., a multi-task learning approach [3, 10, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach involving multi-task learning. The cited papers also do not provide specific dataset names.",
      "processing_time": 57.163936138153076,
      "citing_paper_id": "47019137",
      "cited_paper_id": 6617228
    },
    {
      "context_text": "We develop a joint tensor factorization solution to integrate two complementary tasks of user preference modeling for recommendation and opinionated content modeling for explanation , i.e., a multi-task learning approach [3, 10, 20].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a methodological approach involving multi-task learning. The cited papers also do not provide specific dataset names.",
      "processing_time": 57.163936138153076,
      "citing_paper_id": "47019137",
      "cited_paper_id": 51985230
    },
    {
      "context_text": "l. evaluated 21 types of explanation interfaces for a collaborative filtering based system [13] and found a histogram showing the ratings from similar users was the most persuasive. Bilgic and Mooney [5] studied three explanation generation strategies, i.e., keyword-style, neighbor-style, and influence-style, in a content-based book recommendation system. Sharma and Cosley [30] conducted users studie",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions studies on explanation interfaces and strategies for recommendation systems, but does not specify any named datasets. The focus is on the methodologies and findings of the cited works.",
      "processing_time": 57.849523305892944,
      "citing_paper_id": "47019137",
      "cited_paper_id": 3228904
    },
    {
      "context_text": "nations is not to convince users to accept the customized results (i.e., promotion), but to allow them to make more informed and accurate decisions about which results to utilize (i.e., satisfaction) [5]. Existing recommendation algorithms emphasize end-to-end optimization of performance metrics, such as Root-Mean-Square Error and Normalized Discounted Cumulative Gain, which are defined on numerical",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the distinction between satisfaction and promotion in recommendation systems.",
      "processing_time": 56.536211013793945,
      "citing_paper_id": "47019137",
      "cited_paper_id": 3228904
    },
    {
      "context_text": "A follow-up work [7] introduces aspect-level topic modeling to capture users’ finer-grained sentiment on different aspects of an item, so that aspect-level explanations become possible.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for aspect-level topic modeling. No verifiable resources are identified.",
      "processing_time": 56.11234188079834,
      "citing_paper_id": "47019137",
      "cited_paper_id": 14193088
    },
    {
      "context_text": "JMARS: A probabilistic model that jointly models aspects, ratings, and sentiments by collaborative filtering and topic modeling [7].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions JMARS, which is a model, not a dataset. There are no specific, verifiable datasets mentioned in the citation context.",
      "processing_time": 56.718379974365234,
      "citing_paper_id": "47019137",
      "cited_paper_id": 14193088
    },
    {
      "context_text": "It is known that in natural language the distribution of words is highly skewed, e.g., Zipf’s law [19]; we hypothesize that the distribution of opinion phrases that an item often receives for describing its features, and that a user often uses to describe a type of items’ features are also highly…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses a general observation about word distribution in natural language, which is not a dataset.",
      "processing_time": 57.11555218696594,
      "citing_paper_id": "47019137",
      "cited_paper_id": 14952761
    },
    {
      "context_text": "The lack of transparency [31] leaves users in a dilemma: a user can only assess the recommendation quality by taking the suggested actions, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the issue of transparency in recommender systems.",
      "processing_time": 56.13024687767029,
      "citing_paper_id": "47019137",
      "cited_paper_id": 17114407
    },
    {
      "context_text": "One typical solution is to combine rating prediction with topic models [23, 36, 39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach combining rating prediction with topic models.",
      "processing_time": 55.19588375091553,
      "citing_paper_id": "47019137",
      "cited_paper_id": 31931583
    },
    {
      "context_text": "Similar decomposition structure design can be found in [20, 38].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers. There is no indication of dataset usage or specific research context.",
      "processing_time": 56.00103569030762,
      "citing_paper_id": "47019137",
      "cited_paper_id": 51985230
    },
    {
      "context_text": "Extensive research effort has been devoted to improving the effectiveness of recommendation algorithms [13, 17, 22, 29]; but one fundamental question of “how a system should explain those recommendations to its users” has not received enough attention [38].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about recommendation algorithms and their explanations.",
      "processing_time": 55.017138957977295,
      "citing_paper_id": "47019137",
      "cited_paper_id": 207731647
    },
    {
      "context_text": "In one of the early studies of explanations in recommender systems, Herlocker et al. evaluated 21 types of explanation interfaces for a collaborative filtering based system [13] and found a histogram showing the ratings from similar users was the most persuasive.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a study by Herlocker et al. but does not specify a dataset. It describes an evaluation of explanation interfaces for a collaborative filtering system, which is a method or finding rather than a dataset.",
      "processing_time": 58.99798941612244,
      "citing_paper_id": "47019137",
      "cited_paper_id": 207731647
    },
    {
      "context_text": "3.3.3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.245434045791626,
      "citing_paper_id": "260926600",
      "cited_paper_id": 1955875
    },
    {
      "context_text": "3.3.3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not contain any specific dataset names or verifiable resources. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.245434045791626,
      "citing_paper_id": "260926600",
      "cited_paper_id": 220871030
    },
    {
      "context_text": "5* as T2I diffusion model, which is trained on LAION-5B, the VQA model BLIP [60] pretrained on 129M images from multiple datasets [14, 56, 69, 75, 78, 93] and fine-tuned on VQA2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B",
        "VQA2"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train the T2I diffusion model, providing a large-scale image-text dataset for generating high-quality images.",
        "VQA2": "Used to fine-tune the VQA model BLIP, focusing on improving visual question answering performance with a diverse set of image-question pairs."
      },
      "confidence_score": 1.0,
      "reasoning": "LAION-5B is a large-scale image-text dataset, and VQA2 is a specific version of the Visual Question Answering dataset. Both are used for training and fine-tuning models.",
      "processing_time": 73.07691264152527,
      "citing_paper_id": "260926600",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "For ECON, we use ECON EX , due to its superior performance on both “OOD poses” and “OOD outfits” cases, as reported in the original paper [121].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ECON EX' which seems to be a specific dataset or model, but given the cited paper titles, it is more likely a method or model rather than a dataset. No other specific datasets are mentioned.",
      "processing_time": 59.96282148361206,
      "citing_paper_id": "260926600",
      "cited_paper_id": 4637184
    },
    {
      "context_text": "For ECON, we use ECON EX , due to its superior performance on both “OOD poses” and “OOD outfits” cases, as reported in the original paper [121].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ECON EX' which seems to be a specific dataset or model, but given the cited paper titles, it is more likely a method or model rather than a dataset. No other specific datasets are mentioned.",
      "processing_time": 59.96282148361206,
      "citing_paper_id": "260926600",
      "cited_paper_id": 54446451
    },
    {
      "context_text": "Here, “high-fidelity” refers to the inclusion of consistent texture with intricate patterns, as well as detailed full-body geometry.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only describes aspects of high-fidelity human parsing. No verifiable resources are identified.",
      "processing_time": 55.88899326324463,
      "citing_paper_id": "260926600",
      "cited_paper_id": 6361694
    },
    {
      "context_text": "Here, “high-fidelity” refers to the inclusion of consistent texture with intricate patterns, as well as detailed full-body geometry.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only describes aspects of high-fidelity human parsing. No verifiable resources are identified.",
      "processing_time": 55.88899326324463,
      "citing_paper_id": "260926600",
      "cited_paper_id": 11618478
    },
    {
      "context_text": "0 [35], Seg-Former * [117] pretrained from [10, 20, 23, 137] and fine-tuned on ATR[69], PIXIE [26] trained on human images from multiple datasets [19, 74, 90, 115, 140], and the normal predictor of ICON [120] trained on AGORA [91].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ATR",
        "AGORA"
      ],
      "dataset_descriptions": {
        "ATR": "Used for fine-tuning Seg-Former, focusing on human segmentation tasks in the context of personalized text generation.",
        "AGORA": "Used for training the normal predictor of ICON, focusing on 3D human body shape and pose estimation."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several datasets and models, but only 'ATR' and 'AGORA' are specific datasets. Others are either models or not clearly identified as datasets.",
      "processing_time": 69.46088576316833,
      "citing_paper_id": "260926600",
      "cited_paper_id": 8081284
    },
    {
      "context_text": "0 [35], Seg-Former * [117] pretrained from [10, 20, 23, 137] and fine-tuned on ATR[69], PIXIE [26] trained on human images from multiple datasets [19, 74, 90, 115, 140], and the normal predictor of ICON [120] trained on AGORA [91].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ATR",
        "AGORA"
      ],
      "dataset_descriptions": {
        "ATR": "Used for fine-tuning Seg-Former, focusing on human segmentation tasks in the context of personalized text generation.",
        "AGORA": "Used for training the normal predictor of ICON, focusing on 3D human body shape and pose estimation."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions several datasets and models, but only 'ATR' and 'AGORA' are specific datasets. Others are either models or not clearly identified as datasets.",
      "processing_time": 69.46088576316833,
      "citing_paper_id": "260926600",
      "cited_paper_id": 234470190
    },
    {
      "context_text": "5 * as T2I diffusion model, which is trained on LAION-5B, the VQA model BLIP [65] pre-trained on 129M images from multiple datasets [15, 61, 74, 84, 87, 105] and fine-tuned on VQA2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B",
        "VQA2"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train a T2I diffusion model, focusing on generating high-quality images from text prompts using a large-scale image-text dataset.",
        "VQA2": "Used to fine-tune the BLIP model for visual question answering, enhancing the model's ability to understand and generate answers to questions about images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions LAION-5B and VQA2 as specific datasets used for training and fine-tuning models. LAION-5B is a large-scale image-text dataset, and VQA2 is a dataset for visual question answering.",
      "processing_time": 75.28565502166748,
      "citing_paper_id": "260926600",
      "cited_paper_id": 14579301
    },
    {
      "context_text": "5 * as T2I diffusion model, which is trained on LAION-5B, the VQA model BLIP [65] pre-trained on 129M images from multiple datasets [15, 61, 74, 84, 87, 105] and fine-tuned on VQA2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B",
        "VQA2"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train a T2I diffusion model, focusing on generating high-quality images from text prompts using a large-scale image-text dataset.",
        "VQA2": "Used to fine-tune the BLIP model for visual question answering, enhancing the model's ability to understand and generate answers to questions about images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions LAION-5B and VQA2 as specific datasets used for training and fine-tuning models. LAION-5B is a large-scale image-text dataset, and VQA2 is a dataset for visual question answering.",
      "processing_time": 75.28565502166748,
      "citing_paper_id": "260926600",
      "cited_paper_id": 227305870
    },
    {
      "context_text": "5 * as T2I diffusion model, which is trained on LAION-5B, the VQA model BLIP [65] pre-trained on 129M images from multiple datasets [15, 61, 74, 84, 87, 105] and fine-tuned on VQA2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B",
        "VQA2"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train a T2I diffusion model, focusing on generating high-quality images from text prompts using a large-scale image-text dataset.",
        "VQA2": "Used to fine-tune the BLIP model for visual question answering, enhancing the model's ability to understand and generate answers to questions about images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions LAION-5B and VQA2 as specific datasets used for training and fine-tuning models. LAION-5B is a large-scale image-text dataset, and VQA2 is a dataset for visual question answering.",
      "processing_time": 75.28565502166748,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "A line of works regresses the free-form implicit surface in an end-to-end manner [5, 103, 104], leverages a 3D geometric prior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively builds up the 3D human using a “sandwich-like” structure and implicit shape completion [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for 3D human modeling and reconstruction.",
      "processing_time": 55.3372745513916,
      "citing_paper_id": "260926600",
      "cited_paper_id": 51929974
    },
    {
      "context_text": "A line of works regresses the free-form implicit surface in an end-to-end manner [5, 103, 104], leverages a 3D geometric prior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively builds up the 3D human using a “sandwich-like” structure and implicit shape completion [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for 3D human modeling and reconstruction.",
      "processing_time": 55.3372745513916,
      "citing_paper_id": "260926600",
      "cited_paper_id": 129945462
    },
    {
      "context_text": "A line of works regresses the free-form implicit surface in an end-to-end manner [5, 103, 104], leverages a 3D geometric prior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively builds up the 3D human using a “sandwich-like” structure and implicit shape completion [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for 3D human modeling and reconstruction.",
      "processing_time": 55.3372745513916,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "A line of works regresses the free-form implicit surface in an end-to-end manner [5, 103, 104], leverages a 3D geometric prior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively builds up the 3D human using a “sandwich-like” structure and implicit shape completion [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for 3D human modeling and reconstruction.",
      "processing_time": 55.3372745513916,
      "citing_paper_id": "260926600",
      "cited_paper_id": 219964093
    },
    {
      "context_text": "A line of works regresses the free-form implicit surface in an end-to-end manner [5, 103, 104], leverages a 3D geometric prior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively builds up the 3D human using a “sandwich-like” structure and implicit shape completion [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for 3D human modeling and reconstruction.",
      "processing_time": 55.3372745513916,
      "citing_paper_id": "260926600",
      "cited_paper_id": 248366560
    },
    {
      "context_text": "However, the scarcity of datasets containing real 3D clothed humans [11, 18, 47, 126, 135] limits the model’s generalization ability and may lead to overfitting on small datasets.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the scarcity of datasets containing real 3D clothed humans, indicating a limitation in the field. No specific dataset names are provided.",
      "processing_time": 56.84167814254761,
      "citing_paper_id": "260926600",
      "cited_paper_id": 80628414
    },
    {
      "context_text": "However, the scarcity of datasets containing real 3D clothed humans [11, 18, 47, 126, 135] limits the model’s generalization ability and may lead to overfitting on small datasets.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the scarcity of datasets containing real 3D clothed humans, indicating a limitation in the field. No specific dataset names are provided.",
      "processing_time": 56.84167814254761,
      "citing_paper_id": "260926600",
      "cited_paper_id": 248426884
    },
    {
      "context_text": "However, the scarcity of datasets containing real 3D clothed humans [11, 18, 47, 126, 135] limits the model’s generalization ability and may lead to overfitting on small datasets.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the scarcity of datasets containing real 3D clothed humans, indicating a limitation in the field. No specific dataset names are provided.",
      "processing_time": 56.84167814254761,
      "citing_paper_id": "260926600",
      "cited_paper_id": 258587812
    },
    {
      "context_text": "Efforts to predict invisible regions ( e.g . back-side) based on visible visual cues ( e.g . colors [5, 46, 103], normal estimates [104, 120, 121]) have proven unsuccessful, resulting in blurry texture and smoothed-out geometry, see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and visual cues. No verifiable resources are identified.",
      "processing_time": 55.34352684020996,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "We compare TeCH with state-of-the-art image-based 3D clothed human reconstruction methods, including body-agnostic methods, such as PIFu [103], PIFuHD [104] and PHORHUM [5], as well as methods that utilize SMPL-(X) [78, 92] body prior, such as PaMIR [136], ICON [120] and ECON [121].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.35307312011719,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "8, we show more qualitative comparisons between TeCH and other baselines (PIFu [103], PIFuHD [104], PaMIR [136], PHORHUM [5], ICON [120], ECON [121]) on CAPE, THuman2.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CAPE",
        "THuman2"
      ],
      "dataset_descriptions": {
        "CAPE": "Used for qualitative comparisons of 3D human models, focusing on high-resolution clothed human digitization and evaluating model performance.",
        "THuman2": "Used for qualitative comparisons of 3D human models, focusing on high-resolution clothed human digitization and evaluating model performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions CAPE and THuman2 as datasets used for qualitative comparisons. These are specific datasets used in the field of 3D human modeling.",
      "processing_time": 70.2037341594696,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "Among these works, PIFu [103], ARCH(++) [39, 46], and PaMIR [136] infer the full texture from the input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models/methods (PIFu, ARCH(++), PaMIR) rather than datasets. No specific datasets are referenced.",
      "processing_time": 56.17843413352966,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "Among these works, PIFu [103], ARCH(++) [39, 46], and PaMIR [136] infer the full texture from the input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models/methods (PIFu, ARCH(++), PaMIR) rather than datasets. No specific datasets are referenced.",
      "processing_time": 56.17843413352966,
      "citing_paper_id": "260926600",
      "cited_paper_id": 202120683
    },
    {
      "context_text": "Participants were shown videos showcasing rotating 3D humans reconstructed by TeCH, as well as the baselines (PaMIR [136], PIFu [103], ICON [120], ECON [121] and PHORHUM [5]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods used for 3D human reconstruction.",
      "processing_time": 55.19460844993591,
      "citing_paper_id": "260926600",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "Also, non-parametric explicit representations, such as depth maps [29, 108], normal maps [121], and point clouds [127] could be leveraged to reconstruct the clothed human.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of non-parametric explicit representations used in 3D human shape estimation.",
      "processing_time": 56.16728138923645,
      "citing_paper_id": "260926600",
      "cited_paper_id": 199064499
    },
    {
      "context_text": "To account for the 3D garments, 3D clothing offsets [1– 4, 63, 116, 139] or deformable garment templates [9, 49] are used on top of a body model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for handling 3D garments. No verifiable resources are identified.",
      "processing_time": 56.14583134651184,
      "citing_paper_id": "260926600",
      "cited_paper_id": 201106783
    },
    {
      "context_text": "To account for the 3D garments, 3D clothing offsets [1– 4, 63, 116, 139] or deformable garment templates [9, 49] are used on top of a body model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for handling 3D garments. No verifiable resources are identified.",
      "processing_time": 56.14583134651184,
      "citing_paper_id": "260926600",
      "cited_paper_id": 221836240
    },
    {
      "context_text": "In contrast to 3D data, large-scale 2D human images are widely avaible from DeepFashion [34, 77], SHHQ [28] and LAION-5B [105].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DeepFashion",
        "SHHQ",
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "DeepFashion": "Used for robust clothes recognition and retrieval, providing rich annotations for 2D human images.",
        "SHHQ": "Utilized for high-quality 2D human image data, supporting various applications in image processing and analysis.",
        "LAION-5B": "Leveraged for large-scale 2D human image data, enhancing the diversity and scale of training datasets."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three datasets: DeepFashion, SHHQ, and LAION-5B. These are specific, verifiable datasets used for 2D human image data.",
      "processing_time": 75.32111072540283,
      "citing_paper_id": "260926600",
      "cited_paper_id": 206593370
    },
    {
      "context_text": "Implicit representations (occupancy/distance field) are topology-agnostic, thus, can represent 3D clothed humans, with arbitrary topologies, such as open jackets and loose skirts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the capabilities of implicit representations in 3D human modeling.",
      "processing_time": 55.62314510345459,
      "citing_paper_id": "260926600",
      "cited_paper_id": 209323955
    },
    {
      "context_text": "Implicit representations (occupancy/distance field) are topology-agnostic, thus, can represent 3D clothed humans, with arbitrary topologies, such as open jackets and loose skirts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the capabilities of implicit representations in 3D human modeling.",
      "processing_time": 55.62314510345459,
      "citing_paper_id": "260926600",
      "cited_paper_id": 227228054
    },
    {
      "context_text": "However, the lack of multi-view supervision often results in depth ambiguities or inconsistent textures.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general issue in the field.",
      "processing_time": 53.43121910095215,
      "citing_paper_id": "260926600",
      "cited_paper_id": 215548941
    },
    {
      "context_text": "Human re-constructors could be grouped as: 1) Explicit-shape-based, 2) Implicit-function-based, and 3) NeRF-based methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of human reconstruction methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.35255002975464,
      "citing_paper_id": "260926600",
      "cited_paper_id": 215548941
    },
    {
      "context_text": "2.3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not contain any specific, verifiable datasets or resources. It is too short and lacks context to identify any datasets.",
      "processing_time": 56.12877893447876,
      "citing_paper_id": "260926600",
      "cited_paper_id": 215548941
    },
    {
      "context_text": "3, given the input image of a human, SegFormer [117], which is fine-tuned on ATR dataset [70, 71], is applied to recognize each part of the garments ( e.g . hat, skirt, pants, belt, shoes).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ATR dataset"
      ],
      "dataset_descriptions": {
        "ATR dataset": "Used to fine-tune SegFormer for recognizing garment parts in images, specifically focusing on items like hats, skirts, pants, belts, and shoes."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ATR dataset, which is used for fine-tuning SegFormer to recognize garment parts in images. The dataset is specific and relevant to the research.",
      "processing_time": 65.66151118278503,
      "citing_paper_id": "260926600",
      "cited_paper_id": 216056379
    },
    {
      "context_text": "Statistical body models [51, 78, 92, 123] can be considered as 3D generative models of the human body.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models. No dataset names are present in the citation context.",
      "processing_time": 55.945109367370605,
      "citing_paper_id": "260926600",
      "cited_paper_id": 219964093
    },
    {
      "context_text": "Initially, CLIP [97] semantic consistency loss [48], Score Jacobian Chaining (SJC) [112] and Score Distillation Sampling (SDS) [94] What is the color of the tank top?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.708672761917114,
      "citing_paper_id": "260926600",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Initially, CLIP [97] semantic consistency loss [48], Score Jacobian Chaining (SJC) [112] and Score Distillation Sampling (SDS) [94] What is the color of the tank top?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.708672761917114,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "Initially, CLIP [97] semantic consistency loss [48], Score Jacobian Chaining (SJC) [112] and Score Distillation Sampling (SDS) [94] What is the color of the tank top?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.708672761917114,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254125253
    },
    {
      "context_text": "While SHERF complements missing information from partial 2D observations, ELICIT leverages appearance prior from CLIP [97].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.13047432899475,
      "citing_paper_id": "260926600",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "While SHERF complements missing information from partial 2D observations, ELICIT leverages appearance prior from CLIP [97].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.13047432899475,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257663613
    },
    {
      "context_text": "For a fair comparison, all methods ( i.e ., PIFu, PaMIR, ICON, ECON) utilize the same normal estimator from ICON.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.2214515209198,
      "citing_paper_id": "260926600",
      "cited_paper_id": 231951742
    },
    {
      "context_text": "Pretrained text-to-Image diffusion models [87, 88, 90] lack the ability to mimic the appearance of subjects in a given reference set and synthesize novel renditions of them in different contexts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pretrained models. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 56.10171341896057,
      "citing_paper_id": "260926600",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "…on the SMPL-D model, from registered human scans, Chupa [55] “carves” the SMPL mesh by dual normal maps generated by pose-conditioned diffusion model; Alternatively, gDNA [17], NPMs [88], and SPAMs [89], learn the implicit clothed avatars from normalized raw captures ( i.e ., scans, depth maps).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and methods such as SMPL-D, Chupa, gDNA, NPMs, and SPAMs, which are not datasets.",
      "processing_time": 59.442901372909546,
      "citing_paper_id": "260926600",
      "cited_paper_id": 233004645
    },
    {
      "context_text": "…on the SMPL-D model, from registered human scans, Chupa [55] “carves” the SMPL mesh by dual normal maps generated by pose-conditioned diffusion model; Alternatively, gDNA [17], NPMs [88], and SPAMs [89], learn the implicit clothed avatars from normalized raw captures ( i.e ., scans, depth maps).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and methods such as SMPL-D, Chupa, gDNA, NPMs, and SPAMs, which are not datasets.",
      "processing_time": 59.442901372909546,
      "citing_paper_id": "260926600",
      "cited_paper_id": 261277214
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 233296899
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 248239659
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 250113850
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252780848
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257834153
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257912580
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 258461509
    },
    {
      "context_text": "Related human generators represent 3D humans using meshes [36, 40, 50], DMTet [33], Tri-planes [8, 25, 85, 109, 132], implicit functions [118], or neural fields [13, 41, 60, 128].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.42603015899658,
      "citing_paper_id": "260926600",
      "cited_paper_id": 259187900
    },
    {
      "context_text": "Some methods adapt GANs [54] by integrating diff-renderer [8, 25, 36, 85, 109, 110, 118, 132], while others leverage diffusion models [13, 40, 44, 60, 130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 55.66017460823059,
      "citing_paper_id": "260926600",
      "cited_paper_id": 233296899
    },
    {
      "context_text": "Some methods adapt GANs [54] by integrating diff-renderer [8, 25, 36, 85, 109, 110, 118, 132], while others leverage diffusion models [13, 40, 44, 60, 130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 55.66017460823059,
      "citing_paper_id": "260926600",
      "cited_paper_id": 248239659
    },
    {
      "context_text": "Some methods adapt GANs [54] by integrating diff-renderer [8, 25, 36, 85, 109, 110, 118, 132], while others leverage diffusion models [13, 40, 44, 60, 130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 55.66017460823059,
      "citing_paper_id": "260926600",
      "cited_paper_id": 250113850
    },
    {
      "context_text": "Some methods adapt GANs [54] by integrating diff-renderer [8, 25, 36, 85, 109, 110, 118, 132], while others leverage diffusion models [13, 40, 44, 60, 130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 55.66017460823059,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257912580
    },
    {
      "context_text": "Some methods adapt GANs [54] by integrating diff-renderer [8, 25, 36, 85, 109, 110, 118, 132], while others leverage diffusion models [13, 40, 44, 60, 130].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No dataset names are present in the text.",
      "processing_time": 55.66017460823059,
      "citing_paper_id": "260926600",
      "cited_paper_id": 260681445
    },
    {
      "context_text": "Using PIXIE [26], we estimate an initial body Mbody.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called PIXIE. The context and title do not provide information about a dataset.",
      "processing_time": 56.22292637825012,
      "citing_paper_id": "260926600",
      "cited_paper_id": 234470190
    },
    {
      "context_text": "Using PIXIE [26], we estimate an initial body M body .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called PIXIE. The context does not provide enough information to infer the use of a dataset.",
      "processing_time": 57.16341829299927,
      "citing_paper_id": "260926600",
      "cited_paper_id": 234470190
    },
    {
      "context_text": "0 [35], SegFormer* [117] pretrained from [10, 20, 23, 137] and finetuned on ATR[69], PIXIE [26] trained on human images from multiple datasets [19, 74, 90, 115, 140], and the normal predictor of ICON [120] trained on AGORA [91].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ATR",
        "AGORA"
      ],
      "dataset_descriptions": {
        "ATR": "Used to fine-tune SegFormer for human image segmentation, enhancing model performance on specific human attributes.",
        "AGORA": "Used to train the normal predictor of ICON, focusing on generating accurate normal maps for 3D human models."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions multiple datasets but does not specify their usage in the context of personalized text generation. The datasets are mentioned in the context of training models for other purposes.",
      "processing_time": 68.91104912757874,
      "citing_paper_id": "260926600",
      "cited_paper_id": 234470190
    },
    {
      "context_text": "If PIXIE [26] is predicting a wrong initial pose, the error propagates to TeCH.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (PIXIE) and another component (TeCH). There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.795828342437744,
      "citing_paper_id": "260926600",
      "cited_paper_id": 234470190
    },
    {
      "context_text": "DMTet [32, 106] is a hybrid 3D representation designed for high-resolution 3D shape synthesis and reconstruction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called DMTet. The context is about a 3D representation method, not a dataset.",
      "processing_time": 56.686676025390625,
      "citing_paper_id": "260926600",
      "cited_paper_id": 243848115
    },
    {
      "context_text": "To efficiently represent the 3D clothed human at a high resolution, we embed DMTet [32, 106] around the SMPL-X body mesh [86].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions DMTet and SMPL-X, but these are not datasets. DMTet is a method for 3D shape synthesis, and SMPL-X is a model for representing 3D human bodies. No datasets are mentioned.",
      "processing_time": 60.15325164794922,
      "citing_paper_id": "260926600",
      "cited_paper_id": 243848115
    },
    {
      "context_text": "To represent a high resolution geometry at an affordable cost, we propose a hybrid 3D representation based on DMTet [32, 106].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DMTet) for 3D shape synthesis. The context focuses on the methodology rather than data.",
      "processing_time": 57.34763479232788,
      "citing_paper_id": "260926600",
      "cited_paper_id": 243848115
    },
    {
      "context_text": "These questions cover garment styles, colors, facial features, and hairstyles, with the corresponding answers denoted as { A i } .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes the types of questions covered in some unspecified data.",
      "processing_time": 55.774229288101196,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246016186
    },
    {
      "context_text": "Empirically, we found that the BLIP [65] VQA model tends to use 1 ∼ 3 words to answer these questions, so we simply concatenate all the answers and remove repeated words to construct P VQA .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BLIP) and a method for constructing P VQA. No verifiable datasets are referenced.",
      "processing_time": 56.911888122558594,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "To obtain detailed descriptions ( i.e . color and style) of the parsed garments, we utilize the vision-language model BLIP [65] as VQA captioner.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using BLIP, which is a vision-language model, not a dataset. No specific dataset is mentioned.",
      "processing_time": 55.24710154533386,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "BLIP [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called BLIP. No verifiable resources are identified.",
      "processing_time": 55.58357095718384,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "To achieve this, TeCH follows a two-step procedure: Firstly, a text prompt that describes the human in the input image is obtained via the human parsing model SegFormer [117] and the VQA model BLIP [65] (Sec.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.00563073158264,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "First, we use BLIP [65] and a series of general questions Q general to parse genders, facial appearance, hair colors, hairstyles, facial hairs, and body poses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BLIP) and a series of general questions. The citation is used to describe a method for parsing various visual attributes.",
      "processing_time": 57.95922613143921,
      "citing_paper_id": "260926600",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "The silhouette loss [125, 133] enforces pixel-alignment with the foreground mask S of the input image I under the input camera view k : It consists of (1) a pixel-wise L2 loss over the foreground mask S and the rendered silhouette M , and (2) an edge distance loss, based on the distance of each…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (silhouette loss) and its components. No verifiable resources are identified.",
      "processing_time": 56.09528875350952,
      "citing_paper_id": "260926600",
      "cited_paper_id": 247292343
    },
    {
      "context_text": "To facilitate the creation of digital humans from easily accessible in-the-wild photos, numerous approaches focus on reconstructing a 3D clothed human shape from a single image [12, 38, 39, 46, 67, 72, 102–104, 119– 121, 136].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that focus on 3D human shape reconstruction from single images.",
      "processing_time": 56.091448068618774,
      "citing_paper_id": "260926600",
      "cited_paper_id": 248366560
    },
    {
      "context_text": "Based on these information sources, we optimize the 3D human using multi-view Score Distillation Sampling (SDS)[94], reconstruction losses based on the original observations, and regularization obtained from off-the-shelf normal estimators, to enhance the fidelity of the reconstructed 3D human…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on optimization techniques and loss functions, which do not qualify as datasets.",
      "processing_time": 56.90865468978882,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "3.2), is optimized with SDS losses [94] computed from the personalized DreamBooth (Sec.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DreamBooth) which is part of the cited paper. No verifiable datasets are referenced.",
      "processing_time": 57.26201319694519,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "To mitigate the occurrence of mirrored appearance artifacts ( i.e ., Janus-head), we incorporate view-aware prompts (“ front/side/back/overhead view ”) w.r.t. the viewing angle in the diffusion-based generation process, whose effectiveness has been demonstrated in DreamBooth [94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DreamBooth) and a technique (view-aware prompts).",
      "processing_time": 55.782177209854126,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "The Score Distillation Sampling (SDS) loss has been introduced in DreamFusion [94] for the task of Text-to-3D generation of general objects, by optimizing a neural radiance field (NeRF) with gradients from a frozen diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SDS loss) and a task (Text-to-3D generation).",
      "processing_time": 56.508708477020264,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "As mentioned in DreamFusion [94], the SDS loss may result in over-saturated colors which will cause a noticeable color disparity between visible and invisible regions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a potential issue with the SDS loss in the DreamFusion method.",
      "processing_time": 55.41383981704712,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "DreamFusion [94] introduces Score Distillation Sampling (SDS) loss, to perform Text-to-3D synthesis by using pretrained 2D Text-to-Image diffusion model ϕ .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Score Distillation Sampling) and a model (pretrained 2D Text-to-Image diffusion model).",
      "processing_time": 57.63676452636719,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "Inspired by Fantasia3D [16], our approach integrates normal renderings with the SDS loss [94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SDS loss) and a reference to another paper (Fantasia3D).",
      "processing_time": 56.59184408187866,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "To mitigate the occurrence of mirrored appearance artifacts ( i.e ., Janus-head), we incorporate view-aware prompts, “ front/side/back/overhead view ”, w.r.t. the viewing angle during generation process, whose effectiveness has been demonstrated in DreamBooth [94].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DreamBooth) and a concept (view-aware prompts).",
      "processing_time": 55.92205762863159,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252596091
    },
    {
      "context_text": "Also, how to compositionally generate the separate components, such as haircut [107], accessories [31], and decoupled outfits [27], is still an un-solved problem.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to unsolved problems in generating components of personal appearance.",
      "processing_time": 55.399333238601685,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252715745
    },
    {
      "context_text": "In addition to aforementioned “reconstruct via multi-view SDS” scheme, recent attention has been drawn to the “reconstruct via direct view-conditional generation” [14, 75, 76, 95, 114, 138].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited papers' titles also do not provide additional information about datasets.",
      "processing_time": 56.58241105079651,
      "citing_paper_id": "260926600",
      "cited_paper_id": 252780361
    },
    {
      "context_text": "In addition to aforementioned “reconstruct via multi-view SDS” scheme, recent attention has been drawn to the “reconstruct via direct view-conditional generation” [14, 75, 76, 95, 114, 138].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches. The cited papers' titles also do not provide additional information about datasets.",
      "processing_time": 56.58241105079651,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254125457
    },
    {
      "context_text": "It achieves this by efficiently rasterizing a triangular mesh into high-resolution image patches using a differentiable renderer [57], enabling interaction with pre-trained high-resolution latent diffusion models, such as eDiff-I [7], and Stable Diffusion [88].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the technical aspects of rendering and interaction with pre-trained models.",
      "processing_time": 57.08173489570618,
      "citing_paper_id": "260926600",
      "cited_paper_id": 253254800
    },
    {
      "context_text": "Subsequently, there is a line of works [22, 81, 98, 111, 122] that address this problem, by incorporating textural inversion [30], DreamBooth [101], CLIP-guided diffusion prior, depth prior, and reconstruction loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.37624263763428,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254069839
    },
    {
      "context_text": "Subsequently, there is a line of works [22, 81, 98, 111, 122] that address this problem, by incorporating textural inversion [30], DreamBooth [101], CLIP-guided diffusion prior, depth prior, and reconstruction loss.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.37624263763428,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257757320
    },
    {
      "context_text": "Rodin [113] has recently employed large-scale 3D synthetic head avatars in combination with a diffusion model to develop a high-fidelity head avatar generator.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of 'large-scale 3D synthetic head avatars' but does not specify a named dataset. The cited paper title confirms the focus on generating 3D digital avatars but does not mention a specific dataset.",
      "processing_time": 59.90106225013733,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254564523
    },
    {
      "context_text": "Some methods adapt GANs [49] by integrating differential rendering [8, 23, 33, 76, 97, 103, 115], while others leverage diffusion models [13, 37, 98].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.3576717376709,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Some methods adapt GANs [49] by integrating differential rendering [8, 23, 33, 76, 97, 103, 115], while others leverage diffusion models [13, 37, 98].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.3576717376709,
      "citing_paper_id": "260926600",
      "cited_paper_id": 258461509
    },
    {
      "context_text": "Current human generators trained on 2D images, represent 3D humans using meshes [33, 37, 45], DMTet [30], Tri-planes [8, 23, 76, 97, 115], implicit functions [103], or Neural fields [13, 38, 55, 112].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.1305992603302,
      "citing_paper_id": "260926600",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Current human generators trained on 2D images, represent 3D humans using meshes [33, 37, 45], DMTet [30], Tri-planes [8, 23, 76, 97, 115], implicit functions [103], or Neural fields [13, 38, 55, 112].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for representing 3D humans. No verifiable resources are identified.",
      "processing_time": 56.1305992603302,
      "citing_paper_id": "260926600",
      "cited_paper_id": 258461509
    },
    {
      "context_text": "Leveraging controllable T2I models [52, 82, 96, 134] may help to improve the controllability and stability of generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on improving controllability and stability in text-to-image generation using certain models.",
      "processing_time": 57.230921030044556,
      "citing_paper_id": "260926600",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "SHERF [43] and ELICIT [45] optimize a generalized human NeRF, incorporating model-based priors (SMPL-X [92]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on optimizing a generalized human NeRF using model-based priors.",
      "processing_time": 56.682732343673706,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257663613
    },
    {
      "context_text": "In addition to aforementioned “reconstruct via multiview SDS” scheme, recent attention has been drawn to the “reconstruct via direct view-conditional generation” [14, 75, 76, 95, 114, 138].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers discussing methods for view-conditional generation.",
      "processing_time": 54.823437213897705,
      "citing_paper_id": "260926600",
      "cited_paper_id": 257952179
    },
    {
      "context_text": "To account for the outfits, CAPE [79] learns a clothing offset layer based on the SMPL-D model, from registered human scans, Chupa [55] “carves” the SMPL mesh by dual normal maps generated by pose-conditioned diffusion model; Alternatively, gDNA [17], NPMs [88], and SPAMs [89], learn the implicit…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses models and methods such as CAPE, Chupa, gDNA, NPMs, and SPAMs, but does not reference any datasets by name.",
      "processing_time": 59.431092500686646,
      "citing_paper_id": "260926600",
      "cited_paper_id": 258823360
    },
    {
      "context_text": "Despite the demonstrated quality of these methods in generating textured avatars, a significant gap still exists in achieving “lifelike” avatars with detailed geometry and texture, consistent with the input.",
      "catation_intent": "gap",
      "resource_type": "gap",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a gap in existing methods for generating lifelike avatars.",
      "processing_time": 55.203041553497314,
      "citing_paper_id": "260926600",
      "cited_paper_id": 261705909
    },
    {
      "context_text": "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers are about generating sequences and text using RNNs, which are not datasets.",
      "processing_time": 57.79655051231384,
      "citing_paper_id": "16282767",
      "cited_paper_id": 748227
    },
    {
      "context_text": "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers are about generating sequences and text using RNNs, which are not datasets.",
      "processing_time": 57.79655051231384,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "Recently, recurrent neural networks (RNNs) have been proved to very effective in natural language generation (Graves 2013; Sutskever, Martens, and Hinton 2011; Bowman et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers are about generating sequences and text using RNNs, which are not datasets.",
      "processing_time": 57.79655051231384,
      "citing_paper_id": "16282767",
      "cited_paper_id": 263865370
    },
    {
      "context_text": "(Bowman et al. 2015) investigated generating sentences from continuous semantic spaces with a variational auto-encoder, in which RNN is used for both the encoder and the encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving variational auto-encoders and RNNs.",
      "processing_time": 55.683210134506226,
      "citing_paper_id": "16282767",
      "cited_paper_id": 748227
    },
    {
      "context_text": "…of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of neural networks. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 56.8203763961792,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "…of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of neural networks. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 56.8203763961792,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1918428
    },
    {
      "context_text": "…of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation (Bahdanau, Cho, and Bengio 2014) and image caption (Xu et al. 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of neural networks. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 56.8203763961792,
      "citing_paper_id": "16282767",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "(Graves 2013) studied sequence generation, including text, using recurrent neural networks (RNN) with long-short term memory unit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets used in the research. It focuses on the methodology of using RNNs with LSTM units for sequence generation.",
      "processing_time": 56.989935636520386,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scien-tiﬁc papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various types of data used in RNNs with LSTM, but does not specify named datasets. It refers to general categories of data such as Wikipedia articles, linux source codes, scientific papers, and NSF abstracts.",
      "processing_time": 59.67175650596619,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scien-tiﬁc papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions various types of data used in RNNs with LSTM, but does not specify named datasets. It refers to general categories of data such as Wikipedia articles, linux source codes, scientific papers, and NSF abstracts.",
      "processing_time": 59.67175650596619,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "RNNs with LSTM have shown very promising results on various data sets with different structures including Wikipedia articles (Graves 2013), linux source codes (Karpathy, Johnson, and Li 2015), scientific papers (Karpathy, Johnson, and Li 2015), NSF abstracts (Karpathy, Johnson, and Li 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several types of data but does not specify named datasets. It refers to general categories of data such as Wikipedia articles, linux source codes, scientific papers, and NSF abstracts. These are not specific, verifiable datasets.",
      "processing_time": 59.84357500076294,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of natural language generation.",
      "processing_time": 54.37951445579529,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "There are usually two types of approaches for natural language generation: beam search (Bahdanau, Cho, and Bengio 2014), which is widely used in neural machine translation, and random sample (Graves 2013).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (beam search and random sampling). The cited papers are used to reference these methods, not datasets.",
      "processing_time": 57.39042139053345,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "There are usually two types of approaches for natural language generation: beam search (Bahdanau, Cho, and Bengio 2014), which is widely used in neural machine translation, and random sample (Graves 2013).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (beam search and random sampling). The cited papers are used to reference these methods, not datasets.",
      "processing_time": 57.39042139053345,
      "citing_paper_id": "16282767",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Natural language generation is potentially useful in a variety of applications such as natural language understanding (Graves 2013), response generation in dialogue systems (Wen et al. 2015a; 2015b; Sordoni et al. 2015), text summarization (Rush, Chopra, and Weston 2015), machine translation…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of natural language generation. No verifiable resources are identified.",
      "processing_time": 55.993900775909424,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1697424
    },
    {
      "context_text": "The core idea of LSTM is introducing the memory state and multiple gating functions to control the information written to the memory sate, reading from the memory state, and removed (or forgotten) from the memory state.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only describes the functionality of LSTM.",
      "processing_time": 54.340710401535034,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "(Hochreiter and Schmidhuber 1997) effectively addresses this problem through the long-short term memory (LSTM) unit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to the LSTM unit, which is a method, not a dataset.",
      "processing_time": 56.4316623210907,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "(Hochreiter and Schmidhuber 1997) effectively addresses this problem through the long-short term memory (LSTM) unit.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to the LSTM unit, which is a method, not a dataset.",
      "processing_time": 56.4316623210907,
      "citing_paper_id": "16282767",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Though traditional RNNs suffer from the problem of gradient vanishing or exploding, the long-short term memory (LSTM) (Hochreiter and Schmidhuber 1997) unit effectively addresses this problem and is able to capture the long-range dependency in natural languages.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the LSTM model and its ability to address issues in RNNs.",
      "processing_time": 56.180381774902344,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Though traditional RNNs suffer from the problem of gradient vanishing or exploding, the long-short term memory (LSTM) (Hochreiter and Schmidhuber 1997) unit effectively addresses this problem and is able to capture the long-range dependency in natural languages.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only discusses the LSTM model and its ability to address issues in RNNs.",
      "processing_time": 56.180381774902344,
      "citing_paper_id": "16282767",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The ﬁrst one is the vanilla recurrent neural networks with LSTM unit, and the initiate state of the RNN is set as the context embedding h C .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (LSTM). No verifiable resources are identified.",
      "processing_time": 55.49382448196411,
      "citing_paper_id": "16282767",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Table 5: Results of fake review detection in TripAdvisor with the approach in (Ott et al. 2011 Automatic Classiﬁcation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TripAdvisor"
      ],
      "dataset_descriptions": {
        "TripAdvisor": "Used to detect fake reviews, focusing on deceptive opinion spam through classification methods. The dataset likely contains user-generated reviews and labels indicating authenticity."
      },
      "confidence_score": 0.7,
      "reasoning": "The citation mentions 'TripAdvisor' in the context of fake review detection, which suggests the use of a specific dataset from TripAdvisor. However, the exact name of the dataset is not provided, only the platform is mentioned.",
      "processing_time": 66.46909070014954,
      "citing_paper_id": "16282767",
      "cited_paper_id": 2510724
    },
    {
      "context_text": "We adopt the approach in (Ott et al. 2011), which trains a classiﬁer with 800 real reviews from TripAdvisor and 800 fake reviews written by the Amazon Mechanical Turkers",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "TripAdvisor",
        "Amazon Mechanical Turkers"
      ],
      "dataset_descriptions": {
        "TripAdvisor": "Used to train a classifier with 800 real reviews to detect deceptive opinion spam, focusing on distinguishing authentic from fabricated content.",
        "Amazon Mechanical Turkers": "Used to generate 800 fake reviews for training a classifier to detect deceptive opinion spam, focusing on identifying characteristics of fabricated content."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset used for training a classifier to detect deceptive opinion spam, including both real and fake reviews.",
      "processing_time": 69.085458278656,
      "citing_paper_id": "16282767",
      "cited_paper_id": 2510724
    },
    {
      "context_text": "Specifically, for each vertex, we use beta-blended shape [29] to compute the shape-related vertex deformation T β trg ∈ SE(3) from { θ 0 , β 0 } to { θ 0 , β trg } .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing vertex deformation using a shape model.",
      "processing_time": 55.24422478675842,
      "citing_paper_id": "257834153",
      "cited_paper_id": 5328073
    },
    {
      "context_text": "To represent 3D objects, previous approaches mainly utilize explicit representations like point clouds [3, 28], voxels [10, 23], and meshes [21, 15, 11, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for representing 3D objects. No verifiable resources are identified.",
      "processing_time": 56.52014946937561,
      "citing_paper_id": "257834153",
      "cited_paper_id": 7437409
    },
    {
      "context_text": "To represent 3D objects, previous approaches mainly utilize explicit representations like point clouds [3, 28], voxels [10, 23], and meshes [21, 15, 11, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for representing 3D objects. No verifiable resources are identified.",
      "processing_time": 56.52014946937561,
      "citing_paper_id": "257834153",
      "cited_paper_id": 211839696
    },
    {
      "context_text": "To represent 3D objects, previous approaches mainly utilize explicit representations like point clouds [3, 28], voxels [10, 23], and meshes [21, 15, 11, 58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for representing 3D objects. No verifiable resources are identified.",
      "processing_time": 56.52014946937561,
      "citing_paper_id": "257834153",
      "cited_paper_id": 227251884
    },
    {
      "context_text": "Our key idea is that the object silhouette could serve as a proxy for its geometry, while allowing for non-convex details to be sculpted [24].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to image understanding.",
      "processing_time": 53.779627323150635,
      "citing_paper_id": "257834153",
      "cited_paper_id": 27660563
    },
    {
      "context_text": "Recently, led by the pioneering work of NeRF [32], the advances in neural implicit fields [32, 54, 55, 33, 40, 37] have sparked research into representing 3D objects via a continuous function, producing photo-realistic novel views by volume rendering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models such as NeRF and neural implicit fields. The context is focused on the advancements in these areas rather than the use of specific datasets.",
      "processing_time": 58.990392446517944,
      "citing_paper_id": "257834153",
      "cited_paper_id": 234364556
    },
    {
      "context_text": "Recently, led by the pioneering work of NeRF [32], the advances in neural implicit fields [32, 54, 55, 33, 40, 37] have sparked research into representing 3D objects via a continuous function, producing photo-realistic novel views by volume rendering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models such as NeRF and neural implicit fields. The context is focused on the advancements in these areas rather than the use of specific datasets.",
      "processing_time": 58.990392446517944,
      "citing_paper_id": "257834153",
      "cited_paper_id": 235605960
    },
    {
      "context_text": "Recent works [40, 37, 48, 42] extend the static neural implicit fields to dynamic ones with an implicit deformation network to warp sampled points along the ray to deform a template.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on extending static neural implicit fields to dynamic ones using an implicit deformation network.",
      "processing_time": 56.721519947052,
      "citing_paper_id": "257834153",
      "cited_paper_id": 234364556
    },
    {
      "context_text": "Specifically, NeuS [54] and VolSDF [55] are proposed to improve NeRF [32] by representing the geometry using a sign distance field (SDF) instead of occupancy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on improvements to NeRF by using SDF representations.",
      "processing_time": 56.88940382003784,
      "citing_paper_id": "257834153",
      "cited_paper_id": 235605960
    },
    {
      "context_text": "We postulate that it may be due to CLIP constraint that perceives and embeds the image as a whole, while being weak in providing location-aware super-vision [61, 26] that is necessary for allocating correct texture locally.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) and a need for location-aware supervision. No verifiable resources are identified.",
      "processing_time": 57.22434091567993,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245218534
    },
    {
      "context_text": "Text-driven diffusion models have demonstrated unprecedented capabilities in generating diverse high-quality semantically relevant images, such as GLIDE [35], Imagen [45], and Stable Diffusion Model (SDM) [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (GLIDE, Imagen, Stable Diffusion Model) but does not refer to any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 57.87126064300537,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Text-driven diffusion models have demonstrated unprecedented capabilities in generating diverse high-quality semantically relevant images, such as GLIDE [35], Imagen [45], and Stable Diffusion Model (SDM) [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (GLIDE, Imagen, Stable Diffusion Model) but does not refer to any specific datasets. The cited papers are about models, not datasets.",
      "processing_time": 57.87126064300537,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Recently, diffusion models have emerged as promising and widely-attracted image generators due to their impressive generative performance [14, 36, 35, 45, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only diffusion models. No dataset names are present in the context.",
      "processing_time": 56.106305837631226,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335086
    },
    {
      "context_text": "Recently, diffusion models have emerged as promising and widely-attracted image generators due to their impressive generative performance [14, 36, 35, 45, 43].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only diffusion models. No dataset names are present in the context.",
      "processing_time": 56.106305837631226,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Given a text prompt, we use diffusion models [43] to guide the creation of a human avatar by updating the template with geometry and texture that are consistent with the text.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion models). The context focuses on the use of diffusion models for generating human avatars, which is not a dataset.",
      "processing_time": 58.369725704193115,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "However, directly applying diffusion models [43] can lead to distorted geometry and textures as the diffusion loss [39] is sensitive to the input resolution and biased towards geometry modeling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the limitations of diffusion models and their sensitivity to input resolution.",
      "processing_time": 57.212016105651855,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "For Score Distillation, we use pre-trained Stable Diffusion [43] v1.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'pre-trained Stable Diffusion' but does not refer to a specific dataset. It is a reference to a model or method, not a dataset.",
      "processing_time": 57.544352769851685,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "In addition to directly converting Gaussian noise into images with learned data distributions through iterative denoising, these models can also generate desired images conditioned on the guidance like class labels, text, and low-resolution images [6, 43, 44, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods for image synthesis. No verifiable resources are identified.",
      "processing_time": 56.260435342788696,
      "citing_paper_id": "257834153",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "These models have been successfully applied in various domains, including image styl-ization, image editing, video generation, and 3D scene generation [17, 59, 12, 13, 49, 39, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various application domains of certain models. There are no clear identifiers for datasets.",
      "processing_time": 56.670494556427,
      "citing_paper_id": "257834153",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "We adopt similar augmentation strategies to those used in recent text-to-3D generative models [16, 39, 25, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only augmentation strategies from other models. No verifiable resources are identified.",
      "processing_time": 56.29068350791931,
      "citing_paper_id": "257834153",
      "cited_paper_id": 252668646
    },
    {
      "context_text": "We adopt similar augmentation strategies to those used in recent text-to-3D generative models [16, 39, 25, 19, 27].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only augmentation strategies from other models. No verifiable resources are identified.",
      "processing_time": 56.29068350791931,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Aside from creating 3D objects with reference images [5, 34, 18, 1], the recent works propose to add detailed styles to a bare 3D body mesh given a text prompt as guidance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the concept of adding detailed styles to 3D body meshes using text prompts.",
      "processing_time": 57.47520661354065,
      "citing_paper_id": "257834153",
      "cited_paper_id": 255546292
    },
    {
      "context_text": "Aside from creating 3D objects with reference images [5, 34, 18, 1], the recent works propose to add detailed styles to a bare 3D body mesh given a text prompt as guidance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses the concept of adding detailed styles to 3D body meshes using text prompts.",
      "processing_time": 57.47520661354065,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "To mitigate cloudy artifacts due to inaccurate warping, we adopt a mask function η ( p ( t )) [4] to set the density of the points far from the target mesh surface to zero.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method to mitigate artifacts in a mesh surface.",
      "processing_time": 54.55899381637573,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Thanks to the diffusion constraints, our AvatarCraft can generate novel avatars (i.e., to dream avatars [16, 39]) by mixing different identities together.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method or capability of generating avatars, which is not a dataset.",
      "processing_time": 57.34069299697876,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Regarding avatar animation, unlike skeleton-driven mesh animation in previous approaches [16], our method defines an explicit warping field that maps the target human parameterization to the template human avatar and uses the warping field to deform the neural implicit field directly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to avatar animation.",
      "processing_time": 55.312036991119385,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Avatar-CLIP [16] and NeRF-Art [53] stylize a pre-trained neural implicit field guided by CLIP, producing photo-realistic renderings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of Avatar-CLIP and NeRF-Art for stylizing neural implicit fields.",
      "processing_time": 58.550204277038574,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "We compare AvatarCraft with state-of-the-art methods for text-driven human avatar creation, including the mesh based method of CLIP-Actor [56] and implicit-field based methods of AvatarCLIP [16] and NeRF-Art [53].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.78518629074097,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "In particular, text-to-3D avatar creation [16, 56, 31, 53] is explored by leveraging the zero-shot generation ability of Contrastive Language-Image Pre-Training (CLIP) [41].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP) and a research area (text-to-3D avatar creation).",
      "processing_time": 57.249861001968384,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing methods [31, 56, 16] address these challenges by adopting cross-modal supervision to guide the generation and modeling avatars as explicit meshes to support skeleton-driven animation [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.791725397109985,
      "citing_paper_id": "257834153",
      "cited_paper_id": null
    },
    {
      "context_text": "Previous approaches have relied on expensive and complex acquisition equipment to reconstruct high-fidelity avatar models [Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only previous works that used complex equipment for avatar model reconstruction.",
      "processing_time": 55.864704847335815,
      "citing_paper_id": "259187900",
      "cited_paper_id": 3248785
    },
    {
      "context_text": "Previous approaches have relied on expensive and complex acquisition equipment to reconstruct high-fidelity avatar models [Alexander et al. 2010; Guo et al. 2017; Xiao et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only previous works that used complex equipment for avatar model reconstruction.",
      "processing_time": 55.864704847335815,
      "citing_paper_id": "259187900",
      "cited_paper_id": 10320049
    },
    {
      "context_text": "In the avatar modeling module, a bare rendering of SMPL model [Loper et al. 2015; Pavlakos et al. 2019] is trained into a neural implicit field [Wang et al. 2021] that consists of an SDF network 𝑓 ( 𝑥 ; 𝜃 ) and a color network 𝑐 ( 𝑥 ; 𝜃 ) , following prior works [Hong et al. 2022; Jiang et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the technical aspects of the avatar modeling module, using the SMPL model and neural implicit fields.",
      "processing_time": 58.67572641372681,
      "citing_paper_id": "259187900",
      "cited_paper_id": 5328073
    },
    {
      "context_text": "In the avatar modeling module, a bare rendering of SMPL model [Loper et al. 2015; Pavlakos et al. 2019] is trained into a neural implicit field [Wang et al. 2021] that consists of an SDF network 𝑓 ( 𝑥 ; 𝜃 ) and a color network 𝑐 ( 𝑥 ; 𝜃 ) , following prior works [Hong et al. 2022; Jiang et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the technical aspects of the avatar modeling module, using the SMPL model and neural implicit fields.",
      "processing_time": 58.67572641372681,
      "citing_paper_id": "259187900",
      "cited_paper_id": 235490453
    },
    {
      "context_text": "We believe that this is due to the fact that previous methods rely on the constraints from SMPL [Loper et al. 2015; Pavlakos et al. 2019] Figure 7: Qualitative comparison-II.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (SMPL). No verifiable resources are identified.",
      "processing_time": 56.05894660949707,
      "citing_paper_id": "259187900",
      "cited_paper_id": 5328073
    },
    {
      "context_text": "LoRA [Hu et al. 2022] proposes to fine-tune large language models, which freezes the pre-trained model weights and meanwhile injects learnable rank decomposition matrices into the layers of the Transformer network [Vaswani et al. 2017].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.79042887687683,
      "citing_paper_id": "259187900",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "2022] proposes to fine-tune large language models, which freezes the pre-trained model weights and meanwhile injects learnable rank decomposition matrices into the layers of the Transformer network [Vaswani et al. 2017].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fine-tuning large language models and a reference to the Transformer architecture.",
      "processing_time": 56.787495374679565,
      "citing_paper_id": "259187900",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Then, we employ a keypoint ControlNet to produce multiview facial images I guided by a skeleton constraint generated by OpenPose [Cao et al. 2021, 2017], which are rendered in surround views.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods (ControlNet, OpenPose).",
      "processing_time": 55.83981966972351,
      "citing_paper_id": "259187900",
      "cited_paper_id": 16224674
    },
    {
      "context_text": "In the avatar modeling module, a bare rendering of SMPL model [Loper et al. 2015; Pavlakos et al. 2019] is trained into a neural implicit field [Wang et al. 2021] that consists of an SDF network 𝑓 ( 𝑥 ; 𝜃 ) and a color network 𝑐 ( 𝑥 ; 𝜃 ) , following prior works [Hong et al. 2022; Jiang et al. 2023b].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the technical aspects of neural implicit fields and the SMPL model.",
      "processing_time": 57.36923432350159,
      "citing_paper_id": "259187900",
      "cited_paper_id": 58007025
    },
    {
      "context_text": "Then, pixel colors can be calculated using the volume rendering equation: where 𝑝 ( 𝑡 ) is a sampled point, and 𝑅 contains 𝑛 sampled points along the ray 𝑜 + 𝑡 · 𝑑 , and 𝑤 ( 𝑡 ) is formulated as: where 𝜙 𝑠 is the logistic density distribution, 𝑓 is an SDF network.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only mathematical equations and a method (SDF network).",
      "processing_time": 55.65778660774231,
      "citing_paper_id": "259187900",
      "cited_paper_id": 58007025
    },
    {
      "context_text": "NeuS is a neural implicit representation that represents a 3D surface as the zero-level set of a signed distance function (SDF) [Park et al. 2019].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (NeuS) and a reference to another method (DeepSDF).",
      "processing_time": 57.196924924850464,
      "citing_paper_id": "259187900",
      "cited_paper_id": 58007025
    },
    {
      "context_text": "Given a coordinate ( 𝑥,𝑦,𝑧 ) and viewing location/direction ( 𝑜,𝑑 ) , two MLPs are used for predicting the SDF value and the RGB value respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method using MLPs for predicting SDF and RGB values.",
      "processing_time": 57.02582621574402,
      "citing_paper_id": "259187900",
      "cited_paper_id": 58007025
    },
    {
      "context_text": "To model the neural implicit surface, we use a 6-layer MLP for the SDF network and a 4-layer MLP for the color network.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.372530460357666,
      "citing_paper_id": "259187900",
      "cited_paper_id": 58007025
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.34675073623657,
      "citing_paper_id": "259187900",
      "cited_paper_id": 80628414
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.34675073623657,
      "citing_paper_id": "259187900",
      "cited_paper_id": 236926395
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.34675073623657,
      "citing_paper_id": "259187900",
      "cited_paper_id": 240354085
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.34675073623657,
      "citing_paper_id": "259187900",
      "cited_paper_id": 257767358
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.469812631607056,
      "citing_paper_id": "259187900",
      "cited_paper_id": 80628414
    },
    {
      "context_text": "Additionally, some explicit methods [Alldieck et al. 2019; Han et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.469812631607056,
      "citing_paper_id": "259187900",
      "cited_paper_id": 257767358
    },
    {
      "context_text": "Alternatively, other methods leverage a neural network to predict plausible avatar models from a single image input [Saito et al. 2019; Xiu et al. 2022; Zheng et al. 2021].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.466575622558594,
      "citing_paper_id": "259187900",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "Alternatively, other methods leverage a neural network to predict plausible avatar models from a single image input [Saito et al. 2019; Xiu et al. 2022; Zheng et al. 2021].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles also do not indicate the use of specific datasets.",
      "processing_time": 57.466575622558594,
      "citing_paper_id": "259187900",
      "cited_paper_id": 220404307
    },
    {
      "context_text": "…al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for generating human avatars. No verifiable resources are identified.",
      "processing_time": 56.82325220108032,
      "citing_paper_id": "259187900",
      "cited_paper_id": 152282359
    },
    {
      "context_text": "…al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and approaches for generating human avatars. No verifiable resources are identified.",
      "processing_time": 56.82325220108032,
      "citing_paper_id": "259187900",
      "cited_paper_id": 214743286
    },
    {
      "context_text": "Other methods [Wang et al. 2023; Wu et al. 2023; Zhang et al. 2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al. 2021a; Rombach et al. 2022], or large-scale synthetic data [Wood et al. 2021] to achieve the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large-scale synthetic data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.286845207214355,
      "citing_paper_id": "259187900",
      "cited_paper_id": 214728393
    },
    {
      "context_text": "Other methods [Wang et al. 2023; Wu et al. 2023; Zhang et al. 2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al. 2021a; Rombach et al. 2022], or large-scale synthetic data [Wood et al. 2021] to achieve the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large-scale synthetic data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.286845207214355,
      "citing_paper_id": "259187900",
      "cited_paper_id": 236926395
    },
    {
      "context_text": "Other methods [Wang et al. 2023; Wu et al. 2023; Zhang et al. 2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al. 2021a; Rombach et al. 2022], or large-scale synthetic data [Wood et al. 2021] to achieve the…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large-scale synthetic data' but does not specify a named dataset. The cited papers do not provide additional specific dataset names.",
      "processing_time": 57.286845207214355,
      "citing_paper_id": "259187900",
      "cited_paper_id": 240354085
    },
    {
      "context_text": "2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific dataset names, only references to models and methods. The cited papers' titles suggest the use of the FaceScape dataset, but the context itself does not explicitly state its use.",
      "processing_time": 59.86808490753174,
      "citing_paper_id": "259187900",
      "cited_paper_id": 214728393
    },
    {
      "context_text": "2023] leverage the 3D parametric face model [Yang et al. 2020; Zhu et al. 2021a], large pre-trained language-vision models [Radford et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context does not mention any specific dataset names, only references to models and methods. The cited papers' titles suggest the use of the FaceScape dataset, but the context itself does not explicitly state its use.",
      "processing_time": 59.86808490753174,
      "citing_paper_id": "259187900",
      "cited_paper_id": 240354085
    },
    {
      "context_text": "…et al. 2023; Varol et al. 2018; Xiu et al. 2023; Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for generating content. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.58356213569641,
      "citing_paper_id": "259187900",
      "cited_paper_id": 215548941
    },
    {
      "context_text": "2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for generating human avatars from a single input image.",
      "processing_time": 56.64529061317444,
      "citing_paper_id": "259187900",
      "cited_paper_id": 215548941
    },
    {
      "context_text": "2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models for generating human avatars from a single input image.",
      "processing_time": 56.64529061317444,
      "citing_paper_id": "259187900",
      "cited_paper_id": 220404307
    },
    {
      "context_text": "…Zheng et al. 2019; Zhu et al. 2016, 2018, 2019, 2021b] as well as implicit methods [He et al. 2021; Huang et al. 2020; Peng et al. 2021; Saito et al. 2019, 2020; Tan et al. 2020; Xiu et al. 2022; Zheng et al. 2021] have been developed to generate human avatars conditioned on a single input image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models for generating human avatars from a single input image.",
      "processing_time": 56.45471739768982,
      "citing_paper_id": "259187900",
      "cited_paper_id": 220404307
    },
    {
      "context_text": "In recent years, diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020] have rapidly developed due to their remarkable performance in synthesizing high-quality images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance in image synthesis.",
      "processing_time": 55.612082719802856,
      "citing_paper_id": "259187900",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "In recent years, diffusion models [Dhariwal and Nichol 2021; Ho et al. 2020; Song et al. 2020] have rapidly developed due to their remarkable performance in synthesizing high-quality images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance in image synthesis.",
      "processing_time": 55.612082719802856,
      "citing_paper_id": "259187900",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Neural Implicit Surfaces (NeuS) [Wang et al. 2021].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation refers to a method (NeuS) rather than a dataset. No dataset names are mentioned in the citation context.",
      "processing_time": 56.90842413902283,
      "citing_paper_id": "259187900",
      "cited_paper_id": 235490453
    },
    {
      "context_text": "To this end, a neural implicit surface [Wang et al. 2021] is learned to represent the shape and appearance of the human avatar, which is supervised with dual pre-trained or fine-tuned latent diffusion models for the face and body respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the neural implicit surface representation and latent diffusion models.",
      "processing_time": 57.36625957489014,
      "citing_paper_id": "259187900",
      "cited_paper_id": 235490453
    },
    {
      "context_text": "To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARF-GAN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (EG3D, GNARF, EVA3D, HumanGen) but does not refer to any specific datasets. The cited papers' titles confirm these are models, not datasets.",
      "processing_time": 60.247087240219116,
      "citing_paper_id": "259187900",
      "cited_paper_id": 245144673
    },
    {
      "context_text": "To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARF-GAN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (EG3D, GNARF, EVA3D, HumanGen) but does not refer to any specific datasets. The cited papers' titles confirm these are models, not datasets.",
      "processing_time": 60.247087240219116,
      "citing_paper_id": "259187900",
      "cited_paper_id": 250113850
    },
    {
      "context_text": "To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARF-GAN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (EG3D, GNARF, EVA3D, HumanGen) but does not refer to any specific datasets. The cited papers' titles confirm these are models, not datasets.",
      "processing_time": 60.247087240219116,
      "citing_paper_id": "259187900",
      "cited_paper_id": 252780848
    },
    {
      "context_text": "To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al. 2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al. 2023a], ENARF-GAN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (EG3D, GNARF, EVA3D, HumanGen) but does not refer to any specific datasets. The cited papers' titles confirm these are models, not datasets.",
      "processing_time": 60.247087240219116,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "To overcome this challenge, recent methods have utilized cheaper 2D data to train a neural field, such as EG3D [Chan et al. 2022], GNARF [Bergman et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. No verifiable resources are identified.",
      "processing_time": 56.06585097312927,
      "citing_paper_id": "259187900",
      "cited_paper_id": 245144673
    },
    {
      "context_text": "2022], EVA3D [Hong et al. 2023b], HumanGen [Jiang et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets. It mentions models or methods, which are excluded according to the instructions.",
      "processing_time": 56.898404121398926,
      "citing_paper_id": "259187900",
      "cited_paper_id": 252780848
    },
    {
      "context_text": "Except for using SDS for guidance, other methods [Hong et al. 2023a; Wang et al. 2022] also use Score Jacobian Chaining to generate 3D assets with text, which takes into consideration the gradient of diffusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of Score Jacobian Chaining for generating 3D assets with text, which is a methodological discussion rather than a dataset reference.",
      "processing_time": 60.82616901397705,
      "citing_paper_id": "259187900",
      "cited_paper_id": 252780848
    },
    {
      "context_text": "Recently, 3D content generation based on large-scale pre-trained vision-language models has shown promising performance [Lin et al. 2022; Poole et al. 2023; Raj et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing 3D content generation using pre-trained models.",
      "processing_time": 57.36201453208923,
      "citing_paper_id": "259187900",
      "cited_paper_id": 253708074
    },
    {
      "context_text": "When it comes to 3D content generation, existing methods [Chen et al. 2023; Lin et al. 2022; Poole et al. 2023] leveraged pre-trained text-to-image diffusion models to supervise coordinate-based networks with score distillation loss (SDS) [Poole et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on leveraging pre-trained models for 3D content generation.",
      "processing_time": 57.894612312316895,
      "citing_paper_id": "259187900",
      "cited_paper_id": 253708074
    },
    {
      "context_text": "As directly rendering high-resolution images from neural implicit filed is very computationally expensive, a common solution is to render a low-resolution image, then up-sample it to a higher resolution for SDS training [Chen et al. 2023; Lin et al. 2022].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for up-sampling images.",
      "processing_time": 56.037602186203,
      "citing_paper_id": "259187900",
      "cited_paper_id": 253708074
    },
    {
      "context_text": "When it comes to 3D content generation, existing methods [Chen et al. 2023; Lin et al. 2022; Poole et al. 2023] leveraged pre-trained text-to-image diffusion models to supervise coordinate-based networks with score distillation loss (SDS) [Poole et al. 2023].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on leveraging pre-trained models for 3D content generation.",
      "processing_time": 58.051063776016235,
      "citing_paper_id": "259187900",
      "cited_paper_id": 253708074
    },
    {
      "context_text": "…modeling module, a bare rendering of SMPL model [Loper et al. 2015; Pavlakos et al. 2019] is trained into a neural implicit field [Wang et al. 2021] that consists of an SDF network 𝑓 ( 𝑥 ; 𝜃 ) and a color network 𝑐 ( 𝑥 ; 𝜃 ) , following prior works [Hong et al. 2022; Jiang et al. 2023b].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the technical aspects of training a neural implicit field using the SMPL model.",
      "processing_time": 58.74765872955322,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "It is worth noting that, our method enables the generation of avatars with loose garments and accessories, which cannot be achieved by CLIP-Actor [Youwang et al. 2022], AvatarCLIP [Hong et al. 2022], TEXTure [Richardson et al. 2023], AvatarCraft [Jiang et al. 2023b].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing the capabilities of different models in generating avatars.",
      "processing_time": 57.87801790237427,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Though the previous works [Hong et al. 2022; Jiang et al. 2023b] augment the rendering samples around the face to improve the facial details, they do not exploit the potential of the fine-tuned vision-language models, so their attempts cannot enhance the performance of personalized avatar…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the limitations of previous works in enhancing personalized avatars using fine-tuned vision-language models.",
      "processing_time": 59.113263845443726,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "As there are no official implementations for Avatar-Craft [Jiang et al. 2023b] and DreamAvatar [Cao et al. 2023], we compare the performance with the same setting in their experiments, as shown in Fig.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only comparisons with experimental settings from other papers.",
      "processing_time": 55.86758017539978,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Similar to AvatarCLIP [Hong et al. 2022] and AvatarCraft [Jiang et al. 2023b], we use only text prompts as input to generate avatars that conform to the description without fine-tuning the pre-trained diffusion models.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only text prompts as input for generating avatars. No verifiable resources are identified.",
      "processing_time": 57.12562680244446,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "More recently, following works like AvatarCraft [Jiang et al. 2023b] and DreamAvatar [Cao et al. 2023] have utilized diffusion models to produce high-quality 3D avatars.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of diffusion models for generating 3D avatars.",
      "processing_time": 58.16870093345642,
      "citing_paper_id": "259187900",
      "cited_paper_id": 254564767
    },
    {
      "context_text": "Moreover, some approaches [Metzer et al. 2022; Seo et al. 2023] adopt a neural radiance field to represent the latent space of Stable Diffusion, which enables the synthesis of novel views from text descriptions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of neural radiance fields and Stable Diffusion for generating 3D models from text.",
      "processing_time": 59.52082324028015,
      "citing_paper_id": "259187900",
      "cited_paper_id": 257985152
    },
    {
      "context_text": "* denotes that PATG achieves better performance than NRT [21] with statistical significance test with α = 0.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of performance between two methods.",
      "processing_time": 56.01302886009216,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "[21] propose a unified framework to jointly conduct rating prediction and abstractive tips generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for rating prediction and tips generation.",
      "processing_time": 56.128873348236084,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "and has not be considered in the previous work [21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only indicates that a certain aspect has not been considered in previous work.",
      "processing_time": 57.95403289794922,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "• The baseline methods used in tips quality evaluation: NRT [21], CTR [37], HFT [26].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions baseline methods but does not refer to any specific datasets. The context is focused on methods rather than datasets.",
      "processing_time": 57.11580419540405,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "Abstractive tips generation: We compare our framework PATG with the following baseline and state-of-the-art methods: • NRT [21]: It is a recent multi-task learning framework for rating prediction and abstractive tips generation achieving state-of-the-art performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'abstractive tips generation' and 'rating prediction', which suggests the use of a dataset containing ratings and tips. However, no specific dataset name is mentioned.",
      "processing_time": 59.464643716812134,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "In [21], the tip information was explored for tips generation and rating prediction for the first time.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the exploration of tip information for tips generation and rating prediction.",
      "processing_time": 56.69600200653076,
      "citing_paper_id": "70350032",
      "cited_paper_id": 304614
    },
    {
      "context_text": "• PMF: ProbabilisticMatrix Factorization [32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'ProbabilisticMatrix Factorization' which is a method, not a dataset. No specific dataset is mentioned in the context.",
      "processing_time": 57.9556622505188,
      "citing_paper_id": "70350032",
      "cited_paper_id": 467086
    },
    {
      "context_text": "• LexRank [7] is a classical method in the field of text summarization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions LexRank as a classical method in text summarization but does not refer to any specific dataset. LexRank itself is a method, not a dataset.",
      "processing_time": 58.67180633544922,
      "citing_paper_id": "70350032",
      "cited_paper_id": 506350
    },
    {
      "context_text": "Some previous works [9, 27, 43] have also shown that the performance of z is likely to be disturbed during the training procedure, especially when combining VAEs with the RNN based text generation framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context discusses the combination of VAEs and RNNs in text generation, which is relevant to personalized text generation but does not specify any datasets.",
      "processing_time": 60.75991916656494,
      "citing_paper_id": "70350032",
      "cited_paper_id": 605416
    },
    {
      "context_text": "Some previous works [9, 27, 43] have also shown that the performance of z is likely to be disturbed during the training procedure, especially when combining VAEs with the RNN based text generation framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context discusses the combination of VAEs and RNNs in text generation, which is relevant to personalized text generation but does not specify any datasets.",
      "processing_time": 60.75991916656494,
      "citing_paper_id": "70350032",
      "cited_paper_id": 8316629
    },
    {
      "context_text": "disturbed during the training procedure, especially when combining VAEs with the RNN based text generation framework. In order to enhance the performance of the typical VAEs, inspired by the ideas in [8] and [15], we employ the adversarial strategy for the training of VAEs. Generally, we design a discriminator network DaVAE with a vector x˜ as input, and the target is to recognize if x˜ is from the t",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of adversarial strategies in training VAEs, which is not a dataset.",
      "processing_time": 59.38042330741882,
      "citing_paper_id": "70350032",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "and Gated Recurrent Unit (GRU) [5] demonstrates high capability in text generation related tasks, such as abstractive summarization [19, 28, 31], dialogue systems [3, 33] and image caption generation [39]. Intheareaofrecommendationsystems,someresearchersalsoapply LSTM or GRU based RNN models on abstractive text generation. Tang et al. [35] propose a framework to generate context-aware reviews. Sentime",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 56.462918519973755,
      "citing_paper_id": "70350032",
      "cited_paper_id": 1055111
    },
    {
      "context_text": "[17] propose two methods to conduct the persona modeling for text generation in the area of dialog systems, but dialog systems have different characteristics with recommendation system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods for persona modeling in dialog systems.",
      "processing_time": 55.929362773895264,
      "citing_paper_id": "70350032",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "original output sˆt+1 according to the gate pд: sˆ′ t +1 = pд×t+1 +(1−д)× p t 1 (32) Then the tips sampling process can be conducted on sˆ′ t+1 . 2.3.4 Tips Quality Discriminator. Some previous works [40, 41] show that adversarial training strategy is beneficial to the text generation problem. To further improve the performance, we also employ this training strategy in our framework. The tips discriminato",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (adversarial training strategy) used in text generation.",
      "processing_time": 56.862932443618774,
      "citing_paper_id": "70350032",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "0] . We utilize the REINFORCE [38] method to integrate the tips quality signal V(S)into the tips generation framework to conduct the parameter learning. The details can be found in the existing works [18, 40, 41]. 3 EXPERIMENTAL SETUP 3.1 Datasets In our experiments, we use five datasets from different domains to evaluate our framework. The ratings of all these datasets are integers in the range of [1,5]. The",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions using 'five datasets from different domains' but does not specify their names. The citation is likely referring to datasets used for evaluation, but without specific names, they cannot be included.",
      "processing_time": 59.673110485076904,
      "citing_paper_id": "70350032",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "The details can be found in the existing works [18, 40, 41].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide any specific information about datasets or their usage. It only refers to existing works without mentioning any dataset names or details.",
      "processing_time": 57.55220150947571,
      "citing_paper_id": "70350032",
      "cited_paper_id": 4451272
    },
    {
      "context_text": "If the rating range is [0, 5], we will get the rating vector r̂u,i = (0, 0, 0, 0, 1, 0)T .",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only describes a rating vector in a specific context.",
      "processing_time": 56.67623519897461,
      "citing_paper_id": "70350032",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "f (·) can be the vanilla RNN, Long Short-Term Memory (LSTM) [10], or Gated Recurrent Unit (GRU) [5].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (RNN, LSTM, GRU).",
      "processing_time": 55.95798993110657,
      "citing_paper_id": "70350032",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "The ratings of all these datasets are integers in the range of [1, 5].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a generic reference to 'all these datasets'. No specific, verifiable datasets are identified.",
      "processing_time": 57.547831773757935,
      "citing_paper_id": "70350032",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Recently, sequence modeling based on the gated recurrent neural networks such as Long Short-Term Memory (LSTM) [10] and Gated Recurrent Unit (GRU) [5] demonstrates high capability in text generation related tasks, such as abstractive summarization [19, 28, 31], dialogue systems [3, 33] and image caption generation [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 56.503726959228516,
      "citing_paper_id": "70350032",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Recently, sequence modeling based on the gated recurrent neural networks such as Long Short-Term Memory (LSTM) [10] and Gated Recurrent Unit (GRU) [5] demonstrates high capability in text generation related tasks, such as abstractive summarization [19, 28, 31], dialogue systems [3, 33] and image caption generation [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 56.503726959228516,
      "citing_paper_id": "70350032",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Recently, sequence modeling based on the gated recurrent neural networks such as Long Short-Term Memory (LSTM) [10] and Gated Recurrent Unit (GRU) [5] demonstrates high capability in text generation related tasks, such as abstractive summarization [19, 28, 31], dialogue systems [3, 33] and image caption generation [39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and tasks. No verifiable resources are identified.",
      "processing_time": 56.503726959228516,
      "citing_paper_id": "70350032",
      "cited_paper_id": 52281331
    },
    {
      "context_text": "For the step of tips generation, we design a sequence decoding model based on a neural network of Gated Recurrent Units (GRUs) [5].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It only refers to a method (Gated Recurrent Units) used in the model.",
      "processing_time": 56.98233723640442,
      "citing_paper_id": "70350032",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "are employed to model the user preference from a generative perspective. It still only uses the rating matrix as input. •The baseline methods used in tips quality evaluation: NRT [21], CTR [37], HFT [26]. Ablation experiments: In order to demonstrate the performance of each component of our framework, we conduct the ablation experiments on the dataset Home. We compare the performance of our integrate",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Home"
      ],
      "dataset_descriptions": {
        "Home": "Used for ablation experiments to demonstrate the performance of each component of the framework, focusing on user preference modeling from a generative perspective."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Home' as a dataset used for ablation experiments, but it does not provide enough detail to confirm its nature or specific characteristics. The other names mentioned are models or methods, not datasets.",
      "processing_time": 66.38973331451416,
      "citing_paper_id": "70350032",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "ains a topic model component and it can generate topics for items. Then the most topic-similar sentence from the item historical tips is extracted as the tips. •HFTt: Hidden Factors and Hidden Topics [26] utilizes a topicmodelingtechniquetomodelthereviewtextsforrating prediction. Then we can design a tips extraction method HFTt using the similar technique in CTRt. Rating prediction: We compare of rati",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses methods and models for rating prediction and topic modeling but does not reference any named datasets.",
      "processing_time": 57.202661991119385,
      "citing_paper_id": "70350032",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "ned by Yelp1, as a compacted and concise form of reviews, have unique advantages for helping users get a quick insight over an item. Conventional reviews are extensively studied for rating prediction [26, 37] and review generation [6, 29, 35, 40], while tips are paid relatively less attention. In [21], the tip information was explored for tips generation and rating prediction for the first time. The ratio",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp"
      ],
      "dataset_descriptions": {
        "Yelp": "Used to explore tip information for tips generation and rating prediction, focusing on the unique advantages of tips over conventional reviews."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Yelp' as a source of data, specifically referring to 'tips' and 'reviews'. However, there is no specific dataset name mentioned, only a general reference to data from Yelp.",
      "processing_time": 65.87543487548828,
      "citing_paper_id": "70350032",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Adadelta [42] is used for gradient based optimization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Adadelta as a method for gradient-based optimization, which is not a dataset but an optimization algorithm.",
      "processing_time": 56.672613859176636,
      "citing_paper_id": "70350032",
      "cited_paper_id": 7365802
    },
    {
      "context_text": "d during the training procedure, especially when combining VAEs with the RNN based text generation framework. In order to enhance the performance of the typical VAEs, inspired by the ideas in [8] and [15], we employ the adversarial strategy for the training of VAEs. Generally, we design a discriminator network DaVAE with a vector x˜ as input, and the target is to recognize if x˜ is from the true data",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of VAEs and RNNs in text generation, but does not reference any named datasets.",
      "processing_time": 58.7657208442688,
      "citing_paper_id": "70350032",
      "cited_paper_id": 8785311
    },
    {
      "context_text": "[11], Liao et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide specific details about datasets used. The title suggests controlled text generation, but no dataset names are mentioned in the citation context.",
      "processing_time": 57.82135057449341,
      "citing_paper_id": "70350032",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "• SVD++: It extends Singular Value Decomposition by considering implicit feedback information for latent factor modeling [14].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SVD++).",
      "processing_time": 55.6295166015625,
      "citing_paper_id": "70350032",
      "cited_paper_id": 207168823
    },
    {
      "context_text": "Some previous works in recommendation systems [26, 30, 37] employ topic modeling methods such as Latent Dirichlet Allocation (LDA) [2] or its variants to analyze the text corpus and use the latent topic distribution to represent each document.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'text corpus' but does not specify a named dataset. It focuses on methods (LDA) rather than a specific dataset.",
      "processing_time": 57.4424569606781,
      "citing_paper_id": "70350032",
      "cited_paper_id": 215924728
    },
    {
      "context_text": "Attention mechanism is ﬁrst proposed for machine translation (Bahdanau et al., 2014; Cho et al., 2015), and is quickly applied to single-turn response generation afterwards (Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.042643785476685,
      "citing_paper_id": "14247119",
      "cited_paper_id": 1179542
    },
    {
      "context_text": "Attention mechanism is ﬁrst proposed for machine translation (Bahdanau et al., 2014; Cho et al., 2015), and is quickly applied to single-turn response generation afterwards (Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.042643785476685,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "In practice, we employ the beam search (Tillmann and Ney, 2003) technique to generate the n -best responses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a technique (beam search) but does not reference any specific dataset. The context is about the methodology used for generating responses.",
      "processing_time": 57.24888515472412,
      "citing_paper_id": "14247119",
      "cited_paper_id": 1782520
    },
    {
      "context_text": "A common practice to build a chatbot is to learn a response generation model within an encoder-decoder framework from large scale message-response pairs (Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general practice of using large scale message-response pairs for training chatbots.",
      "processing_time": 56.8760871887207,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "A common practice to build a chatbot is to learn a response generation model within an encoder-decoder framework from large scale message-response pairs (Shang et al., 2015; Vinyals and Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general practice of using large scale message-response pairs for training chatbots.",
      "processing_time": 56.8760871887207,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12985528
    },
    {
      "context_text": "Starting from the basic sequence to sequence model (Sutskever et al., 2014), various models (Shang et al., 2015; Vinyals and Le, 2015; Li et al., 2015; Xing et al., 2016; Li et al., 2016; ?",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.25367450714111,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "We followed the existing work and employed the following metrics: Perplexity : following (Vinyals and Le, 2015), we employed perplexity as an evaluation metric.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions perplexity as an evaluation metric but does not reference any specific dataset. The focus is on the method or metric itself.",
      "processing_time": 57.32795572280884,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "(Jafarpour et al., 2010).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not provide any specific dataset names or details about the usage of datasets. The title 'Filter, Rank, and Transfer the Knowledge: Learning to Chat' suggests a focus on chatbot development but does not mention specific datasets.",
      "processing_time": 60.15193772315979,
      "citing_paper_id": "14247119",
      "cited_paper_id": 12985528
    },
    {
      "context_text": "The decoder of HRAN is a RNN language model (Mikolov et al., 2010) conditioned on the context vectors { c t } Tt =1 given by Equation (4).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RNN language model).",
      "processing_time": 55.797261238098145,
      "citing_paper_id": "14247119",
      "cited_paper_id": 17048224
    },
    {
      "context_text": "Conversational agents include task-oriented dialog systems which are built in vertical domains for speciﬁc tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people ∗ The work was…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of conversational agents. No verifiable resources are identified.",
      "processing_time": 56.86940407752991,
      "citing_paper_id": "14247119",
      "cited_paper_id": 141250619
    },
    {
      "context_text": "Conversational agents include task-oriented dialog systems which are built in vertical domains for speciﬁc tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people ∗ The work was…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general types of conversational agents. No verifiable resources are identified.",
      "processing_time": 56.86940407752991,
      "citing_paper_id": "14247119",
      "cited_paper_id": 269035337
    },
    {
      "context_text": "the-art models for multi-turn response generation. 1 Introduction Conversational agents include task-oriented dialog systems which are built in vertical domains for speciﬁc tasks (Young et al., 2013; Boden, 2006; Wallace, 2009; Young et al., 2010), and non-task-oriented chatbots which aim to realize natural and human-like conversations with people The work was done when the ﬁrst author was an intern in Micro",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only introduces the concept of conversational agents and their types.",
      "processing_time": 57.5859637260437,
      "citing_paper_id": "14247119",
      "cited_paper_id": 269035337
    },
    {
      "context_text": "Numerous works have explored learning user embeddings as a way to capture syntactic and semantic properties of the language of individuals (Hovy, 2015; Ould Amer et al., 2016; Zeng et al., 2017; Welch et al., 2020b,a; Rocca and Yarkoni, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works exploring user embeddings. No verifiable resources are identified.",
      "processing_time": 57.062363624572754,
      "citing_paper_id": "267523283",
      "cited_paper_id": 9135567
    },
    {
      "context_text": "Numerous works have explored learning user embeddings as a way to capture syntactic and semantic properties of the language of individuals (Hovy, 2015; Ould Amer et al., 2016; Zeng et al., 2017; Welch et al., 2020b,a; Rocca and Yarkoni, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works exploring user embeddings. No verifiable resources are identified.",
      "processing_time": 57.062363624572754,
      "citing_paper_id": "267523283",
      "cited_paper_id": 14021168
    },
    {
      "context_text": "Numerous works have explored learning user embeddings as a way to capture syntactic and semantic properties of the language of individuals (Hovy, 2015; Ould Amer et al., 2016; Zeng et al., 2017; Welch et al., 2020b,a; Rocca and Yarkoni, 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works exploring user embeddings. No verifiable resources are identified.",
      "processing_time": 57.062363624572754,
      "citing_paper_id": "267523283",
      "cited_paper_id": 256631001
    },
    {
      "context_text": "IMDb62 contains 62,000 movie reviews written by 62 prolific users of the Internet Movie Database (IMDb) where each user wrote 1,000 reviews (Seroussi et al., 2014).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "IMDb62"
      ],
      "dataset_descriptions": {
        "IMDb62": "Used to study authorship attribution and personalized text generation, focusing on the stylistic and thematic consistency in movie reviews written by prolific users."
      },
      "confidence_score": 1.0,
      "reasoning": "IMDb62 is a specific dataset containing movie reviews, which is relevant for personalized text generation research.",
      "processing_time": 64.10389280319214,
      "citing_paper_id": "267523283",
      "cited_paper_id": 11860229
    },
    {
      "context_text": "We model 3 RST relations in total.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only RST relations which are part of a theoretical framework.",
      "processing_time": 56.275941610336304,
      "citing_paper_id": "267523283",
      "cited_paper_id": 60514661
    },
    {
      "context_text": "We use the publicly available RST parser 3 introduced by Heilman and Sagae (2015) to obtain RST relations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a parser tool, not a dataset. The RST parser is a method or tool, not a dataset.",
      "processing_time": 56.732683181762695,
      "citing_paper_id": "267523283",
      "cited_paper_id": 60514661
    },
    {
      "context_text": "Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the most influential approaches for document-level discourse analysis.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Rhetorical Structure Theory (RST) but does not refer to a specific dataset. It is cited as a method or theoretical framework.",
      "processing_time": 57.79514527320862,
      "citing_paper_id": "267523283",
      "cited_paper_id": 60514661
    },
    {
      "context_text": "In our work, we consider the frequency of the RST relations for each author’s text to capture discourse coherence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the use of RST relations in capturing discourse coherence, which is a methodological approach rather than a dataset.",
      "processing_time": 58.995792388916016,
      "citing_paper_id": "267523283",
      "cited_paper_id": 60514661
    },
    {
      "context_text": "Our work is closely related to King and Cook (2020), who analyzed various methods, such as interpolation, fine-tuning, and priming language models for the personalization of general-purpose language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the analysis of various methods for personalizing language models.",
      "processing_time": 57.62960481643677,
      "citing_paper_id": "267523283",
      "cited_paper_id": 218973759
    },
    {
      "context_text": "The models were pretrained using the Pile dataset (Gao et al., 2020), an 825GB English dataset containing texts from 22 diverse sources, roughly broken down into five categories: academic writing, internet, prose, dialogue, and miscellaneous.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "The Pile"
      ],
      "dataset_descriptions": {
        "The Pile": "Used to pretrain models, focusing on a diverse 825GB English dataset with texts from 22 sources across academic, internet, prose, dialogue, and miscellaneous categories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'The Pile dataset' as a specific, verifiable dataset used for pretraining models. It provides details about the dataset's size and content, which aligns with the criteria for inclusion.",
      "processing_time": 69.1422860622406,
      "citing_paper_id": "267523283",
      "cited_paper_id": 230435736
    },
    {
      "context_text": "…and Author 2, respectively. and Yeung, 2018; Bingel et al., 2018), Dialogue Modeling (Zhang et al., 2018; Mazaré et al., 2018; Madotto et al., 2019; Ma et al., 2021; Zhong et al., 2022), Machine Translation (Rabinovich et al., 2017), Grammatical Error Correction (Nadejde and Tetreault, 2019),…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and papers. No verifiable resources are identified.",
      "processing_time": 56.96516132354736,
      "citing_paper_id": "267523283",
      "cited_paper_id": 235792273
    },
    {
      "context_text": "Underlined and bold words refer to verbs. studies (Mireshghallah et al., 2022; Zhong et al., 2021; Oba et al., 2023) have explored leveraging techniques such as adapters for adapting LLMs for various personalized NLP and NLG tasks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies exploring techniques for personalizing NLP and NLG tasks.",
      "processing_time": 57.382033348083496,
      "citing_paper_id": "267523283",
      "cited_paper_id": 238252929
    },
    {
      "context_text": "…models by fine-tuning pretrained models with attribute-specific corpora or training conditional generative networks (Keskar et al., 2019; Dathathri et al., 2020; Lample et al., 2019; Lo-geswaran et al., 2018; Krause et al., 2021; Russo et al., 2020; Yu et al., 2021; Kulkarni et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers discussing methods and models. No verifiable resources are identified.",
      "processing_time": 57.36961483955383,
      "citing_paper_id": "267523283",
      "cited_paper_id": 239050492
    },
    {
      "context_text": "Some of the noteworthy lines of research have proposed techniques such as Prefix-Tuning (Qian et al., 2022), Adapters (Houlsby et al., 2019), and Prompt-tuning (Yang et al., 2023; Chen et al., 2023) to approach the problem.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and techniques. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.80718207359314,
      "citing_paper_id": "267523283",
      "cited_paper_id": 257353840
    },
    {
      "context_text": "To evaluate our proposed benchmark and given the ubiquitous adoption of causal language models (CLMs) for various NLP tasks recently, we consider the models from the Pythia Scaling Suite (Biderman et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Pythia Scaling Suite' but does not indicate it is a dataset. It is referenced as a suite of models, which is excluded according to the instructions.",
      "processing_time": 59.386600732803345,
      "citing_paper_id": "267523283",
      "cited_paper_id": 257921893
    },
    {
      "context_text": "In recent years, there has been a flurry of writing and conversational assistants that harness the power of LLMs at each stage of the writing life-cycle (Raheja et al., 2023; Gómez-Rodríguez and Williams, 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to writing and conversational assistants using LLMs.",
      "processing_time": 56.82835555076599,
      "citing_paper_id": "267523283",
      "cited_paper_id": 258741409
    },
    {
      "context_text": "Hyperparameters We use Hugging Face’s trans-formers (Wolf et al., 2020) to fine-tune our models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using Hugging Face’s transformers library but does not reference any specific datasets. The context is about the tools and methods used, not datasets.",
      "processing_time": 58.29251432418823,
      "citing_paper_id": "267523283",
      "cited_paper_id": null
    },
    {
      "context_text": "P CS , we use Llama-2-7B (Touvron et al., 2023) as the base LLM and BM25 (Trotman et al., 2014) for retrieval operations to ensure efficient and fair comparisons.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Llama-2-7B and BM25, but both are models/methods and not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 59.09389686584473,
      "citing_paper_id": "270560467",
      "cited_paper_id": 207220720
    },
    {
      "context_text": "…user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al., 2021; Pratama et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications and research areas. The cited paper titles do not provide additional context to identify a specific dataset.",
      "processing_time": 58.44356083869934,
      "citing_paper_id": "270560467",
      "cited_paper_id": 210883769
    },
    {
      "context_text": "…be available at https://github.com/ TamSiuhin/Per-Pcs 2023b; Wu et al., 2023; Baek et al., 2023), user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It primarily references methods, tools, and research works.",
      "processing_time": 56.990025997161865,
      "citing_paper_id": "270560467",
      "cited_paper_id": 219708533
    },
    {
      "context_text": "…1 Code will be available at https://github.com/ TamSiuhin/Per-Pcs 2023b; Wu et al., 2023; Baek et al., 2023), user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023),…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and research works. No verifiable resources are identified.",
      "processing_time": 56.98761224746704,
      "citing_paper_id": "270560467",
      "cited_paper_id": 219708533
    },
    {
      "context_text": "P CS is built with the help of many existing scientific artifacts, including PyTorch (Paszke et al., 2019), Numpy (Harris et al., 2020), huggingface, and transformers (Wolf et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several tools and libraries but does not refer to any specific datasets. The context is about building a system using these tools, not about using datasets for research.",
      "processing_time": 58.771438121795654,
      "citing_paper_id": "270560467",
      "cited_paper_id": 219792763
    },
    {
      "context_text": "…user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al., 2021; Pratama et al., 2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application such as user simulation, personalized chatbot, user profiling, healthcare, and education. No verifiable resources are named.",
      "processing_time": 59.55308771133423,
      "citing_paper_id": "270560467",
      "cited_paper_id": 221863152
    },
    {
      "context_text": "…user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al., 2021; Pratama et al., 2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application such as user simulation, personalized chatbot, user profiling, healthcare, and education. No verifiable resources are named.",
      "processing_time": 59.55308771133423,
      "citing_paper_id": "270560467",
      "cited_paper_id": 225484271
    },
    {
      "context_text": "…user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al., 2021; Pratama et al., 2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application such as user simulation, personalized chatbot, user profiling, healthcare, and education. No verifiable resources are named.",
      "processing_time": 59.55308771133423,
      "citing_paper_id": "270560467",
      "cited_paper_id": 257766541
    },
    {
      "context_text": "…user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al., 2021), and education (Alamri et al., 2021; Pratama et al., 2023).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application such as user simulation, personalized chatbot, user profiling, healthcare, and education. No verifiable resources are named.",
      "processing_time": 59.55308771133423,
      "citing_paper_id": "270560467",
      "cited_paper_id": 268216550
    },
    {
      "context_text": "These abilities include step-by-step reasoning (Wei et al., 2022b), in-context learning (Min et al., 2022), and instruction following (Wei et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only abilities of models. No verifiable resources are identified.",
      "processing_time": 56.50984287261963,
      "citing_paper_id": "270560467",
      "cited_paper_id": 221863152
    },
    {
      "context_text": "These abilities include step-by-step reasoning (Wei et al., 2022b), in-context learning (Min et al., 2022), and instruction following (Wei et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only abilities of models. No verifiable resources are identified.",
      "processing_time": 56.50984287261963,
      "citing_paper_id": "270560467",
      "cited_paper_id": 237416585
    },
    {
      "context_text": "Model-level composition methods treat the entire model parameter as the minimum composition unit (Wortsman et al., 2022; Choshen et al., 2022; Ramé et al., 2023; Jin et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model-level composition methods. No verifiable resources are identified.",
      "processing_time": 56.93258547782898,
      "citing_paper_id": "270560467",
      "cited_paper_id": 248006458
    },
    {
      "context_text": "Model-level composition methods treat the entire model parameter as the minimum composition unit (Wortsman et al., 2022; Choshen et al., 2022; Ramé et al., 2023; Jin et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model-level composition methods. No verifiable resources are identified.",
      "processing_time": 56.93258547782898,
      "citing_paper_id": "270560467",
      "cited_paper_id": 273901551
    },
    {
      "context_text": "Prompt-based personalization involves designing prompt templates to help LLMs understand user preferences, using methods such as vanilla personalized prompting (Dai et al., 2023), retrieval-augmented prompting (Mysore et al., 2023), and profile-augmented prompting (Richardson et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for personalized prompting. No verifiable resources are identified.",
      "processing_time": 56.73275542259216,
      "citing_paper_id": "270560467",
      "cited_paper_id": 258461170
    },
    {
      "context_text": "Vanilla personalized prompting leverages LLMs’ in-context learning and few-shot learning abilities by encoding either complete or randomly sampled user history behaviors as contextual examples (Dai et al., 2023; Wang et al., 2023; Kang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses methods and approaches for personalized prompting using LLMs.",
      "processing_time": 57.53610062599182,
      "citing_paper_id": "270560467",
      "cited_paper_id": 258461170
    },
    {
      "context_text": "Vanilla personalized prompting leverages LLMs’ in-context learning and few-shot learning abilities by encoding either complete or randomly sampled user history behaviors as contextual examples (Dai et al., 2023; Wang et al., 2023; Kang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only discusses methods and approaches for personalized prompting using LLMs.",
      "processing_time": 57.53610062599182,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "LoRAHub (Huang et al., 2023) uses a black-box optimizer to integrate specialized LoRAs, facilitating generalization to unseen tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LoRAHub) and its application. The context focuses on the method's ability to generalize to unseen tasks.",
      "processing_time": 59.01537489891052,
      "citing_paper_id": "270560467",
      "cited_paper_id": 260155012
    },
    {
      "context_text": "Consequently, personalizing LLMs to align with users’ unique needs has become a crucial research focus (Li et al., 2023a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general research focus on personalizing LLMs.",
      "processing_time": 56.31969738006592,
      "citing_paper_id": "270560467",
      "cited_paper_id": 260926523
    },
    {
      "context_text": "Another line of work focuses on designing personalized alignment methods via parameter merging (Jang et al., 2023a), personalized RLHF (Li et al., 2024; Park et al., 2024), personalized reward models (Cheng et al., 2023), and black-box LLM personalization (Zhuang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 57.23411846160889,
      "citing_paper_id": "270560467",
      "cited_paper_id": 264289231
    },
    {
      "context_text": "Another line of work focuses on designing personalized alignment methods via parameter merging (Jang et al., 2023a), personalized RLHF (Li et al., 2024; Park et al., 2024), personalized reward models (Cheng et al., 2023), and black-box LLM personalization (Zhuang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 57.23411846160889,
      "citing_paper_id": "270560467",
      "cited_paper_id": 267547503
    },
    {
      "context_text": "Another line of work focuses on designing personalized alignment methods via parameter merging (Jang et al., 2023a), personalized RLHF (Li et al., 2024; Park et al., 2024), personalized reward models (Cheng et al., 2023), and black-box LLM personalization (Zhuang et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 57.23411846160889,
      "citing_paper_id": "270560467",
      "cited_paper_id": 270257981
    },
    {
      "context_text": "…important in content recommendation (Li et al., 1 Code will be available at https://github.com/ TamSiuhin/Per-Pcs 2023b; Wu et al., 2023; Baek et al., 2023), user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021),…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications and papers. No clear, verifiable datasets are identified.",
      "processing_time": 56.89491057395935,
      "citing_paper_id": "270560467",
      "cited_paper_id": 265150332
    },
    {
      "context_text": "By composing PEFT parameters, models can achieve task and domain generalization (Shah et al., 2023; Gou et al., 2023; Zhang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 57.56509184837341,
      "citing_paper_id": "270560467",
      "cited_paper_id": 265351656
    },
    {
      "context_text": "By composing PEFT parameters, models can achieve task and domain generalization (Shah et al., 2023; Gou et al., 2023; Zhang et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to models and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 57.56509184837341,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "…(Li et al., 1 Code will be available at https://github.com/ TamSiuhin/Per-Pcs 2023b; Wu et al., 2023; Baek et al., 2023), user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers and methods. There are no clear identifiers for datasets.",
      "processing_time": 56.91881990432739,
      "citing_paper_id": "270560467",
      "cited_paper_id": 266135411
    },
    {
      "context_text": "To manage the rapidly growing user behavior and LLMs’ limited context window, researchers have proposed retrieval-augmented methods for personalized LLMs (Salemi et al., 2023), and enhance the calibration (Mysore et al., 2023) and optimize retrieval (Salemi et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalizing LLMs. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 58.570847034454346,
      "citing_paper_id": "270560467",
      "cited_paper_id": 269009728
    },
    {
      "context_text": "Personalization involves mining user’s history data to tailor and customize a system’s interaction, content, or recommendations to meet the specific needs, preferences, and characteristics, of individual users (Tan and Jiang, 2023; Chen et al., 2023; Kirk et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of personalization using user history data. No verifiable resources are identified.",
      "processing_time": 57.25613284111023,
      "citing_paper_id": "270560467",
      "cited_paper_id": 269348048
    },
    {
      "context_text": "Personalization involves mining user’s history data to tailor and customize a system’s interaction, content, or recommendations to meet the specific needs, preferences, and characteristics, of individual users (Tan and Jiang, 2023; Chen et al., 2023; Kirk et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of personalization using user history data. No verifiable resources are identified.",
      "processing_time": 57.25613284111023,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "Personalization involves mining user’s history data to tailor and customize a system’s interaction, content, or recommendations to meet the specific needs, preferences, and characteristics, of individual users (Tan and Jiang, 2023; Chen et al., 2023; Kirk et al., 2024).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of personalization using user history data. No verifiable resources are identified.",
      "processing_time": 57.25613284111023,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "Inspired by the exhaustiveness of human preferences (Lee et al., 2024), we propose the P ER - SONALIZED P IECES (P ER -",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not provide specific dataset names, only a reference to a method or system (PERSONALIZED PIECES). No clear, verifiable datasets are mentioned.",
      "processing_time": 58.41052556037903,
      "citing_paper_id": "270560467",
      "cited_paper_id": 270067579
    },
    {
      "context_text": "Motivated by the exhaustiveness of human preferences (Lee et al., 2024), we assemble PEFT modules for target users using the PEFT modules and gate vectors from sharers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to PEFT modules and gate vectors, which are methods or components, not datasets.",
      "processing_time": 57.94163489341736,
      "citing_paper_id": "270560467",
      "cited_paper_id": 270067579
    },
    {
      "context_text": "Despite these capabilities, current LLMs adhere to a \"one-size-fits-all\" paradigm, being trained on broad, domain-agnostic data, which limits their effectiveness in adapting to individual user preferences (Chen et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'domain-agnostic data'. No verifiable resources are identified.",
      "processing_time": 57.53137540817261,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "Ilharco et al. (2022) propose the task vector, which sub-tracts the weights of a fine-tuned model from the pre-trained weights and conducts task vector arithmetic to enable generalization across tasks and domains.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving task vectors and model weights.",
      "processing_time": 55.9849693775177,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing work has shown that performing weighted linear interpolation of model parameters leads to the composition of each model ability (Li et al., 2022; Tam et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to existing work on model parameter interpolation.",
      "processing_time": 56.03588104248047,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "…TamSiuhin/Per-Pcs 2023b; Wu et al., 2023; Baek et al., 2023), user simulation (Dejescu et al., 2023; Zhang and Balog, 2020), personalized chatbot (Srivastava et al., 2020; Ma et al., 2021), user profiling (Gu et al., 2020; Gao et al., 2023), healthcare (Johnson et al., 2021; Gold-enberg et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 58.12836980819702,
      "citing_paper_id": "270560467",
      "cited_paper_id": null
    },
    {
      "context_text": "Recent works [Li et al., 2016a; Wen et al., 2015] mitigate this by either using a diversity promoting objective or re-ranking multiple responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on techniques for generating diverse responses in dialogue systems.",
      "processing_time": 57.49955725669861,
      "citing_paper_id": "20956365",
      "cited_paper_id": 1139492
    },
    {
      "context_text": "In particular, dialog or conversational models too have received a lot of attention due to its wide range of applications in human-machine interaction such as personal assistants, technical support for products and services, entertainment, to name a few [Lowe et al., 2015; Serban et al., 2016a; Wen et al., 2015; Li et al., 2016b; Sordoni et al., 2015].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Ubuntu Dialogue Corpus"
      ],
      "dataset_descriptions": {
        "Ubuntu Dialogue Corpus": "Used to develop and evaluate unstructured multi-turn dialogue systems, providing a large dataset for training and testing conversational models."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions dialog and conversational models but does not specify any datasets. However, the cited paper 'The Ubuntu Dialogue Corpus' suggests a relevant dataset.",
      "processing_time": 64.02613258361816,
      "citing_paper_id": "20956365",
      "cited_paper_id": 1139492
    },
    {
      "context_text": "In particular, dialog or conversational models too have received a lot of attention due to its wide range of applications in human-machine interaction such as personal assistants, technical support for products and services, entertainment, to name a few [Lowe et al., 2015; Serban et al., 2016a; Wen et al., 2015; Li et al., 2016b; Sordoni et al., 2015].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Ubuntu Dialogue Corpus"
      ],
      "dataset_descriptions": {
        "Ubuntu Dialogue Corpus": "Used to develop and evaluate unstructured multi-turn dialogue systems, providing a large dataset for training and testing conversational models."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions dialog and conversational models but does not specify any datasets. However, the cited paper 'The Ubuntu Dialogue Corpus' suggests a relevant dataset.",
      "processing_time": 64.02613258361816,
      "citing_paper_id": "20956365",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "…received a lot of attention due to its wide range of applications in human-machine interaction such as personal assistants, technical support for products and services, entertainment, to name a few [Lowe et al., 2015; Serban et al., 2016a; Wen et al., 2015; Li et al., 2016b; Sordoni et al., 2015].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 58.52276015281677,
      "citing_paper_id": "20956365",
      "cited_paper_id": 1139492
    },
    {
      "context_text": "…received a lot of attention due to its wide range of applications in human-machine interaction such as personal assistants, technical support for products and services, entertainment, to name a few [Lowe et al., 2015; Serban et al., 2016a; Wen et al., 2015; Li et al., 2016b; Sordoni et al., 2015].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 58.52276015281677,
      "citing_paper_id": "20956365",
      "cited_paper_id": 8379583
    },
    {
      "context_text": "…received a lot of attention due to its wide range of applications in human-machine interaction such as personal assistants, technical support for products and services, entertainment, to name a few [Lowe et al., 2015; Serban et al., 2016a; Wen et al., 2015; Li et al., 2016b; Sordoni et al., 2015].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. The cited papers' titles do not provide additional context to identify specific datasets.",
      "processing_time": 58.52276015281677,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "GRU [Cho et al., 2014] is used to model a sequence of inputs, e.g., words in a sentence in our case.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions GRU but does not refer to any specific dataset. GRU is a method, not a dataset.",
      "processing_time": 56.55638122558594,
      "citing_paper_id": "20956365",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "In this section, we first introduce notation and briefly overview GRU [Cho et al., 2014], the underlying recurrent neural network (RNN) for all the models.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GRU).",
      "processing_time": 55.41534161567688,
      "citing_paper_id": "20956365",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Further improvements on this generative modeling idea were later explored in [Li et al., 2016b], [Sordoni et al., 2015], [Serban et al., 2015a] and [Galley et al., 2015].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.8643913269043,
      "citing_paper_id": "20956365",
      "cited_paper_id": 6078795
    },
    {
      "context_text": "Works on distributed representation of language [Mikolov and Dean, 2013], neural language models [Bengio et al., 2003] and sequence-tosequence learning [Sutskever et al., 2014] have significantly changed the state-of-the-art landscape in NLP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 56.637269020080566,
      "citing_paper_id": "20956365",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Works on distributed representation of language [Mikolov and Dean, 2013], neural language models [Bengio et al., 2003] and sequence-tosequence learning [Sutskever et al., 2014] have significantly changed the state-of-the-art landscape in NLP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. No verifiable resources are identified.",
      "processing_time": 56.637269020080566,
      "citing_paper_id": "20956365",
      "cited_paper_id": null
    },
    {
      "context_text": "Context-only: The Hierarchical Recurrent EncoderDecoder [Serban et al., 2016b] captures contextual cues through a high level, context RNN that updates its hidden for every turn in a conversation and is learnt on top of encoderdecoder RNNs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Hierarchical Recurrent Encoder-Decoder). The cited paper title suggests a dataset but it is not mentioned in the context.",
      "processing_time": 59.21642804145813,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "Our model Context-aware, Persona-based Hierarchical Encoder-Decoder (CoPerHED) is a hybrid of persona-based [Li et al., 2016b] and context-aware [Serban et al., 2016b] neural conversational models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about the architecture of a neural conversational model.",
      "processing_time": 57.407975912094116,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "Recently proposed models [Sordoni et al., 2015; Li et al., 2016b; Serban et al., 2016b] have shown success in training neural dialog systems that return semantic and syntactic responses for a given input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance in training neural dialog systems.",
      "processing_time": 56.08301520347595,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "High frequency of such phrases in the train dataset could be a potential reason for such a phenomenon [Serban et al., 2016b].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset name. It only refers to a 'train dataset' without providing a specific name or identifier.",
      "processing_time": 57.82007837295532,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "Similar to [Serban et al., 2016b], CoPerHED constitutes a hierarchy of RNNs with context RNN working at a turn-level in a conversation while language RNN (GRU in this case) works at a word-level in every turn.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CoPerHED) which is a hierarchy of RNNs. The cited paper title suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 60.51541757583618,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "Following [Serban et al., 2016b], we use perplexity in our study.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a metric (perplexity). The cited paper title suggests a dataset, but it is not mentioned in the citation context.",
      "processing_time": 59.12403464317322,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "Context-awareness is achieved similar to the hierarchical model in [Serban et al., 2016b], where an additional RNN summarizes information at the level of turns.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context is about achieving context-awareness using a hierarchical model, which is not a dataset.",
      "processing_time": 59.10902738571167,
      "citing_paper_id": "20956365",
      "cited_paper_id": 12241221
    },
    {
      "context_text": "We use the deep learning framework Torch [Collobert et al., 2011], to build and train our models by minimizing the loglikelihood of tokens predicted by the language model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using Torch, which is a deep learning framework, not a dataset. No datasets are explicitly mentioned.",
      "processing_time": 56.669084548950195,
      "citing_paper_id": "20956365",
      "cited_paper_id": 14365368
    },
    {
      "context_text": "For instance, Ameixa et al [Ameixa et al., 2014] used movie subtitles to retrieve good answers to out-of-domain questions.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'movie subtitles' as a data source used to retrieve answers to out-of-domain questions. However, it does not specify a named dataset.",
      "processing_time": 58.63465452194214,
      "citing_paper_id": "20956365",
      "cited_paper_id": 15956725
    },
    {
      "context_text": "SubTle dataset The third dataset we consider is the SubTle [Ameixa et al., 2014], which is an enormous, non-dialog corpus extracted from movie subtitles.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SubTle"
      ],
      "dataset_descriptions": {
        "SubTle": "Used to study non-dialog text extracted from movie subtitles, focusing on the linguistic patterns and structures in large-scale subtitle data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'SubTle' as a specific dataset used in the research, which is a corpus of movie subtitles. It is clearly identified and used for research purposes.",
      "processing_time": 64.99990797042847,
      "citing_paper_id": "20956365",
      "cited_paper_id": 15956725
    },
    {
      "context_text": ", 2015b] and tag named entities using the NER tagger from the standard NLTK library [Bird et al., 2009] and replace them with placeholders (e.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 57.302464723587036,
      "citing_paper_id": "20956365",
      "cited_paper_id": null
    },
    {
      "context_text": "We follow [Serban et al., 2015b] and tag named entities using the NER tagger from the standard NLTK library [Bird et al., 2009] and replace them with placeholders (e.g.<PERSON>, <PLACE>, etc.).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (NER tagger from NLTK) and a process (replacing named entities with placeholders).",
      "processing_time": 59.56362533569336,
      "citing_paper_id": "20956365",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2003] and sequence-tosequence learning [Sutskever et al., 2014] have significantly changed the state-of-the-art landscape in NLP.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in NLP techniques.",
      "processing_time": 55.81909394264221,
      "citing_paper_id": "20956365",
      "cited_paper_id": null
    },
    {
      "context_text": "Focusing on text generation, the methods in Miao et al. (2017, 2016); Srivastava and Sutton (2017) represent texts as bag-of-words, and Bowman et al. (2015) proposed the usage of an RNN as the encoder and decoder, and found some negative results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and findings. The context focuses on describing various approaches to text generation, including bag-of-words representations and RNN-based encoders and decoders.",
      "processing_time": 59.93408727645874,
      "citing_paper_id": "81982679",
      "cited_paper_id": 10796
    },
    {
      "context_text": ", 2014), dialogue generation (Li et al., 2017a), and text summarization (Nallapati et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and papers. No verifiable resources are identified.",
      "processing_time": 56.81752109527588,
      "citing_paper_id": "81982679",
      "cited_paper_id": 98180
    },
    {
      "context_text": "…also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image cap-tioning (Pu et al., 2016), dialogue generation (Ser-ban et al., 2017; Shen et al., 2017b; Zhao et al., 2017) and text summarization (Li et al., 2017b; Miao and Blunsom, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of conditional text generation. No dataset names are present in the text.",
      "processing_time": 57.450058698654175,
      "citing_paper_id": "81982679",
      "cited_paper_id": 98180
    },
    {
      "context_text": "…also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image cap-tioning (Pu et al., 2016), dialogue generation (Ser-ban et al., 2017; Shen et al., 2017b; Zhao et al., 2017) and text summarization (Li et al., 2017b; Miao and Blunsom, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of conditional text generation. No dataset names are present in the text.",
      "processing_time": 57.450058698654175,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1240989
    },
    {
      "context_text": "…also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image cap-tioning (Pu et al., 2016), dialogue generation (Ser-ban et al., 2017; Shen et al., 2017b; Zhao et al., 2017) and text summarization (Li et al., 2017b; Miao and Blunsom, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of conditional text generation. No dataset names are present in the text.",
      "processing_time": 57.450058698654175,
      "citing_paper_id": "81982679",
      "cited_paper_id": 10480989
    },
    {
      "context_text": "Text generation plays an important role in various natural language processing (NLP) applications, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), dialogue generation (Li et al., 2017a), and text summarization (Nallapati et al., 2016; Rush et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation in NLP. No verifiable resources are identified.",
      "processing_time": 57.65841460227966,
      "citing_paper_id": "81982679",
      "cited_paper_id": 98180
    },
    {
      "context_text": "Text generation plays an important role in various natural language processing (NLP) applications, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), dialogue generation (Li et al., 2017a), and text summarization (Nallapati et al., 2016; Rush et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation in NLP. No verifiable resources are identified.",
      "processing_time": 57.65841460227966,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "“LT” denotes a linear transformation. using less conditional information while generating each word) (Yang et al., 2017; Shen et al., 2017a), or bridging the amortization gap (between the log-likelihood and the ELBO) using semi-amortized inference networks (Kim et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on techniques for improving text generation, such as linear transformations and semi-amortized inference networks.",
      "processing_time": 59.19996523857117,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1240989
    },
    {
      "context_text": "In order to improve the performance, different convolutional designs (Semeniuta et al., 2017; Shen et al., 2017a; Yang et al., 2017) have been proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria.",
      "processing_time": 57.44019961357117,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1240989
    },
    {
      "context_text": "using less conditional information while generating each word) (Yang et al., 2017; Shen et al., 2017a), or bridging the amortization gap (between the loglikelihood and the ELBO) using semi-amortized inference networks (Kim et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.276689767837524,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1240989
    },
    {
      "context_text": "Though no closedform solutions exist, the KL divergence has an explicit upper bound (Dilokthanakul et al., 2016), shown in Proposition 1.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical result about KL divergence.",
      "processing_time": 55.705732345581055,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1327363
    },
    {
      "context_text": "Though no closed-form solutions exist, the KL divergence has an explicit upper bound (Dilokthanakul et al., 2016), shown in Proposition 1.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical result about KL divergence.",
      "processing_time": 55.57280993461609,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1327363
    },
    {
      "context_text": ", 2016), they are often designed with pre-defined parameter settings without incorporating semantic meanings into the latent codes, which may lead to over-regularization (Dilokthanakul et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects of clustering with variational autoencoders.",
      "processing_time": 56.731241941452026,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1327363
    },
    {
      "context_text": "Although some variants of VAE try to impose some structure on the latent codes (Jiang et al., 2016; Dilokthanakul et al., 2016), they are often designed with pre-defined parameter settings without incorporating semantic meanings into the latent codes, which may lead to over-regularization (Dilokthanakul et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological approaches and potential issues with VAE variants.",
      "processing_time": 56.360817432403564,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1327363
    },
    {
      "context_text": "…of VAE try to impose some structure on the latent codes (Jiang et al., 2016; Dilok-thanakul et al., 2016), they are often designed with pre-deﬁned parameter settings without incorporating semantic meanings into the latent codes, which may lead to over-regularization (Dilokthanakul et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects of VAEs and clustering.",
      "processing_time": 56.358378887176514,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1327363
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1481635
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1930231
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 13663262
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 13692090
    },
    {
      "context_text": "The VAE was proposed by Kingma and Welling (2013), and since then, it has been applied successfully in a variety of applications (Gregor et al., 2015; Kingma et al., 2014; Chen et al., 2017; Wang et al., 2018b; Shen et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of VAEs. No dataset names are present in the text.",
      "processing_time": 57.04777932167053,
      "citing_paper_id": "81982679",
      "cited_paper_id": 48353867
    },
    {
      "context_text": "It is worthwhile to note that recently several much more complex CNN/RNN architectures have been proposed for abstract text summarization, such as SEASS (Zhou et al., 2017), ConvS2S (Gehring et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.928932428359985,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1770102
    },
    {
      "context_text": "It is worthwhile to note that recently several much more complex CNN/RNN architectures have been proposed for abstract text summarization, such as SEASS (Zhou et al., 2017), ConvS2S (Gehring et al., 2017), and Reinforced-ConvS2S (Wang et al., 2018a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (SEASS, ConvS2S, Reinforced-ConvS2S) but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 60.03047513961792,
      "citing_paper_id": "81982679",
      "cited_paper_id": 1770102
    },
    {
      "context_text": "It is worthwhile to note that recently several much more complex CNN/RNN architectures have been proposed for abstract text summarization, such as SEASS (Zhou et al., 2017), ConvS2S (Gehring et al., 2017), and Reinforced-ConvS2S (Wang et al., 2018a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (SEASS, ConvS2S, Reinforced-ConvS2S) but does not refer to any specific datasets. The cited papers also do not provide additional context about datasets.",
      "processing_time": 60.03047513961792,
      "citing_paper_id": "81982679",
      "cited_paper_id": 13663262
    },
    {
      "context_text": "Following Xie et al. (2015); Miao et al. (2017), we apply a topic diversity regularization while carrying out the inference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for applying topic diversity regularization.",
      "processing_time": 55.506447315216064,
      "citing_paper_id": "81982679",
      "cited_paper_id": 2326916
    },
    {
      "context_text": "The idea of using learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embed-dings…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the combination of topic models and neural language models, which are not datasets.",
      "processing_time": 58.23361158370972,
      "citing_paper_id": "81982679",
      "cited_paper_id": 2600027
    },
    {
      "context_text": "The idea of using learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embeddings (Liu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.80560564994812,
      "citing_paper_id": "81982679",
      "cited_paper_id": 2600027
    },
    {
      "context_text": "The idea of using learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embeddings (Liu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.80560564994812,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3505576
    },
    {
      "context_text": "The idea of using learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embeddings (Liu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.80560564994812,
      "citing_paper_id": "81982679",
      "cited_paper_id": 6039192
    },
    {
      "context_text": "The idea of using learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embeddings (Liu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.80560564994812,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7074119
    },
    {
      "context_text": "…the VAE has also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image cap-tioning (Pu et al., 2016), dialogue generation (Ser-ban et al., 2017; Shen et al., 2017b; Zhao et al., 2017) and text summarization (Li et al., 2017b; Miao…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various applications of VAEs in conditional text generation tasks.",
      "processing_time": 55.980212926864624,
      "citing_paper_id": "81982679",
      "cited_paper_id": 2665144
    },
    {
      "context_text": ", 2016), image captioning (Pu et al., 2016), dialogue generation (Serban et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas. No verifiable resources are identified.",
      "processing_time": 56.1899573802948,
      "citing_paper_id": "81982679",
      "cited_paper_id": 2665144
    },
    {
      "context_text": ", those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful latent codes from text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods such as GANs and VAEs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.096291065216064,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3389583
    },
    {
      "context_text": ", those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful latent codes from text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods such as GANs and VAEs. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.096291065216064,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "Compared with other potential methods, e.g. , those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models such as GANs and VAE. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.74379825592041,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3389583
    },
    {
      "context_text": "Compared with other potential methods, e.g. , those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models such as GANs and VAE. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.74379825592041,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3439214
    },
    {
      "context_text": "Compared with other potential methods, e.g. , those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models such as GANs and VAE. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.74379825592041,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3765778
    },
    {
      "context_text": "…to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embed-dings (Liu et al., 2015; Xu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 56.21093511581421,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3505576
    },
    {
      "context_text": "…to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embed-dings (Liu et al., 2015; Xu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 56.21093511581421,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7074119
    },
    {
      "context_text": "…the generated sentences, noted as test -BLEU and self -BLEU. test - BLEU (higher is better) evaluates the quality of generated sentences using a group of real test-set sentences as the reference, and self -BLEU (lower is better) mainly measures the diversity of generated samples (Zhu et al., 2018).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (test-BLEU and self-BLEU).",
      "processing_time": 56.38234257698059,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3636178
    },
    {
      "context_text": "1 Householder Flow for Approximate Posterior Householder flow (Zhang et al., 2017a; Tomczak and Welling, 2016) is a volume-preserving normalizing flow (Rezende and Mohamed, 2015), capable of constructing an arbitrarily complex posterior qK(zK |y) from an initial random variable z0 with distribution q0, by composing a sequence of invertible mappings, i.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about a method called Householder flow, which is a type of normalizing flow used in machine learning.",
      "processing_time": 58.56007957458496,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3765778
    },
    {
      "context_text": "Householder ﬂow (Zhang et al., 2017a; Tomczak and Welling, 2016) is a volume-preserving normalizing ﬂow (Rezende and Mohamed, 2015), capable of constructing an arbitrarily complex posterior q K ( z K | y ) from an initial random variable z 0 with distribution q 0 , by composing a sequence of…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on explaining the Householder flow method.",
      "processing_time": 56.71840739250183,
      "citing_paper_id": "81982679",
      "cited_paper_id": 3765778
    },
    {
      "context_text": "Householder ﬂow (Zhang et al., 2017a; Tomczak and Welling, 2016) is a volume-preserving normalizing ﬂow (Rezende and Mohamed, 2015), capable of constructing an arbitrarily complex posterior q K ( z K | y ) from an initial random variable z 0 with distribution q 0 , by composing a sequence of…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on explaining the Householder flow method.",
      "processing_time": 56.71840739250183,
      "citing_paper_id": "81982679",
      "cited_paper_id": 216079008
    },
    {
      "context_text": ", K), and compare it with six baselines: (i) a standard language model (LM); (ii) a standard variational RNN auto-encoder (VAE); (iii) a Gaussian priorbased VAE with Householder Flow (VAE+HF); (iv) a standard LSTM language model with LDA as additional feature (LDA+LSTM); (v) Topic-RNN (Dieng et al., 2016), a joint learning framework which learns a topic model and a language model simultaneously; (vi) TDLM (Lau et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.00698375701904,
      "citing_paper_id": "81982679",
      "cited_paper_id": 6039192
    },
    {
      "context_text": "…learned topics to improve NLP tasks has been explored previously, including methods combining topic and neural language models (Ahn et al., 2016; Dieng et al., 2016; Lau et al., 2017; Mikolov and Zweig, 2012; Wang et al., 2017), as well as leveraging topic and word embed-dings (Liu et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and previous works. There are no clear identifiers for datasets.",
      "processing_time": 56.905710220336914,
      "citing_paper_id": "81982679",
      "cited_paper_id": 6039192
    },
    {
      "context_text": "Though this paper focuses on generating coherent topic-specific sentences rather than the learned topics themselves, we also evaluate the topic coherence (Lau et al., 2017) to show the rationality of our joint learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating topic coherence. No verifiable resources are identified.",
      "processing_time": 56.39888954162598,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7074119
    },
    {
      "context_text": ", 2016), a joint learning framework which learns a topic model and a language model simultaneously; (vi) TDLM (Lau et al., 2017), a joint learning framework which learns a convolutional based topic model and a language model simultaneously.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions joint learning frameworks but does not specify any datasets. The context focuses on methods and models rather than datasets.",
      "processing_time": 56.40910983085632,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7074119
    },
    {
      "context_text": "Though this paper focuses on generating coherent topic-speciﬁc sentences rather than the learned topics themselves, we also evaluate the topic coherence (Lau et al., 2017) to show the rationality of our joint learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating topic coherence. No verifiable resources are identified.",
      "processing_time": 56.48309946060181,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7074119
    },
    {
      "context_text": "By injecting the topics learned by our model (semantic information), we are able to make better use of the source document and improve a sequence-to-sequence summarization model (Sutskever et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to a sequence-to-sequence model. No verifiable resources are identified.",
      "processing_time": 57.02413749694824,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Text generation plays an important role in various natural language processing (NLP) applications, such as machine translation (Cho et al., 2014; Sutskever et al., 2014), dialogue generation (Li et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of text generation in NLP.",
      "processing_time": 55.915942907333374,
      "citing_paper_id": "81982679",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Additionally, the VAE has also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image cap-tioning (Pu et al., 2016), dialogue generation (Ser-ban et al., 2017; Shen et al., 2017b; Zhao et al., 2017) and text summarization (Li et al., 2017b;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various applications of VAE in conditional text generation tasks. No clear, verifiable datasets are identified.",
      "processing_time": 57.42952227592468,
      "citing_paper_id": "81982679",
      "cited_paper_id": 9134916
    },
    {
      "context_text": "Additionally, the VAE has also been considered for conditional text generation tasks, including machine translation (Zhang et al., 2016), image captioning (Pu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and methods. No clear, verifiable datasets are identified.",
      "processing_time": 56.30797219276428,
      "citing_paper_id": "81982679",
      "cited_paper_id": 9134916
    },
    {
      "context_text": "…other potential methods, e.g. , those based on generative adversarial networks (GANs) (Yu et al., 2017; Guo et al., 2017; Zhang et al., 2017b, 2018; Chen et al., 2018), VAE is of particular interest when one desires not only text generation, but also the capacity to infer meaningful latent codes…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the given context.",
      "processing_time": 56.84810543060303,
      "citing_paper_id": "81982679",
      "cited_paper_id": 52292169
    },
    {
      "context_text": "Householder Flow Householder ﬂow (Tomczak and Welling, 2016) is a series of Householder transformations, deﬁned as follows.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called Householder Flow. No verifiable resources are identified.",
      "processing_time": 56.294055461883545,
      "citing_paper_id": "81982679",
      "cited_paper_id": 216079008
    },
    {
      "context_text": "To make matters worse, recent works (e.g., [30, 49]) have shown that widely used metrics, such as CLIP [51] or FID [24], correlate poorly with human preferences, even for subject-agnostic de novo image generations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context focuses on the poor correlation of metrics with human preferences.",
      "processing_time": 56.851531982421875,
      "citing_paper_id": "266163420",
      "cited_paper_id": 326772
    },
    {
      "context_text": "To make matters worse, recent works (e.g., [30, 49]) have shown that widely used metrics, such as CLIP [51] or FID [24], correlate poorly with human preferences, even for subject-agnostic de novo image generations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context focuses on the poor correlation of metrics with human preferences.",
      "processing_time": 56.851531982421875,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "…methods for T2I models (personalized or not) use a combination of metrics originally designed for unconditional image generation, (e.g., FID [24] or IS [59]), and bimodal metrics such as CLIP [51] (or extensions [23, 77, 79]) which are optimized for de novo T2I generations that are not…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context focuses on evaluation methods for T2I models using metrics like FID and IS, and bimodal metrics like CLIP.",
      "processing_time": 59.561665773391724,
      "citing_paper_id": "266163420",
      "cited_paper_id": 326772
    },
    {
      "context_text": "…for T2I models (personalized or not) use a combination of metrics originally designed for unconditional image generation, (e.g., FID [24] or IS [59]), and bimodal metrics such as CLIP [51] (or extensions [23, 77, 79]) which are optimized for de novo T2I generations that are not subject-driven.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only metrics and models. The cited papers do not introduce new datasets either.",
      "processing_time": 56.48420715332031,
      "citing_paper_id": "266163420",
      "cited_paper_id": 1687220
    },
    {
      "context_text": "…for T2I models (personalized or not) use a combination of metrics originally designed for unconditional image generation, (e.g., FID [24] or IS [59]), and bimodal metrics such as CLIP [51] (or extensions [23, 77, 79]) which are optimized for de novo T2I generations that are not subject-driven.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only metrics and models. The cited papers do not introduce new datasets either.",
      "processing_time": 56.48420715332031,
      "citing_paper_id": "266163420",
      "cited_paper_id": 233296711
    },
    {
      "context_text": "…for T2I models (personalized or not) use a combination of metrics originally designed for unconditional image generation, (e.g., FID [24] or IS [59]), and bimodal metrics such as CLIP [51] (or extensions [23, 77, 79]) which are optimized for de novo T2I generations that are not subject-driven.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only metrics and models. The cited papers do not introduce new datasets either.",
      "processing_time": 56.48420715332031,
      "citing_paper_id": "266163420",
      "cited_paper_id": 258079316
    },
    {
      "context_text": "…for T2I models (personalized or not) use a combination of metrics originally designed for unconditional image generation, (e.g., FID [24] or IS [59]), and bimodal metrics such as CLIP [51] (or extensions [23, 77, 79]) which are optimized for de novo T2I generations that are not subject-driven.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only metrics and models. The cited papers do not introduce new datasets either.",
      "processing_time": 56.48420715332031,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "For, we we can restrict nouns associated with [food] to specific (e.g., detectable [31]) food items.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'specific food items'. No clear, verifiable resource is identified.",
      "processing_time": 56.89107298851013,
      "citing_paper_id": "266163420",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "There is a vast literature and accompanying datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various types of datasets but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 57.14278817176819,
      "citing_paper_id": "266163420",
      "cited_paper_id": 4492210
    },
    {
      "context_text": "There is a vast literature and accompanying datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various types of datasets but does not specify any particular dataset names. The cited paper titles do not provide additional specific dataset names.",
      "processing_time": 57.14278817176819,
      "citing_paper_id": "266163420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "Besides the many existing methods and corresponding data (e.g., [28, 34–37, 65, 71] specializing in editing/describing such characteristics - the derived invariance to such attributes eases the evaluation between the input/output subject (by assessing if such characteristics remained unchanged).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and data. No clear, verifiable dataset names are provided.",
      "processing_time": 57.13841009140015,
      "citing_paper_id": "266163420",
      "cited_paper_id": 12275803
    },
    {
      "context_text": "Besides the many existing methods and corresponding data (e.g., [28, 34–37, 65, 71] specializing in editing/describing such characteristics - the derived invariance to such attributes eases the evaluation between the input/output subject (by assessing if such characteristics remained unchanged).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and data. No clear, verifiable dataset names are provided.",
      "processing_time": 57.13841009140015,
      "citing_paper_id": "266163420",
      "cited_paper_id": 226281395
    },
    {
      "context_text": "To make matters worse, humans exhibit a heightened critical awareness of nuanced distortions or inaccuracies in representations of human faces [10, 12].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about human perception of facial representations.",
      "processing_time": 55.23080778121948,
      "citing_paper_id": "266163420",
      "cited_paper_id": 22727274
    },
    {
      "context_text": "Secondarily, we wish to highlight the potential of leveraging more powerful open-source, non-personalized T2I systems (e.g., SDXL [49]) in combination with well-curated data (e.g., CelebAMask-HQ [36]) to assist personalized generators.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebAMask-HQ"
      ],
      "dataset_descriptions": {
        "CelebAMask-HQ": "Used to enhance personalized text-to-image generation by providing high-quality, well-curated facial image data for training and fine-tuning models."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'CelebAMask-HQ' as an example of well-curated data that can be used in combination with powerful T2I systems to assist personalized generators.",
      "processing_time": 66.89574193954468,
      "citing_paper_id": "266163420",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "The Dynamic Textual Inversion ( DTI ) module inverts the foreground-masked identity image (found in CelebAMask-HQ [36]) into textual embeddings, S ∗ ( left ).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebAMask-HQ"
      ],
      "dataset_descriptions": {
        "CelebAMask-HQ": "Used to train the DTI module by providing foreground-masked identity images, focusing on generating textual embeddings for personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "CelebAMask-HQ is mentioned as a source of masked identity images used for training the DTI module. It is a specific dataset with a clear name and is directly used in the research.",
      "processing_time": 67.05471158027649,
      "citing_paper_id": "266163420",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "Equipped with the above prompt-based part of Stellar, we further couple these prompts with high-resolution human-portraying images existing in CelebAMask-HQ’s [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebAMask-HQ"
      ],
      "dataset_descriptions": {
        "CelebAMask-HQ": "Used to provide high-resolution human-portraying images for facial image manipulation, enhancing the diversity and interactivity of generated images."
      },
      "confidence_score": 1.0,
      "reasoning": "CelebAMask-HQ is mentioned as a source of high-resolution human-portraying images used in conjunction with prompts for facial image manipulation.",
      "processing_time": 65.42878937721252,
      "citing_paper_id": "266163420",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "More-over, Stellar’s prompts are paired to 400 unique human identities, selected from CelebAMask-HQ [36, 37]; and all paired items come with rich meta-annotations promoting a rigorous evaluation of personalized T2I systems, as described in the following paragraphs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CelebAMask-HQ"
      ],
      "dataset_descriptions": {
        "CelebAMask-HQ": "Used to select 400 unique human identities for pairing with prompts, enhancing the evaluation of personalized text-to-image systems through rich meta-annotations."
      },
      "confidence_score": 1.0,
      "reasoning": "CelebAMask-HQ is mentioned as a source of human identities used in the research, which is relevant for personalized text-to-image generation.",
      "processing_time": 66.47720861434937,
      "citing_paper_id": "266163420",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "In summary, we introduce 1 a large-scale multimodal dataset, Stellar , with prompts of imaginary human-centric depictions, grounding publicly available images of celebrities [36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stellar"
      ],
      "dataset_descriptions": {
        "Stellar": "Introduced as a large-scale multimodal dataset with prompts of imaginary human-centric depictions, using publicly available images of celebrities for training and evaluation."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Stellar', which appears to be a specific, named dataset. However, the description is brief and does not provide detailed usage information.",
      "processing_time": 65.07815670967102,
      "citing_paper_id": "266163420",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "Our metric uses a face detector [15] to isolate the identity’s face in both input and generated images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a face detector but does not refer to a specific dataset. The cited paper title suggests a method or tool rather than a dataset.",
      "processing_time": 56.72465133666992,
      "citing_paper_id": "266163420",
      "cited_paper_id": 219964874
    },
    {
      "context_text": "…datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions predicated on visual stimuli [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various types of datasets but does not specify any particular dataset names. The references are too generic and do not meet the criteria for inclusion.",
      "processing_time": 57.72406458854675,
      "citing_paper_id": "266163420",
      "cited_paper_id": 220714131
    },
    {
      "context_text": "…datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions predicated on visual stimuli [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various types of datasets but does not specify any particular dataset names. The references are too generic and do not meet the criteria for inclusion.",
      "processing_time": 57.72406458854675,
      "citing_paper_id": "266163420",
      "cited_paper_id": 231639297
    },
    {
      "context_text": "…datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions predicated on visual stimuli [47].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions various types of datasets but does not specify any particular dataset names. The references are too generic and do not meet the criteria for inclusion.",
      "processing_time": 57.72406458854675,
      "citing_paper_id": "266163420",
      "cited_paper_id": 248227685
    },
    {
      "context_text": "…CLIP-based metrics and human-preference [30, 49], and the fact that subjective quantities such as an image’s aesthetic value are intrinsically hard to compute [5]; all the above metrics are not designed to take into account the additional image portraying the subject that is driving the generation.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses metrics and subjective qualities. No clear identifiers for datasets are present.",
      "processing_time": 56.72786855697632,
      "citing_paper_id": "266163420",
      "cited_paper_id": 231639297
    },
    {
      "context_text": "Applications ranging from creating de novo fictional images [53, 55, 58] to editing them via text [11, 83], or similarly manipulating higher-dimensional visual representations (e.g., video [22, 42, 67, 74] or 3D objects [6, 52, 60, 82]); have vastly pushed the envelope for computer-aided creativity…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 57.37814664840698,
      "citing_paper_id": "266163420",
      "cited_paper_id": 232035663
    },
    {
      "context_text": "Applications ranging from creating de novo fictional images [53, 55, 58] to editing them via text [11, 83], or similarly manipulating higher-dimensional visual representations (e.g., video [22, 42, 67, 74] or 3D objects [6, 52, 60, 82]); have vastly pushed the envelope for computer-aided creativity…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 57.37814664840698,
      "citing_paper_id": "266163420",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Applications ranging from creating de novo fictional images [53, 55, 58] to editing them via text [11, 83], or similarly manipulating higher-dimensional visual representations (e.g., video [22, 42, 67, 74] or 3D objects [6, 52, 60, 82]); have vastly pushed the envelope for computer-aided creativity…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications and methods. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 57.37814664840698,
      "citing_paper_id": "266163420",
      "cited_paper_id": 254408758
    },
    {
      "context_text": "For example, metrics such as CLIP T [73] and CLIP-Score [23] attempt to evaluate holistically the semantic alignment between a sentence and a given image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (CLIP T and CLIP-Score) but does not refer to any specific datasets. The context is about evaluating semantic alignment between text and images, which is a methodological approach rather than a dataset.",
      "processing_time": 60.50946092605591,
      "citing_paper_id": "266163420",
      "cited_paper_id": 233296711
    },
    {
      "context_text": "For example, metrics such as CLIP T [73] and CLIP-Score [23] attempt to evaluate holistically the semantic alignment between a sentence and a given image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics (CLIP T and CLIP-Score) but does not refer to any specific datasets. The context is about evaluating semantic alignment between text and images, which is a methodological approach rather than a dataset.",
      "processing_time": 60.50946092605591,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Despite that, many recent and indispensable metrics for de novo text-to-image systems exist [23, 51, 62]; such metrics are typically blind to fundamental aspects that affect the overall performance and quality of personalized, subject-driven generators.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general metrics for evaluating text-to-image systems. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.707815408706665,
      "citing_paper_id": "266163420",
      "cited_paper_id": 233296711
    },
    {
      "context_text": "Despite that, many recent and indispensable metrics for de novo text-to-image systems exist [23, 51, 62]; such metrics are typically blind to fundamental aspects that affect the overall performance and quality of personalized, subject-driven generators.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general metrics for evaluating text-to-image systems. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.707815408706665,
      "citing_paper_id": "266163420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "Despite that, many recent and indispensable metrics for de novo text-to-image systems exist [23, 51, 62]; such metrics are typically blind to fundamental aspects that affect the overall performance and quality of personalized, subject-driven generators.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general metrics for evaluating text-to-image systems. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.707815408706665,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "Efficient and stable training for the larger SDXL is achieved using Low-Rank Adaptation (LoRA) [26] weight offsets (Fig.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LoRA) for adapting large language models. The context is about training stability and efficiency, not about dataset usage.",
      "processing_time": 58.60384130477905,
      "citing_paper_id": "266163420",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Additionally, we finetune the UNet backbone of SDXL using LoRA weight-offsets [26] for efficient and stable training ( middle bottom ).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LoRA) for adapting large language models. The context focuses on the technical details of model training.",
      "processing_time": 58.02682447433472,
      "citing_paper_id": "266163420",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "…and accompanying datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions predicated on visual…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span mentions various types of datasets but does not specify any particular dataset names. It refers to general categories of data such as 'objective multimodal captioning of images', '3D objects and scenes', and 'subjective bi-modal data'. No specific dataset names are provided.",
      "processing_time": 62.68381881713867,
      "citing_paper_id": "266163420",
      "cited_paper_id": 244908764
    },
    {
      "context_text": "…and accompanying datasets concerning objective multimodal captioning of images [13, 29, 31, 33, 40, 48, 62, 64], 3D objects and scenes [1, 3, 4, 14, 43, 70], or even more subjective bi-modal data concerning e.g., visually grounded emotions [5, 7, 44], or dynamic actions predicated on visual…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation span mentions various types of datasets but does not specify any particular dataset names. It refers to general categories of data such as 'objective multimodal captioning of images', '3D objects and scenes', and 'subjective bi-modal data'. No specific dataset names are provided.",
      "processing_time": 62.68381881713867,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "Such personalized models have recently started to emerge [19, 57, 66, 73] as a natural and powerful extension of de novo image generators [49, 56], inspiring radically novel applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to personalized models and de novo image generators. No verifiable resources are identified.",
      "processing_time": 57.46958637237549,
      "citing_paper_id": "266163420",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Such personalized models have recently started to emerge [19, 57, 66, 73] as a natural and powerful extension of de novo image generators [49, 56], inspiring radically novel applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to personalized models and de novo image generators. No verifiable resources are identified.",
      "processing_time": 57.46958637237549,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Such personalized models have recently started to emerge [19, 57, 66, 73] as a natural and powerful extension of de novo image generators [49, 56], inspiring radically novel applications.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to personalized models and de novo image generators. No verifiable resources are identified.",
      "processing_time": 57.46958637237549,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Capitalizing on the remarkable capabilities and modularity shown recently by T2I models [49, 54, 56, 58], powerful extensions personalizing their output generations have emerged.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.11335515975952,
      "citing_paper_id": "266163420",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "…sharing multimodal deep learning models trained with internet scale data, comprised of billions of textual tokens and images (e.g., [49, 51, 56]), have given rise to an unprecedented explosion of novel solutions for many long-standing problems at the intersection of computer vision and…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generic references to 'internet scale data' and 'billions of textual tokens and images'. No specific, verifiable datasets are named.",
      "processing_time": 58.743860721588135,
      "citing_paper_id": "266163420",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Similar to existing research [19, 57], our architecture relies on large pre-trained T2I diffusion models [49, 56] tapping on their excellent capacity for prompt-guided image generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their capabilities. The context focuses on the use of pre-trained T2I diffusion models for image generation, which are methods, not datasets.",
      "processing_time": 59.68889093399048,
      "citing_paper_id": "266163420",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Similar to existing research [19, 57], our architecture relies on large pre-trained T2I diffusion models [49, 56] tapping on their excellent capacity for prompt-guided image generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their capabilities. The context focuses on the use of pre-trained T2I diffusion models for image generation, which are methods, not datasets.",
      "processing_time": 59.68889093399048,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "…fictional images [53, 55, 58] to editing them via text [11, 83], or similarly manipulating higher-dimensional visual representations (e.g., video [22, 42, 67, 74] or 3D objects [6, 52, 60, 82]); have vastly pushed the envelope for computer-aided creativity and further de-mocratized this field…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It discusses various applications of AI in generating and editing visual content but does not reference any particular dataset.",
      "processing_time": 58.027469635009766,
      "citing_paper_id": "266163420",
      "cited_paper_id": 247292765
    },
    {
      "context_text": "Comparison of Stellar with existing datasets focusing on personalized, or de novo (Parti [81]), image generations.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'existing datasets' but does not specify any particular dataset names. The term 'Stellar' is mentioned but is likely a method or model, not a dataset, given the context and the cited paper title.",
      "processing_time": 60.363614082336426,
      "citing_paper_id": "266163420",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "DreamBooth [57] 35 40 ✗ ✗ CustomConcept101 [32] 617 101 ✗ ✗ Parti [81] 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models or methods. The titles of the cited papers also do not provide clear evidence of dataset usage.",
      "processing_time": 57.494086503982544,
      "citing_paper_id": "266163420",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "DreamBooth [57] 35 40 ✗ ✗ CustomConcept101 [32] 617 101 ✗ ✗ Parti [81] 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models or methods. The titles of the cited papers also do not provide clear evidence of dataset usage.",
      "processing_time": 57.494086503982544,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "DreamBooth [57] 35 40 ✗ ✗ CustomConcept101 [32] 617 101 ✗ ✗ Parti [81] 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models or methods. The titles of the cited papers also do not provide clear evidence of dataset usage.",
      "processing_time": 57.494086503982544,
      "citing_paper_id": "266163420",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Even if one is to lift the constraint of coupling the descriptions of images to specific individuals [32, 57, 81], there is yet no common ground for building large-scale collections of textual prompts in natural language that can be deemed applicable for generating i) interesting fictional…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the challenge of building large-scale collections of textual prompts but does not name any existing datasets.",
      "processing_time": 57.87139821052551,
      "citing_paper_id": "266163420",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "Even if one is to lift the constraint of coupling the descriptions of images to specific individuals [32, 57, 81], there is yet no common ground for building large-scale collections of textual prompts in natural language that can be deemed applicable for generating i) interesting fictional…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the challenge of building large-scale collections of textual prompts but does not name any existing datasets.",
      "processing_time": 57.87139821052551,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Even if one is to lift the constraint of coupling the descriptions of images to specific individuals [32, 57, 81], there is yet no common ground for building large-scale collections of textual prompts in natural language that can be deemed applicable for generating i) interesting fictional…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the challenge of building large-scale collections of textual prompts but does not name any existing datasets.",
      "processing_time": 57.87139821052551,
      "citing_paper_id": "266163420",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Recent personalized datasets (e.g., [32, 57, 81]) are not focused on human-centric actions, cover few subjects, and/or lack rich meta-annotations accompanying the paired prompts/images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'personalized datasets' but does not provide specific names. It only describes general characteristics and limitations of such datasets.",
      "processing_time": 56.92794632911682,
      "citing_paper_id": "266163420",
      "cited_paper_id": 249926846
    },
    {
      "context_text": "Recent personalized datasets (e.g., [32, 57, 81]) are not focused on human-centric actions, cover few subjects, and/or lack rich meta-annotations accompanying the paired prompts/images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'personalized datasets' but does not provide specific names. It only describes general characteristics and limitations of such datasets.",
      "processing_time": 56.92794632911682,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Recent personalized datasets (e.g., [32, 57, 81]) are not focused on human-centric actions, cover few subjects, and/or lack rich meta-annotations accompanying the paired prompts/images.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'personalized datasets' but does not provide specific names. It only describes general characteristics and limitations of such datasets.",
      "processing_time": 56.92794632911682,
      "citing_paper_id": "266163420",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Starting with the pi-oneering works of Gal et. al [19] and Ruiz et. al [57], personalized networks with admirable capabilities have started to appear.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to pioneering works and personalized networks. No verifiable resources are identified.",
      "processing_time": 56.74012851715088,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "In all the experiments we evaluate along StellarNet, DreamBooth (DB) [57], Textual Inversion (TI) [19], and ELITE [73].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'StellarNet', 'DreamBooth', 'Textual Inversion', and 'ELITE'. However, none of these are datasets; they are methods or models. The context does not provide any specific dataset names.",
      "processing_time": 60.804314613342285,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "In all the experiments we evaluate along StellarNet, DreamBooth (DB) [57], Textual Inversion (TI) [19], and ELITE [73].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'StellarNet', 'DreamBooth', 'Textual Inversion', and 'ELITE'. However, none of these are datasets; they are methods or models. The context does not provide any specific dataset names.",
      "processing_time": 60.804314613342285,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "…efforts make it easy to find vast quantities of objective captioning data for images (e.g., [33, 62]); however, curating images of individuals performing fictional actions accompanied by textual descriptions is practically impossible given the open-ended and imaginary nature of this task [57].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the general availability of objective captioning data for images and the difficulty of curating specific types of image-text pairs.",
      "processing_time": 58.67979907989502,
      "citing_paper_id": "266163420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Orthogonally to these metrics, unimodal (vision-based) metrics, such as the Aesthetic-Score [62], try to quantify the aesthetic quality of a given image.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a metric called 'Aesthetic-Score'. No datasets are referenced for use in the research context.",
      "processing_time": 57.42578959465027,
      "citing_paper_id": "266163420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "Current efforts make it easy to find vast quantities of objective captioning data for images (e.g., [33, 62]); however, curating images of individuals performing fictional actions accompanied by textual descriptions is practically impossible given the open-ended and imaginary nature of this task…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the general availability of objective captioning data for images, which is too generic.",
      "processing_time": 57.588292598724365,
      "citing_paper_id": "266163420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "We quantitatively compare our proposed metrics with metrics typically used in existing T2I studies (Aesthetic Score [62], CLIP I [51], DreamSim [18], CLIP T [51], HPSv1 [77], HPSv2 [76], ImageReward [79] and PickScore [30]) over the entire Stellar Dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stellar Dataset"
      ],
      "dataset_descriptions": {
        "Stellar Dataset": "Used to quantitatively compare proposed metrics with existing metrics in text-to-image generation studies, focusing on aesthetic and perceptual quality assessments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stellar Dataset' which is a specific dataset used for quantitative comparison of metrics in text-to-image generation studies.",
      "processing_time": 64.00279021263123,
      "citing_paper_id": "266163420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "We quantitatively compare our proposed metrics with metrics typically used in existing T2I studies (Aesthetic Score [62], CLIP I [51], DreamSim [18], CLIP T [51], HPSv1 [77], HPSv2 [76], ImageReward [79] and PickScore [30]) over the entire Stellar Dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stellar Dataset"
      ],
      "dataset_descriptions": {
        "Stellar Dataset": "Used to quantitatively compare proposed metrics with existing metrics in text-to-image generation studies, focusing on aesthetic and perceptual quality assessments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stellar Dataset' which is a specific dataset used for quantitative comparison of metrics in text-to-image generation studies.",
      "processing_time": 64.00279021263123,
      "citing_paper_id": "266163420",
      "cited_paper_id": 258079316
    },
    {
      "context_text": "We quantitatively compare our proposed metrics with metrics typically used in existing T2I studies (Aesthetic Score [62], CLIP I [51], DreamSim [18], CLIP T [51], HPSv1 [77], HPSv2 [76], ImageReward [79] and PickScore [30]) over the entire Stellar Dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stellar Dataset"
      ],
      "dataset_descriptions": {
        "Stellar Dataset": "Used to quantitatively compare proposed metrics with existing metrics in text-to-image generation studies, focusing on aesthetic and perceptual quality assessments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stellar Dataset' which is a specific dataset used for quantitative comparison of metrics in text-to-image generation studies.",
      "processing_time": 64.00279021263123,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "We quantitatively compare our proposed metrics with metrics typically used in existing T2I studies (Aesthetic Score [62], CLIP I [51], DreamSim [18], CLIP T [51], HPSv1 [77], HPSv2 [76], ImageReward [79] and PickScore [30]) over the entire Stellar Dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Stellar Dataset"
      ],
      "dataset_descriptions": {
        "Stellar Dataset": "Used to quantitatively compare proposed metrics with existing metrics in text-to-image generation studies, focusing on aesthetic and perceptual quality assessments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Stellar Dataset' which is a specific dataset used for quantitative comparison of metrics in text-to-image generation studies.",
      "processing_time": 64.00279021263123,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "DiffusionDB [72] is a large-scale dataset that includes user prompts that are not personalized and typically contain multiple complex modifiers (e.g., famous painters, photographic techniques, etc.), resulting in obscure, not natural language.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DiffusionDB"
      ],
      "dataset_descriptions": {
        "DiffusionDB": "Used to study non-personalized, complex user prompts in text-to-image generative models, focusing on the impact of multiple modifiers on prompt clarity and naturalness."
      },
      "confidence_score": 1.0,
      "reasoning": "DiffusionDB is explicitly mentioned as a dataset and described with specific characteristics relevant to the research context.",
      "processing_time": 63.33556866645813,
      "citing_paper_id": "266163420",
      "cited_paper_id": 253116574
    },
    {
      "context_text": "More recently, works like ELITE [73], or similar ones( [20, 27]), have lifted this bottleneck by learning more general and robust mappings between visual and textual embeddings capable of generalizing simultaneously to many individual inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The citation is about learning mappings between visual and textual embeddings, which is not directly related to the topic of personalized text generation.",
      "processing_time": 59.032145261764526,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "More recently, works like ELITE [73], or similar ones( [20, 27]), have lifted this bottleneck by learning more general and robust mappings between visual and textual embeddings capable of generalizing simultaneously to many individual inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models or methods. The citation is about learning mappings between visual and textual embeddings, which is not directly related to the topic of personalized text generation.",
      "processing_time": 59.032145261764526,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257952647
    },
    {
      "context_text": "Those latter masks are helpful for evaluation purposes and for training modern T2I generators where background-free grounding images typically result in qualitatively superior generations [73].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'background-free grounding images' which is too generic.",
      "processing_time": 56.66501021385193,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "To achieve the above goal efficiently and overcome the need for per-subject optimization, we incorporate an encoder-based inversion technique (similar to [20, 73]).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.007147789001465,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Following recent works [20, 73], originally inspired by GAN-Inversion techniques [78], we also aim to derive a latent representation for the provided subject image, I , in the pre-trained word-embedding space of a T2I model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only techniques and models. There are no clear identifiers for datasets.",
      "processing_time": 56.2689266204834,
      "citing_paper_id": "266163420",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "…deep learning models trained with internet scale data, comprised of billions of textual tokens and images (e.g., [49, 51, 56]), have given rise to an unprecedented explosion of novel solutions for many long-standing problems at the intersection of computer vision and natural language [8, 80].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generic references to 'internet scale data' without naming any particular resource.",
      "processing_time": 56.644237995147705,
      "citing_paper_id": "266163420",
      "cited_paper_id": 259243718
    },
    {
      "context_text": "Moreover, we advise proper content moderation and relevant regulation [25] to prevent malpractice.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only advises on content moderation and regulation.",
      "processing_time": 56.263288259506226,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "These applications include personalized story-telling via character animation of specific individuals, aiding people to attend otherwise inaccessible real-world events virtually, or simply lifting the burden to capture photographs physically [17, 21, 68].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It describes potential applications of personalized text generation but does not reference any datasets used for these purposes.",
      "processing_time": 57.69853377342224,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "…in evaluative standards underscores the need for personalization systems focused on human-driven generations to adhere to a higher level of precision and accuracy, as human faces are not just scrutinized more closely but also evoke stronger emotional responses [69] upon perceived inaccuracies [39].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses the importance of precision and accuracy in personalization systems, particularly in generating human faces.",
      "processing_time": 57.937657594680786,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "Specifically, we utilize CLIP T [51], to independently measure the similarity between the prompt and both the input and output images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CLIP) which is excluded according to the instructions.",
      "processing_time": 56.445207357406616,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "…publicly sharing multimodal deep learning models trained with internet scale data, comprised of billions of textual tokens and images (e.g., [49, 51, 56]), have given rise to an unprecedented explosion of novel solutions for many long-standing problems at the intersection of computer vision and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It refers to general internet-scale data and multimodal deep learning models, which are not specific datasets.",
      "processing_time": 58.093915700912476,
      "citing_paper_id": "266163420",
      "cited_paper_id": null
    },
    {
      "context_text": "Different algorithms are utilized, such as Genetic algorithms [Manurung, 2003; Levy, 2001] and Statistical Machine Translation approaches [He et al., 2012].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.35121703147888,
      "citing_paper_id": "199466228",
      "cited_paper_id": 341808
    },
    {
      "context_text": "Different algorithms are utilized, such as Genetic algorithms [Manurung, 2003; Levy, 2001] and Statistical Machine Translation approaches [He et al., 2012].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.35121703147888,
      "citing_paper_id": "199466228",
      "cited_paper_id": 9222660
    },
    {
      "context_text": "Different algorithms are utilized, such as Genetic algorithms [Manurung, 2003; Levy, 2001] and Statistical Machine Translation approaches [He et al., 2012].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only algorithms and methods. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 57.35121703147888,
      "citing_paper_id": "199466228",
      "cited_paper_id": 170162691
    },
    {
      "context_text": ", 2018a], we use TextRank [Mihalcea and Tarau, 2004] to extract three keywords from each poem and build three <keyword, poem> pairs to enable the model to cope with different keywords.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using TextRank to extract keywords from poems, but TextRank is a method, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.34805727005005,
      "citing_paper_id": "199466228",
      "cited_paper_id": 577937
    },
    {
      "context_text": "As in [Yi et al., 2018a], we use TextRank [Mihalcea and Tarau, 2004] to extract three keywords from each poem and build three <keyword, poem> pairs to enable the model to cope with different keywords.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using TextRank to extract keywords from poems, but TextRank is a method, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.16538166999817,
      "citing_paper_id": "199466228",
      "cited_paper_id": 577937
    },
    {
      "context_text": "Different algorithms are utilized, such as Genetic algorithms [Manurung, 2003; Levy, 2001] and Statistical Machine Translation approaches [He et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 56.24716758728027,
      "citing_paper_id": "199466228",
      "cited_paper_id": 5063877
    },
    {
      "context_text": "Different algorithms are utilized, such as Genetic algorithms [Manurung, 2003; Levy, 2001] and Statistical Machine Translation approaches [He et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only algorithms and methods. No verifiable resources are identified.",
      "processing_time": 56.24716758728027,
      "citing_paper_id": "199466228",
      "cited_paper_id": 9222660
    },
    {
      "context_text": "We use the five criteria designed by [Manurung, 2003]: Fluency (is the generated poem well-formed?), Coherence (are the meaning and theme of the poem consistent?), Meaningfulness (does the poem convey some certain messages?), Poeticness (does the poem have some poetic attributes?), Overall (the general impression of the poem).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation criteria for generated poems.",
      "processing_time": 55.24465847015381,
      "citing_paper_id": "199466228",
      "cited_paper_id": 5063877
    },
    {
      "context_text": "Adam [Kingma and Ba, 2015] with mini-batches (batch size 64) is used for optimization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions Adam, which is a method for stochastic optimization, not a dataset. No datasets are mentioned.",
      "processing_time": 56.01815867424011,
      "citing_paper_id": "199466228",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "After that, more effective sequence-to-sequence models with attention mechanism [Bahdanau et al., 2015] are also adopted to generate poetry [Wang et al., 2016a].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the adoption of sequence-to-sequence models with attention mechanisms for generating poetry.",
      "processing_time": 58.26641130447388,
      "citing_paper_id": "199466228",
      "cited_paper_id": 6828501
    },
    {
      "context_text": "Though recent neural poetry generation models have achieved significant improvements in different aspects of poetry quality, such as fluency [Zhang and Lapata, 2014] and coherence [Wang et al., 2016b; Yi et al., 2018b], they all neglected to generate sentiment-controllable poetry.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to improvements in neural poetry generation models. No verifiable resources are identified.",
      "processing_time": 57.345677852630615,
      "citing_paper_id": "199466228",
      "cited_paper_id": 6828501
    },
    {
      "context_text": "We use the five criteria designed by [Manurung, 2003]: Fluency (is the generated poem well-formed?)",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only criteria for evaluating generated poems.",
      "processing_time": 55.40720891952515,
      "citing_paper_id": "199466228",
      "cited_paper_id": 170162691
    },
    {
      "context_text": "Our work is also related to the task of text generation with controllable sentiments [Hu et al., 2017; Cagan et al., 2017; Wang and Wan, 2018].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only related work on controllable sentiment in text generation.",
      "processing_time": 56.78259229660034,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Besides, we also use the annealing trick [Yang et al., 2018b] and BOW-loss [Zhao et al., 2017] to alleviate the vanishing latent variable problem in VAE training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (annealing trick and BOW-loss) used to address a technical issue in VAE training.",
      "processing_time": 58.03396773338318,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Besides, we also use the annealing trick [Yang et al., 2018b] and BOW-loss [Zhao et al., 2017] to alleviate the vanishing latent variable problem in VAE training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (annealing trick and BOW-loss) used to address a technical issue in VAE training.",
      "processing_time": 58.03396773338318,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "We consider VAE as a feasible method, which has shown great promise in text generation tasks such as dialogue generation [Zhao et al., 2017] and poetry generation [Yang et al., 2018b].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (VAE) and their applications in text generation tasks.",
      "processing_time": 56.90612554550171,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "We consider VAE as a feasible method, which has shown great promise in text generation tasks such as dialogue generation [Zhao et al., 2017] and poetry generation [Yang et al., 2018b].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (VAE) and their applications in text generation tasks.",
      "processing_time": 56.90612554550171,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "As studied in [Yi et al., 2018a], perplexity or BLEU departs from the human evaluation manner, therefore we directly conduct human evaluations following the settings in [Yang et al., 2018a; Yi et al., 2018a; Zhang et al., 2017].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to human evaluation methods and settings from other studies.",
      "processing_time": 56.53536033630371,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "CVAE [Yang et al., 2018b]: a conditional variational autoencoder with a hybrid decoder to learn the implicit topic information within poems lines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CVAE) and its application to learn topic information within poem lines.",
      "processing_time": 57.62778997421265,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "CVAE [Yang et al., 2018b]: a conditional variational autoencoder with a hybrid decoder to learn the implicit topic information within poems lines.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CVAE) and its application to learn topic information within poem lines.",
      "processing_time": 57.62778997421265,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Different from previous VAE models which learn a context-based latent variable [Yang et al., 2018b] or suppose the independence of latent variable and the required attributes (e.g., sentiment) [Hu et al., 2017], we make the latent space conditioned on both sentiment and content to capture…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous models and their approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.61840581893921,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Different from previous VAE models which learn a context-based latent variable [Yang et al., 2018b] or suppose the independence of latent variable and the required attributes (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a method or model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.54975509643555,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "As previous work [Yang et al., 2018b] did, we assume latent variable z follows the isotropic Gaussian distribution, i.e., p(z|w, y) ∼ N (µprior, σpriorI) and q(z|x,w, y) ∼ N (µpost, σpostI).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the statistical assumptions of latent variables.",
      "processing_time": 55.954378843307495,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "As previous work [Yang et al., 2018b] did, we assume latent variable z follows the isotropic Gaussian distribution, i.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological assumption about a latent variable.",
      "processing_time": 56.291510820388794,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Besides, we also use the annealing trick [Yang et al., 2018b] and BOW-loss [Zhao et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (annealing trick and BOW-loss).",
      "processing_time": 56.46251654624939,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": ", polarities and intensities) in poetry sentiment expression, and the sentiment of each line could be different under certain holistic sentiment [Janowitz, 1973].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or other verifiable resources. It only discusses the concept of sentiment expression in poetry.",
      "processing_time": 57.91603207588196,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Each poem is annotated with sentiments of not only the whole poem but also each line because there are varying granularities (e.g., polarities and intensities) in poetry sentiment expression, and the sentiment of each line could be different under certain holistic sentiment [Janowitz, 1973].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions sentiment annotation in poetry but does not specify a named dataset. The reference to Janowitz, 1973, does not provide a specific dataset name.",
      "processing_time": 59.186649322509766,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "For the labelled data, inspired by [Kingma et al., 2014], to maximize the distribution p(x, y|w), we involve z and derive the lower bound as: log p(x, y|w) ≥ Eq(z|x,w,y)[log p(x|z, w, y)]\n−KL[q(z|x,w, y)||p(z|w, y)] + log p(y|w) = −L(x, y, w)\n(1)\nwhere q(z|x,w, y) and p(z|w, y) are the estimations…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical formulations and distributions. There are no clear identifiers for datasets, models, or other resources.",
      "processing_time": 57.89808964729309,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Different from previous VAE models which learn a context-based latent variable [Yang et al., 2018b] or suppose the independence of latent variable and the required attributes (e.g., sentiment) [Hu et al., 2017], we make the latent space conditioned on both sentiment and content to capture generalized sentiment-related semantics, because the sentiment is coupled with semantics especially for poetry [Chari, 1976].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous models and their approaches. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.60922431945801,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Instead, we propose to adopt a semi-supervised Variational AutoEncoder (VAE) [Kingma et al., 2014], which utilizes the labelled data more efficiently and has been widely used for image generation, to generate sentimental poetry.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Variational AutoEncoder) and a general application (image generation).",
      "processing_time": 58.807679414749146,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Concretely, we extend the semi-supervised version of VAE [Kingma et al., 2014] to the conditional version and deduce a different lower bound for our task to capture generalized sentiment-related semantics more efficiently.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VAE).",
      "processing_time": 55.862839221954346,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "Note that during this process, we don’t suppose the independence of z and y as [Kingma et al., 2014], instead, we directly draw z from the sentiment and keyword since the sentiment is coupled with semantics in poetry as discussed in Section 1.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.32123374938965,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "We are the first effort at sentimental poetry generation with a semi-supervised sentiment-conditioned VAE, which makes latent space conditioned on the sentiments, instead of learning a context-based or attribute-independent latent variable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.08112454414368,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "For the labelled data, inspired by [Kingma et al., 2014], to maximize the distribution p(x, y|w), we involve z and derive the lower bound as: log p(x, y|w) ≥ Eq(z|x,w,y)[log p(x|z, w, y)]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a mathematical derivation involving distributions and variables, which is not indicative of a dataset.",
      "processing_time": 57.99948024749756,
      "citing_paper_id": "199466228",
      "cited_paper_id": null
    },
    {
      "context_text": "For testing quantitative results, we methodically selected one image per individual for a total of 500 individuals from the VGGFace2 dataset [5] as reference for all methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to select one image per individual for a total of 500 individuals, serving as a reference for testing quantitative results in face recognition methods."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the VGGFace2 dataset, which is a specific, verifiable dataset used for face recognition research. It is clearly identified and used in the context of selecting images for testing.",
      "processing_time": 67.50803637504578,
      "citing_paper_id": "268537084",
      "cited_paper_id": 216009
    },
    {
      "context_text": "We first encode all of the reference images into visual patch features { f } and a learned class embedding { f } using the vision model of CLIP [29], where f Here, ( p, p ) is the patch size, and d v is the dimension of these embedded patches through a linear projection.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CLIP) for encoding visual features. No verifiable dataset names are present.",
      "processing_time": 57.57335329055786,
      "citing_paper_id": "268537084",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "3.3 using the vision model of CLIP [29], we also obtain a learned class embedding { f } simultaneously.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method (CLIP) which is excluded according to the instructions.",
      "processing_time": 56.67001485824585,
      "citing_paper_id": "268537084",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "This was followed by GLIDE [27], which introduced more realistic and high-resolution images using diffusion models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GLIDE) that uses diffusion models for generating high-resolution images.",
      "processing_time": 57.225035190582275,
      "citing_paper_id": "268537084",
      "cited_paper_id": 231979499
    },
    {
      "context_text": "Models such as Imagen proposed by [35], DALL-E2 by [31], and Stable Diffusion by [33] have gained attention for their ability to generate realistic images from natural language prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions models but does not refer to any specific datasets. The context is about image generation models and their capabilities, not about datasets.",
      "processing_time": 57.93695092201233,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Models such as Imagen proposed by [35], DALL-E2 by [31], and Stable Diffusion by [33] have gained attention for their ability to generate realistic images from natural language prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions models but does not refer to any specific datasets. The context is about image generation models and their capabilities, not about datasets.",
      "processing_time": 57.93695092201233,
      "citing_paper_id": "268537084",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "The success of Stable Diffusion [33] in the open-source community has led to its widespread use and the development of various fine-tuned models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the success of a model and its derivatives.",
      "processing_time": 55.912269592285156,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "It can be observed that IDAdapter effectively leverages the base model to generate more diverse results with identity preserved.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (IDAdapter). There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.75914430618286,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "A potential cause for this pitfall could be the limitation of the textual space features in capturing identity (ID) characteristics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a general limitation related to textual space features.",
      "processing_time": 57.26957607269287,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Techniques like Generative Adversarial Networks (GANs) [22, 45], auto-regressive models [30], and diffusion models [17, 33] have played a crucial role.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.75757908821106,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "…embedding C = Θ( T ) , which is generated using a text encoder Θ and a text prompt T and then employed in the intermediate layers of the UNet through a cross-attention mechanism: where is the hidden states through the UNet implementation, d is the scale factor utilized for attention mechanisms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method involving a text encoder and a UNet. No verifiable resources are identified.",
      "processing_time": 57.62006855010986,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Recent developments like DALL-E 2 [31], Imagen [35], and LDM [33] have further enhanced these models, offering more realism, better language understanding and diverse outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.84338879585266,
      "citing_paper_id": "268537084",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Recent developments like DALL-E 2 [31], Imagen [35], and LDM [33] have further enhanced these models, offering more realism, better language understanding and diverse outputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.84338879585266,
      "citing_paper_id": "268537084",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "An alternative strategy, discussed in studies like [7, 8, 44, 46], involves augmenting pre-trained This CVPR Workshop paper is the Open Access version, provided by the Computer Vision Foundation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to an alternative strategy discussed in other studies.",
      "processing_time": 57.40838837623596,
      "citing_paper_id": "268537084",
      "cited_paper_id": 252596087
    },
    {
      "context_text": "Facilitating object replacement and style variation, Instruct-Pix2Pix [4] integrates latent features of reference images into the noise injection process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Instruct-Pix2Pix) and its functionality. No verifiable resources are identified.",
      "processing_time": 58.13987445831299,
      "citing_paper_id": "268537084",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "Consequently, we assessed Pos-Div and Expr-Div metrics exclusively on open-source models requiring fine-tuning [13, 23, 34, 48].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.9298632144928,
      "citing_paper_id": "268537084",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "In this regard, some existing research [14, 23] have emphasized the significance of attention layers.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the significance of attention layers in existing research.",
      "processing_time": 55.81023049354553,
      "citing_paper_id": "268537084",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Following this, several meth-ods like CustomDiffusion [23], SVDiff [15], LoRa [1, 19], StyleDrop [39], and the approach by [18] proposed partial optimizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.60909843444824,
      "citing_paper_id": "268537084",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Following this, several meth-ods like CustomDiffusion [23], SVDiff [15], LoRa [1, 19], StyleDrop [39], and the approach by [18] proposed partial optimizations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.60909843444824,
      "citing_paper_id": "268537084",
      "cited_paper_id": 257631648
    },
    {
      "context_text": "Personalization in text-to-image (T2I) synthesis has been primarily achieved through methodologies employing pre-trained models, as outlined in works such as [2, 13, 16, 23, 34, 38, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and pre-trained models. No verifiable resources are identified.",
      "processing_time": 56.68378496170044,
      "citing_paper_id": "268537084",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Personalization in text-to-image (T2I) synthesis has been primarily achieved through methodologies employing pre-trained models, as outlined in works such as [2, 13, 16, 23, 34, 38, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and pre-trained models. No verifiable resources are identified.",
      "processing_time": 56.68378496170044,
      "citing_paper_id": "268537084",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "Personalization in text-to-image (T2I) synthesis has been primarily achieved through methodologies employing pre-trained models, as outlined in works such as [2, 13, 16, 23, 34, 38, 40].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methodologies and pre-trained models. No verifiable resources are identified.",
      "processing_time": 56.68378496170044,
      "citing_paper_id": "268537084",
      "cited_paper_id": null
    },
    {
      "context_text": "Several studies, such as UMM [25], ELITE [44], and SuTI [7], have demonstrated subject image generation without fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions UMM, ELITE, and SuTI, but these are likely methods or models rather than datasets. No specific datasets are mentioned in the context.",
      "processing_time": 58.41638946533203,
      "citing_paper_id": "268537084",
      "cited_paper_id": 257557302
    },
    {
      "context_text": "UMM-Diffusion [25], leveraging LAION-400M dataset [36], proposed a multimodal latent diffusion approach that combines text and image inputs.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-400M"
      ],
      "dataset_descriptions": {
        "LAION-400M": "Used to train a multimodal latent diffusion model that combines text and image inputs, focusing on joint subject and text conditional image generation."
      },
      "confidence_score": 1.0,
      "reasoning": "LAION-400M is a specific, verifiable dataset mentioned in the context. It is used in the research for training a multimodal latent diffusion model.",
      "processing_time": 66.51744675636292,
      "citing_paper_id": "268537084",
      "cited_paper_id": 257557302
    },
    {
      "context_text": "The study by [42] enhanced textual inversion to capture detailed subject information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method enhancement. The context and title do not provide enough information to identify a verifiable dataset.",
      "processing_time": 57.867173194885254,
      "citing_paper_id": "268537084",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Qualitative Results Our methodology was benchmarked against several leading techniques, including Textual Inversion [13], Dreambooth [34], E4T [14], ProFusion [49], and Photoverse [6], as illustrated in Figure 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.42875790596008,
      "citing_paper_id": "268537084",
      "cited_paper_id": 258841022
    },
    {
      "context_text": "Qualitative Results Our methodology was benchmarked against several leading techniques, including Textual Inversion [13], Dreambooth [34], E4T [14], ProFusion [49], and Photoverse [6], as illustrated in Figure 4.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.42875790596008,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "Drawing inspiration from test-time fine-tuning methods utilizing multiple reference images and the adapter series as described in works [26, 43, 47], we introduce IDAdapter.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on introducing a new method called IDAdapter, inspired by other works.",
      "processing_time": 58.05115461349487,
      "citing_paper_id": "268537084",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "and PhotoVerse [6] have also contributed novel approaches in this domain.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide specific details about the usage of a dataset. 'PhotoVerse' is mentioned as a contribution to the domain but does not specify a dataset.",
      "processing_time": 58.22185158729553,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "However, as noted by several studies [6, 20, 24, 37], the sole use of textual space embed-dings constrains the ultimate quality of generated images.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of using textual space embeddings for image generation.",
      "processing_time": 56.08251237869263,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "However, as noted by several studies [6, 20, 24, 37], the sole use of textual space embed-dings constrains the ultimate quality of generated images.",
      "catation_intent": "limitation",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of using textual space embeddings for image generation.",
      "processing_time": 56.08251237869263,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261705666
    },
    {
      "context_text": "For example, as indicated in [6] and [37], this approach often restricts the generated images to the expressions present in the input image, thus limiting the expansive creative potential of diffusion models.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a limitation of an approach in generating images using diffusion models.",
      "processing_time": 56.578007221221924,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "The comparative results were sourced directly from the study of [6], where the “ S* ” in the prompts refers to the “ [class noun] ” we mentioned.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another study. There are no clear identifiers for datasets, models, or other resources.",
      "processing_time": 57.854048013687134,
      "citing_paper_id": "268537084",
      "cited_paper_id": 261696798
    },
    {
      "context_text": "Originally, Generative Adversarial Networks (GANs) were employed for this purpose, as illustrated by [28], who achieved personalization by fine-tuning StyleGAN with around 100 facial images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (GANs) and a model (StyleGAN).",
      "processing_time": 57.37618160247803,
      "citing_paper_id": "268537084",
      "cited_paper_id": null
    },
    {
      "context_text": "Learning rate decay was not necessary due to the monotonic nature of the Adagrad accumulator.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Adagrad) and its properties. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.88556241989136,
      "citing_paper_id": "202573071",
      "cited_paper_id": 538820
    },
    {
      "context_text": "We compared to the Adam optimizer (Kingma & Ba, 2014) while training smaller models, but we noticed comparable convergence rates and signiﬁcant memory savings with Adagrad.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimizers (Adam and Adagrad) but does not refer to any specific datasets. The focus is on comparing optimization methods.",
      "processing_time": 57.48090171813965,
      "citing_paper_id": "202573071",
      "cited_paper_id": 538820
    },
    {
      "context_text": "We compared to the Adam optimizer (Kingma & Ba, 2014) while training smaller models, but we noticed comparable convergence rates and signiﬁcant memory savings with Adagrad.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions optimizers (Adam and Adagrad) but does not refer to any specific datasets. The focus is on comparing optimization methods.",
      "processing_time": 57.48090171813965,
      "citing_paper_id": "202573071",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Training took approximately 2 weeks using Adagrad (Duchi et al., 2011) with a linear warmup from 0 to 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Adagrad) used for training. The context is about the training process and optimization method, not about a dataset.",
      "processing_time": 58.84589076042175,
      "citing_paper_id": "202573071",
      "cited_paper_id": 538820
    },
    {
      "context_text": "Token embeddings were tied with the ﬁnal output embedding layer (Inan et al., 2016; Press & Wolf, 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods related to token embeddings and output embedding layers.",
      "processing_time": 55.719318866729736,
      "citing_paper_id": "202573071",
      "cited_paper_id": 836219
    },
    {
      "context_text": "We train on 140 GB of text drawing from a wide variety of domains: Wikipedia (En, De, Es, Fr), Project Gutenberg 1 , submissions from 45 subreddits, OpenWebText 2 , a large collection of news data (Hermann et al., 2015; Barrault et al., 2019; Sandhaus, 2008; Grusky et al., 2018), Amazon Reviews (McAuley et al., 2015), Europarl and UN data from WMT (En-De, En-Es, En-Fr) (Barrault et al., 2019), question-answer pairs (no context documents) from ELI5 (Fan et al., 2019) and the MRQA shared task 3 , which includes the Stanford Question Answering Dataset (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Wikipedia",
        "Project Gutenberg",
        "Reddit",
        "OpenWebText",
        "news data",
        "Amazon Reviews",
        "Europarl",
        "UN data",
        "ELI5",
        "MRQA",
        "Stanford Question Answering Dataset",
        "NewsQA",
        "TriviaQA",
        "SearchQA",
        "HotpotQA",
        "Natural Questions"
      ],
      "dataset_descriptions": {
        "Wikipedia": "Used to train a large language model, providing multilingual text content across various domains.",
        "Project Gutenberg": "Used to train a large language model, providing a vast collection of public domain books.",
        "Reddit": "Used to train a large language model, providing text from 45 subreddits to capture diverse online discussions.",
        "OpenWebText": "Used to train a large language model, providing web-scraped text data.",
        "news data": "Used to train a large language model, providing news articles from various sources to cover current events and topics.",
        "Amazon Reviews": "Used to train a large language model, providing user reviews to capture sentiment and opinion expressions.",
        "Europarl": "Used to train a large language model, providing multilingual parliamentary proceedings to enhance language diversity.",
        "UN data": "Used to train a large language model, providing multilingual documents from the United Nations to enhance language diversity.",
        "ELI5": "Used to train a large language model, providing question-answer pairs from the Explain Like I'm 5 subreddit to improve explanatory text generation.",
        "MRQA": "Used to train a large language model, providing a collection of question-answer datasets to improve question answering capabilities.",
        "Stanford Question Answering Dataset": "Used to train a large language model, providing context-free question-answer pairs to improve reading comprehension.",
        "NewsQA": "Used to train a large language model, providing context-free question-answer pairs derived from news articles to improve reading comprehension.",
        "TriviaQA": "Used to train a large language model, using the dataset to study the relationship between questions and answers in trivia contexts.",
        "SearchQA": "Used to train a large language model, providing context-free question-answer pairs derived from search engine queries to improve information retrieval.",
        "HotpotQA": "Used to train a large language model, providing complex question-answer pairs requiring multi-hop reasoning to improve deep comprehension.",
        "Natural Questions": "Used to train a large language model, providing context-free question-answer pairs to improve the ability to answer natural language questions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions multiple datasets used for training a large language model, including text from various sources such as Wikipedia, Project Gutenberg, Reddit, news data, Amazon Reviews, and question-answer pairs from ELI5 and MRQA.",
      "processing_time": 129.77157878875732,
      "citing_paper_id": "202573071",
      "cited_paper_id": 1167588
    },
    {
      "context_text": "…from ELI5 (Fan et al., 2019) and the MRQA shared task 3 , which includes the Stanford Question Answering Dataset (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ELI5",
        "Stanford Question Answering Dataset",
        "NewsQA",
        "TriviaQA",
        "SearchQA",
        "HotpotQA",
        "Natural Questions"
      ],
      "dataset_descriptions": {
        "ELI5": "Used for training and evaluating models on explain-like-I'm-5 style questions, focusing on generating clear and concise answers.",
        "Stanford Question Answering Dataset": "Used for training and evaluating question-answering models, focusing on reading comprehension and extracting answers from Wikipedia articles.",
        "NewsQA": "Used for training and evaluating models on news article comprehension, focusing on generating answers to questions based on news content.",
        "TriviaQA": "Used for training and evaluating models on trivia-style questions, focusing on complex reasoning and diverse sources.",
        "SearchQA": "Used for training and evaluating models on search-based question answering, focusing on integrating web search results into answers.",
        "HotpotQA": "Used for training and evaluating models on multi-hop question answering, focusing on reasoning across multiple sentences.",
        "Natural Questions": "Used for training and evaluating models on complex, real-world questions, focusing on comprehensive understanding and diverse answer types."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions multiple datasets used in the MRQA shared task, including ELI5 and several question answering datasets. These are specific, verifiable datasets used for training and evaluation.",
      "processing_time": 99.35685229301453,
      "citing_paper_id": "202573071",
      "cited_paper_id": 1167588
    },
    {
      "context_text": "Commonsense reasoning (Levesque et al., 2012) and abstractive summarization (Rush et al., 2015) represent two areas where these challenges remain readily apparent (Kry ´ sci ´ nski et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only research areas and challenges. No verifiable resources are identified.",
      "processing_time": 56.415831565856934,
      "citing_paper_id": "202573071",
      "cited_paper_id": 1918428
    },
    {
      "context_text": "In natural language processing, language models are often trained as conditional language models for speciﬁc tasks that require text generation (Brants et al., 2007; Sutskever et al., 2014; Rush et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to language models and text generation tasks.",
      "processing_time": 56.193655252456665,
      "citing_paper_id": "202573071",
      "cited_paper_id": 1918428
    },
    {
      "context_text": "…foundation to extend proposals of uniﬁed, multi-task systems for all of NLP (Collobert & Weston, 2008; Collobert et al., 2011), parsing and tagging (Hashimoto et al., 2016), multiple languages (Wu et al., 2016; Johnson et al., 2017), and multiple modalities (Luong et al., 2015; Kaiser et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.25503158569336,
      "citing_paper_id": "202573071",
      "cited_paper_id": 2213896
    },
    {
      "context_text": "…have pointed to natural language as a means for controlling these multi-task systems (McCann et al., 2018; Radford et al., 2019; Keskar et al., 2019), and several point to the beneﬁts of a code book either speciﬁed explicitly (Wu et al., 2016) or learned in a latent space (Kaiser et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in multi-task systems and neural machine translation.",
      "processing_time": 57.04873609542847,
      "citing_paper_id": "202573071",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "This is similar to how codes and natural language sequences have been used in multi-task settings (Wu et al., 2016; Johnson et al., 2017; McCann et al., 2018) to control conditional language models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to multi-task settings in controlling conditional language models.",
      "processing_time": 56.39231014251709,
      "citing_paper_id": "202573071",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "…to extend proposals of uniﬁed, multi-task systems for all of NLP (Collobert & Weston, 2008; Collobert et al., 2011), parsing and tagging (Hashimoto et al., 2016), multiple languages (Wu et al., 2016; Johnson et al., 2017), and multiple modalities (Luong et al., 2015; Kaiser et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 58.068371295928955,
      "citing_paper_id": "202573071",
      "cited_paper_id": 3603249
    },
    {
      "context_text": "…(Bengio et al., 2003) have played an important role in natural language processing through transferrable word vectors (Mikolov et al., 2013), contextualized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and models (Howard & Ruder, 2018; Radford et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.420021057128906,
      "citing_paper_id": "202573071",
      "cited_paper_id": 3626819
    },
    {
      "context_text": "…(Bengio et al., 2003) have played an important role in natural language processing through transferrable word vectors (Mikolov et al., 2013), contextualized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and models (Howard & Ruder, 2018; Radford et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.420021057128906,
      "citing_paper_id": "202573071",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "…(Bengio et al., 2003) have played an important role in natural language processing through transferrable word vectors (Mikolov et al., 2013), contextualized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and models (Howard & Ruder, 2018; Radford et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.420021057128906,
      "citing_paper_id": "202573071",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "They are also used as a means of learning word vectors (Mikolov et al., 2013), document vectors (Kiros et al., 2015), or contextualized word vectors (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) for transfer learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for learning word vectors, document vectors, and contextualized word vectors.",
      "processing_time": 57.92377853393555,
      "citing_paper_id": "202573071",
      "cited_paper_id": 3626819
    },
    {
      "context_text": "They are also used as a means of learning word vectors (Mikolov et al., 2013), document vectors (Kiros et al., 2015), or contextualized word vectors (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) for transfer learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for learning word vectors, document vectors, and contextualized word vectors.",
      "processing_time": 57.92377853393555,
      "citing_paper_id": "202573071",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "They are also used as a means of learning word vectors (Mikolov et al., 2013), document vectors (Kiros et al., 2015), or contextualized word vectors (McCann et al., 2017; Peters et al., 2018; Devlin et al., 2018) for transfer learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models for learning word vectors, document vectors, and contextualized word vectors.",
      "processing_time": 57.92377853393555,
      "citing_paper_id": "202573071",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "This potential showed that language models can offer an alternative to supervised multi-task learning as framed by several recent benchmarks (Wang et al., 2018; McCann et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to benchmarks which are excluded according to the instructions.",
      "processing_time": 56.72272276878357,
      "citing_paper_id": "202573071",
      "cited_paper_id": 5034059
    },
    {
      "context_text": "The language models themselves have been transferred to new tasks through ﬁne-tuning as well (Dai & Le, 2015; Radford et al., 2018; Howard & Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to fine-tuning language models. No verifiable resources are identified.",
      "processing_time": 57.37785983085632,
      "citing_paper_id": "202573071",
      "cited_paper_id": 7138078
    },
    {
      "context_text": "The language models themselves have been transferred to new tasks through ﬁne-tuning as well (Dai & Le, 2015; Radford et al., 2018; Howard & Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to fine-tuning language models. No verifiable resources are identified.",
      "processing_time": 57.37785983085632,
      "citing_paper_id": "202573071",
      "cited_paper_id": 49313245
    },
    {
      "context_text": "…with ReLU activation (Nair & Hinton, 2010) that projects inputs to an inner dimension f , with parameters U ∈ R d × f and V ∈ R f × d : Each block precedes core functionality with layer normalization (Ba et al., 2016; Child et al., 2019) and follows it with a residual connection (He et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.531439781188965,
      "citing_paper_id": "202573071",
      "cited_paper_id": 8236317
    },
    {
      "context_text": "…1 , submissions from 45 subreddits, OpenWebText 2 , a large collection of news data (Hermann et al., 2015; Barrault et al., 2019; Sandhaus, 2008; Grusky et al., 2018), Amazon Reviews (McAuley et al., 2015), Europarl and UN data from WMT (En-De, En-Es, En-Fr) (Barrault et al., 2019),…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenWebText",
        "news data (Hermann et al., 2015)",
        "news data (Barrault et al., 2019)",
        "news data (Sandhaus, 2008)",
        "news data (Grusky et al., 2018)",
        "Amazon Reviews",
        "Europarl",
        "UN data from WMT (En-De, En-Es, En-Fr)"
      ],
      "dataset_descriptions": {
        "OpenWebText": "Used to train models for personalized text generation, providing a diverse and large-scale web-based text corpus.",
        "news data (Hermann et al., 2015)": "Used to train models for personalized text generation, focusing on news articles and summaries.",
        "news data (Barrault et al., 2019)": "Used to train models for personalized text generation, focusing on news articles and summaries.",
        "news data (Sandhaus, 2008)": "Used to train models for personalized text generation, focusing on news articles and summaries.",
        "news data (Grusky et al., 2018)": "Used to train models for personalized text generation, focusing on news articles and summaries.",
        "Amazon Reviews": "Used to train models for personalized text generation, focusing on user reviews and ratings.",
        "Europarl": "Used to train models for personalized text generation, focusing on multilingual parliamentary proceedings.",
        "UN data from WMT (En-De, En-Es, En-Fr)": "Used to train models for personalized text generation, focusing on multilingual United Nations documents."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions several datasets that are used for training and evaluation in the context of personalized text generation. These datasets are specific and have clear identifiers.",
      "processing_time": 118.13985824584961,
      "citing_paper_id": "202573071",
      "cited_paper_id": 13752552
    },
    {
      "context_text": "…Lample et al., 2019) has improved perplexities on the most common benchmarks, and even without these memories, large Transformer architectures (Vaswani et al., 2017) like GPT-2 (Rad-ford et al., 2019), OpenGPT-2 5 , and Megatron 6 can achieve state-of-the-art results without directly training…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks. The context focuses on model performance and improvements in perplexity.",
      "processing_time": 57.51778864860535,
      "citing_paper_id": "202573071",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Each vector is the sum of a learned token embedding and a sinusoidal positional embedding as in the original Transformer architecture (Vaswani et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer architecture).",
      "processing_time": 55.81282711029053,
      "citing_paper_id": "202573071",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "This sequence of vectors is stacked into a matrix X 0 ∈ R n × d so that it can be processed by l attention layers (Vaswani et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention layers) from the cited paper 'Attention is All you Need'.",
      "processing_time": 57.53192400932312,
      "citing_paper_id": "202573071",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "…k heads that uses a causal mask to preclude attending to future tokens: The core of the second block is a feedforward network with ReLU activation (Nair & Hinton, 2010) that projects inputs to an inner dimension f , with parameters U ∈ R d × f and V ∈ R f × d : Each block precedes core…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only model architectures and components. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.839688539505005,
      "citing_paper_id": "202573071",
      "cited_paper_id": 15539264
    },
    {
      "context_text": "Language models (Bengio et al., 2003) have played an important role in natural language processing through transferrable word vectors (Mikolov et al., 2013), contextualized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and models (Howard & Ruder, 2018; Radford et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 57.694819688797,
      "citing_paper_id": "202573071",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Language models (Bengio et al., 2003) have played an important role in natural language processing through transferrable word vectors (Mikolov et al., 2013), contextualized word vectors (Peters et al., 2018; Devlin et al., 2018; Lample & Conneau, 2019), and models (Howard & Ruder, 2018; Radford et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 57.694819688797,
      "citing_paper_id": "202573071",
      "cited_paper_id": 221275765
    },
    {
      "context_text": "Recent work in sampling methods for text generation has focused on reducing repetition by replacing it with novel, coherent text (Fan et al., 2018; Holtzman et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 56.3671817779541,
      "citing_paper_id": "202573071",
      "cited_paper_id": 44134226
    },
    {
      "context_text": "Recent work in sampling methods for text generation has focused on reducing repetition by replacing it with novel, coherent text (Fan et al., 2018; Holtzman et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses methods and findings related to text generation.",
      "processing_time": 56.3671817779541,
      "citing_paper_id": "202573071",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Typically prompts generated by models (Fan et al., 2018) or written by humans can only be used to provide a rough guide or starting point for the generated text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods for generating prompts. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 57.677311182022095,
      "citing_paper_id": "202573071",
      "cited_paper_id": 44134226
    },
    {
      "context_text": "…text and consistency with prompts and prior generated text remains a difﬁcult challenge, but this work found that relying on inference-time methods (Fan et al., 2018; Holtzman et al., 2019) that are closer in behavior to context-based losses (See et al., 2017; Welleck et al., 2019) provides a…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.05078458786011,
      "citing_paper_id": "202573071",
      "cited_paper_id": 44134226
    },
    {
      "context_text": "Current state-of-the-art meth-ods (Dai et al., 2019; Radford et al., 2019) train a neural network with parameters θ to minimize the negative log-likelihood over a dataset D = { x 1 , . . . , x | D | } : Because language models learn p θ ( x i | x <i ) , a new ˜ x of length m can be generated by…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a generic 'dataset D' without providing a name or specific identifier.",
      "processing_time": 58.19728636741638,
      "citing_paper_id": "202573071",
      "cited_paper_id": 57759363
    },
    {
      "context_text": "Current state-of-the-art meth-ods (Dai et al., 2019; Radford et al., 2019) train a neural network with parameters θ to minimize the negative log-likelihood over a dataset D = { x 1 , . . . , x | D | } : Because language models learn p θ ( x i | x <i ) , a new ˜ x of length m can be generated by…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a generic 'dataset D' without providing a name or specific identifier.",
      "processing_time": 58.19728636741638,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Recent work on memory mechanisms (Dai et al., 2019; Lample et al., 2019) has improved perplexities on the most common benchmarks, and even without these memories, large Transformer architectures (Vaswani et al., 2017) like GPT-2 (Rad-ford et al., 2019), OpenGPT-2 5 , and Megatron 6 can achieve…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and benchmarks which are excluded according to the instructions.",
      "processing_time": 56.71521806716919,
      "citing_paper_id": "202573071",
      "cited_paper_id": 57759363
    },
    {
      "context_text": "…with prompts and prior generated text remains a difﬁcult challenge, but this work found that relying on inference-time methods (Fan et al., 2018; Holtzman et al., 2019) that are closer in behavior to context-based losses (See et al., 2017; Welleck et al., 2019) provides a reasonable solution as…",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and findings. The context focuses on inference-time methods and context-based losses, which are not datasets.",
      "processing_time": 58.88006949424744,
      "citing_paper_id": "202573071",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "…with prompts and prior generated text remains a difﬁcult challenge, but this work found that relying on inference-time methods (Fan et al., 2018; Holtzman et al., 2019) that are closer in behavior to context-based losses (See et al., 2017; Welleck et al., 2019) provides a reasonable solution…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 58.52946662902832,
      "citing_paper_id": "202573071",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Rather than choosing a ﬁxed value of k , as is common practice, Holtzman et al. (2019) suggested adapting k heuristically.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting k heuristically in neural text generation.",
      "processing_time": 56.84340691566467,
      "citing_paper_id": "202573071",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "However, this is known to create repetitions of phrases or sentences even for large well-trained models (Radford et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and their behavior. No verifiable resources are identified.",
      "processing_time": 57.03077054023743,
      "citing_paper_id": "202573071",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "However, this is known to create repetitions of phrases or sentences even for large well-trained models (Radford et al., 2019; Holtzman et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to models and their behavior. No verifiable resources are identified.",
      "processing_time": 57.03077054023743,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "(2019), among others, have argued artificial intelligence has such an omni-use character and have suggested governance policies emerging from the responsible innovation literature (Brundage, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the discussion of AI's omni-use character and governance policies.",
      "processing_time": 58.38361144065857,
      "citing_paper_id": "202573071",
      "cited_paper_id": 152125702
    },
    {
      "context_text": "Brundage et al. (2019), among others, have argued artificial intelligence has such an omni-use character and have suggested governance policies emerging from the responsible innovation literature (Brundage, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the discussion of AI's omni-use character and governance policies.",
      "processing_time": 58.38947105407715,
      "citing_paper_id": "202573071",
      "cited_paper_id": 152125702
    },
    {
      "context_text": "Brundage et al. (2019), among others, have argued artiﬁcial intelligence has such an omni-use character and have suggested governance policies emerging from the responsible innovation literature (Brundage, 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the argument about AI's omni-use character and governance policies.",
      "processing_time": 58.21921467781067,
      "citing_paper_id": "202573071",
      "cited_paper_id": 152125702
    },
    {
      "context_text": "Yet language models show potential for mitigating these problems directly (Trinh & Le, 2018; Radford et al., 2019) or indirectly (Ra-jani et al., 2019; Xenouleas et al., 2019; Scialom et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to language models and their potential for mitigating certain problems. No verifiable resources are identified.",
      "processing_time": 58.394981384277344,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Yet language models show potential for mitigating these problems directly (Trinh & Le, 2018; Radford et al., 2019) or indirectly (Ra-jani et al., 2019; Xenouleas et al., 2019; Scialom et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to language models and their potential for mitigating certain problems. No verifiable resources are identified.",
      "processing_time": 58.394981384277344,
      "citing_paper_id": "202573071",
      "cited_paper_id": 202541447
    },
    {
      "context_text": "Because these latter language models are trained on far more diverse data than is used in the supervised setting, they demonstrate impressive text generation capabilities (Radford et al., 2019; Zellers et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to language models and their training data. No verifiable resource names are provided.",
      "processing_time": 57.41826033592224,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Several works have pointed to natural language as a means for controlling these multi-task systems (McCann et al., 2018; Radford et al., 2019; Keskar et al., 2019), and several point to the beneﬁts of a code book either speciﬁed explicitly (Wu et al., 2016) or learned in a latent space (Kaiser et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches. No verifiable resources are identified.",
      "processing_time": 57.162259101867676,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "These models demonstrate the potential to learn multiple tasks as well as quick adaptation to patterns in input prompts (Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. The context is about the models' ability to learn multiple tasks and adapt to input patterns.",
      "processing_time": 58.819910764694214,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Inspired by the degree of control available in image generation as well as the recent progress in text generation (Radford et al., 2019) and multitask learning McCann et al. (2018), we train a language model that is conditioned on a variety of control codes (Pfaff, 1979; Poplack, 1980) that make…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers and concepts. No verifiable resources are identified.",
      "processing_time": 56.93834638595581,
      "citing_paper_id": "202573071",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "We also experimented with explicit memory-saving optimizers including SM3 (Anil et al., 2019), Adafactor (Shazeer & Stern, 2018), and NovoGrad (Ginsburg et al., 2019) with mixed results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several optimizers but does not refer to any specific datasets. The context is about experimenting with different optimizers, which are methods, not datasets.",
      "processing_time": 58.817676067352295,
      "citing_paper_id": "202573071",
      "cited_paper_id": 166228676
    },
    {
      "context_text": "All other control codes are injected into the data without such special treatment (Moryossef et al., 2019; Caswell et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only control codes and their treatment in data. No verifiable resources are identified.",
      "processing_time": 57.31646513938904,
      "citing_paper_id": "202573071",
      "cited_paper_id": 189928248
    },
    {
      "context_text": "Much research then focused on methods for controlling the generation process and improving estimation of generative distributions (Arjovsky et al., 2017; Chen et al., 2016; Kingma & Welling, 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and research works.",
      "processing_time": 55.981725454330444,
      "citing_paper_id": "202573071",
      "cited_paper_id": 211146177
    },
    {
      "context_text": "Because x is a sequence, it is natural to factorize this distribution using the chain rule of probability (Bengio et al., 2003): This decomposes language modeling into next-word prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for language modeling.",
      "processing_time": 56.091001987457275,
      "citing_paper_id": "202573071",
      "cited_paper_id": 221275765
    },
    {
      "context_text": "• Finally, the README asks users to develop appropriate documentation (PAI, 2019; Arnold et al., 2018; Mitchell et al., 2019) when building on CTRL and to tell the research team how they are using CTRL by emailing ctrl-monitoring@salesforce.com .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only guidelines for documentation and communication with the research team.",
      "processing_time": 56.713918924331665,
      "citing_paper_id": "202573071",
      "cited_paper_id": 263502962
    },
    {
      "context_text": "• Finally, the README asks users to develop appropriate documentation (PAI, 2019; Arnold et al., 2018; Mitchell et al., 2019) when building on CTRL and to tell the research team how they are using CTRL by emailing ctrl-monitoring@salesforce.com .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only guidelines for documentation and communication with the research team.",
      "processing_time": 56.713918924331665,
      "citing_paper_id": "202573071",
      "cited_paper_id": 263502965
    },
    {
      "context_text": "o the included questions, to pose further questions, and suggest solutions by emailing ctrl-monitoring@salesforce.com. Finally, the README asks users to develop appropriate documentation (PAI, 2019; Arnold et al., 2018; Mitchell et al., 2019) when building on CTRL and to tell the research team how they are using CTRL by emailing ctrl-monitoring@salesforce.com. This facilitates a post-release monitoring plan that ob",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to guidelines and best practices for AI development and monitoring.",
      "processing_time": 56.95060443878174,
      "citing_paper_id": "202573071",
      "cited_paper_id": 263502965
    },
    {
      "context_text": "…dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of personalized text generation. No dataset names are present in the text.",
      "processing_time": 58.095649003982544,
      "citing_paper_id": "246426909",
      "cited_paper_id": 13679932
    },
    {
      "context_text": "…dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of personalized text generation. No dataset names are present in the text.",
      "processing_time": 58.095649003982544,
      "citing_paper_id": "246426909",
      "cited_paper_id": 168169995
    },
    {
      "context_text": "…dialogue (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), translation (Kreutzer et al., 2018; Bahdanau et al., 2016), semantic parsing (Lawrence and Riezler, 2018), story generation (Zhou and Xu, 2020), review generation (Cho et al., 2018), and evidence extraction (Perez et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of personalized text generation. No dataset names are present in the text.",
      "processing_time": 58.095649003982544,
      "citing_paper_id": "246426909",
      "cited_paper_id": 202572744
    },
    {
      "context_text": "Again following Stiennon et al. (2020), we fine-tuned the SFT model using PPO (Schulman et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the methodology used for fine-tuning the model.",
      "processing_time": 58.13060784339905,
      "citing_paper_id": "246426909",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "Again following Stiennon et al. (2020), we fine-tuned the SFT model using PPO (Schulman et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the methodology used for fine-tuning the model.",
      "processing_time": 58.13060784339905,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Finally, we use this RM as a reward function and fine-tune our supervised learning baseline to maximize this reward using the PPO algorithm (Schulman et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PPO algorithm).",
      "processing_time": 56.10802149772644,
      "citing_paper_id": "246426909",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "We fine-tune the supervised policy to optimize this reward using the PPO algorithm (Schulman et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the PPO algorithm but does not refer to any specific dataset. The context is about the method used for fine-tuning a policy, not a dataset.",
      "processing_time": 59.079529762268066,
      "citing_paper_id": "246426909",
      "cited_paper_id": 28695052
    },
    {
      "context_text": "Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and applications. The cited papers do not introduce new datasets either.",
      "processing_time": 57.47809886932373,
      "citing_paper_id": "246426909",
      "cited_paper_id": 53424488
    },
    {
      "context_text": "Originally developed for training simple robots in simulated environments and Atari games (Christiano et al., 2017; Ibarz et al., 2018), it has recently been applied to fine-tuning language models to summarize text (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and applications. The cited papers do not introduce new datasets either.",
      "processing_time": 57.47809886932373,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "We make progress on aligning language models by training them to act in accordance with the user’s intention (Leike et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to training language models. The cited paper title suggests a method rather than a dataset.",
      "processing_time": 58.489014863967896,
      "citing_paper_id": "246426909",
      "cited_paper_id": 53424488
    },
    {
      "context_text": "There are many ways to mitigate these harms, including by fine-tuning on a small, value-targeted dataset (Solaiman and Dennison, 2021), filtering the pretraining dataset (Ngo et al., 2021), or human-in-the-loop data collection (Dinan et al., 2019; Xu et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'value-targeted dataset' but does not provide a specific name. The context focuses on methods to mitigate harms in AI systems, which is relevant to personalized text generation.",
      "processing_time": 59.61560416221619,
      "citing_paper_id": "246426909",
      "cited_paper_id": 201070022
    },
    {
      "context_text": "There are many ways to mitigate these harms, including by fine-tuning on a small, value-targeted dataset (Solaiman and Dennison, 2021), filtering the pretraining dataset (Ngo et al., 2021), or human-in-the-loop data collection (Dinan et al., 2019; Xu et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'value-targeted dataset' but does not provide a specific name. The context focuses on methods to mitigate harms in AI systems, which is relevant to personalized text generation.",
      "processing_time": 59.61560416221619,
      "citing_paper_id": "246426909",
      "cited_paper_id": 222341902
    },
    {
      "context_text": "We believe both these behaviors could be dramatically reduced with adversarial data collection (Dinan et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'adversarial data collection' but does not specify a dataset name. The context suggests a method or approach rather than a specific dataset.",
      "processing_time": 57.831058740615845,
      "citing_paper_id": "246426909",
      "cited_paper_id": 201070022
    },
    {
      "context_text": "Specifically, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to fine-tune GPT-3 to follow a broad class of written instructions (see Figure 2).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on using reinforcement learning from human feedback to fine-tune GPT-3.",
      "processing_time": 58.78329086303711,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "In Stiennon et al. (2020), the RM is trained on a dataset of comparisons between two model outputs on the same input.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "dataset of comparisons between two model outputs"
      ],
      "dataset_descriptions": {
        "dataset of comparisons between two model outputs": "Used to train a reward model on human-preferred outputs, focusing on improving text summarization quality through direct feedback."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions a dataset of comparisons between two model outputs, which is specific enough to be considered a dataset, though it lacks a formal name.",
      "processing_time": 65.80733585357666,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "For comparison, in the summarization work of Stiennon et al. (2020) researcher-researcher agreement was 73 ± 4% .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison of researcher-researcher agreement in summarization work.",
      "processing_time": 57.18019890785217,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Our methodology follows that of Ziegler et al. (2019) and Stiennon et al. (2020), who applied it in the stylistic continuation and summarization domains.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methodologies from other papers. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.335036277770996,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "Compared to earlier work that collects human preference data on the task of summarization (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), our inputs span a much broader range of tasks, and can occasionally include controversial and sensitive topics.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions human preference data on summarization tasks but does not specify a named dataset. The reference to 'human preference data' is too generic and lacks a specific identifier.",
      "processing_time": 58.905389070510864,
      "citing_paper_id": "246426909",
      "cited_paper_id": 221665105
    },
    {
      "context_text": "InstructGPT does not significantly improve over GPT-3 on the Winogender (Rudinger et al., 2018) and CrowSPairs (Nangia et al., 2020) datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Winogender",
        "CrowSPairs"
      ],
      "dataset_descriptions": {
        "Winogender": "Used to evaluate gender bias in language models, specifically measuring pronoun resolution in gendered contexts.",
        "CrowSPairs": "Applied to assess social biases in masked language models, focusing on stereotypical associations and fairness."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions two specific datasets, Winogender and CrowSPairs, which are used to measure performance on social biases in language models.",
      "processing_time": 68.60102558135986,
      "citing_paper_id": "246426909",
      "cited_paper_id": 222090785
    },
    {
      "context_text": "…produce biased outputs (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak private data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al., 2021), and be used maliciously; for a thorough review we…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only issues related to large language models. No verifiable resources are identified.",
      "processing_time": 57.31672382354736,
      "citing_paper_id": "246426909",
      "cited_paper_id": 229156229
    },
    {
      "context_text": "…intervals. used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely” (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their training objectives. The context focuses on the difference in training objectives between large language models and instruction-following models.",
      "processing_time": 58.958415031433105,
      "citing_paper_id": "246426909",
      "cited_paper_id": 231573431
    },
    {
      "context_text": "We start with a pretrained language model (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), a distribution of prompts on which we want our model to produce aligned outputs, and a team of trained human labelers (see Section 3.3 for details).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.66422486305237,
      "citing_paper_id": "246426909",
      "cited_paper_id": 231573431
    },
    {
      "context_text": "Language models can produce biased outputs (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak private data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al., 2021), and be used maliciously; for a…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets by name. It only references issues with language models such as bias, privacy leaks, and misinformation. The cited paper title 'BOLD' suggests a dataset, but it is not explicitly mentioned in the citation context.",
      "processing_time": 61.91075420379639,
      "citing_paper_id": "246426909",
      "cited_paper_id": 231719337
    },
    {
      "context_text": "These risks have been extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to risks documented in other works. No verifiable resources are identified.",
      "processing_time": 57.70347619056702,
      "citing_paper_id": "246426909",
      "cited_paper_id": 232404883
    },
    {
      "context_text": "These risks have been extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to risks documented in other works. No verifiable resources are identified.",
      "processing_time": 57.70347619056702,
      "citing_paper_id": "246426909",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general issues with language models. No verifiable resources are identified.",
      "processing_time": 57.34920930862427,
      "citing_paper_id": "246426909",
      "cited_paper_id": 232404883
    },
    {
      "context_text": "However, these models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general issues with language models. No verifiable resources are identified.",
      "processing_time": 57.34920930862427,
      "citing_paper_id": "246426909",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "We additionally compare InstructGPT to fine-tuning 175B GPT-3 on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets, which both consist of a variety of NLP tasks, combined with natural language instructions for each task (they differ in the NLP datasets included, and the style of…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FLAN",
        "T0"
      ],
      "dataset_descriptions": {
        "FLAN": "Used to fine-tune GPT-3 on a variety of NLP tasks with natural language instructions, focusing on zero-shot task generalization.",
        "T0": "Used to fine-tune GPT-3 on a variety of NLP tasks with natural language instructions, focusing on zero-shot task generalization."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'FLAN' and 'T0' as datasets used for fine-tuning GPT-3. These are specific datasets consisting of various NLP tasks with natural language instructions.",
      "processing_time": 73.40532803535461,
      "citing_paper_id": "246426909",
      "cited_paper_id": 239009562
    },
    {
      "context_text": "We compare GPT-3 fine-tuned on our human preference data (i.e. InstructGPT) to GPT-3 fine-tuned on two different compilations of public NLP tasks: the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) (in particular, the T0++ variant).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human preference data' and 'public NLP tasks' but does not specify any named datasets. FLAN and T0 are mentioned but are models, not datasets.",
      "processing_time": 59.17623257637024,
      "citing_paper_id": "246426909",
      "cited_paper_id": 239009562
    },
    {
      "context_text": "There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei et al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training and evaluation data, formatting of instructions, size of pretrained models, and other experimental…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to training and evaluation data. No multi-word proper nouns, acronyms, or hyphenated names with digits are present.",
      "processing_time": 60.03070783615112,
      "citing_paper_id": "246426909",
      "cited_paper_id": 239009562
    },
    {
      "context_text": "In Figure 5a, we also compare InstructGPT to our 175B GPT-3 baselines fine-tuned on the FLAN (Wei et al., 2021) and T0 (Sanh et al., 2021) datasets (see Appendix D for details).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FLAN",
        "T0"
      ],
      "dataset_descriptions": {
        "FLAN": "Used to fine-tune GPT-3 baselines, focusing on multitask prompted training for zero-shot task generalization.",
        "T0": "Used to fine-tune GPT-3 baselines, focusing on multitask prompted training for zero-shot task generalization."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions 'FLAN' and 'T0' datasets, which are specific and verifiable resources used for fine-tuning models.",
      "processing_time": 70.52159094810486,
      "citing_paper_id": "246426909",
      "cited_paper_id": 239009562
    },
    {
      "context_text": "Following Askell et al. (2021), we say our models are aligned if they are helpful, truthful, and harmless (we elaborate in Appendix C.2).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only alignment criteria for models. No verifiable resources are referenced.",
      "processing_time": 56.53682732582092,
      "citing_paper_id": "246426909",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "Using the language of Askell et al. (2021), we want language models to be helpful (they should help the user solve their task), honest (they shouldn’t fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general qualities that language models should have. There are no verifiable resources or datasets mentioned.",
      "processing_time": 58.10716772079468,
      "citing_paper_id": "246426909",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "In concurrent work, Askell et al. (2021); Bai et al. (2022) propose language assistants as a testbed for alignment research, and train models using RLHF.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The focus is on the use of language assistants and reinforcement learning for alignment research.",
      "processing_time": 58.47901272773743,
      "citing_paper_id": "246426909",
      "cited_paper_id": 244799619
    },
    {
      "context_text": "…planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), case-based reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.088629722595215,
      "citing_paper_id": "51729727",
      "cited_paper_id": 2126816
    },
    {
      "context_text": "…planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), case-based reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.088629722595215,
      "citing_paper_id": "51729727",
      "cited_paper_id": 16270356
    },
    {
      "context_text": "…planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), case-based reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 58.088629722595215,
      "citing_paper_id": "51729727",
      "cited_paper_id": 18228913
    },
    {
      "context_text": "With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for controllable generation. No verifiable resources are identified.",
      "processing_time": 57.90734791755676,
      "citing_paper_id": "51729727",
      "cited_paper_id": 5002792
    },
    {
      "context_text": "With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for controllable generation. No verifiable resources are identified.",
      "processing_time": 57.90734791755676,
      "citing_paper_id": "51729727",
      "cited_paper_id": 5024767
    },
    {
      "context_text": "With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for controllable generation. No verifiable resources are identified.",
      "processing_time": 57.90734791755676,
      "citing_paper_id": "51729727",
      "cited_paper_id": 13226696
    },
    {
      "context_text": "With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works on controllable generation of images and dialog responses.",
      "processing_time": 57.61184740066528,
      "citing_paper_id": "51729727",
      "cited_paper_id": 5002792
    },
    {
      "context_text": "With the recent successes on controllable generation of images (Chen et al., 2016; Siddharth et al., 2017; Lample et al., 2017), dialog responses (Wang et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works on controllable generation of images and dialog responses.",
      "processing_time": 57.61184740066528,
      "citing_paper_id": "51729727",
      "cited_paper_id": 5024767
    },
    {
      "context_text": ", 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to different styles of text and style transfer methods.",
      "processing_time": 57.10671424865723,
      "citing_paper_id": "51729727",
      "cited_paper_id": 6484065
    },
    {
      "context_text": ", 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to different styles of text and style transfer methods.",
      "processing_time": 57.10671424865723,
      "citing_paper_id": "51729727",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "…2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and personalized stories.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only general types of text such as dialog responses, poems, and different styles of text. No clear, verifiable datasets are identified.",
      "processing_time": 59.74766802787781,
      "citing_paper_id": "51729727",
      "cited_paper_id": 6484065
    },
    {
      "context_text": "…2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and personalized stories.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets by name, only general types of text such as dialog responses, poems, and different styles of text. No clear, verifiable datasets are identified.",
      "processing_time": 59.74766802787781,
      "citing_paper_id": "51729727",
      "cited_paper_id": 7296803
    },
    {
      "context_text": "Martin et al. (2017) train a recurrent encoder-decoder neural network (Sutskever et al., 2014) to predict the next event in the story.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (recurrent encoder-decoder neural network).",
      "processing_time": 56.951805114746094,
      "citing_paper_id": "51729727",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Roemmele et al. (2017) use skip-thought vectors (Kiros et al., 2015) to encode sentences, and a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) erate stories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not introduce new dataset names either.",
      "processing_time": 57.40225434303284,
      "citing_paper_id": "51729727",
      "cited_paper_id": 9126867
    },
    {
      "context_text": "Roemmele et al. (2017) use skip-thought vectors (Kiros et al., 2015) to encode sentences, and a Long Short-Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) erate stories.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not introduce new dataset names either.",
      "processing_time": 57.40225434303284,
      "citing_paper_id": "51729727",
      "cited_paper_id": 11293070
    },
    {
      "context_text": "…2017; Lample et al., 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al., 2017), and different styles of text (Hu et al., 2017; Ficler and Goldberg, 2017; Shen et al., 2017; Fu et al., 2017). people would want to control a story generation system to produce interesting and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only various types of text generation tasks. No clear, verifiable datasets are identified.",
      "processing_time": 57.21885585784912,
      "citing_paper_id": "51729727",
      "cited_paper_id": 11054023
    },
    {
      "context_text": ", 2017), dialog responses (Wang et al., 2017), poems (Ghazvininejad et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'dialog responses' which could imply a dataset, but lacks a specific name. No other specific datasets are mentioned.",
      "processing_time": 57.43742632865906,
      "citing_paper_id": "51729727",
      "cited_paper_id": 13226696
    },
    {
      "context_text": "Li et al. (2013) introduced plot graphs which contain events and their relations to represent story-line.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'plot graphs' but does not refer to a specific, verifiable dataset. It describes a method or concept rather than a reusable dataset.",
      "processing_time": 58.08715295791626,
      "citing_paper_id": "51729727",
      "cited_paper_id": 16270356
    },
    {
      "context_text": ", 2005), or generalizing knowledge from existing stories to assemble new ones (Swanson and Gordon, 2012; Li et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references general concepts and methods.",
      "processing_time": 56.44870710372925,
      "citing_paper_id": "51729727",
      "cited_paper_id": 16270356
    },
    {
      "context_text": "We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence1 (Egidi and Gerrig, 2009) and storyline.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using RNNs to control story ending valence. No verifiable dataset names are present.",
      "processing_time": 58.78320050239563,
      "citing_paper_id": "51729727",
      "cited_paper_id": 42277086
    },
    {
      "context_text": "We adapt the RAKE algorithm (Rose et al., 2010) for keyword extraction, which builds document graphs and weights the importance of each word combining several word-level and graph-level criteria.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the RAKE algorithm but does not refer to any specific dataset. The RAKE algorithm is a method for keyword extraction, not a dataset.",
      "processing_time": 57.9341094493866,
      "citing_paper_id": "51729727",
      "cited_paper_id": 58960973
    },
    {
      "context_text": "Early attempts in this ﬁeld relied on symbolic planning (Meehan, 1977; Lebowitz, 1987; Turner, 1993; Bringsjord and Ferrucci, 1999; Perez and Sharples, 2001; Riedl and Young, 2010), case-based reasoning (Gervas et al., 2005), or generalizing knowledge from existing stories to assemble new ones…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches used in early attempts at personalized text generation.",
      "processing_time": 56.589073181152344,
      "citing_paper_id": "51729727",
      "cited_paper_id": null
    },
    {
      "context_text": "Similar techniques have been used for tasks such as style transfer (Gatys et al., 2016; Johnson et al., 2016), colorization (Zhang et al., 2016; Larsson et al., 2016; Zhang et al., 2017), and super-resolution (Ledig et al., 2017; Johnson et al., 2016; Wang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and tasks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.75930953025818,
      "citing_paper_id": "258999614",
      "cited_paper_id": 980236
    },
    {
      "context_text": "Similar techniques have been used for tasks such as style transfer (Gatys et al., 2016; Johnson et al., 2016), colorization (Zhang et al., 2016; Larsson et al., 2016; Zhang et al., 2017), and super-resolution (Ledig et al., 2017; Johnson et al., 2016; Wang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only techniques and tasks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.75930953025818,
      "citing_paper_id": "258999614",
      "cited_paper_id": 7023610
    },
    {
      "context_text": "…et al., 2014; Brock et al., 2019; Karras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and image manipulation using text (Gal…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works on text-to-image generation and image manipulation using text.",
      "processing_time": 57.22984290122986,
      "citing_paper_id": "258999614",
      "cited_paper_id": 1563370
    },
    {
      "context_text": "The diffusion U-Net (Ronneberger et al., 2015) contains encoder, middle, and decoder layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model architecture (diffusion U-Net) but does not reference any specific dataset. The citation is about the model structure, not a dataset.",
      "processing_time": 58.480828285217285,
      "citing_paper_id": "258999614",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "…have the loss where t is the timestep, z t is the latent code at timestep t, c π is the text encoder that maps text prompts y into text embeddings, ϵ is the noise sampled from Gaussian distribution, and ϵ θ is the denoising network ( i.e. , U-Net (Ronneberger et al., 2015)) that predicts the noise.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (U-Net) and a loss function. No verifiable datasets are referenced.",
      "processing_time": 57.587276220321655,
      "citing_paper_id": "258999614",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "…2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and image manipulation using text (Gal et al., 2022; Patashnik et al., 2021; Xia et al., 2021;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works in text-to-image generation and image manipulation using text.",
      "processing_time": 57.21160435676575,
      "citing_paper_id": "258999614",
      "cited_paper_id": 8858625
    },
    {
      "context_text": "Visual condition is commonly used in image-to-image translation (Isola et al., 2017; Zhu et al., 2017a;b; Choi et al., 2018; Park et al., 2020), which involves training a model to map an input image to an output image based on a certain condition, e.g. , edge, sketch, or semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses image-to-image translation methods and does not mention any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 57.42214584350586,
      "citing_paper_id": "258999614",
      "cited_paper_id": 9417016
    },
    {
      "context_text": "Visual condition is commonly used in image-to-image translation (Isola et al., 2017; Zhu et al., 2017a;b; Choi et al., 2018; Park et al., 2020), which involves training a model to map an input image to an output image based on a certain condition, e.g. , edge, sketch, or semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses image-to-image translation methods and does not mention any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 57.42214584350586,
      "citing_paper_id": "258999614",
      "cited_paper_id": 19046372
    },
    {
      "context_text": "Visual condition is commonly used in image-to-image translation (Isola et al., 2017; Zhu et al., 2017a;b; Choi et al., 2018; Park et al., 2020), which involves training a model to map an input image to an output image based on a certain condition, e.g. , edge, sketch, or semantic segmentation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses image-to-image translation methods and does not mention any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 57.42214584350586,
      "citing_paper_id": "258999614",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "In the literature of GANs (Goodfellow et al., 2014; Brock et al., 2019; Karras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and image manipulation using text (Gal et al., 2022; Patashnik et al., 2021; Xia et al., 2021; Abdal et al., 2022), advancing the generation of images conditioned on plain text.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various works on GANs and their applications in text-to-image generation and image manipulation.",
      "processing_time": 58.108344316482544,
      "citing_paper_id": "258999614",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "In the literature of GANs (Goodfellow et al., 2014; Brock et al., 2019; Karras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to GAN-related works and text-to-image generation. No verifiable resources are identified.",
      "processing_time": 57.939204692840576,
      "citing_paper_id": "258999614",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "In the literature of GANs (Goodfellow et al., 2014; Brock et al., 2019; Karras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to GAN-related works and text-to-image generation. No verifiable resources are identified.",
      "processing_time": 57.939204692840576,
      "citing_paper_id": "258999614",
      "cited_paper_id": 52889459
    },
    {
      "context_text": "In the literature of GANs (Goodfellow et al., 2014; Brock et al., 2019; Karras et al., 2019; 2020; 2021), plenty of works have gained remarkable progress in text-to-image generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to GAN-related works and text-to-image generation. No verifiable resources are identified.",
      "processing_time": 57.939204692840576,
      "citing_paper_id": "258999614",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "Our image attention module follows the standard attention-feedforward fashion (Vaswani et al., 2017), and has the same structure as the text cross-attention used in LDMs (Rombach et al., 2022) only differing in the dimension of the condition projection layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No dataset names are present in the citation span.",
      "processing_time": 56.777334213256836,
      "citing_paper_id": "258999614",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "The proposed image cross-attention blocks, designed to accept visual conditions with the standard attention architecture as in (Vaswani et al., 2017), are integrated into specific attention layers within the decoder of the diffusion U-Net architecture.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only architectural components and methods. The cited paper 'Attention is All you Need' is a methodological reference, not a dataset.",
      "processing_time": 58.74812078475952,
      "citing_paper_id": "258999614",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "DreamBooth and Custom Diffusion both meet the issue of language drift (Lee et al., 2019; Lu et al., 2020) because finetuning the pretrained model on new data can lead to a loss of the preformed language knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the issue of language drift during fine-tuning.",
      "processing_time": 57.598342180252075,
      "citing_paper_id": "258999614",
      "cited_paper_id": 214713859
    },
    {
      "context_text": "Stable Diffusion (SD) (Rombach et al., 2022) is a latent text-to-image diffusion model derived from classic Denoising Diffusion Probabilistic Models (DDPMs) (Ho et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.039222955703735,
      "citing_paper_id": "258999614",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Nowadays, people can easily generate unprecedentedly high-quality photorealistic images with text prompts using fast-growing text-to-image diffusion models (Ho et al., 2020; Song et al., 2021; Ramesh et al., 2022; Nichol et al., 2022; Saharia et al., 2022; Rombach et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.38096308708191,
      "citing_paper_id": "258999614",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "Nowadays, people can easily generate unprecedentedly high-quality photorealistic images with text prompts using fast-growing text-to-image diffusion models (Ho et al., 2020; Song et al., 2021; Ramesh et al., 2022; Nichol et al., 2022; Saharia et al., 2022; Rombach et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.38096308708191,
      "citing_paper_id": "258999614",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "Ho et al. (Ho et al., 2020) first presents DDPMs to progressively denoise from random noise to a synthesized image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDPMs).",
      "processing_time": 55.69746232032776,
      "citing_paper_id": "258999614",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "The use of diffusion-based methods (Ho et al., 2020) has pushed the boundaries of text-to-image generation to a new level.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion-based methods) and its impact on text-to-image generation.",
      "processing_time": 57.366549253463745,
      "citing_paper_id": "258999614",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "DDIMs (Song et al., 2021) accelerate the sampling process of DDPMs. Latent diffusion models (LDMS) (Rombach et al., 2022) introduce multiple conditions in latent diffusion space, producing realistic and high-fidelity text-to-image synthesis results.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the models and their capabilities.",
      "processing_time": 56.7763774394989,
      "citing_paper_id": "258999614",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "…generation (Reed et al., 2016; Zhu et al., 2019; Tao et al., 2022; Xu et al., 2018; Zhang et al., 2021; Ye et al., 2021) and image manipulation using text (Gal et al., 2022; Patashnik et al., 2021; Xia et al., 2021; Abdal et al., 2022), advancing the generation of images conditioned on plain text.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that discuss text-guided image generation and manipulation. No verifiable resources are named.",
      "processing_time": 58.08118486404419,
      "citing_paper_id": "258999614",
      "cited_paper_id": 235702618
    },
    {
      "context_text": "We follow (Gal et al., 2023a) to learn the embedding S ⋆ .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for learning an embedding.",
      "processing_time": 55.27687358856201,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Recent works (Gal et al., 2023a; Ruiz et al., 2023; Kumari et al., 2023) make this fantasy come true, terming the task personalized text-to-image generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of personalized text-to-image generation. No verifiable resources are identified.",
      "processing_time": 57.44224214553833,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Recent works (Gal et al., 2023a; Ruiz et al., 2023; Kumari et al., 2023) make this fantasy come true, terming the task personalized text-to-image generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of personalized text-to-image generation. No verifiable resources are identified.",
      "processing_time": 57.44224214553833,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "In this work, we follow (Gal et al., 2023a) (Gal et al., 2023a). to represent the novel concept instead of the form of “[V] class” used in (Ruiz et al., 2023; Kumari et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other works. There are no clear identifiers for datasets, and the context focuses on methodological choices rather than data sources.",
      "processing_time": 58.92674684524536,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "In this work, we follow (Gal et al., 2023a) (Gal et al., 2023a). to represent the novel concept instead of the form of “[V] class” used in (Ruiz et al., 2023; Kumari et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to other works. There are no clear identifiers for datasets, and the context focuses on methodological choices rather than data sources.",
      "processing_time": 58.92674684524536,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Recently, some works consider personalized text-to-image generation by learning a token embedding (Gal et al., 2023a) and finetuning (Ruiz et al., 2023) or partially finetuning (Kumari et al., 2023) a diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalized text-to-image generation.",
      "processing_time": 56.12765574455261,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Recently, some works consider personalized text-to-image generation by learning a token embedding (Gal et al., 2023a) and finetuning (Ruiz et al., 2023) or partially finetuning (Kumari et al., 2023) a diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for personalized text-to-image generation.",
      "processing_time": 56.12765574455261,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Following the attempt of textual inversion (Gal et al., 2023a), we adopt a placeholder ( S ⋆ ) as the learnable text embedding to capture the unique visual object.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (textual inversion) and a placeholder (S ⋆) for a learnable text embedding.",
      "processing_time": 58.226003885269165,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Previous works ( e.g. , Textual Inversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al., 2023)) use different datasets for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'different datasets' but does not specify any particular dataset names. The context is too generic and lacks specific identifiers.",
      "processing_time": 57.25322246551514,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Previous works ( e.g. , Textual Inversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al., 2023)) use different datasets for evaluation.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'different datasets' but does not specify any particular dataset names. The context is too generic and lacks specific identifiers.",
      "processing_time": 57.25322246551514,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Textual Inversion (Gal et al., 2023a) adopts minimal optimization by exclusively learning a novel text embedding to represent the given object, showing enhanced performance using latent diffusion models (Rombach et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the technique of Textual Inversion and its application in personalizing text-to-image generation.",
      "processing_time": 58.649975299835205,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We compare our method ViCo with three state-of-the-art models, namely Textual Inversion (Gal et al., 2023a), DreamBooth (Ruiz et al., 2023), and Custom Diffusion (Kumari et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is focused on comparing methods, not using datasets.",
      "processing_time": 57.03361439704895,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "…introducing an automatic object mask generation mechanism from the cross-attention map; (3) providing quantitative and qualitative comparisons with state-of-the-art methods (Ruiz et al., 2023; Kumari et al., 2023; Gal et al., 2023a) and demonstrating the efficiency of ViCo in multiple applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with other methods and applications. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.092445611953735,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "…introducing an automatic object mask generation mechanism from the cross-attention map; (3) providing quantitative and qualitative comparisons with state-of-the-art methods (Ruiz et al., 2023; Kumari et al., 2023; Gal et al., 2023a) and demonstrating the efficiency of ViCo in multiple applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only comparisons with other methods and applications. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 58.092445611953735,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Based on the prompt list provided in (Ruiz et al., 2023), we remove one undesirable prompt “a cube shaped S ⋆ ” because we are more interested in keeping the appearance of the unique object.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a prompt list which is not a verifiable resource according to the guidelines.",
      "processing_time": 55.97834849357605,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "DreamBooth (Ruiz et al., 2023) incorporates a unique identifier before the category word in the text embedding space and finetunes the entire diffusion model during training.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DreamBooth) and its functionality. No verifiable datasets are referenced.",
      "processing_time": 57.483362674713135,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "As the language knowledge remains intact without requiring fine-tuning of the diffusion model, our method avoids the problem of language drift, which eliminates the need for heavy preprocessing like image generation (Ruiz et al., 2023) and retrieval (Kumari et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the advantages of the proposed method over other approaches.",
      "processing_time": 57.47970414161682,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Our method stands out for not requiring any modifications or fine-tuning of any layers in the original diffusion model, setting it apart from most existing methods like DreamBooth (Ruiz et al., 2023) and Custom Diffusion (Kumari et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on comparing the method to other approaches.",
      "processing_time": 56.90127778053284,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "They leverage a preservation loss to address this problem, which requires manually generating (Ruiz et al., 2023) or retrieving massive class-specific images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating or retrieving images. The context is about addressing a problem using a preservation loss, which is not a dataset.",
      "processing_time": 58.19166898727417,
      "citing_paper_id": "258999614",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "In the context of diffusion models, visual condition is also used for image editing (Brooks et al., 2023) and controllable conditioning (Mou et al., 2023; Zhang & Agrawala, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.20047473907471,
      "citing_paper_id": "258999614",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "In the context of diffusion models, visual condition is also used for image editing (Brooks et al., 2023) and controllable conditioning (Mou et al., 2023; Zhang & Agrawala, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.20047473907471,
      "citing_paper_id": "258999614",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Common approaches for conditioning diffusion models on images include feature concatenation (Brooks et al., 2023) and direct element-wise addition (Mou et al., 2023; Zhang & Agrawala, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for conditioning diffusion models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.471333503723145,
      "citing_paper_id": "258999614",
      "cited_paper_id": 253581213
    },
    {
      "context_text": "Common approaches for conditioning diffusion models on images include feature concatenation (Brooks et al., 2023) and direct element-wise addition (Mou et al., 2023; Zhang & Agrawala, 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for conditioning diffusion models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.471333503723145,
      "citing_paper_id": "258999614",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "We use three datasets as our testbed, PororoSV [17], FlintstonesSV [19], and VIST [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV",
        "VIST"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence.",
        "FlintstonesSV": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence.",
        "VIST": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets by name, which are used as testbeds for the research. These datasets are likely relevant to personalized text generation as they involve storytelling and visualization.",
      "processing_time": 75.06617379188538,
      "citing_paper_id": "253734226",
      "cited_paper_id": 2574224
    },
    {
      "context_text": "We use three datasets as our testbed, PororoSV [17], FlintstonesSV [19], and VIST [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV",
        "VIST"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence.",
        "FlintstonesSV": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence.",
        "VIST": "Used as a testbed for evaluating personalized text generation, focusing on visual storytelling and narrative coherence."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets by name, which are used as testbeds for the research. These datasets are likely relevant to personalized text generation as they involve storytelling and visualization.",
      "processing_time": 75.06617379188538,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "We propose to use the Visual Story Telling (VIST) dataset [11] for real-world story synthesizing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Visual Story Telling (VIST)"
      ],
      "dataset_descriptions": {
        "Visual Story Telling (VIST)": "Used for real-world story synthesizing, focusing on generating coherent narratives from image sequences."
      },
      "confidence_score": 1.0,
      "reasoning": "The VIST dataset is explicitly mentioned and is relevant to the topic of personalized text generation, specifically for real-world story synthesizing.",
      "processing_time": 63.798890113830566,
      "citing_paper_id": "253734226",
      "cited_paper_id": 2574224
    },
    {
      "context_text": "While all existing story visualization and continuation datasets are cartoon images2, we introduce a new dataset VIST [11] to better evaluate real-world story synthesis capacity.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VIST"
      ],
      "dataset_descriptions": {
        "VIST": "Introduced to evaluate real-world story synthesis capacity, specifically using visual storytelling to enhance the evaluation of narrative generation systems."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the introduction of a new dataset called VIST, which is used to evaluate real-world story synthesis capacity, making it highly relevant to personalized text generation.",
      "processing_time": 63.79580640792847,
      "citing_paper_id": "253734226",
      "cited_paper_id": 2574224
    },
    {
      "context_text": "PororoSV and FlintstonesSV The PororoSV [17] and FlintstonesSV [19] datasets are adapted from Pororo video question answering dataset [12] and Flintstones text-tovideo synthesis dataset [7], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Adapted from the Pororo video question answering dataset, used for evaluating story visualization and question answering tasks.",
        "FlintstonesSV": "Derived from the Flintstones text-to-video synthesis dataset, utilized for assessing the generation of video content from textual stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PororoSV and FlintstonesSV, which are adaptations of existing datasets. These are clearly identified and used in the research.",
      "processing_time": 70.28876423835754,
      "citing_paper_id": "253734226",
      "cited_paper_id": 9096634
    },
    {
      "context_text": "PororoSV and FlintstonesSV The PororoSV [17] and FlintstonesSV [19] datasets are adapted from Pororo video question answering dataset [12] and Flintstones text-tovideo synthesis dataset [7], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Adapted from the Pororo video question answering dataset, used for evaluating story visualization and question answering tasks.",
        "FlintstonesSV": "Derived from the Flintstones text-to-video synthesis dataset, utilized for assessing the generation of video content from textual stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PororoSV and FlintstonesSV, which are adaptations of existing datasets. These are clearly identified and used in the research.",
      "processing_time": 70.28876423835754,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "Recently advanced diffusion models [31] such as DALL·E 2 [24], Imagen [29], and Stable Diffusion [26] have shown unprecedented text-to-image synthetic capacities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.42135429382324,
      "citing_paper_id": "253734226",
      "cited_paper_id": 14888175
    },
    {
      "context_text": "Recently advanced diffusion models [31] such as DALL·E 2 [24], Imagen [29], and Stable Diffusion [26] have shown unprecedented text-to-image synthetic capacities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.42135429382324,
      "citing_paper_id": "253734226",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "Recently advanced diffusion models [31] such as DALL·E 2 [24], Imagen [29], and Stable Diffusion [26] have shown unprecedented text-to-image synthetic capacities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 56.42135429382324,
      "citing_paper_id": "253734226",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Diffusion models [31] deﬁne a Markov chain of forward diffusion process q to gradually add Gaussian noise sampled to real data z 0 ⇠ q ( z ) in T steps.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (diffusion models).",
      "processing_time": 55.21705889701843,
      "citing_paper_id": "253734226",
      "cited_paper_id": 14888175
    },
    {
      "context_text": "VP-CSV [1] leverages transformer [35], and VQ-VAE [34] to preserve characters across generated images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of transformer and VQ-VAE in preserving characters across generated images.",
      "processing_time": 58.15997815132141,
      "citing_paper_id": "253734226",
      "cited_paper_id": 20282961
    },
    {
      "context_text": "What’s more, Inception-V3 [34] backbone may also cause a mismatch between the FID score and human preference for its locality inductive bias and spatial translation invariance [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Inception-V3) and its properties. No verifiable resources are identified.",
      "processing_time": 57.2462203502655,
      "citing_paper_id": "253734226",
      "cited_paper_id": 46935302
    },
    {
      "context_text": "However, as for more challenging PororoSV and FlintstonesSV datasets whose frames are sampled from videos, we ﬁnd that few synthesized visual stories are as consistent as ground truth references.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to evaluate the consistency of synthesized visual stories from video frames, comparing them to ground truth references.",
        "FlintstonesSV": "Used to evaluate the consistency of synthesized visual stories from video frames, comparing them to ground truth references."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PororoSV and FlintstonesSV, which are used to evaluate the consistency of synthesized visual stories compared to ground truth references.",
      "processing_time": 68.95571780204773,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "3, DII captions are more like the ones in PororoSV and FlintstonesSV, every single caption contains detailed information about the image.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only compares captions in DII to those in PororoSV and FlintstonesSV, which appear to be datasets or methods but are not clearly identified as reusable datasets.",
      "processing_time": 60.06762075424194,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "We also carry out large-scale human evaluations for the story continuation task on PororoSV, FlintstonesSV, and VIST-SIS datasets in terms of visual quality, relevance, and consistency.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV",
        "VIST-SIS"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used for evaluating story continuation tasks, focusing on visual quality, relevance, and consistency in generated stories.",
        "FlintstonesSV": "Used for evaluating story continuation tasks, focusing on visual quality, relevance, and consistency in generated stories.",
        "VIST-SIS": "Used for evaluating story continuation tasks, focusing on visual quality, relevance, and consistency in generated stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used for evaluating story continuation tasks, which are relevant to personalized text generation.",
      "processing_time": 73.56482791900635,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "In this paper, we mainly focus on two tasks: story visualization [17] and story continuation [21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tasks. No verifiable resources are identified.",
      "processing_time": 55.71249961853027,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "Visual story synthesis results on PororoSV.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used for evaluating visual story synthesis, focusing on generating coherent image sequences from textual stories using a sequential conditional GAN."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'PororoSV' which appears to be a specific dataset used for visual story synthesis. The cited paper title supports this as it discusses story visualization.",
      "processing_time": 64.44611382484436,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "In particular, AR-LDM achieves an FID score of 16.59 on PororoSV, with a relative improvement of 55% over previous story visualization methods.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to evaluate story visualization methods, specifically measuring the FID score to compare performance against previous approaches."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'PororoSV' which appears to be a specific dataset used for evaluating story visualization methods. However, there is no additional information about the dataset's content or structure.",
      "processing_time": 64.27878308296204,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "We use three datasets as our testbed, PororoSV [16], FlintstonesSV [18], and VIST [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV",
        "VIST"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization.",
        "FlintstonesSV": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization.",
        "VIST": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used as a testbed for the research. These datasets are likely relevant to personalized text generation as they involve story visualization and structured data.",
      "processing_time": 75.23235511779785,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "We use three datasets as our testbed, PororoSV [16], FlintstonesSV [18], and VIST [10].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV",
        "VIST"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization.",
        "FlintstonesSV": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization.",
        "VIST": "Used to evaluate personalized text generation models, focusing on visuospatial and linguistic structure in story visualization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets used as a testbed for the research. These datasets are likely relevant to personalized text generation as they involve story visualization and structured data.",
      "processing_time": 75.23235511779785,
      "citing_paper_id": "253734226",
      "cited_paper_id": 239049842
    },
    {
      "context_text": "While FlintstonesSV is relatively harder than PororoSV, for there are many un-seen characters within the stories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlintstonesSV",
        "PororoSV"
      ],
      "dataset_descriptions": {
        "FlintstonesSV": "Used to evaluate the difficulty of story visualization, focusing on the presence of many unseen characters within the stories.",
        "PororoSV": "Used as a comparative dataset to FlintstonesSV, evaluating the difficulty of story visualization with fewer unseen characters."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'FlintstonesSV' and 'PororoSV', which appear to be specific datasets used for story visualization. However, without additional context, it is unclear if these are verifiable resources or just part of the discussion.",
      "processing_time": 72.04326677322388,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "To evaluate our method, we utilize two widely accepted datasets, FlintstonesSV and PororoSV, as our test bed.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FlintstonesSV",
        "PororoSV"
      ],
      "dataset_descriptions": {
        "FlintstonesSV": "Used to evaluate the method for story visualization, focusing on sequential conditional GAN performance in generating coherent visual stories.",
        "PororoSV": "Used to evaluate the method for story visualization, focusing on sequential conditional GAN performance in generating coherent visual stories."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, FlintstonesSV and PororoSV, which are used to evaluate the method. These names are specific and appear to be verifiable resources.",
      "processing_time": 70.68236446380615,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "Story visualization FID score results on PororoSV.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to evaluate the performance of story visualization models, focusing on the FID score to measure the quality of generated images in the context of story sequences."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'PororoSV' which appears to be a specific dataset or evaluation metric used for story visualization. However, without additional context, it is unclear if 'PororoSV' is a dataset or a metric. Given the topic of personalized text generation, it is plausible that 'PororoSV' could be a dataset, but this is not definitively clear.",
      "processing_time": 72.98033785820007,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "StoryGAN [17] firstly propose the story visualization task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (StoryGAN) and a task (story visualization).",
      "processing_time": 56.72613310813904,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "2, we present our story visualization results on PororoSV, which is the most commonly Figure 4.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Used to visualize stories through sequential conditional GANs, focusing on generating coherent visual narratives from textual stories."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'PororoSV' as a dataset used for story visualization results. The name is specific and appears to be a verifiable resource.",
      "processing_time": 63.499313831329346,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "The PororoSV [16] and FlintstonesSV [18] datasets are adapted from Pororo video question answering dataset [11] and Flintstones text-to-video synthesis dataset [6], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Adapted from the Pororo video question answering dataset, used to enhance story visualization with visuospatial and linguistic structures.",
        "FlintstonesSV": "Derived from the Flintstones text-to-video synthesis dataset, utilized to integrate commonsense structure into story visualization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PororoSV and FlintstonesSV, which are adaptations of existing datasets. These are clearly identified and used in the research context.",
      "processing_time": 70.78935098648071,
      "citing_paper_id": "253734226",
      "cited_paper_id": 54457433
    },
    {
      "context_text": "The PororoSV [16] and FlintstonesSV [18] datasets are adapted from Pororo video question answering dataset [11] and Flintstones text-to-video synthesis dataset [6], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PororoSV",
        "FlintstonesSV"
      ],
      "dataset_descriptions": {
        "PororoSV": "Adapted from the Pororo video question answering dataset, used to enhance story visualization with visuospatial and linguistic structures.",
        "FlintstonesSV": "Derived from the Flintstones text-to-video synthesis dataset, utilized to integrate commonsense structure into story visualization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, PororoSV and FlintstonesSV, which are adaptations of existing datasets. These are clearly identified and used in the research context.",
      "processing_time": 70.78935098648071,
      "citing_paper_id": "253734226",
      "cited_paper_id": 239049842
    },
    {
      "context_text": "GANs like Stackgan [41], Attngan [38], Mirrorgan [21], and MXC-GAN [40] perform adversarial training between generators and discriminators to learn to generate high-quality images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several GAN models but does not refer to any specific datasets. The context is about the performance of these models in generating high-quality images.",
      "processing_time": 57.92361879348755,
      "citing_paper_id": "253734226",
      "cited_paper_id": 76661216
    },
    {
      "context_text": "GANs like Stackgan [41], Attngan [38], Mirrorgan [22], and MXCGAN [40] perform adversarial training between generators and discriminators to learn to generate high-quality images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited paper titles do not provide additional information about datasets.",
      "processing_time": 57.105297803878784,
      "citing_paper_id": "253734226",
      "cited_paper_id": 76661216
    },
    {
      "context_text": "During inference, we sample images using the DDIM scheduler [32] for 250 inference steps with guidance scale w set to 6.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDIM scheduler) used during inference. No dataset names are present in the citation context.",
      "processing_time": 57.72568345069885,
      "citing_paper_id": "253734226",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "In contrast, SIS captions describe Models FID StoryGAN ( 2019 ) [16] 158.06 CP-CSV ( 2020 ) [33] 149.29 DUCO-StoryGAN ( 2021 ) [19] 96.51 VLC-StoryGAN ( 2021 ) [18] 84.96 VP-CSV ( 2022 ) [1] 65.51 Word-Level SV ( 2022 ) [14] 56.08 Make-A-Story ( 2022 ) [23] 36.64 AR-LDM (Ours) 16.59 Table 2.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only model names and their performance metrics. There are no clear identifiers for datasets.",
      "processing_time": 56.79456090927124,
      "citing_paper_id": "253734226",
      "cited_paper_id": 239049842
    },
    {
      "context_text": "VLC-StoryGAN [18] and Word-Level SV [14] focus on text inputs, and propose to use structured input and sentence representation to better guide visual story generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of structured input and sentence representation for visual story generation.",
      "processing_time": 57.791959285736084,
      "citing_paper_id": "253734226",
      "cited_paper_id": 239049842
    },
    {
      "context_text": "BLIP utilizes the cross-attention module to deeply integrate visual and language modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (cross-attention module) used in the BLIP model.",
      "processing_time": 56.654895305633545,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Specifically, we employ a history-aware encoding module containing a CLIP text encoder [23], and a BLIP multimodal encoder [16].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using CLIP and BLIP encoders but does not refer to them as datasets. These are models or methods, not datasets.",
      "processing_time": 57.43354558944702,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "(5) can be simpliﬁed as p ✓ ( z The conditioning network consists of CLIP [22] and BLIP [15], in charge of current caption encoding and previous caption-image encoding, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (CLIP and BLIP) which are excluded according to the rules.",
      "processing_time": 56.63519811630249,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Speciﬁcally, we employ a history-aware encoding module containing a CLIP text encoder [22], and a BLIP multimodal encoder [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using CLIP and BLIP encoders but does not refer to them as datasets. These are models or methods, not datasets.",
      "processing_time": 57.42694354057312,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "The conditioning network consists of CLIP [23] and BLIP [16], in charge of current caption encoding and previous caption-image encoding, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions CLIP and BLIP, which are models, not datasets. No specific datasets are mentioned in the context.",
      "processing_time": 56.61994791030884,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "If we further leverage the source frame image caption pair and use BLIP to encode it into a multimodal embedding to condition the diffusion process, we can observe an improvement of 2.16 compared to the baseline model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (BLIP) and a performance metric (improvement of 2.16).",
      "processing_time": 58.90197205543518,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "BLIP is pre-trained using vision-language understanding and generation tasks with large-scale ﬁltered clean web data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale filtered clean web data' but does not provide a specific, identifiable dataset name. The term is too generic and lacks a clear identifier.",
      "processing_time": 58.536028146743774,
      "citing_paper_id": "253734226",
      "cited_paper_id": 246411402
    },
    {
      "context_text": "Large auto-regressive models like DALL·E [25], Make-AScene [4], and Parti [39] can be easily scaled up and have also shown their excellent image synthetic capacity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.660733222961426,
      "citing_paper_id": "253734226",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "Concurrent work M-VADER [37] shares a similar idea with us, but they focus on single image synthesis and do not utilize the auto-regressive method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (M-VADER) and a method (auto-regressive).",
      "processing_time": 57.034316062927246,
      "citing_paper_id": "253734226",
      "cited_paper_id": 254275172
    },
    {
      "context_text": "Meta-analyses have emphasized the importance of personality traits in understanding individual differences in psychopathology [3-4] and well-being [5-6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to meta-analyses and personality traits. No verifiable resources are identified.",
      "processing_time": 57.27542686462402,
      "citing_paper_id": "275471754",
      "cited_paper_id": 13614077
    },
    {
      "context_text": "These outcomes align with findings from PersonaLLM [27], which demonstrated that while LLMs can approximate personality traits, their accuracy varies by trait, and human evaluations of LLM outputs can differ in perception.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (PersonaLLM) and its findings. The context focuses on the outcomes and human evaluations of LLM outputs.",
      "processing_time": 59.256837129592896,
      "citing_paper_id": "275471754",
      "cited_paper_id": 268032940
    },
    {
      "context_text": "These findings are supported by studies such as Predicting the Big Five Personality Traits in Chinese Counseling Dialogues [28], which showed that role-play and structured frameworks prompts enhance prediction accuracy.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a study that used role-play and structured frameworks to predict personality traits.",
      "processing_time": 56.858476877212524,
      "citing_paper_id": "275471754",
      "cited_paper_id": 270711015
    },
    {
      "context_text": "Research also indicates that larger parameter sets in LLMs exhibit nuanced personality traits, such as increased openness and conscientiousness, while fine-tuned models display minor modulations in specific traits [15].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model characteristics.",
      "processing_time": 55.17481517791748,
      "citing_paper_id": "275471754",
      "cited_paper_id": null
    },
    {
      "context_text": "Despite these advancements, in the use of Large Language Models (LLMs) for personality inference, most existing approaches [12-15] rely on directly predicting personality traits without leveraging the structure relationship between traits and widely accepted psychological tools like the Big Five…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach using Large Language Models (LLMs) for personality inference. No verifiable resources are identified.",
      "processing_time": 58.347506284713745,
      "citing_paper_id": "275471754",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, by observing the trait value distributions for five personality dimensions illustrated in Figue 1, we found the data is unbalanced, so we tried to apply SMOTE [11]",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to an imbalance in the data for personality dimensions, which is too generic.",
      "processing_time": 57.63152194023132,
      "citing_paper_id": "235666838",
      "cited_paper_id": 1554582
    },
    {
      "context_text": "Moreover, by observing the trait value distributions for (cid:12)ve personality dimensions illustrated in Figue 1, we found the data is unbalanced, so we tried to apply SMOTE [11] over-sampling algorithm to deal with this problem.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only a method (SMOTE) for dealing with unbalanced data. No verifiable dataset is identified.",
      "processing_time": 58.00888204574585,
      "citing_paper_id": "235666838",
      "cited_paper_id": 1554582
    },
    {
      "context_text": "When we set the trait values to be [2,4,3,3,4] where corresponding dimensions are EXT, OPN, CON, AGR and NEU, the chatbot reveals a negative attitude and is quite emotional that most generated responses contain the word \\not\" since the prescribed NEU value is high.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for setting trait values in a chatbot, which is not a dataset.",
      "processing_time": 57.248300313949585,
      "citing_paper_id": "235666838",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "A lot of previous studies simpli(cid:12)ed the issue to generate responses based on users’ pro(cid:12)les that contain entities like age and location [1] [2] [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general user profiles containing entities like age and location.",
      "processing_time": 57.25707983970642,
      "citing_paper_id": "235666838",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "A lot of previous studies simpli(cid:12)ed the issue to generate responses based on users’ pro(cid:12)les that contain entities like age and location [1] [2] [3].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general user profiles containing entities like age and location.",
      "processing_time": 57.25707983970642,
      "citing_paper_id": "235666838",
      "cited_paper_id": 51608471
    },
    {
      "context_text": "When the trait value is set to be [4,4,3,3,2] with higher EXT value, the chatbot tends to generate positive responses with more in-terjections that show passion like \\Oh my God\" and \\Yes!\".",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method or experimental setup but does not reference a named dataset.",
      "processing_time": 57.253257274627686,
      "citing_paper_id": "235666838",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "[7] define personality by using a set of descriptive sentences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for defining personality using descriptive sentences.",
      "processing_time": 55.24625611305237,
      "citing_paper_id": "235666838",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In this study, we applied and modified four methods that are proposed in previous studies and compared their performances [12] [13] [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods from previous studies. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.07123613357544,
      "citing_paper_id": "235666838",
      "cited_paper_id": 8182454
    },
    {
      "context_text": "Several prior studies have found close correlations between these (cid:12)ve traits and linguistic behavior via lexical and syntax analysis [5] [6].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to linguistic behavior analysis. No clear, verifiable resource names are provided.",
      "processing_time": 57.49047064781189,
      "citing_paper_id": "235666838",
      "cited_paper_id": 29567532
    },
    {
      "context_text": "HyperNetworks, introduced as auxiliary networks predicting weights for neural networks [12], have been applied in image generation tasks akin to personalization, such as StyleGAN inversion [3], resembling methods that aim to invert an image’s latent code for editing in GAN spaces [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.166836738586426,
      "citing_paper_id": "259847576",
      "cited_paper_id": 233033581
    },
    {
      "context_text": "HyperNetworks, introduced as auxiliary networks predicting weights for neural networks [12], have been applied in image generation tasks akin to personalization, such as StyleGAN inversion [3], resembling methods that aim to invert an image’s latent code for editing in GAN spaces [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.166836738586426,
      "citing_paper_id": "259847576",
      "cited_paper_id": 244729249
    },
    {
      "context_text": "HyperNetworks, introduced as auxiliary networks predicting weights for neural networks [12], have been applied in image generation tasks akin to personalization, such as StyleGAN inversion [3], resembling methods that aim to invert an image’s latent code for editing in GAN spaces [2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.166836738586426,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "Casanova et al. [6] condition a GAN with an input image to produce vari-6528 Figure 3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (GAN). No verifiable resources are identified.",
      "processing_time": 56.33909559249878,
      "citing_paper_id": "259847576",
      "cited_paper_id": 237491885
    },
    {
      "context_text": "In previous work [3, 12], this dependency is not rigorously modeled in the HyperNetwork, whereas with a transformer decoder with a positional embedding, this positional dependency is modeled - similar to dependencies between words in a language model transformer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological aspects of modeling positional dependencies in transformers.",
      "processing_time": 56.16071557998657,
      "citing_paper_id": "259847576",
      "cited_paper_id": 244729249
    },
    {
      "context_text": "Iterative Prediction We find that the HyperNetwork achieves better and more confident predictions given an iterative learning and prediction scenario [3], where intermediate weight predictions are fed to the HyperNetwork and the network’s task is to improve that initial prediction.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (HyperNetwork) and its application in an iterative learning and prediction scenario.",
      "processing_time": 57.27917265892029,
      "citing_paper_id": "259847576",
      "cited_paper_id": 244729249
    },
    {
      "context_text": "In this work, we use Stable Diffusion [24], a specific instatiation of LDM [24].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Stable Diffusion' and 'LDM', which are models, not datasets. No datasets are explicitly mentioned or used in the context.",
      "processing_time": 58.33427691459656,
      "citing_paper_id": "259847576",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "See [24] for more details.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation span does not provide any specific information about datasets or their usage. The cited paper title is unrelated to personalized text generation.",
      "processing_time": 57.27316355705261,
      "citing_paper_id": "259847576",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "In the case of Stable Diffusion [24], this amounts to finetuning the entire denoising UNet has over 1GB of parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (Stable Diffusion) and its architecture (denoising UNet).",
      "processing_time": 57.81350302696228,
      "citing_paper_id": "259847576",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Text-to-Image Models Several recent models such as Im-agen [26], DALL-E2 [22], Stable Diffusion (SD) [24], Muse [7], Parti [33], etc., demonstrate excellent image generation capabilities given a text prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on describing the capabilities of these models rather than their training or evaluation data.",
      "processing_time": 58.19095325469971,
      "citing_paper_id": "259847576",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Text-to-Image Models Several recent models such as Im-agen [26], DALL-E2 [22], Stable Diffusion (SD) [24], Muse [7], Parti [33], etc., demonstrate excellent image generation capabilities given a text prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on describing the capabilities of these models rather than their training or evaluation data.",
      "processing_time": 58.19095325469971,
      "citing_paper_id": "259847576",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "Text-to-Image Models Several recent models such as Im-agen [26], DALL-E2 [22], Stable Diffusion (SD) [24], Muse [7], Parti [33], etc., demonstrate excellent image generation capabilities given a text prompt.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models but does not refer to any specific datasets. The focus is on describing the capabilities of these models rather than their training or evaluation data.",
      "processing_time": 58.19095325469971,
      "citing_paper_id": "259847576",
      "cited_paper_id": 255372955
    },
    {
      "context_text": "Our method achieves strong personalization results for widely diverse faces, with performance that is identically or surpasses that of the state-of-the art optimization driven methods [10, 11, 25].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other papers for comparative performance.",
      "processing_time": 55.62765884399414,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "DreamBooth [25] adapts entire network weights for subject fidelity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DreamBooth) for fine-tuning text-to-image diffusion models.",
      "processing_time": 57.497307777404785,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "The impressive diversity of styles is owed to the strong prior of pre-trained diffusion model, and one of the key properties of works such as DreamBooth [25], is the ability to implant a new subject into the model without damaging the model’s prior.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DreamBooth) and its capabilities. No verifiable datasets are referenced.",
      "processing_time": 57.49414777755737,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "DreamBooth [25] provides a network fine-tuning strategy to adapt a given T2I denoising network D θ to generate images of a specific subject.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fine-tuning a text-to-image diffusion model.",
      "processing_time": 56.368995904922485,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Qualitative Comparisons We compare our method to Textual Inversion [10], DreamBooth [25] and E4T [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods/models. No dataset names are present in the text.",
      "processing_time": 56.528653383255005,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "We simply propose to condition the learning process “a [V] face” for all samples, where [V] is a rare identifier described in [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to a method or approach for conditioning the learning process.",
      "processing_time": 57.12699866294861,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Recent work on text-to-image (T2I) personalization [25] has opened the door for a new class of creative applications.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for text-to-image personalization.",
      "processing_time": 56.17436099052429,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Quantitative Comparisons and Ablations We compare our method to Textual Inversion and DreamBooth using face recognition metrics (“Face Rec.” from a VGGFace2 Inception ResNet), along with DINO, CLIP-I, and CLIP-T metrics [25].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and metrics. The context focuses on comparing methods using various metrics.",
      "processing_time": 56.76934599876404,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "We find that given the HyperNet-work initialization, fast finetuning can be done in 40 iterations, which is 25x faster than DreamBooth [25] and LoRA DreamBooth [1].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the efficiency of fast finetuning compared to other methods.",
      "processing_time": 57.78969216346741,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "At a high-level, DreamBooth optimizes all the diffusion network weights θ on a few given subject images while also retaining the generalization ability of the original model with class-specific prior preservation loss [25].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DreamBooth) and its optimization process. No verifiable datasets are referenced.",
      "processing_time": 57.62273454666138,
      "citing_paper_id": "259847576",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "They have also been used in other tasks such as language modeling [15, 19, 21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other tasks such as language modeling.",
      "processing_time": 55.81718397140503,
      "citing_paper_id": "259847576",
      "cited_paper_id": 253761398
    },
    {
      "context_text": "Various methods, like CustomDiffusion [18], SVDiff [13], LoRa [1, 14], Style-Drop [28], and DreamArtist [9], optimize specific network parts or use specialized tuning strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 56.104464292526245,
      "citing_paper_id": "259847576",
      "cited_paper_id": 257631648
    },
    {
      "context_text": "Various methods, like CustomDiffusion [18], SVDiff [13], LoRa [1, 14], Style-Drop [28], and DreamArtist [9], optimize specific network parts or use specialized tuning strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 56.104464292526245,
      "citing_paper_id": "259847576",
      "cited_paper_id": 258999204
    },
    {
      "context_text": "Various methods, like CustomDiffusion [18], SVDiff [13], LoRa [1, 14], Style-Drop [28], and DreamArtist [9], optimize specific network parts or use specialized tuning strategies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. No verifiable resources are identified.",
      "processing_time": 56.104464292526245,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "Techniques like [4], Face0 [29], and Celeb-basis [34] explore different conditioning or guidance approaches for efficient T2I personalization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Celeb-basis' as a technique for T2I personalization, but does not indicate it is a dataset. No other specific datasets are mentioned.",
      "processing_time": 58.53564977645874,
      "citing_paper_id": "259847576",
      "cited_paper_id": 258999486
    },
    {
      "context_text": "We find that our method preserves identity and subject fidelity more closely, while achieving a higher score in prompt fidelity. weight updates.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the performance of the authors' method.",
      "processing_time": 56.730061769485474,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "T2I Personalization via Finetuning Recent techniques enhance T2I models for improved subject fidelity and versatile text-based recontextualization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general reference to techniques for enhancing T2I models.",
      "processing_time": 56.38367438316345,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "Yet, the proposed technique is generic and applicable to any T2I model.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a generic applicability statement.",
      "processing_time": 54.89410161972046,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "Low Rank Adaptation (LoRA) [1, 14] provides a memory-efficient and faster technique for DreamBooth.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called Low Rank Adaptation (LoRA).",
      "processing_time": 56.02153420448303,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "This is achieved by training a Dream-Booth model in a low-dimensional weight-space generated by a random orthogonal incomplete basis inside of a low-rank adaptation [14] weight space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Dream-Booth model) and a technique (low-rank adaptation).",
      "processing_time": 57.59615921974182,
      "citing_paper_id": "259847576",
      "cited_paper_id": null
    },
    {
      "context_text": "Diffusion models [9,10,51] perform better in generating photorealistic and diverse images compared with generative adversarial networks (GANs) [8, 14], leading to increasing attention over the past two years.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance comparisons. No verifiable resources are identified.",
      "processing_time": 56.920074462890625,
      "citing_paper_id": "264590753",
      "cited_paper_id": 1033682
    },
    {
      "context_text": "Diffusion models [9,10,51] perform better in generating photorealistic and diverse images compared with generative adversarial networks (GANs) [8, 14], leading to increasing attention over the past two years.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance comparisons. No verifiable resources are identified.",
      "processing_time": 56.920074462890625,
      "citing_paper_id": "264590753",
      "cited_paper_id": 234357997
    },
    {
      "context_text": "Diffusion models [9,10,51] perform better in generating photorealistic and diverse images compared with generative adversarial networks (GANs) [8, 14], leading to increasing attention over the past two years.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance comparisons. No verifiable resources are identified.",
      "processing_time": 56.920074462890625,
      "citing_paper_id": "264590753",
      "cited_paper_id": 252199918
    },
    {
      "context_text": "The ﬁne-tuning process was performed for 10 epochs using AdamW [27], which took approximately 40 minutes to complete.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AdamW) used for fine-tuning. No datasets are referenced in the context.",
      "processing_time": 57.59055709838867,
      "citing_paper_id": "264590753",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "GAN-based panorama generation algorithms [7, 25, 26, 31, 43, 47, 49] have been extensively studied.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to GAN-based algorithms for image extension.",
      "processing_time": 56.00911021232605,
      "citing_paper_id": "264590753",
      "cited_paper_id": 201106503
    },
    {
      "context_text": "Next, we employed the image encoder of CLIP [34] to extract embeddings of these patches.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the image encoder of CLIP, which is a model, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.00193405151367,
      "citing_paper_id": "264590753",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "360-degree panoramic images [11, 33, 36, 54] are extensively utilized in virtual reality (VR) devices, such as head mount displays [3].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to 360-degree panoramic images in the context of VR devices, which is a general technology reference.",
      "processing_time": 58.98426127433777,
      "citing_paper_id": "264590753",
      "cited_paper_id": 233647684
    },
    {
      "context_text": "The paired image-text data used during the ﬁne-tuning process are from our collected 360PanoI dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "360PanoI"
      ],
      "dataset_descriptions": {
        "360PanoI": "Used for fine-tuning paired image-text data, focusing on improving model performance in generating personalized text based on visual inputs."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions a specific dataset '360PanoI' which is used for fine-tuning paired image-text data. The dataset name is specific and plausible.",
      "processing_time": 65.96870183944702,
      "citing_paper_id": "264590753",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Speciﬁcally, we focus on customization of a T2I diffusion model for synthesizing 360-degree panoramas with inherent globally geometric properties.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a focus on customizing a T2I diffusion model. No verifiable resources are identified.",
      "processing_time": 57.62039113044739,
      "citing_paper_id": "264590753",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "We will only introduce several representative works here; for more comprehensive information, we refer readers to the survey paper [52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a survey paper for more comprehensive information.",
      "processing_time": 57.34826064109802,
      "citing_paper_id": "264590753",
      "cited_paper_id": 245123664
    },
    {
      "context_text": "We will only introduce several representative works here; for more comprehensive information, we refer readers to the survey paper [52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a survey paper for more comprehensive information.",
      "processing_time": 57.34826064109802,
      "citing_paper_id": "264590753",
      "cited_paper_id": 251196614
    },
    {
      "context_text": "We will only introduce several representative works here; for more comprehensive information, we refer readers to the survey paper [52].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to a survey paper for more comprehensive information.",
      "processing_time": 57.34826064109802,
      "citing_paper_id": "264590753",
      "cited_paper_id": 257505012
    },
    {
      "context_text": "It is important to highlight that the practical implementation of our StitchDiffusion process operates on J and I within latent space.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a process operating in latent space. No verifiable resources are identified.",
      "processing_time": 56.122962951660156,
      "citing_paper_id": "264590753",
      "cited_paper_id": 249879327
    },
    {
      "context_text": "Thanks to their excellent generation quality and controllability, diffusion models have been widely explored for tackling numerous challenging tasks [4,15,18,24,30,37,45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to diffusion models and their applications.",
      "processing_time": 55.8237407207489,
      "citing_paper_id": "264590753",
      "cited_paper_id": 249889060
    },
    {
      "context_text": "To handle this challenge, several personalized T2I generation algorithms [12, 17, 20, 38, 44] have been proposed.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to algorithms. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.01141023635864,
      "citing_paper_id": "264590753",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Given one or multiple images of a speciﬁc subject or style provided by users, personalized text-to-image (T2I) generation [2, 12, 13, 17, 20, 38, 41, 42, 44, 46, 48] based on diffusion models aims to synthesize instances of the speciﬁc subject or style in diverse contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of personalized text-to-image generation using diffusion models.",
      "processing_time": 56.54416370391846,
      "citing_paper_id": "264590753",
      "cited_paper_id": 258041269
    },
    {
      "context_text": "Given one or multiple images of a speciﬁc subject or style provided by users, personalized text-to-image (T2I) generation [2, 12, 13, 17, 20, 38, 41, 42, 44, 46, 48] based on diffusion models aims to synthesize instances of the speciﬁc subject or style in diverse contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general description of personalized text-to-image generation using diffusion models.",
      "processing_time": 56.54416370391846,
      "citing_paper_id": "264590753",
      "cited_paper_id": 258078844
    },
    {
      "context_text": "Different from GLIDE requiring to train its text encoder, Im-agen [40] utilizes a pre-trained large transformer language model to encode textual input for image generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Im-agen) and a comparison to another method (GLIDE).",
      "processing_time": 57.2169725894928,
      "citing_paper_id": "264590753",
      "cited_paper_id": null
    },
    {
      "context_text": "Notably, diffusion models applied to text-to-image (T2I) synthesis [29,35,37,40] can produce high-quality images corresponding to descriptive text prompts, making them highly popular on social media.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a class of models (diffusion models) and their application to text-to-image synthesis. No verifiable resources are identified.",
      "processing_time": 58.57063031196594,
      "citing_paper_id": "264590753",
      "cited_paper_id": null
    },
    {
      "context_text": "Text-to-image (T2I) synthesis based on diffusion models [16, 29, 35, 37, 37, 40] can generate images that align with the provided text prompts, which have showcased unprecedented levels of diversity and ﬁdelity.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general capability of T2I synthesis using diffusion models.",
      "processing_time": 56.75544595718384,
      "citing_paper_id": "264590753",
      "cited_paper_id": null
    },
    {
      "context_text": "DiffCollage [53] utilizes a semantic segmentation map as the condition for the diffusion model and generates 360-degree panoramas based on a complex factor graph.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method (DiffCollage) and its functionality, but does not reference any dataset used for training or evaluation.",
      "processing_time": 58.937084674835205,
      "citing_paper_id": "264590753",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, diffusion models have also shown promising results in panorama synthesis [5, 22, 53].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers showing results in panorama synthesis using diffusion models.",
      "processing_time": 56.412028312683105,
      "citing_paper_id": "264590753",
      "cited_paper_id": null
    },
    {
      "context_text": "Then we extract an embedding from each detected face with an Inception-ResnetV1 [35] pre-trained on VGGFace2 [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to pre-train the Inception-ResnetV1 model for face recognition, focusing on pose and age variations in the dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VGGFace2' as a dataset used for pre-training the Inception-ResnetV1 model. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 65.94619035720825,
      "citing_paper_id": "258041269",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Then we extract an embedding from each detected face with an Inception-ResnetV1 [35] pre-trained on VGGFace2 [3].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to pre-train the Inception-ResnetV1 model for face recognition, focusing on pose and age variations in the dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VGGFace2' as a dataset used for pre-training the Inception-ResnetV1 model. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 65.94619035720825,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "Then we extract an embedding from each detected face with an Inception-ResnetV1 [37] pre-trained on VGGFace2 [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to pre-train the Inception-ResnetV1 model for face recognition, focusing on pose and age variations in the dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VGGFace2' as a dataset used for pre-training the Inception-ResnetV1 model. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 66.3387668132782,
      "citing_paper_id": "258041269",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Then we extract an embedding from each detected face with an Inception-ResnetV1 [37] pre-trained on VGGFace2 [7].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace2"
      ],
      "dataset_descriptions": {
        "VGGFace2": "Used to pre-train the Inception-ResnetV1 model for face recognition, focusing on pose and age variations in the dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'VGGFace2' as a dataset used for pre-training the Inception-ResnetV1 model. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 66.3387668132782,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "Generative Adversarial Networks [31, 41, 1, 15] are usually used for text-to-image generation in early years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general application area (text-to-image generation). No verifiable resources are identified.",
      "processing_time": 57.292073249816895,
      "citing_paper_id": "258041269",
      "cited_paper_id": 1563370
    },
    {
      "context_text": "Generative Adversarial Networks [31, 41, 1, 15] are usually used for text-to-image generation in early years.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general application area (text-to-image generation). No verifiable resources are identified.",
      "processing_time": 57.292073249816895,
      "citing_paper_id": "258041269",
      "cited_paper_id": 245117814
    },
    {
      "context_text": "The specific list of prompts is in the Supplement.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a list of prompts in the supplement. No verifiable resources are identified.",
      "processing_time": 56.96125864982605,
      "citing_paper_id": "258041269",
      "cited_paper_id": 233323280
    },
    {
      "context_text": "More following work are proposed using transformers [10, 11] or diffusion models [18, 3, 5, 17, 19, 23, 25] to generate high-fidelity images from text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for generating high-fidelity images from text.",
      "processing_time": 56.67534828186035,
      "citing_paper_id": "258041269",
      "cited_paper_id": 244714366
    },
    {
      "context_text": "More following work are proposed using transformers [10, 11] or diffusion models [18, 3, 5, 17, 19, 23, 25] to generate high-fidelity images from text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods for generating high-fidelity images from text.",
      "processing_time": 56.67534828186035,
      "citing_paper_id": "258041269",
      "cited_paper_id": 248239727
    },
    {
      "context_text": "Modern visual synthesis systems are usually powered by large text-to-image foundation models [2, 4, 7, 8, 10, 16, 24, 27, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. There are no clear identifiers for datasets within the text.",
      "processing_time": 57.12257170677185,
      "citing_paper_id": "258041269",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "Recent advances in personalized image generation have enabled text-to-image models [2, 4, 7, 8, 10, 14, 16, 24, 27, 28, 34] to learn a new concept from a small set of images then create images of that concept in different poses, perspectives, styles, and scenes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general advancement in personalized image generation. No verifiable resources are named.",
      "processing_time": 57.12793564796448,
      "citing_paper_id": "258041269",
      "cited_paper_id": 247628171
    },
    {
      "context_text": "We encode X into a global concept embedding c g via Concept Encoder E g and obtain",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a method for encoding concepts into embeddings.",
      "processing_time": 55.779794692993164,
      "citing_paper_id": "258041269",
      "cited_paper_id": 248097655
    },
    {
      "context_text": "We encode X into a global concept embedding c g via Concept Encoder E g and obtain",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a method for encoding concepts into embeddings.",
      "processing_time": 55.779794692993164,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257631648
    },
    {
      "context_text": "We use the official code base for Textual Inversion and ELITE, and a third-party replication [40] for DreamBooth, where the base text-to-image model is changed from Imagen [32] to Stable Diffusion [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of code bases and models, not datasets.",
      "processing_time": 57.710055112838745,
      "citing_paper_id": "258041269",
      "cited_paper_id": 248986576
    },
    {
      "context_text": "We use the official code base for Textual Inversion and ELITE, and a third-party replication [40] for DreamBooth, where the base text-to-image model is changed from Imagen [32] to Stable Diffusion [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of code bases and models, not datasets.",
      "processing_time": 57.710055112838745,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "We use the official code base for Textual Inversion and ELITE, and a third-party replication [40] for DreamBooth, where the base text-to-image model is changed from Imagen [32] to Stable Diffusion [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of code bases and models, not datasets.",
      "processing_time": 57.710055112838745,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "We use the official code base for Textual Inversion and ELITE, and a third-party replication [40] for DreamBooth, where the base text-to-image model is changed from Imagen [32] to Stable Diffusion [29].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of code bases and models, not datasets.",
      "processing_time": 57.710055112838745,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "Align ↑ Face Sim ↑ Recon ↑ Time (s) ↓ TI [11] 0.2556 0.1130 0.7832 ∼ 1500 DB [30] 0.3088 0.2408 0.8335 ∼ 600 ELITE [38] 1.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only performance metrics and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.70600962638855,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Align ↑ Face Sim ↑ Recon ↑ Time (s) ↓ TI [11] 0.2556 0.1130 0.7832 ∼ 1500 DB [30] 0.3088 0.2408 0.8335 ∼ 600 ELITE [38] 1.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only performance metrics and methods. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.70600962638855,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "The first is test-time finetuning-based methods [5, 9, 11–13, 17, 30, 31, 36, 37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, and the context is focused on methods rather than data.",
      "processing_time": 58.46565389633179,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "The first is test-time finetuning-based methods [5, 9, 11–13, 17, 30, 31, 36, 37].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, and the context is focused on methods rather than data.",
      "processing_time": 58.46565389633179,
      "citing_paper_id": "258041269",
      "cited_paper_id": 259847576
    },
    {
      "context_text": "Quality ↑ Alignment ↑ Identity ↑ TI [11] 2.89 3.04 2.97 DB [30] 3.50 3.50 3.55 ELITE [38] 3.14 3.08 3.08 Ours 3.53 3.58 3.55 Table 2.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model performance metrics. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.38520383834839,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Quality ↑ Alignment ↑ Identity ↑ TI [11] 2.89 3.04 2.97 DB [30] 3.50 3.50 3.55 ELITE [38] 3.14 3.08 3.08 Ours 3.53 3.58 3.55 Table 2.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only model performance metrics. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.38520383834839,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "In this section, we compare our approach with Textual Inversion (TI) [11] and DreamBooth (DB) [30], both of which require heavy test-time finetuning, as well as with ELITE [38], a zero-shot personalization method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.215184450149536,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "In this section, we compare our approach with Textual Inversion (TI) [11] and DreamBooth (DB) [30], both of which require heavy test-time finetuning, as well as with ELITE [38], a zero-shot personalization method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.215184450149536,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "In this section, we compare our approach with Textual Inversion (TI) [11] and DreamBooth (DB) [30], both of which require heavy test-time finetuning, as well as with ELITE [38], a zero-shot personalization method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 57.215184450149536,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "Existing approaches [11, 17, 30] for this task can be mainly categorized into two types.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only categorizes existing approaches into two types.",
      "processing_time": 56.772478103637695,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "First, many existing models [5, 9, 11–13, 17, 30, 31, 36, 37] require individual finetuning for each new concept.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about existing models requiring fine-tuning. No verifiable resources are identified.",
      "processing_time": 57.38422632217407,
      "citing_paper_id": "258041269",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "First, many existing models [5, 9, 11–13, 17, 30, 31, 36, 37] require individual finetuning for each new concept.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general statement about existing models requiring fine-tuning. No verifiable resources are identified.",
      "processing_time": 57.38422632217407,
      "citing_paper_id": "258041269",
      "cited_paper_id": 259847576
    },
    {
      "context_text": "More recently, approaches built upon pre-trained text-to-image models have been proposed for more controllable multi-modal image generation [20, 23, 42].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained models and their applications. No verifiable resources are identified.",
      "processing_time": 56.75510931015015,
      "citing_paper_id": "258041269",
      "cited_paper_id": 256900833
    },
    {
      "context_text": "Note that, since ELITE can accept only a single image, we compare ELITE using a single image as input, whereas all other methods use multiple images as inputs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison of methods. The cited paper title does not provide additional dataset information.",
      "processing_time": 57.05944299697876,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "ELITE [38] finetunes the pretrained parameters in the attention layers and sometimes fails to generate images with diverse poses/viewpoints.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ELITE) and its limitations in generating diverse images.",
      "processing_time": 56.76050662994385,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Quantitative comparison in “ person ” category of TI (Textual Inversion), DB (DreamBooth), ELITE and our method.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to methods or models (TI, DB, ELITE) and a comparison of their performance.",
      "processing_time": 58.50293159484863,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Quantitative comparison in “ person ” category of TI (Textual Inversion), DB (DreamBooth), ELITE and our method.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to methods or models (TI, DB, ELITE) and a comparison of their performance.",
      "processing_time": 58.50293159484863,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "6 showcases our method’s performance with a single input image, indicating the superiority of our methods over ELITE in a single image input penalization setting.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a comparison between methods. The cited paper title confirms 'ELITE' is a method, not a dataset.",
      "processing_time": 57.96001601219177,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "DreamBooth ELITE ”open arms,” while maintaining the identity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called DreamBooth ELITE. No verifiable resources are identified.",
      "processing_time": 57.426631927490234,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "DreamBooth ELITE ”open arms,” while maintaining the identity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model called DreamBooth ELITE. No verifiable resources are identified.",
      "processing_time": 57.426631927490234,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "ELITE cannot maintain the identity, but note it is not a completely fair comparison since ELITE is trained on OpenIm-ages [18] of general categories, while ours are trained on specific categories.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "OpenImages"
      ],
      "dataset_descriptions": {
        "OpenImages": "Used to train ELITE on general categories, contrasting with the specific categories used in the current research."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'OpenImages' as a dataset used for training ELITE, which is a multi-word proper noun and fits the criteria for inclusion.",
      "processing_time": 62.91278290748596,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "To allow the pretrained text-to-image generator to adapt visual input, ELITE [39] finetunes the pretrained parameters in the attention layers and UMM-Diffusion [24] only learns a visual mapping layer but freeze the weight of the pretrained generator.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the adaptation of pretrained models for text-to-image generation.",
      "processing_time": 57.35499429702759,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "We also recognize several concurrent works [39, 24, 9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only acknowledges concurrent works.",
      "processing_time": 56.00083088874817,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "The second is encoder-based methods [6, 15, 19, 22, 38, 39, 41], which learn image embeddings from the input images as the representation of the concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for learning image embeddings. No verifiable resources are identified.",
      "processing_time": 56.49785256385803,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257913352
    },
    {
      "context_text": "The second is encoder-based methods [6, 15, 19, 22, 38, 39, 41], which learn image embeddings from the input images as the representation of the concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods for learning image embeddings. No verifiable resources are identified.",
      "processing_time": 56.49785256385803,
      "citing_paper_id": "258041269",
      "cited_paper_id": 264815845
    },
    {
      "context_text": "SuTI [6] learns the concept from a massive amount of paired images generated by subject-driven expert models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to 'paired images' but does not provide a specific name or identifier.",
      "processing_time": 57.16422414779663,
      "citing_paper_id": "258041269",
      "cited_paper_id": 257913352
    },
    {
      "context_text": "2 outlines the inference pipeline of our method for generating images from image set X = { x i } Ni =1 ( N can be 1 or larger) and text prompt P .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a general method for generating images from an image set and a text prompt.",
      "processing_time": 57.702845335006714,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "DreamBooth can generate high-quality images; however, when the input person subject occupies a small portion (e.g., row 2 and 4 of Fig.4), it tends to generated the person within a small portion of the image, thus limiting the identity preservation ability.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a limitation of the DreamBooth method.",
      "processing_time": 55.68144226074219,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "All the comparison methods tend to replicate the pose of the foreground person, and Textual Inversion and DreamBooth also replicate the background.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and their outcomes.",
      "processing_time": 54.99980139732361,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "We adopt the official code base for Textual Inversion and third-party replication[40] for DreamBooth where the base text-to-image model is replaced from Imagen [35] to Stable Diffusion [33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and code bases. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.001402139663696,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "For all experiments of our model, we use “sks” as the unique identifier V̂ as suggested in DreamBooth [40].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a unique identifier used in experiments. No verifiable resources are referenced.",
      "processing_time": 56.14283871650696,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "Moreover, even when the input image contains a large portion of the person object (e.g., row 5 of Fig.4), DreamBooth often only preserves the person’s outfit but distorts the face identity.",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It describes a limitation of DreamBooth in preserving face identity.",
      "processing_time": 56.93425893783569,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "We conduct a user study to compare our method with DreamBooth and Textual Inversion perceptually.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (DreamBooth and Textual Inversion).",
      "processing_time": 56.215322494506836,
      "citing_paper_id": "258041269",
      "cited_paper_id": null
    },
    {
      "context_text": "To measure identity consistency, we use a face detector [21] to detect faces in both the generated images and the reference images.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It only refers to a face detector, which is a method or tool, not a dataset.",
      "processing_time": 57.472739458084106,
      "citing_paper_id": "268512710",
      "cited_paper_id": 6155330
    },
    {
      "context_text": "During training, our encoder is trained on FFHQ [18] and AFHQ [19] for 100k iterations on four A100s with a batch size of 2 and a learning rate of 1e-5.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FFHQ",
        "AFHQ"
      ],
      "dataset_descriptions": {
        "FFHQ": "Used to train the encoder for 100k iterations, focusing on high-resolution face images to improve generative model performance.",
        "AFHQ": "Used to train the encoder for 100k iterations, focusing on animal faces to enhance the diversity and quality of generated images."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions FFHQ and AFHQ, which are known datasets used for training generative models. However, they are not directly related to personalized text generation.",
      "processing_time": 70.70841670036316,
      "citing_paper_id": "268512710",
      "cited_paper_id": 54482423
    },
    {
      "context_text": "The structure of our encoder can be represented as Equation(6), where the Encoder is the image encoder from CLIP [17], extracting the rich detailed features embeddings (257 × 768) from the penultimate layer, and using an MLP as the projection layer to project the rich feature embeddings into the…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (CLIP) which is excluded according to the instructions.",
      "processing_time": 56.394190311431885,
      "citing_paper_id": "268512710",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Tuning-based methods [1, 2, 3, 4, 5, 6, 7] mainly convert a concept into a text embedding or convert the concept within the parameters of a generative model through fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.483781814575195,
      "citing_paper_id": "268512710",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Tuning-based methods [1, 2, 3, 4, 5, 6, 7] mainly convert a concept into a text embedding or convert the concept within the parameters of a generative model through fine-tuning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.483781814575195,
      "citing_paper_id": "268512710",
      "cited_paper_id": 254408780
    },
    {
      "context_text": "Inspired by recent works [14, 15, 16] in image-to-image translation, which typically edits images through diffusion’s attention layers, we found it is not necessary to train any additional text embedding for each concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to image-to-image translation methods and concepts.",
      "processing_time": 55.77113747596741,
      "citing_paper_id": "268512710",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "Generative Quality: We score the mean aesthetics of the generated images using an aesthetic predictor [23] to measure the generative quality.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset name, only a general reference to an aesthetic predictor. The cited paper title suggests a large-scale dataset, but it is not explicitly used in the given context.",
      "processing_time": 58.683614015579224,
      "citing_paper_id": "268512710",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "Encoder-based methods [8, 9, 10, 11, 12, 13] mainly train a mapping between the concept images and the text embed-dings.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general training process between concept images and text embeddings. No verifiable resources are identified.",
      "processing_time": 57.29599618911743,
      "citing_paper_id": "268512710",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Encoder-based methods [8, 9, 10, 11, 12, 13] mainly train a mapping between the concept images and the text embed-dings.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general training process between concept images and text embeddings. No verifiable resources are identified.",
      "processing_time": 57.29599618911743,
      "citing_paper_id": "268512710",
      "cited_paper_id": 257952647
    },
    {
      "context_text": "Encoder-based methods [8, 9, 10, 11, 12, 13] mainly train a mapping between the concept images and the text embed-dings.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a general training process between concept images and text embeddings. No verifiable resources are identified.",
      "processing_time": 57.29599618911743,
      "citing_paper_id": "268512710",
      "cited_paper_id": 259847716
    },
    {
      "context_text": "One clear limitation of the present study is that the prototypes might be stereotypical and might not fully represent how emotions are communicated in real life [17, 18].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a limitation regarding the representation of emotions in prototypes.",
      "processing_time": 55.841108560562134,
      "citing_paper_id": "233739719",
      "cited_paper_id": 3403061
    },
    {
      "context_text": "One clear limitation of the present study is that the prototypes might be stereotypical and might not fully represent how emotions are communicated in real life [17, 18].",
      "catation_intent": "limitation",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a limitation regarding the representation of emotions in prototypes.",
      "processing_time": 55.841108560562134,
      "citing_paper_id": "233739719",
      "cited_paper_id": 150798263
    },
    {
      "context_text": "A reference encoder [10] is added, which compresses the Mel spectrogram to a fixed length embedding.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (reference encoder) used in the research.",
      "processing_time": 55.667038440704346,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4425995
    },
    {
      "context_text": "GST Tacotron [3] is one of the most prominent examples of such TTS systems and is capable of learning prosodic features relevant to the communication of affect [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (GST Tacotron) and its capabilities. No verifiable resources are identified.",
      "processing_time": 57.125991106033325,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "GST Tacotron [3] is one of the most prominent examples of such TTS systems and is capable of learning prosodic features relevant to the communication of affect [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (GST Tacotron) and its capabilities. No verifiable resources are identified.",
      "processing_time": 57.125991106033325,
      "citing_paper_id": "233739719",
      "cited_paper_id": 239709387
    },
    {
      "context_text": "It is an extension of Tacotron, which is a sequence-to-sequence model learning the TTS task solely on pairs of recordings and transcripts (see online supplementary Figure S1 for the architecture).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (Tacotron) and its architecture.",
      "processing_time": 56.73634314537048,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "In this paper, we used Gibbs Sampling with People together with a trained GST Tacotron model in order to explore prototypes of emotional prosody.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to methods and models.",
      "processing_time": 55.81779098510742,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "In GST Tacotron a few components are added to Tacotron.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only components added to Tacotron. The context is about model architecture, not data.",
      "processing_time": 56.918452978134155,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "[2] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards end-to-end speech synthesis,” in Interspeech 2017.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Tacotron) for end-to-end speech synthesis.",
      "processing_time": 56.205833196640015,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "More recently, models have been proposed that are able to generate expressive speech, such as Tacotron with Global Style Tokens (GST Tacotron) [3], Mellotron [4] and Flowtron [5].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 57.18248963356018,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "Together with the text,\nCopyright © 2021 ISCA\nhttp://dx.doi.org/10.21437/Interspeech.2021-15383870\nthe style embedding is passed to the Tacotron model which creates the predicted spectrogram.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (Tacotron) and a general process of passing style embeddings to create predicted spectrograms.",
      "processing_time": 58.49088764190674,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "In the last decade, the quality of text-to-speech (TTS) has greatly been improved by the introduction of neural vocoders in combination with end-to-end TTS models like Tacotron [1, 2].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the improvement of TTS quality through neural vocoders and end-to-end models.",
      "processing_time": 58.1529495716095,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "All in all, GSP in combination with GST Tacotron seems to be a useful and efficient tool for studying emotional prototypes, for exploring speaking styles in existing TTS systems, and for generating new emotional sentences based on pre-existing speech recordings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. The focus is on the combination of GSP and GST Tacotron for generating emotional sentences.",
      "processing_time": 57.78301215171814,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "All in all, GSP in combination with GST Tacotron seems to be a useful and efficient tool for studying emotional prototypes, for exploring speaking styles in existing TTS systems, and for generating new emotional sentences based on pre-existing speech recordings.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only tools and methods. The focus is on the combination of GSP and GST Tacotron for generating emotional sentences.",
      "processing_time": 57.78301215171814,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Here we use GSP to search the prosodic latent space in a trained GST Tacotron model to explore prototypes of emotional prosody.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method (GSP) and a model (GST Tacotron), which are not datasets.",
      "processing_time": 58.14499878883362,
      "citing_paper_id": "233739719",
      "cited_paper_id": 4689304
    },
    {
      "context_text": "Here we use GSP to search the prosodic latent space in a trained GST Tacotron model to explore prototypes of emotional prosody.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a method (GSP) and a model (GST Tacotron), which are not datasets.",
      "processing_time": 58.14499878883362,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "To complement this hand-crafted feature set, we added all features from the larger standard feature set (eGeMAPS) developed for detecting emotions from speech [15, 16].",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions adding features from eGeMAPS, which is a feature set, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.16788148880005,
      "citing_paper_id": "233739719",
      "cited_paper_id": 5205529
    },
    {
      "context_text": "All features were extracted with Praat [13] through a Python wrapper [14].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions tools (Praat and a Python wrapper) but does not reference any specific datasets.",
      "processing_time": 56.0051794052124,
      "citing_paper_id": "233739719",
      "cited_paper_id": 53493607
    },
    {
      "context_text": "There are several psychological paradigms that can sample from such spaces using human participants, such as reverse correlation [6], Markov Chain Monte Carlo with People (MCMCP) [7] and Gibbs Sampling with People (GSP) [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for sampling from psychological spaces using human participants.",
      "processing_time": 55.835742235183716,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "In a previous study [8], we used GSP to sample prototypes of emotional prosody when explicitly manipulating duration, loudness and pitch related features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'GSP' which could be a method or tool, but does not provide enough information to confirm it as a dataset. No other specific datasets are mentioned.",
      "processing_time": 58.49207425117493,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "This grouping emerges relatively early on, providing additional support for early convergence of the GSP process (Figure 2E).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to a figure and a process, which does not provide enough information to identify a verifiable resource.",
      "processing_time": 58.70269441604614,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "GSP is a particularly recent paradigm that uses a continuous-sampling task instead of the binary choice task used by the other methods.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.821635484695435,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Again the correlation between both experiments is low (r(256) = .31, p < .001), indicating that our GST experiment identifies different regions of the prosodic space than the experiment described in Harrison et al. (2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a correlation between two experiments. The cited paper title 'Gibbs Sampling with People' does not indicate a dataset.",
      "processing_time": 58.099525928497314,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "GSP is an adaptive procedure whereby many participants collaborate to explore a high-dimensional sample space (Figure 1).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method (Gibbs Sampling with People) but does not reference any dataset.",
      "processing_time": 57.590627670288086,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "These results suggests that both GSP methods generate samples with emotional states that occupy distinct parts of the feature space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GSP) and its outcomes. No verifiable resources are identified.",
      "processing_time": 57.08557438850403,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Finally, we found interesting acoustic differences between the current study and Harrison et al. (2020), which should be explored in future research by carefully comparing emotional prototypes created with hand-crafted acoustic manipulations versus those created by TTS models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between methods (hand-crafted acoustic manipulations vs. TTS models).",
      "processing_time": 57.04584360122681,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "In order to compare the current results with our findings from previous work [8], we computed a similar set of acoustic features that were manipulated in the previous experiment.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to previous work and the computation of acoustic features. No verifiable resource names are provided.",
      "processing_time": 57.57901310920715,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "However, Figure 2F shows that profiles computed on the stimuli from Harrison et al. (2020) look rather different from the profiles in the current study (r(16) = .27, p = ns).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a comparison of profiles from a previous study. No clear, verifiable dataset is identified.",
      "processing_time": 57.096463680267334,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "GSP provides a way to overcome this independence assumption: it leverages a well-established algorithm from computational statistics (Gibbs sampling) to identify regions of stimulus spaces associated with given semantic labels, while avoiding any independence assumptions [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Gibbs sampling). The context focuses on the algorithm and its application, not on a particular dataset.",
      "processing_time": 57.872167110443115,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Participant recruitment was managed by PsyNet [8], an under-development framework for implementing complex experiment paradigms such as GSP and MCMCP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a framework called PsyNet which is not a dataset. The citation is about participant recruitment and experiment paradigms, not data.",
      "processing_time": 58.595887422561646,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "When synthesizing from the model we set the attention weights (Figure 1E) directly from the current location of the relevant GSP chain in the sample space (Figure 1B), generating one output for each of the 32 possible slider positions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating outputs using a model. No verifiable resources are identified.",
      "processing_time": 56.66266489028931,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "The resulting process can be interpreted as a Gibbs sampler, a well-known algorithm from computational statistics for sampling from highdimensional probability distributions [8].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Gibbs sampler).",
      "processing_time": 55.57023859024048,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Previous work [8] showed that a simple handcrafted feature space was sufficient for generating distinctive, well-recognized prosodic prototypes of emotions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach using handcrafted features for generating emotional prosody.",
      "processing_time": 56.501237869262695,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "We created 156 transfer stimuli by applying the median attention weights of the final GSP iteration to four novel sentences from the Harvard sentence corpus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Harvard sentence corpus"
      ],
      "dataset_descriptions": {
        "Harvard sentence corpus": "Used to generate novel sentences for creating transfer stimuli, focusing on the application of median attention weights from the GSP iteration to study personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Harvard sentence corpus' as a source of novel sentences for creating transfer stimuli. This corpus is a well-known linguistic resource.",
      "processing_time": 65.46307110786438,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Future research could address this issue by replacing the discrete emotion labels in the GSP paradigm with descriptions of real-life emotional situations.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only suggests a future direction for research involving the GSP paradigm.",
      "processing_time": 56.72303056716919,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "Participant recruitment was managed by PsyNet [8], an under-development framework for implementing complex ex-\nperiment paradigms such as GSP and MCMCP.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a framework called PsyNet which is not a dataset. The title 'Gibbs Sampling with People' does not provide additional information about a dataset.",
      "processing_time": 59.302096128463745,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "We include the last two iterations from Harrison et al. (2020) and iteration 9 – 20 from the current experiment to have a similar number of stimuli per experiment and made sure every emotion occurs equally often in every fold.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only iterations from experiments. There are no clear identifiers for datasets.",
      "processing_time": 56.16886329650879,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "First, the constrained feature set used in Harrison et al. (2020) might have forced participants to rely heavily on particular prosodic features that otherwise might be treated only as secondary emotional cues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach used in a study. There are no clear identifiers for datasets in the text.",
      "processing_time": 57.72031116485596,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "However, when predicting Harrison et al. (2020) with the current results or vice versa, we obtain a lower UAR (49.1% and 48.6% respectively).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only reports performance metrics (UAR) when comparing results across studies.",
      "processing_time": 57.44724917411804,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": "We observed that the Unweighted Average Recall (UAR) is high for both experiments: Harrison et al. (2020) obtains 75.0% UAR and the current experiment 79.4% UAR (chance: 33.3% UAR).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics (UAR). There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.70216631889343,
      "citing_paper_id": "233739719",
      "cited_paper_id": 221005871
    },
    {
      "context_text": ", 2003] has attracted a lot of attention, since such dense word vector representation in high-dimensional (but much lower dimensional than one-hot representation) space can provide reduced computational complexity and improve generalization ability of machine learning models for many downstream tasks [Collobert et al., 2011].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the benefits of dense word vector representations.",
      "processing_time": 55.73595404624939,
      "citing_paper_id": "9135567",
      "cited_paper_id": 351666
    },
    {
      "context_text": "Later, it was further scaled up and studied [Morin and Bengio, 2005; Mnih and Hinton, 2008; Collobert et al., 2011] and pushed to limit to train on Webscale corpus [Józefowicz et al., 2016].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'Webscale corpus' but does not provide a specific name or identifier. It is a generic reference to a large web-based dataset, which does not meet the criteria for inclusion.",
      "processing_time": 59.383644342422485,
      "citing_paper_id": "9135567",
      "cited_paper_id": 351666
    },
    {
      "context_text": "…of attention, since such dense word vector representation in high-dimensional (but much lower dimensional than one-hot representation) space can provide reduced computational complexity and improve generalization ability of machine learning models for many downstream tasks [Collobert et al., 2011].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the benefits of dense word vector representations.",
      "processing_time": 55.72997212409973,
      "citing_paper_id": "9135567",
      "cited_paper_id": 351666
    },
    {
      "context_text": "Later, it was further scaled up and studied [Morin and Bengio, 2005; Mnih and Hinton, 2008; Collobert et al., 2011] and pushed to limit to train on Webscale corpus [Józefowicz et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a 'Webscale corpus' without providing a specific name or identifier.",
      "processing_time": 57.742666244506836,
      "citing_paper_id": "9135567",
      "cited_paper_id": 351666
    },
    {
      "context_text": "Thus, there\nis a need of development of computational sociolinguistics for social media texts [Nguyen et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general need for computational sociolinguistics in social media texts.",
      "processing_time": 56.68255305290222,
      "citing_paper_id": "9135567",
      "cited_paper_id": 583389
    },
    {
      "context_text": "” Thus, there is a need of development of computational sociolinguistics for social media texts [Nguyen et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general need for computational sociolinguistics in social media texts.",
      "processing_time": 56.67904996871948,
      "citing_paper_id": "9135567",
      "cited_paper_id": 583389
    },
    {
      "context_text": "Thus, it can adopt efficient training algorithm to train over large-scale corpus and is widely used in many applications, such as information extraction [Turian et al., 2010], sentiment analysis [Tang et al., 2015a], search engine [Mitra and Craswell, 2017], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general applications of a method. No dataset names are present in the citation span.",
      "processing_time": 56.67691254615784,
      "citing_paper_id": "9135567",
      "cited_paper_id": 629094
    },
    {
      "context_text": "Thus, it can adopt efficient training algorithm to train over large-scale corpus and is widely used in many applications, such as information extraction [Turian et al., 2010], sentiment analysis [Tang et al., 2015a], search engine [Mitra and Craswell, 2017], etc.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general applications of a method. No dataset names are present in the citation span.",
      "processing_time": 56.67691254615784,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Thus, it can adopt efficient training algorithm to train over large-scale corpus and is widely used in many applications, such as information extraction [Turian et al., 2010], sentiment analysis [Tang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to general applications of a method.",
      "processing_time": 56.33623147010803,
      "citing_paper_id": "9135567",
      "cited_paper_id": 629094
    },
    {
      "context_text": "We follow the semi-supervised learning setting claimed by Turian et al. (2010), where the word embeddings are trained in an unsupervised way with larger corpus, then it can be used for downstream tasks with smaller number of training examples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a general method for training word embeddings using a larger corpus, which is not a specific dataset.",
      "processing_time": 58.59168291091919,
      "citing_paper_id": "9135567",
      "cited_paper_id": 629094
    },
    {
      "context_text": "In practice, we solve this by SGD with re-projection [Goodfellow et al., 2016].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (SGD with re-projection).",
      "processing_time": 55.89760732650757,
      "citing_paper_id": "9135567",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "We compared with different deep learning architectures, i.e., convolutional neural networks (CNN) and long short term memory (LSTM) recurrent neural networks (see [Goodfellow et al., 2016] for more details), and their hierarchical versions (HCNN and HLSTM) [Tang et al., 2015b; Chen et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.23260855674744,
      "citing_paper_id": "9135567",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "We compared with different deep learning architectures, i.e., convolutional neural networks (CNN) and long short term memory (LSTM) recurrent neural networks (see [Goodfellow et al., 2016] for more details), and their hierarchical versions (HCNN and HLSTM) [Tang et al., 2015b; Chen et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 56.23260855674744,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": ", convolutional neural networks (CNN) and long short term memory (LSTM) recurrent neural networks (see [Goodfellow et al., 2016] for more details), and their hierarchical versions (HCNN and HLSTM) [Tang et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.79719114303589,
      "citing_paper_id": "9135567",
      "cited_paper_id": 1779661
    },
    {
      "context_text": "Inspired by word2vec, many alternatives of embedding approaches have been proposed [Pennington et al., 2014; Levy and Goldberg, 2014] and a comprehensive study has shown that word2vec can be very powerful when all the models are tuned on the same corpus with best hyper-parameters [Levy et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The citation is focused on embedding approaches and their performance, not on the use of a particular dataset.",
      "processing_time": 58.10754203796387,
      "citing_paper_id": "9135567",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Inspired by word2vec, many alternatives of embedding approaches have been proposed [Pennington et al., 2014; Levy and Goldberg, 2014] and a comprehensive study has shown that word2vec can be very powerful when all the models are tuned on the same corpus with best hyper-parameters [Levy et al.,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on embedding approaches and their performance.",
      "processing_time": 56.5867919921875,
      "citing_paper_id": "9135567",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Personalized language models were mainly applied to Web search [Croft et al., 2001; Song et al., 2010; Sontag et al., 2012] or collaborative search (where user groups are clustered based on user behaviors instead of explicit connections) [Sun et al., 2005; Teevan et al., 2009; Xue et al., 2009].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized language models. No verifiable resources are identified.",
      "processing_time": 56.84064221382141,
      "citing_paper_id": "9135567",
      "cited_paper_id": 2982180
    },
    {
      "context_text": "Personalized language models were mainly applied to Web search [Croft et al., 2001; Song et al., 2010; Sontag et al., 2012] or collaborative search (where user groups are clustered based on user behaviors instead of explicit connections) [Sun et al., 2005; Teevan et al., 2009; Xue et al., 2009].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized language models. No verifiable resources are identified.",
      "processing_time": 56.84064221382141,
      "citing_paper_id": "9135567",
      "cited_paper_id": 7062883
    },
    {
      "context_text": "Personalized language models were mainly applied to Web search [Croft et al., 2001; Song et al., 2010; Sontag et al., 2012] or collaborative search (where user groups are clustered based on user behaviors instead of explicit connections) [Sun et al., 2005; Teevan et al., 2009; Xue et al., 2009].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized language models. No verifiable resources are identified.",
      "processing_time": 56.84064221382141,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "To train the word embeddings, we consider the continuous bag-of-words (CBOW) model [Mikolov et al., 2013a] as our base model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (CBOW) and a method for training word embeddings.",
      "processing_time": 57.770766735076904,
      "citing_paper_id": "9135567",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "To train the word embeddings, we consider the continuous bag-of-words (CBOW) model [Mikolov et al., 2013a] as our base model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (CBOW) and a method for training word embeddings.",
      "processing_time": 57.770766735076904,
      "citing_paper_id": "9135567",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Word embedding, i.e., word2vec [Mikolov et al., 2013b; Mikolov et al., 2013a], simplifies the NNLM problem and has been shown to be efficient for training over very large-scale corpus.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'word2vec' and 'very large-scale corpus', but does not specify a named dataset. 'word2vec' is a method, not a dataset.",
      "processing_time": 58.339948415756226,
      "citing_paper_id": "9135567",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "Word embedding, i.e., word2vec [Mikolov et al., 2013b; Mikolov et al., 2013a], simplifies the NNLM problem and has been shown to be efficient for training over very large-scale corpus.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'word2vec' and 'very large-scale corpus', but does not specify a named dataset. 'word2vec' is a method, not a dataset.",
      "processing_time": 58.339948415756226,
      "citing_paper_id": "9135567",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Word embeddings, such as word2vec [Mikolov et al., 2013a], simplifies the NNLM architecture by reducing the latent variables and relaxing the constraint of context words being previous words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only word embedding methods like word2vec. No verifiable datasets are referenced.",
      "processing_time": 56.65004754066467,
      "citing_paper_id": "9135567",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "Word embeddings, such as word2vec [Mikolov et al., 2013a], simplifies the NNLM architecture by reducing the latent variables and relaxing the constraint of context words being previous words.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only word embedding methods like word2vec. No verifiable datasets are referenced.",
      "processing_time": 56.65004754066467,
      "citing_paper_id": "9135567",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Thus, originally there are two techniques used for optimizing the problem, hierarchical softmax [Morin and Bengio, 2005; Mnih and Hinton, 2008] and negative sampling [Mikolov et al., 2013a].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (hierarchical softmax and negative sampling).",
      "processing_time": 55.964335680007935,
      "citing_paper_id": "9135567",
      "cited_paper_id": 5959482
    },
    {
      "context_text": "Thus, originally there are two techniques used for optimizing the problem, hierarchical softmax [Morin and Bengio, 2005; Mnih and Hinton, 2008] and negative sampling [Mikolov et al., 2013a].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods (hierarchical softmax and negative sampling).",
      "processing_time": 55.964335680007935,
      "citing_paper_id": "9135567",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "Socialized languages models have recently been developed, which were also applied to search problems, but to social media text search [Vosecky et al., 2014; Huang et al., 2014; Yan et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to socialized language models and their application to social media text search. No verifiable resources are identified.",
      "processing_time": 58.05456781387329,
      "citing_paper_id": "9135567",
      "cited_paper_id": 7595933
    },
    {
      "context_text": "Socialized languages models have recently been developed, which were also applied to search problems, but to social media text search [Vosecky et al., 2014; Huang et al., 2014; Yan et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to socialized language models and their application to social media text search. No verifiable resources are identified.",
      "processing_time": 58.05456781387329,
      "citing_paper_id": "9135567",
      "cited_paper_id": 15130818
    },
    {
      "context_text": "Socialized languages models have recently been developed, which were also applied to search problems, but to social media text search [Vosecky et al., 2014; Huang et al., 2014; Yan et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to socialized language models and their application to social media text search. No verifiable resources are identified.",
      "processing_time": 58.05456781387329,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Our work is also related to multi-task learning [Caruana, 1997].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to multi-task learning.",
      "processing_time": 55.65165376663208,
      "citing_paper_id": "9135567",
      "cited_paper_id": 45998148
    },
    {
      "context_text": "A network could be loose or tight depending on how members interact with each other [Holmes, 2012] and may affect speech patterns adopted by a speaker [Dubois and Horvath, 1998].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to sociolinguistic concepts and their effects on speech patterns.",
      "processing_time": 57.0183892250061,
      "citing_paper_id": "9135567",
      "cited_paper_id": 141987270
    },
    {
      "context_text": "When user information is desired to be included, previous studies simply preprocess the data and work only on the data containing sufficient user information (e.g., containing only 4,818 users) [Tang et al., 2015b; Chen et al., 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to preprocessing data to include user information, which is too generic.",
      "processing_time": 57.56076240539551,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, deep learning based models also adopt a separate learning mechanism to model different users along global text representation to improve sentiment classification results [Tang et al., 2015c; Tang et al., 2015b], which can be regarded as a multi-task learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a methodological approach in deep learning for sentiment classification.",
      "processing_time": 55.819024324417114,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "We use the one-fifth training set in the previous experiment of YelpR8 data as our training data, and test on both development and test sets, which have same scales with the sets used in [Tang et al., 2015b; Chen et al., 2016] .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YelpR8"
      ],
      "dataset_descriptions": {
        "YelpR8": "Used for training and testing personalized text generation models, focusing on the scalability and performance across development and test sets."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'YelpR8 data' as a specific dataset used for training and testing. The dataset is clearly identified and used in a specific research context.",
      "processing_time": 64.5811014175415,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, deep learning based models also adopt a separate learning mechanism to model different users along global text representation to\nimprove sentiment classification results [Tang et al., 2015c; Tang et al., 2015b], which can be regarded as a multi-task learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a methodological approach in deep learning for sentiment classification.",
      "processing_time": 56.52410364151001,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "We follow the task used in [Tang et al., 2015c; Tang et al., 2015a; Tang et al., 2015b; Chen et al., 2016], which is long document sentiment classification.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a task related to long document sentiment classification.",
      "processing_time": 55.99868202209473,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "For example, social media topic classification and sentiment classification can be personalized [Hu et al., 2013; Song et al., 2013; Wu and Huang, 2016].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general research areas. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 56.922887325286865,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Personalized language models were mainly applied to Web search [Croft et al., 2001; Song et al., 2010; Sontag et al., 2012] or collaborative search (where user groups are clustered based on user behaviors instead of explicit connections) [Sun et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized language models. There are no clear identifiers for datasets.",
      "processing_time": 57.08010506629944,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": ", 2012] or collaborative search (where user groups are clustered based on user behaviors instead of explicit connections) [Sun et al., 2005; Teevan et al., 2009; Xue et al., 2009].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references methods and approaches in collaborative search.",
      "processing_time": 56.46702289581299,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "A well-known effect of social network is the concept of “homophily,” which has been developed in psychology [Lazarsfeld and Merton, 1954] and observed in social network [McPherson et al., 2001].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to concepts and observations in social network studies.",
      "processing_time": 56.10091471672058,
      "citing_paper_id": "9135567",
      "cited_paper_id": null
    },
    {
      "context_text": "Automated agents beneﬁt from adapting their personality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al., 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research findings and methods.",
      "processing_time": 55.40563082695007,
      "citing_paper_id": "34405847",
      "cited_paper_id": 265223
    },
    {
      "context_text": "Automated agents beneﬁt from adapting their personality according to the task at hand (Reeves and Nass, 1996; Tapus and Mataric, 2008) or to the customer (Herzig et al., 2016).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research findings and methods.",
      "processing_time": 55.40563082695007,
      "citing_paper_id": "34405847",
      "cited_paper_id": 16581260
    },
    {
      "context_text": "Li et al. (2016) modiﬁed a S EQ 2S EQ model to encode a per-sona (the character of an artiﬁcial agent).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a modification to a Seq2Seq model for encoding persona. No verifiable resources are identified.",
      "processing_time": 57.230122089385986,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "In this setting, since all the agents that appear in the test data appear also in the training data, we can also test the performance of (Li et al., 2016), which learns a persona vector for each agent in the training data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'test data' and 'training data' but does not provide specific names for these datasets. The citation is about a method (persona-based neural conversation model) rather than a dataset.",
      "processing_time": 59.10085320472717,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Following Li et al. (2016) methodology, the two responses were presented in a random order, and judged on a 5 -point zero-sum scale.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodology for judging responses.",
      "processing_time": 54.920286893844604,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Following (Sordoni et al., 2015; Li et al., 2016) we used B LEU (Papineni et al., 2002) for evaluation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BLEU for evaluation, which is a metric, not a dataset. No specific datasets are mentioned.",
      "processing_time": 56.630967140197754,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Following (Sordoni et al., 2015; Li et al., 2016) we used B LEU (Papineni et al., 2002) for evaluation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of BLEU for evaluation, which is a metric, not a dataset. No specific datasets are mentioned.",
      "processing_time": 56.630967140197754,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "After discarding ties, we found that the high-trait responses generated by our P ERSONALITY - BASED model were judged either more expressive or somewhat more expressive than the low-trait corresponding responses in 61% of cases.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and its performance. The context is about the results of a personality-based model in generating more expressive responses.",
      "processing_time": 57.77070236206055,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "…the lowest performance in terms of both perplexity and B LEU score while the competing models which learn a representation Model Perplexity B LEU S EQ 2S EQ 11.49 6.3% P ERSONA - BASED (Li et al., 2016) 9.25 15.55% P ERSONALITY - BASED 9.62 12.46% for the agents achieved higher performance.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Persona-Based' and 'Personality-Based' models, but does not refer to any specific datasets. The citation is focused on comparing model performance rather than using a dataset.",
      "processing_time": 58.599454164505005,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "In this section we present our P ERSONALITY - BASED model (Figure 2) which generates responses conditioned on a target set of personality traits values which the responses should express.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model for generating responses based on personality traits.",
      "processing_time": 55.69370484352112,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The results in table 1 show that the standard S EQ 2S EQ model achieved the lowest performance in terms of both perplexity and B LEU score while the competing models which learn a representation Model Perplexity B LEU S EQ 2S EQ 11.49 6.3% P ERSONA - BASED (Li et al., 2016) 9.25 15.55% P ERSONALITY - BASED 9.62 12.46% for the agents achieved higher performance.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'P ERSONA - BASED' which is likely referring to the dataset or method described in the cited paper 'A Persona-Based Neural Conversation Model'. However, it is not clear if 'P ERSONA - BASED' is a dataset or a method. The context does not provide enough information to confidently label it as a dataset.",
      "processing_time": 64.85893607139587,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The results in table 1 show that the standard S EQ 2S EQ model achieved the lowest performance in terms of both perplexity and B LEU score while the competing models which learn a representation Model Perplexity B LEU S EQ 2S EQ 11.49 6.3% P ERSONA - BASED (Li et al., 2016) 9.25 15.55% P ERSONALITY - BASED 9.62 12.46% for the agents achieved higher performance.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'P ERSONA - BASED' which is likely referring to the dataset or method described in the cited paper 'A Persona-Based Neural Conversation Model'. However, it is not clear if 'P ERSONA - BASED' is a dataset or a method. The context does not provide enough information to confidently label it as a dataset.",
      "processing_time": 64.85893607139587,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "In this setting, it is not possible to test the P ERSONA - BASED model since no representation is learned during training for agents in the test set.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses a model and its limitations.",
      "processing_time": 55.840482234954834,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "This is reasonable since PERSONA - BASED is not restricted to personality based features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to a model or method. The title 'A Persona-Based Neural Conversation Model' suggests a method rather than a dataset.",
      "processing_time": 58.234461307525635,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "We conducted a human evaluation of our P ERSONALITY - BASED model using a crowd-sourcing service.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a human evaluation using a crowd-sourcing service, which is too generic.",
      "processing_time": 57.041996002197266,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The PERSONA - BASED model achieved similar perplexity but higher B LEU score than our model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only model performance metrics. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.74706983566284,
      "citing_paper_id": "34405847",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "The PERSONA - BASED model achieved similar perplexity but higher B LEU score than our model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only model performance metrics. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.74706983566284,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Neural response generation models (Vinyals and Le, 2015; Shang et al., 2015) are based on a S EQ 2S EQ architecture (Sutskever et al., 2014) and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 55.99940729141235,
      "citing_paper_id": "34405847",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Neural response generation models (Vinyals and Le, 2015; Shang et al., 2015) are based on a S EQ 2S EQ architecture (Sutskever et al., 2014) and employ an encoder to represent the user utterance and an attention-based decoder that generates the agent response one token at a time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and architectures. No verifiable resources are identified.",
      "processing_time": 55.99940729141235,
      "citing_paper_id": "34405847",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Besides B LEU scores, we also report perplexity as an indicator of model capability.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. The context is focused on reporting perplexity alongside BLEU scores.",
      "processing_time": 56.262733936309814,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "We ﬁnd that leveraging personality encoding improves relative performance up to 46% in B LEU score, compared to a baseline S EQ 2S EQ model.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric (BLEU score) and a model (SEQ2SEQ).",
      "processing_time": 56.590147256851196,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "7% decrease in perplexity, and a 46% relative improvement in B LEU score.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No dataset names are present in the citation span.",
      "processing_time": 56.115299224853516,
      "citing_paper_id": "34405847",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "Early work on the PERSONAGE system (Mairesse and Walker, 2007; Mairesse and Walker, 2008; Mairesse and Walker, 2010; Mairesse and Walker, 2011) presented a framework projecting different traits throughout the different modules of an NLG system.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the PERSONAGE system but does not refer to any specific dataset. The focus is on the system and its framework, not on a dataset.",
      "processing_time": 57.20777463912964,
      "citing_paper_id": "34405847",
      "cited_paper_id": 12366617
    },
    {
      "context_text": "Finally, Xu et al. (2017) generated responses for customer service re-\nquests on social media using standard SEQ2SEQ, while we modify it to generate a target personality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (SEQ2SEQ) and a modification for generating a target personality.",
      "processing_time": 57.59580874443054,
      "citing_paper_id": "34405847",
      "cited_paper_id": 207246940
    },
    {
      "context_text": "For our experiments we utilized the dataset presented in (Xu et al., 2017), which exhibits a large variety of customer service properties.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "dataset presented in (Xu et al., 2017)"
      ],
      "dataset_descriptions": {
        "dataset presented in (Xu et al., 2017)": "Used to experiment with a variety of customer service properties, focusing on chatbot interactions on social media platforms."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a specific dataset from Xu et al., 2017, which is used for experiments involving customer service properties. The title confirms it is a chatbot-related dataset.",
      "processing_time": 68.2719292640686,
      "citing_paper_id": "34405847",
      "cited_paper_id": 207246940
    },
    {
      "context_text": "In order to use the item specifications, CTR [40] integrates PMF [31] and Latent Dirichlet Allocation (LDA) [4] into a single framework and employs LDA to model the text.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on integrating PMF and LDA into a framework for modeling text.",
      "processing_time": 57.2431538105011,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "RMR Ratings meet reviews model [21] CTR Collaborative topic regression model [40] NMF Non-negative matrix factorization [18] PMF Probabilistic matrix factorization [31] LRMF List-wise learning to rank for item ranking [35] SVD++ Factorization meets the neighborhood [14] URP User rating profile modeling using LDA [22]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The cited paper titles also do not indicate the presence of datasets.",
      "processing_time": 56.89641857147217,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "RMR Ratings meet reviews model [21] CTR Collaborative topic regression model [40] NMF Non-negative matrix factorization [18] PMF Probabilistic matrix factorization [31] LRMF List-wise learning to rank for item ranking [35] SVD++ Factorization meets the neighborhood [14] URP User rating profile modeling using LDA [22]",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models and methods but does not refer to any specific datasets. The cited paper titles also do not indicate the presence of datasets.",
      "processing_time": 56.89641857147217,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "The reason is that CTR and RMR consider text information such as item specifications and user reviews to improve the representation quality of latent factors, while the traditional CF-based models (e.g. LRMF, NMF, PMF, and SVD++) only consider the rating matrix as the input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on comparing different recommendation systems approaches.",
      "processing_time": 56.00521159172058,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "The reason is that CTR and RMR consider text information such as item specifications and user reviews to improve the representation quality of latent factors, while the traditional CF-based models (e.g. LRMF, NMF, PMF, and SVD++) only consider the rating matrix as the input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on comparing different recommendation systems approaches.",
      "processing_time": 56.00521159172058,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Books Electronics Movies Yelp-2016\nMAE RMSE MAE RMSE MAE RMSE MAE RMSE\nLRMF 1.939 2.153 2.005 2.203 1.977 2.189 1.809 2.038 PMF 0.882 1.219 1.220 1.612 0.927 1.290 1.320 1.752 NMF 0.731 1.035 0.904 1.297 0.794 1.135 1.062 1.454 SVD++ 0.686 0.967 0.847 1.194 0.745 1.049 1.020 1.349 URP 0.704 0.945 0.860 1.126 0.764 1.006 1.030 1.286 CTR 0.736 0.961 0.903 1.154 0.854 1.069 1.174 1.392 RMR 0.681 0.933 0.822 1.123 0.741 1.005 0.994 1.286 NRT 0.667* 0.927* 0.806* 1.107* 0.702* 0.985* 0.985* 1.277*\n*Statistical significance tests show that our method is better than RMR [21].\nwe notice that the topic modeling based methods CTR and RMR are much better than LRMF, NMF, PMF, and SVD++.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp-2016"
      ],
      "dataset_descriptions": {
        "Yelp-2016": "Used to evaluate recommendation algorithms, specifically comparing performance metrics (MAE, RMSE) across different methods (LRMF, PMF, NMF, SVD++, URP, CTR, RMR, NRT) in the context of personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp-2016' as a dataset used for evaluating various recommendation algorithms. No other specific datasets are mentioned.",
      "processing_time": 69.99844312667847,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "Books Electronics Movies Yelp-2016\nMAE RMSE MAE RMSE MAE RMSE MAE RMSE\nLRMF 1.939 2.153 2.005 2.203 1.977 2.189 1.809 2.038 PMF 0.882 1.219 1.220 1.612 0.927 1.290 1.320 1.752 NMF 0.731 1.035 0.904 1.297 0.794 1.135 1.062 1.454 SVD++ 0.686 0.967 0.847 1.194 0.745 1.049 1.020 1.349 URP 0.704 0.945 0.860 1.126 0.764 1.006 1.030 1.286 CTR 0.736 0.961 0.903 1.154 0.854 1.069 1.174 1.392 RMR 0.681 0.933 0.822 1.123 0.741 1.005 0.994 1.286 NRT 0.667* 0.927* 0.806* 1.107* 0.702* 0.985* 0.985* 1.277*\n*Statistical significance tests show that our method is better than RMR [21].\nwe notice that the topic modeling based methods CTR and RMR are much better than LRMF, NMF, PMF, and SVD++.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Yelp-2016"
      ],
      "dataset_descriptions": {
        "Yelp-2016": "Used to evaluate recommendation algorithms, specifically comparing performance metrics (MAE, RMSE) across different methods (LRMF, PMF, NMF, SVD++, URP, CTR, RMR, NRT) in the context of personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Yelp-2016' as a dataset used for evaluating various recommendation algorithms. No other specific datasets are mentioned.",
      "processing_time": 69.99844312667847,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Various MF algorithms have been proposed, such as Singular Value Decomposition (SVD) and SVD++ [14], Non-negative Matrix Factorization (NMF) [18], and Probabilistic Matrix Factorization (PMF) [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several matrix factorization algorithms but does not refer to any specific datasets. The cited papers are methods, not datasets.",
      "processing_time": 56.178874015808105,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "Various MF algorithms have been proposed, such as Singular Value Decomposition (SVD) and SVD++ [14], Non-negative Matrix Factorization (NMF) [18], and Probabilistic Matrix Factorization (PMF) [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several matrix factorization algorithms but does not refer to any specific datasets. The cited papers are methods, not datasets.",
      "processing_time": 56.178874015808105,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "After the tuning process, we set the number of latent factors k = 10 for LRMF, NMF, PMF, and SVD++.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only algorithms and their parameters. The cited papers are about methods, not datasets.",
      "processing_time": 56.004865884780884,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "After the tuning process, we set the number of latent factors k = 10 for LRMF, NMF, PMF, and SVD++.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only algorithms and their parameters. The cited papers are about methods, not datasets.",
      "processing_time": 56.004865884780884,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "• PMF: Probabilistic Matrix Factorization [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Probabilistic Matrix Factorization' which is a method, not a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 57.02305841445923,
      "citing_paper_id": "304614",
      "cited_paper_id": 467086
    },
    {
      "context_text": "• PMF: Probabilistic Matrix Factorization [31].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Probabilistic Matrix Factorization' which is a method, not a dataset. There are no specific datasets mentioned in the citation context.",
      "processing_time": 57.02305841445923,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Recently, some approaches also consider text information in addition to the rating data [1, 21, 23, 26, 40, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only that some approaches consider text information in addition to rating data. No clear, verifiable dataset names are provided.",
      "processing_time": 57.37098145484924,
      "citing_paper_id": "304614",
      "cited_paper_id": 1378406
    },
    {
      "context_text": "Recently, some approaches also consider text information in addition to the rating data [1, 21, 23, 26, 40, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only that some approaches consider text information in addition to rating data. No clear, verifiable dataset names are provided.",
      "processing_time": 57.37098145484924,
      "citing_paper_id": "304614",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "After some investigations, we observe that the text information in most recommendation tasks can be generally classified into two types: item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of text information in recommendation tasks, such as item specifications and user reviews.",
      "processing_time": 57.74267888069153,
      "citing_paper_id": "304614",
      "cited_paper_id": 1378406
    },
    {
      "context_text": "After some investigations, we observe that the text information in most recommendation tasks can be generally classified into two types: item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of text information in recommendation tasks, such as item specifications and user reviews.",
      "processing_time": 57.74267888069153,
      "citing_paper_id": "304614",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "After some investigations, we observe that the text information in most recommendation tasks can be generally classified into two types: item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general types of text information in recommendation tasks, such as item specifications and user reviews.",
      "processing_time": 57.74267888069153,
      "citing_paper_id": "304614",
      "cited_paper_id": 11832773
    },
    {
      "context_text": "Existing models only integrate text information such as item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49] to enhance the performance of latent factor modeling and rating prediction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'user reviews' as a source of text information for enhancing latent factor modeling and rating prediction, but does not specify any named datasets.",
      "processing_time": 57.35552453994751,
      "citing_paper_id": "304614",
      "cited_paper_id": 1378406
    },
    {
      "context_text": "Existing models only integrate text information such as item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49] to enhance the performance of latent factor modeling and rating prediction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'user reviews' as a source of text information for enhancing latent factor modeling and rating prediction, but does not specify any named datasets.",
      "processing_time": 57.35552453994751,
      "citing_paper_id": "304614",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "Existing models only integrate text information such as item specifications [40–42] and user reviews [1, 21, 23, 26, 46, 47, 49] to enhance the performance of latent factor modeling and rating prediction.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'user reviews' as a source of text information for enhancing latent factor modeling and rating prediction, but does not specify any named datasets.",
      "processing_time": 57.35552453994751,
      "citing_paper_id": "304614",
      "cited_paper_id": 11832773
    },
    {
      "context_text": "• NMF:Non-negativeMatrix Factorization [18].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Non-negative Matrix Factorization (NMF) but does not refer to a specific dataset. It is a method, not a dataset.",
      "processing_time": 56.79578614234924,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "• LRMF: Learning to Rank withMatrix Factorization [35].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset. It refers to a method (LRMF) which is a variant of matrix factorization.",
      "processing_time": 56.23299479484558,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Latent Factor Models (LFM) based onMatrix Factorization (MF) [15] play an important role for rating prediction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Matrix Factorization) used for rating prediction. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.14035630226135,
      "citing_paper_id": "304614",
      "cited_paper_id": 2095855
    },
    {
      "context_text": "Moreover, inspired by [11, 41], neural network based models can help learn more effective latent factors when conducting rating prediction and improve the performance of collaborative filtering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using neural networks for improving collaborative filtering.",
      "processing_time": 55.59384369850159,
      "citing_paper_id": "304614",
      "cited_paper_id": 4833213
    },
    {
      "context_text": "[41] employs a hierarchical Bayesian model which jointly performs deep representation learning for the specification text content and collaborative filtering for the rating matrix.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for deep representation learning and collaborative filtering.",
      "processing_time": 55.12147784233093,
      "citing_paper_id": "304614",
      "cited_paper_id": 4833213
    },
    {
      "context_text": "For user review texts, some research works, such as HFT [23], RMR [21], TriRank [10], and sCVR [26], integrate topic models in their frameworks to generate the latent factors for users and items incorporating review texts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several methods (HFT, RMR, TriRank, sCVR) but does not specify any datasets. The context focuses on integrating topic models to generate latent factors using review texts.",
      "processing_time": 58.935818910598755,
      "citing_paper_id": "304614",
      "cited_paper_id": 6440341
    },
    {
      "context_text": "[32] employ a class of two-layer Restricted Boltzmann Machines (RBM) with an efficient learning algorithm to model user interactions and perform collaborative filtering.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (RBM) for collaborative filtering.",
      "processing_time": 54.98461580276489,
      "citing_paper_id": "304614",
      "cited_paper_id": 7285098
    },
    {
      "context_text": "Multi-faceted information can be extracted from reviews and used as user preferences or item features, which otherwise cannot be obtained from the overall ratings [5].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept about extracting information from reviews. No clear, verifiable resource is identified.",
      "processing_time": 56.506715297698975,
      "citing_paper_id": "304614",
      "cited_paper_id": 7847519
    },
    {
      "context_text": "In order to model the relationship between users and items, one may consider to use a neural tensor network [36] to describe the interactions between users and items, such as uT Wv, where W ∈ Rku×d×kv .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (neural tensor network) for modeling user-item interactions.",
      "processing_time": 55.768269062042236,
      "citing_paper_id": "304614",
      "cited_paper_id": 8429835
    },
    {
      "context_text": "Recently, some research works on representation learning from different fields, such as computer vision [9, 16], natural language processing [17, 24], and knowledge base completion [36], demonstrate that non-linear transformations will enhance the representation ability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general fields of research. No dataset names are present in the text.",
      "processing_time": 56.14283037185669,
      "citing_paper_id": "304614",
      "cited_paper_id": 8429835
    },
    {
      "context_text": "Recently, some research works on representation learning from different fields, such as computer vision [9, 16], natural language processing [17, 24], and knowledge base completion [36], demonstrate that non-linear transformations will enhance the representation ability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general fields of research. No dataset names are present in the text.",
      "processing_time": 56.14283037185669,
      "citing_paper_id": "304614",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Recently, some research works on representation learning from different fields, such as computer vision [9, 16], natural language processing [17, 24], and knowledge base completion [36], demonstrate that non-linear transformations will enhance the representation ability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general fields of research. No dataset names are present in the text.",
      "processing_time": 56.14283037185669,
      "citing_paper_id": "304614",
      "cited_paper_id": 195908774
    },
    {
      "context_text": "In the field of neural summarization [19, 30], the context is the encoded document information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the general context of neural summarization.",
      "processing_time": 55.15705871582031,
      "citing_paper_id": "304614",
      "cited_paper_id": 29562039
    },
    {
      "context_text": "At the operational or testing stage, we use a beam search algorithm [13] for decoding and generating the best tips given a trained model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (beam search algorithm) used for decoding and generating text.",
      "processing_time": 55.77728486061096,
      "citing_paper_id": "304614",
      "cited_paper_id": 29758373
    },
    {
      "context_text": "The model can be trained efficiently by an end-to-end paradigm using backpropagation algorithms [29].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for training models.",
      "processing_time": 54.64533591270447,
      "citing_paper_id": "304614",
      "cited_paper_id": 205001834
    },
    {
      "context_text": "We note that reference-based metrics for generation tasks like BLEU and ROUGE are not suitable for evaluation of video comments (Das et al., 2017; Ma et al., 2019; Zhang et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the unsuitability of certain metrics for evaluating video comments.",
      "processing_time": 55.61113715171814,
      "citing_paper_id": "235097581",
      "cited_paper_id": 1820614
    },
    {
      "context_text": "Hence we follow (Das et al., 2017) and focus on the ability to rank the correct comment originally appearing at this point in the video over other comments taken from the dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'the dataset' without providing a specific name. It is unclear which dataset is being referred to, and there are no multi-word proper nouns, acronyms, or hyphenated names with digits.",
      "processing_time": 59.96097445487976,
      "citing_paper_id": "235097581",
      "cited_paper_id": 1820614
    },
    {
      "context_text": "In the encoder, the sequence of video frames is embedded by a CNN (Subhashini et al., 2014) or RNN (Nitish et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods (CNN, RNN, LSTM). The cited papers' titles also do not provide additional dataset information.",
      "processing_time": 57.63359451293945,
      "citing_paper_id": "235097581",
      "cited_paper_id": 11699847
    },
    {
      "context_text": "In the encoder, the sequence of video frames is embedded by a CNN (Subhashini et al., 2014) or RNN (Nitish et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods (CNN, RNN, LSTM). The cited papers' titles also do not provide additional dataset information.",
      "processing_time": 57.63359451293945,
      "citing_paper_id": "235097581",
      "cited_paper_id": 52316421
    },
    {
      "context_text": "For OCR, we use the open-source Tesseract (Kay, 2007) OCR engine on the lower half of the sampled video frames.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using Tesseract for OCR but does not reference any specific dataset. Tesseract is a tool, not a dataset.",
      "processing_time": 56.51042366027832,
      "citing_paper_id": "235097581",
      "cited_paper_id": 59749777
    },
    {
      "context_text": "For the audio signal, we use 20-dimensional mel-frequency cepstral coefﬁcients (MFCCs) and another 20-dimensional MFCCs derivatives as audio frame features (Di Gangi et al., 2019).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the use of MFCCs and their derivatives as features, which are not datasets themselves.",
      "processing_time": 57.63613677024841,
      "citing_paper_id": "235097581",
      "cited_paper_id": 202670644
    },
    {
      "context_text": "Following the success of the Transformer architecture in multi-modal processing (Ma et al., 2019; Chaoqun et al., 2020), we adopt a multi-unit Trans-former module to recursively learn and combine representations from all three modalities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the use of a Transformer architecture for multi-modal processing.",
      "processing_time": 55.742785692214966,
      "citing_paper_id": "235097581",
      "cited_paper_id": 211066479
    },
    {
      "context_text": "A GRU module (Chaoqun et al., 2020) is applied to recursively encode the input audio sequence.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (GRU module).",
      "processing_time": 55.212629079818726,
      "citing_paper_id": "235097581",
      "cited_paper_id": 211066479
    },
    {
      "context_text": "This work has served as a benchmark for the most recent approaches (Zhang et al., 2020; Chaoqun et al., 2020; Weiying et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.29853916168213,
      "citing_paper_id": "235097581",
      "cited_paper_id": 211066479
    },
    {
      "context_text": "This work has served as a benchmark for the most recent approaches (Zhang et al., 2020; Chaoqun et al., 2020; Weiying et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.29853916168213,
      "citing_paper_id": "235097581",
      "cited_paper_id": 222278182
    },
    {
      "context_text": "The models proposed in (Chaoqun et al., 2020; Zhang et al., 2020) are very recent and their code is not publicly available yet, so we do not consider these as one of our baseline methods.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only recent models whose code is not publicly available. No datasets are referenced or used.",
      "processing_time": 56.80473208427429,
      "citing_paper_id": "235097581",
      "cited_paper_id": 211066479
    },
    {
      "context_text": "In previous work, we re-worked the baseline implementationof LiveBot to address several shortcomings in both the original dataset and implementation (Wu et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'the original dataset' but does not provide a specific name or identifier. The reference to 'LiveBot' is likely a method or tool, not a dataset.",
      "processing_time": 59.137967348098755,
      "citing_paper_id": "235097581",
      "cited_paper_id": 219305429
    },
    {
      "context_text": "Speciﬁcally, we use the code from (Wu et al., 2020), trained on our full dataset with only video frames and surrounding comments as input.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'our full dataset' which is too generic and does not provide a specific, identifiable name. No other datasets are mentioned.",
      "processing_time": 57.2842013835907,
      "citing_paper_id": "235097581",
      "cited_paper_id": 219305429
    },
    {
      "context_text": "Part of the data (2,322 videos and 857,993 comments) comes from the publicly available automatic danmu generation Response to Livebot dataset (Wu et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Response to Livebot dataset"
      ],
      "dataset_descriptions": {
        "Response to Livebot dataset": "Used to generate live video comments based on visual and textual contexts, focusing on the interaction between video content and user-generated comments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset with a clear name and reference, which is relevant to the topic of personalized text generation.",
      "processing_time": 64.45911765098572,
      "citing_paper_id": "235097581",
      "cited_paper_id": 219305429
    },
    {
      "context_text": "It is trained on the extended dataset using the implementation provided in (Wu et al., 2020).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'extended dataset' but does not provide a specific name or identifier. The term 'extended dataset' is too generic and lacks the necessary specificity to be considered a verifiable resource.",
      "processing_time": 59.765665769577026,
      "citing_paper_id": "235097581",
      "cited_paper_id": 219305429
    },
    {
      "context_text": "The main focus of the research literature (see Section 2) has so far been on the automatic generation of danmu comments (Lv et al., 2019; Ma et al., 2019; Weiying et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research literature on danmu comment generation.",
      "processing_time": 56.107014179229736,
      "citing_paper_id": "235097581",
      "cited_paper_id": 222278182
    },
    {
      "context_text": "This has historically been explored for generating personalized reviews (Ni et al., 2017), recipes (Majumder et al., 2019), news headlines (Ao et al., 2021) and in dialogue agents (Mazaré et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No dataset names are provided in the context.",
      "processing_time": 57.42357134819031,
      "citing_paper_id": "265213422",
      "cited_paper_id": 2765857
    },
    {
      "context_text": "This has historically been explored for generating personalized reviews (Ni et al., 2017), recipes (Majumder et al., 2019), news headlines (Ao et al., 2021) and in dialogue agents (Mazaré et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No dataset names are provided in the context.",
      "processing_time": 57.42357134819031,
      "citing_paper_id": "265213422",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "This has historically been explored for generating personalized reviews (Ni et al., 2017), recipes (Majumder et al., 2019), news headlines (Ao et al., 2021) and in dialogue agents (Mazaré et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No dataset names are provided in the context.",
      "processing_time": 57.42357134819031,
      "citing_paper_id": "265213422",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "While text completion presents a common form of interaction (Clark et al., 2018), recent work has seen use of infilling, tag-based control (Sun et al., 2021), and instruction guided generations (Chakrabarty et al., 2022) – a deeper examination of control and interaction strategies and their trade…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers are not referenced for their datasets but for their methodologies.",
      "processing_time": 57.83804416656494,
      "citing_paper_id": "265213422",
      "cited_paper_id": 3817960
    },
    {
      "context_text": "…for communication applications, these have been targeted at authors of creative texts like screenplays (Mirowski et al., 2023), stories (Akoury et al., 2020), and poems (Gonçalo Oliveira, 2017) – consequently, they focus on diverse generations and long-range coherence, rather than personalization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers that discuss creative text generation. No verifiable resources are identified.",
      "processing_time": 56.84526038169861,
      "citing_paper_id": "265213422",
      "cited_paper_id": 32461868
    },
    {
      "context_text": "Early work of Chen et al. (2019) explore personalized email completion by mixing user-specific n-gram language models and a global language model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method for personalized email completion using language models.",
      "processing_time": 56.5221688747406,
      "citing_paper_id": "265213422",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "This has also been found to improve acceptance of generated text in various large-scale systems (Chen et al., 2019; Trajanovski et al., 2021).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to improving acceptance of generated text in large-scale systems.",
      "processing_time": 56.35906457901001,
      "citing_paper_id": "265213422",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "…Assistants While writing assistants have seen considerable exploration, only work that has sought to aid communication applications like email (Chen et al., 2019; Trajanovski et al., 2021), social media (Gero et al., 2022), and grammatical error correction (GEC) (Nadejde and Tetreault, 2019)…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of writing assistants. No clear identifiers for datasets are present.",
      "processing_time": 56.187872886657715,
      "citing_paper_id": "265213422",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "…have seen considerable exploration, only work that has sought to aid communication applications like email (Chen et al., 2019; Trajanovski et al., 2021), social media (Gero et al., 2022), and grammatical error correction (GEC) (Nadejde and Tetreault, 2019) have focused on author personalization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of application such as email, social media, and grammatical error correction. No verifiable resources are named.",
      "processing_time": 58.74636912345886,
      "citing_paper_id": "265213422",
      "cited_paper_id": 208166839
    },
    {
      "context_text": "…have seen considerable exploration, only work that has sought to aid communication applications like email (Chen et al., 2019; Trajanovski et al., 2021), social media (Gero et al., 2022), and grammatical error correction (GEC) (Nadejde and Tetreault, 2019) have focused on author personalization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general areas of application such as email, social media, and grammatical error correction. No verifiable resources are named.",
      "processing_time": 58.74636912345886,
      "citing_paper_id": "265213422",
      "cited_paper_id": 239009871
    },
    {
      "context_text": "Personalized Writing Assistants While writing assistants have seen considerable exploration, only work that has sought to aid communication applications like email (Chen et al., 2019; Trajanovski et al., 2021), social media (Gero et al., 2022), and grammatical error correction (GEC) (Nadejde and Tetreault, 2019) have focused on author personalization.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general areas of application for writing assistants. No clear, verifiable resources are identified.",
      "processing_time": 57.23981165885925,
      "citing_paper_id": "265213422",
      "cited_paper_id": 208166839
    },
    {
      "context_text": "Nadejde and Tetreault (2019) explore personalization in GEC models through efficient fine-tuning of expertise and native language specific models.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of personalization in GEC models. The context is about the methodology and approach rather than a specific dataset.",
      "processing_time": 58.739320278167725,
      "citing_paper_id": "265213422",
      "cited_paper_id": 208166839
    },
    {
      "context_text": "…mediated communication suggests that users interpersonal communication patterns are influenced by the tool/medium used for communication (Poddar et al., 2023) with a potential for these influences to have longer term influences on communication in the absence of these tools (Hancock et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research findings about AI-mediated communication and its effects on interpersonal communication patterns.",
      "processing_time": 56.64383888244629,
      "citing_paper_id": "265213422",
      "cited_paper_id": 210989306
    },
    {
      "context_text": "However, understanding of the implications of AI mediated communication, specially those powered by powerful LLMs, is largely developing (Hancock et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general discussion about AI-mediated communication.",
      "processing_time": 55.49281668663025,
      "citing_paper_id": "265213422",
      "cited_paper_id": 210989306
    },
    {
      "context_text": "Given our focus on personalization, we avoid rating generations using specific intrinsic aspects e.g., fluency, non-reduncancy (Celikyilmaz et al., 2021) and instead focus on how well the generated text mimics the user’s real writing sample.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general approach to evaluating text generation. No verifiable resources are named.",
      "processing_time": 56.63815259933472,
      "citing_paper_id": "265213422",
      "cited_paper_id": 220128348
    },
    {
      "context_text": "…found in text simplification and lay summarization in the context of scientific text – this work has explored generating definitions for scientific concepts at varying levels of complexity (August et al., 2022; Murthy et al., 2022) or summarizing scientific text for lay readers (Guo et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.4542441368103,
      "citing_paper_id": "265213422",
      "cited_paper_id": 229363446
    },
    {
      "context_text": "…found in text simplification and lay summarization in the context of scientific text – this work has explored generating definitions for scientific concepts at varying levels of complexity (August et al., 2022; Murthy et al., 2022) or summarizing scientific text for lay readers (Guo et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general research areas and methods. The cited papers' titles do not provide additional dataset names.",
      "processing_time": 57.4542441368103,
      "citing_paper_id": "265213422",
      "cited_paper_id": 248811750
    },
    {
      "context_text": "We introduce two key novelties in training the retriever, both of which are aimed at optimizing f retr directly for the downstream text generation task: 1) Training data creation based on a novel difference of likelihoods from an auxiliary text generation model; and 2) A scale-calibrating training…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches for training a retriever model. There are no clear identifiers for datasets in the given context.",
      "processing_time": 58.15575957298279,
      "citing_paper_id": "265213422",
      "cited_paper_id": 232066861
    },
    {
      "context_text": "In Table 3, we see that this underperforms approaches optimized for target generation – indicating the value of retrievers optimized for generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison of approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.58076000213623,
      "citing_paper_id": "265213422",
      "cited_paper_id": 233296016
    },
    {
      "context_text": "In Table 3, we see that this underperforms approaches optimized for target generation – indicating the value of retrievers optimized for generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a comparison of approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.58076000213623,
      "citing_paper_id": "265213422",
      "cited_paper_id": 249097975
    },
    {
      "context_text": "Cohen et al. (2021) estimate model confidence in retriever predicted rele-vances with monte-carlo dropout.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for estimating model confidence using monte-carlo dropout.",
      "processing_time": 56.13483452796936,
      "citing_paper_id": "265213422",
      "cited_paper_id": 234357857
    },
    {
      "context_text": "Writing assistants have progressed from providing simple syntactic checks, to revising human authored text, to recent assistants being able to fully compose texts on direction from authors (Mahlow, 2023; Dale and Viethen, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes the progression of writing assistants.",
      "processing_time": 56.534679889678955,
      "citing_paper_id": "265213422",
      "cited_paper_id": 235770552
    },
    {
      "context_text": "Writing assistants have progressed from providing simple syntactic checks, to revising human authored text, to recent assistants being able to fully compose texts on direction from authors (Mahlow, 2023; Dale and Viethen, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only describes the progression of writing assistants.",
      "processing_time": 56.534679889678955,
      "citing_paper_id": "265213422",
      "cited_paper_id": 257900891
    },
    {
      "context_text": "This approach, also, closely resembles prior work (Rubin et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to prior work. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 57.56972551345825,
      "citing_paper_id": "265213422",
      "cited_paper_id": 245218561
    },
    {
      "context_text": "This approach extends recent work (Rubin et al., 2022; Gonen et al., 2022) indicating the best prompts to be the ones with the highest conditional likelihood of generating a target output to the novel setting of personalized text generation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to recent work on prompts and conditional likelihood in the context of personalized text generation.",
      "processing_time": 56.86932706832886,
      "citing_paper_id": "265213422",
      "cited_paper_id": 245218561
    },
    {
      "context_text": "Finally, in constructing training data for instance selection models for an LLM, prior work has noted the best empirical performance from matching f aux and f LLM (Rubin et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for constructing training data. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.79871344566345,
      "citing_paper_id": "265213422",
      "cited_paper_id": 245218561
    },
    {
      "context_text": "In contrast with prior work (Dhuliawala et al., 2022; Yan et al., 2022) by top-1 document score, s 1 , and then measure pearson r between the bin start and the average evaluation metric per bin.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating machine reading systems. No verifiable resources are identified.",
      "processing_time": 56.51235055923462,
      "citing_paper_id": "265213422",
      "cited_paper_id": 247594202
    },
    {
      "context_text": "QL-FT5: An approach which ranks documents based on p ( q u | d u ) with a pretrained F LAN T5-B ASE with 250M parameters (Sachan et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (FLAN T5-BASE) and a parameter count. The context is about a model and its application, not a dataset.",
      "processing_time": 59.5998170375824,
      "citing_paper_id": "265213422",
      "cited_paper_id": 248218489
    },
    {
      "context_text": "Creswell et al. (2023) select examples based on the target LLM likelihood - necessitating access to LLM like-lihoods and incurring latency in retrieval.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving LLM likelihoods.",
      "processing_time": 55.18341302871704,
      "citing_paper_id": "265213422",
      "cited_paper_id": 248887351
    },
    {
      "context_text": "1 to select the best candidate documents for a request, i.e. to identify positive training examples.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a general process of selecting candidate documents for training.",
      "processing_time": 56.61506748199463,
      "citing_paper_id": "265213422",
      "cited_paper_id": 250340449
    },
    {
      "context_text": "The poorer calibration of crossen-coders trained without scale calibration also finds support in prior work showing crossencoder scores to lie at extremes of the score distribution (Menon et al., 2022; Yadav et al., 2022).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only findings about cross-encoder models and their calibration issues.",
      "processing_time": 55.87340521812439,
      "citing_paper_id": "265213422",
      "cited_paper_id": 253098237
    },
    {
      "context_text": "We construct a personalized comment generation task from the dataset of Plepi et al. (2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'personalized comment generation task' using a dataset from Plepi et al. (2022). However, the dataset name is not explicitly provided, only a reference to the authors and year.",
      "processing_time": 60.37611198425293,
      "citing_paper_id": "265213422",
      "cited_paper_id": 253116635
    },
    {
      "context_text": "Finally, we assume the presence of a large language model f LLM available via a prompt-based text generation API.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a large language model available via an API, which is not a dataset.",
      "processing_time": 57.76047325134277,
      "citing_paper_id": "265213422",
      "cited_paper_id": 253420743
    },
    {
      "context_text": "This has led current research to explore a new frontier of LLM-based writing assistants for complex and important applications such as knowledge synthesis (Shen et al., 2023), peer review (Chen et al., 2023), and jour-nalism (Wang et al., 2023c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based writing assistants. No verifiable resources are identified.",
      "processing_time": 57.142266511917114,
      "citing_paper_id": "265213422",
      "cited_paper_id": 257952162
    },
    {
      "context_text": "This has led current research to explore a new frontier of LLM-based writing assistants for complex and important applications such as knowledge synthesis (Shen et al., 2023), peer review (Chen et al., 2023), and jour-nalism (Wang et al., 2023c).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of LLM-based writing assistants. No verifiable resources are identified.",
      "processing_time": 57.142266511917114,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "Designs of LLM-based sys-a tems for various tasks have explored either tuning the input prompt (Salemi et al., 2023; Li et al., 2023a) or iteratively modifying output generations through chaining LLM calls (Wu et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches for using LLMs in personalized systems.",
      "processing_time": 56.12296676635742,
      "citing_paper_id": "265213422",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "We also observe that personalization through retrieval generally improves upon non-personalized approaches (NP), which is consistent with prior work (Salemi et al., 2023).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about personalization improving over non-personalized approaches.",
      "processing_time": 56.2462432384491,
      "citing_paper_id": "265213422",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "Gupta et al. (2023) explore selecting sets of examples with dense retrieval models, presenting a complementary approach to ours.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about example selection using dense retrieval models, which is a method rather than a dataset.",
      "processing_time": 58.771321535110474,
      "citing_paper_id": "265213422",
      "cited_paper_id": 258865631
    },
    {
      "context_text": "With current LLMs prone to generating overly generic text (Pu and Demberg, 2023), author personalization of LLM generations presents an important problem for LLM writing assistants.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general problem with LLMs. No verifiable resources are identified.",
      "processing_time": 56.719693183898926,
      "citing_paper_id": "265213422",
      "cited_paper_id": 259145296
    },
    {
      "context_text": "Close work is presented by approaches that leverage repeated LLM calls to verify the reasoning or factuality of previous generations (Shridhar et al., 2023; Dhuliawala et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other research works.",
      "processing_time": 54.94822359085083,
      "citing_paper_id": "265213422",
      "cited_paper_id": 262466051
    },
    {
      "context_text": "In this respect, our approach bears similarity to Zhang et al. (2023b), who progressively run larger LLMs only when necessary for an input.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach similar to another study.",
      "processing_time": 55.47019672393799,
      "citing_paper_id": "265213422",
      "cited_paper_id": 263671677
    },
    {
      "context_text": "Prompt 5: f aux prompt used to compute p aux ( t u | q u ) in Eq (1) for AITA. annotators external to the authors weren’t recruited for authoring requests due to the private nature of the W ORK S M dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "W ORK S M"
      ],
      "dataset_descriptions": {
        "W ORK S M": "Used to compute auxiliary probabilities in Eq (1) for AITA, focusing on personalizing large language models through retrieval augmentation. The dataset contains private user data."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'W ORK S M dataset' which appears to be a specific dataset used in the research. However, the name is not clearly defined and seems to be a placeholder or internal reference.",
      "processing_time": 68.05111575126648,
      "citing_paper_id": "265213422",
      "cited_paper_id": 269009728
    },
    {
      "context_text": "Given D u , q u , and f LLM , the task is to select a subset of historical documents D ′ u ⊂ D u to provide as few-shot examples to the LLM, which then generates a target text t u of up to 300 words: where ϕ is a prompt construction function that inputs the user’s request and retrieved historical…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for personalizing large language models using retrieval augmentation but does not reference any named datasets.",
      "processing_time": 58.207762479782104,
      "citing_paper_id": "265213422",
      "cited_paper_id": 269009728
    },
    {
      "context_text": "…D ′ u ⊂ D u to provide as few-shot examples to the LLM, which then generates a target text t u of up to 300 words: where ϕ is a prompt construction function that inputs the user’s request and retrieved historical documents, and t u reflects the style, preferences, and knowledge of the user u .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for personalizing large language models using retrieval augmentation but does not reference any named datasets.",
      "processing_time": 58.20225167274475,
      "citing_paper_id": "265213422",
      "cited_paper_id": 269009728
    },
    {
      "context_text": "…assistants for communication applications, these have been targeted at authors of creative texts like screenplays (Mirowski et al., 2023), stories (Akoury et al., 2020), and poems (Gonçalo Oliveira, 2017) – consequently, they focus on diverse generations and long-range coherence, rather than…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers that focus on creative text generation. No verifiable resources are named.",
      "processing_time": 57.240323305130005,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "Evaluating text generations under various personalization setups represents a largely emerging body of work (Wang et al., 2023a,d).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to a body of work.",
      "processing_time": 55.552345991134644,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "An important element of effective writing assistants is being able personalize generated text to retain the knowledge, and communicative style of a user – an essential element of interpersonal communication (Pickering and Garrod, 2013).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a general concept in interpersonal communication.",
      "processing_time": 56.61681032180786,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "Contemporaneous work has also explored personalized writing with LLMs. Li et al. (2023b) construct prompts with pre-trained retrieval and summarization models followed by fine-tuning an LLM for personalized completion.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The focus is on the construction of prompts using pre-trained models and fine-tuning an LLM.",
      "processing_time": 58.43009567260742,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, we report text generation evaluation metrics, which, in addition to known biases (He et al., 2023), were not developed for personalized text generation tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to known biases. No verifiable resources are identified.",
      "processing_time": 56.61086177825928,
      "citing_paper_id": "265213422",
      "cited_paper_id": null
    },
    {
      "context_text": "2019), conversing to a dialogue agent is still in its infancy, and one major issue for these datadriven models is the lack of a consistent persona (Vinyals and Le 2015; Li et al. 2016a; Zhang et al. 2018; Song et al. 2019a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers discussing issues in dialogue agents and persona consistency.",
      "processing_time": 56.61310029029846,
      "citing_paper_id": "208006638",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "To evaluate our model, we compared the proposed approach with the following strong models: • S2SA Seq2Seq is a generative dialogue model with the context attention mechanism (Shang, Lu, and Li 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context is about comparing the proposed model with other models.",
      "processing_time": 56.61022186279297,
      "citing_paper_id": "208006638",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Despite the recent success of dialogue generation in open-domain by training from large volumes of human-to-human interaction data (Shang, Lu, and Li 2015; Serban et al. 2016; Li et al. 2017; Zhu et al. 2019), conversing to a dialogue agent is still in its infancy, and one major issue for these…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'large volumes of human-to-human interaction data' but does not specify any named datasets. The reference is too generic and lacks specific identifiers.",
      "processing_time": 58.20583176612854,
      "citing_paper_id": "208006638",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "• Transformer Transformer is one of the state-of-the-art sequence transduction models (Vaswani et al. 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Transformer).",
      "processing_time": 54.74745988845825,
      "citing_paper_id": "208006638",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "2018), paraphrase generation (Li et al. 2018), anaphora resolution (Yin et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and methods. No verifiable resources are identified.",
      "processing_time": 56.21790528297424,
      "citing_paper_id": "208006638",
      "cited_paper_id": 21646317
    },
    {
      "context_text": "Reinforcement Learning In recent years, deep reinforcement learning has been widely applied in natural language processing, such as machine translation (Wu et al. 2018), visual question generation (Fan et al. 2018), paraphrase generation (Li et al. 2018), anaphora resolution (Yin et al. 2018) etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only applications of deep reinforcement learning in NLP tasks. The cited paper title does not introduce a specific dataset either.",
      "processing_time": 58.02465796470642,
      "citing_paper_id": "208006638",
      "cited_paper_id": 21646317
    },
    {
      "context_text": "2015), deep neural network methods have made promising progress (Chen et al. 2017; Gong, Luo, and Zhang 2018; Kim et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural network methods and their progress. No verifiable resources are identified.",
      "processing_time": 57.21292161941528,
      "citing_paper_id": "208006638",
      "cited_paper_id": 34032948
    },
    {
      "context_text": "2015), deep neural network methods have made promising progress (Chen et al. 2017; Gong, Luo, and Zhang 2018; Kim et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to deep neural network methods and their progress. No verifiable resources are identified.",
      "processing_time": 57.21292161941528,
      "citing_paper_id": "208006638",
      "cited_paper_id": 44132329
    },
    {
      "context_text": "To evaluate the action-value Q at an intermediate state s t (t < T), a common strategy is to apply rollout policy, such as Monte Carlo search, to sample the last T − t words for the partially decoded response ˆ Y 1: t (Yu et al. 2017; Li et al. 2017; Fan et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and strategies for evaluating action-values in reinforcement learning.",
      "processing_time": 55.53666877746582,
      "citing_paper_id": "208006638",
      "cited_paper_id": 52012661
    },
    {
      "context_text": "…in open-domain by training from large volumes of human-to-human interaction data (Shang, Lu, and Li 2015; Serban et al. 2016; Li et al. 2017; Zhu et al. 2019), conversing to a dialogue agent is still in its infancy, and one major issue for these data-driven models is the lack of a…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'human-to-human interaction data' but does not specify any named datasets. The reference is too generic and lacks specific identifiers.",
      "processing_time": 57.586766958236694,
      "citing_paper_id": "208006638",
      "cited_paper_id": 56421016
    },
    {
      "context_text": "Dziri et al. (2019) has shown that entailment techniques can be used as a surrogate for human judgment in evaluating dialogue consistency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique for evaluating dialogue consistency.",
      "processing_time": 55.52035617828369,
      "citing_paper_id": "208006638",
      "cited_paper_id": 102351258
    },
    {
      "context_text": "The use of empathy to recognise and express emotions to efficiently convey the affective tone of information could allow computers to influence the mood of their users (Picard, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to computing and empathy.",
      "processing_time": 55.522743225097656,
      "citing_paper_id": "947719",
      "cited_paper_id": 1327952
    },
    {
      "context_text": "In this paper, we discuss our effort to do this in the context of the BabyTalk (Gatt et al., 2009) project.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "BabyTalk"
      ],
      "dataset_descriptions": {
        "BabyTalk": "Used to generate personalized text in the Neonatal Intensive Care Unit, focusing on decision support and information management using NLG technology."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'BabyTalk' as a project, which is likely a dataset or corpus used in the research. The cited paper title supports this by indicating the use of NLG technology in a specific domain.",
      "processing_time": 66.30295443534851,
      "citing_paper_id": "947719",
      "cited_paper_id": 3939596
    },
    {
      "context_text": "The goal of the BabyTalk project (Gatt et al., 2009) is to develop software that generates English summaries of medical data about babies in a NICU.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the BabyTalk project, which is a software development effort rather than a dataset. No specific dataset is named or described.",
      "processing_time": 56.62154841423035,
      "citing_paper_id": "947719",
      "cited_paper_id": 3939596
    },
    {
      "context_text": "Key events are specifically identified by the BT-Nurse document planner whose importance exceeds a preset threshold and these events are placed at the head of a paragraph, with each paragraph being ordered by the time of occurrence of the key events (Gatt et al., 2009).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool (BT-Nurse document planner). The context focuses on the use of this planner for organizing key events in text generation.",
      "processing_time": 59.23035407066345,
      "citing_paper_id": "947719",
      "cited_paper_id": 3939596
    },
    {
      "context_text": "NLG systems have been increasingly used for the creation of e-Health systems (Hüske-Kraus, 2003), such as generating information for smoking cessation patients (Reiter et al., 2000), breast surgery patients (DiMarco et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only applications of NLG systems in e-Health contexts.",
      "processing_time": 56.183013916015625,
      "citing_paper_id": "947719",
      "cited_paper_id": 15039473
    },
    {
      "context_text": "NLG systems have been increasingly used for the creation of e-Health systems (Hüske-Kraus, 2003), such as generating information for smoking cessation patients (Reiter et al., 2000), breast surgery patients (DiMarco et al., 2007), and so forth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only applications of NLG systems in e-Health. No verifiable resources are named.",
      "processing_time": 57.32505178451538,
      "citing_paper_id": "947719",
      "cited_paper_id": 15039473
    },
    {
      "context_text": "NLG systems have been increasingly used for the creation of e-Health systems (Hüske-Kraus, 2003), such as generating information for smoking cessation patients (Reiter et al., 2000), breast surgery patients (DiMarco et al., 2007), and so forth.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only applications of NLG systems in e-Health. No verifiable resources are named.",
      "processing_time": 57.32505178451538,
      "citing_paper_id": "947719",
      "cited_paper_id": 15620623
    },
    {
      "context_text": ", 2000), breast surgery patients (DiMarco et al., 2007), and so forth.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only lists types of patients, which are not datasets.",
      "processing_time": 56.61750030517578,
      "citing_paper_id": "947719",
      "cited_paper_id": 15620623
    },
    {
      "context_text": "One prime example is the HealthDoc project that aimed to customise patient information at an individual level based upon their medical condition, demographic, personality profile, and other relevant factors (DiMarco et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the HealthDoc project but does not specify a dataset. It describes a project goal rather than a reusable dataset.",
      "processing_time": 56.770442724227905,
      "citing_paper_id": "947719",
      "cited_paper_id": 15620623
    },
    {
      "context_text": "based upon their medical condition, demographic, personality profile, and other relevant factors (DiMarco et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general factors used in a personalized e-Health information system.",
      "processing_time": 56.439244747161865,
      "citing_paper_id": "947719",
      "cited_paper_id": 15620623
    },
    {
      "context_text": "NLG systems have been increasingly used for the creation of e-Health systems (Hüske-Kraus, 2003), such as generating information for smoking cessation patients (Reiter et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general usage of NLG systems in e-Health. No clear, verifiable resource names are provided.",
      "processing_time": 57.91963481903076,
      "citing_paper_id": "947719",
      "cited_paper_id": 19803465
    },
    {
      "context_text": "The use of empathy to recognise and express emotions to efficiently convey the affective tone of information could allow computers to influence the\nmood of their users (Picard, 1997).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concept related to emotional computing.",
      "processing_time": 55.366936683654785,
      "citing_paper_id": "947719",
      "cited_paper_id": 23883488
    },
    {
      "context_text": "The level of distress experienced by parents can be significant especially if their child is critically ill (Shields-Poë and Pinelli, 1997).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to parental stress in neonatal intensive care units.",
      "processing_time": 55.80007719993591,
      "citing_paper_id": "947719",
      "cited_paper_id": 27632464
    },
    {
      "context_text": "As one of the main factors of parental stress is the physiological health of the child (Shields-Poë and Pinelli, 1997; Seideman et al., 1997), most of the elements in the PNSS model focus on this particular aspect.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of parental stress in neonatal intensive care units.",
      "processing_time": 55.56548571586609,
      "citing_paper_id": "947719",
      "cited_paper_id": 27632464
    },
    {
      "context_text": "Parents can find the experience of having their child looked after in a technological environment considerably distressing and oppressive (Jämsä and Jämsä, 1998).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a study on parents' experiences, which is not a verifiable resource according to the criteria.",
      "processing_time": 58.40944242477417,
      "citing_paper_id": "947719",
      "cited_paper_id": 34624576
    },
    {
      "context_text": "Such personalisation compares favourably when compared to traditional patient literature which is often limited in its effectiveness by having to address a wide audience (DiMarco et al., 1995).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to traditional patient literature. No verifiable resources are identified.",
      "processing_time": 56.98171138763428,
      "citing_paper_id": "947719",
      "cited_paper_id": 67842456
    },
    {
      "context_text": "These elements were derived from a partial subset of the Parental Stress Scale (PSS): NICU (Miles et al., 1991) and the Neonatal Unit Parental Stress (Reid et al., 2007) questionnaire instruments.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions two questionnaire instruments, PSS: NICU and NUPS, which are specific tools used to measure parental stress in neonatal units. These are not datasets but rather scales or questionnaires.",
      "processing_time": 59.46161699295044,
      "citing_paper_id": "947719",
      "cited_paper_id": 143671044
    },
    {
      "context_text": ", 1991) and the Neonatal Unit Parental Stress (Reid et al., 2007) questionnaire instruments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Neonatal Unit Parental Stress (Reid et al., 2007)' which is a questionnaire instrument, not a dataset. It does not meet the criteria for a verifiable dataset.",
      "processing_time": 60.18987417221069,
      "citing_paper_id": "947719",
      "cited_paper_id": 143671044
    },
    {
      "context_text": "peated stress measurements to obtain an accurate assessment of parental stress (Reid et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a scale (NUPS) which is a method or tool rather than a dataset.",
      "processing_time": 58.11029863357544,
      "citing_paper_id": "947719",
      "cited_paper_id": 143671044
    },
    {
      "context_text": "Since neonatal care is a dynamic environment, the sources and levels of stress for parents can change over time and therefore it is important to obtain repeated stress measurements to obtain an accurate assessment of parental stress (Reid et al., 2007).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a scale (NUPS) which is a method or tool rather than a dataset. The context focuses on the importance of repeated stress measurements but does not specify a dataset.",
      "processing_time": 59.98961329460144,
      "citing_paper_id": "947719",
      "cited_paper_id": 143671044
    },
    {
      "context_text": "Overall gains for both the settings are discussed in Table 18 in Appendix D. Personalized Email Completion: For this task, both settings show improvements, with ROUGE-L showing the highest gains.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and results. The context is focused on reporting performance gains using ROUGE-L, which is a metric, not a dataset.",
      "processing_time": 58.9161262512207,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In the user setting, the best performance is achieved by retrieving 2 profiles, resulting in a 101.8% increase in ROUGE-L scores, while the temporal setting sees an 86.2% gain.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics and settings. No verifiable resources are identified.",
      "processing_time": 56.38753890991211,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In the temporal setting, personalized outcomes improve across all metrics, with ROUGE-L increasing by 4.02%.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (ROUGE-L) which is excluded according to the instructions.",
      "processing_time": 56.99056053161621,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Contriever shows slight advantages in ROUGE-1 and ROUGE-L, while both models perform similarly in METEOR.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics. No verifiable resources are identified.",
      "processing_time": 56.144299268722534,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Overall we see an average improvement of 30.21% with ROUGE-1 metric and 47.5 % with ROUGE-L across all tasks.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only improvements in ROUGE metrics. No verifiable resources are identified.",
      "processing_time": 56.81922173500061,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In the user setting Contriever performs slightly better in ROUGE-1 and ROUGE-L, and BM25 performs better in METEOR.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No verifiable resources are identified.",
      "processing_time": 55.948081731796265,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Evaluation: Following previous works, we use ROUGE-1 and ROUGE-L (Lin, 2004) as task evaluation metrics.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ROUGE-1 and ROUGE-L as evaluation metrics, which are not datasets but rather evaluation tools. No datasets are explicitly mentioned.",
      "processing_time": 57.54311013221741,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Personalized Abstract Generation: The user setting shows better results in ROUGE-1 and ME-TEOR, but a slight decrease in ROUGE-L.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No verifiable resources are identified.",
      "processing_time": 56.16719460487366,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "5 exhibits a slight deterioration in the ROUGE-L score, however, shows performance improvements for ROUGE-1 and ME-TEOR scores.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics. No verifiable resources are identified.",
      "processing_time": 55.77231979370117,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Contriever generally excels in ROUGE-L, and BM25 performs better in ROUGE-1, with both models showing similar METEOR scores.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only performance metrics of models. No verifiable resources are identified.",
      "processing_time": 56.443411350250244,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Except for a very small decline in the ROUGE-L score, performance improvements can be seen across all other metrics while using the GPT-3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (GPT-3) and a metric (ROUGE-L). No verifiable resources are identified.",
      "processing_time": 57.78836536407471,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "5 and LLaMA2 models with the exception of a slight decline in ROUGE-L score while using the LLaMA2 as displayed in Table 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and evaluation metrics. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.80255150794983,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In the user setting, METEOR shows the highest gain of 6.89%, with Contriever performing slightly better in ROUGE-1 and ROUGE-L, and equally in METEOR.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and model performance comparisons.",
      "processing_time": 55.591795444488525,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Retrieving one profile yields the best results, with BM25 excelling in ROUGE-1 and METEOR, while Contriever leads in ROUGE-L.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and models. The context focuses on comparing performance metrics.",
      "processing_time": 56.67319989204407,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "5 and LLaMA2 are utilized, and the evaluation metrics used are ROUGE-1, ROUGE-L, and METEOR.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and evaluation metrics. No verifiable resources are identified.",
      "processing_time": 56.55269193649292,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "In the temporal setting, METEOR achieves the highest gain of 9.89%, with both models performing equally in METEOR, Con-triever showing slightly better results in ROUGE-L, and BM25 outperforming in ROUGE-1.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and models. The context focuses on performance comparisons.",
      "processing_time": 56.36030864715576,
      "citing_paper_id": "271218187",
      "cited_paper_id": 964287
    },
    {
      "context_text": "Data Curation: To generate the data samples, we leverage the Citation Network Dataset (V14) (Tang et al., 2008a), which comprises 5,259,858 papers and 29 features per paper.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Citation Network Dataset (V14)"
      ],
      "dataset_descriptions": {
        "Citation Network Dataset (V14)": "Used to generate data samples for personalized text generation, leveraging 5,259,858 papers and 29 features per paper to create a comprehensive dataset."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Citation Network Dataset (V14) as a specific dataset used for generating data samples, which is relevant to the research topic of personalized text generation.",
      "processing_time": 68.97806406021118,
      "citing_paper_id": "271218187",
      "cited_paper_id": 3348552
    },
    {
      "context_text": "To generate the data samples, we leverage the Citation Network Dataset (V14) (Tang et al., 2008a).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Citation Network Dataset (V14)"
      ],
      "dataset_descriptions": {
        "Citation Network Dataset (V14)": "Used to generate data samples for personalized text generation, focusing on the structure and content of academic citations and networks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Citation Network Dataset (V14)', which is a specific dataset used for generating data samples. The dataset is clearly identified and relevant to the research context.",
      "processing_time": 67.15418291091919,
      "citing_paper_id": "271218187",
      "cited_paper_id": 3348552
    },
    {
      "context_text": "It has been explored across various areas such as product review generation (Li and Tuzhilin, 2019; Li et al., 2020), dialog agents (Zhang et al., 2018; Mazaré et al., 2018; Tang et al., 2008b), sentiment analysis (El-Ansari and Beni-Hssane, 2023; Mireshghallah et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and applications. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.5700159072876,
      "citing_paper_id": "271218187",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "It has been explored across various areas such as product review generation (Li and Tuzhilin, 2019; Li et al., 2020), dialog agents (Zhang et al., 2018; Mazaré et al., 2018; Tang et al., 2008b), sentiment analysis (El-Ansari and Beni-Hssane, 2023; Mireshghallah et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and applications. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.5700159072876,
      "citing_paper_id": "271218187",
      "cited_paper_id": 238252929
    },
    {
      "context_text": "Fine-tuning language models on user data raises privacy risks, including data exposure through memorization of training data (Carlini et al., 2019) and inference attacks exploiting model outputs to reveal sensitive user information (Shokri et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses privacy risks in fine-tuning language models, mentioning memorization and inference attacks. No specific datasets are named, only general concepts.",
      "processing_time": 57.408812284469604,
      "citing_paper_id": "271218187",
      "cited_paper_id": 10488675
    },
    {
      "context_text": "Fine-tuning language models on user data raises privacy risks, including data exposure through memorization of training data (Carlini et al., 2019) and inference attacks exploiting model outputs to reveal sensitive user information (Shokri et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context discusses privacy risks in fine-tuning language models, mentioning memorization and inference attacks. No specific datasets are named, only general concepts.",
      "processing_time": 57.408812284469604,
      "citing_paper_id": "271218187",
      "cited_paper_id": 170076423
    },
    {
      "context_text": "We employ nucleus sampling with temperature 0.8 (Holtzman et al., 2019) as the decoding technique.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a decoding technique but does not reference any specific dataset. The focus is on the method rather than a reusable resource.",
      "processing_time": 56.82008671760559,
      "citing_paper_id": "271218187",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "To generate the data samples, we leverage the Amazon Reviews Dataset (Ni et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Amazon Reviews Dataset"
      ],
      "dataset_descriptions": {
        "Amazon Reviews Dataset": "Used to generate data samples for personalized text generation, focusing on leveraging user reviews to enhance recommendation justifications."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Amazon Reviews Dataset' which is a specific, verifiable dataset used for generating data samples.",
      "processing_time": 61.95812106132507,
      "citing_paper_id": "271218187",
      "cited_paper_id": 202621357
    },
    {
      "context_text": "Data Curation: To generate data samples, we leverage the Amazon Reviews Dataset (Ni et al., 2019), which comprises 150 million reviews { 2 \"name\" : \"Roberto Battiti\", 3 \"input\" : \"Generate an abstract for the title \\\"Reactive and dynamic local search for max-clique: Engineering effective building blocks\\\" using the following items: 1.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Amazon Reviews Dataset"
      ],
      "dataset_descriptions": {
        "Amazon Reviews Dataset": "Used to generate data samples for personalized text generation, leveraging 150 million reviews to train models on user preferences and review content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Amazon Reviews Dataset' as a specific, verifiable dataset used for generating data samples. The dataset is clearly identified and referenced in the context.",
      "processing_time": 64.56523966789246,
      "citing_paper_id": "271218187",
      "cited_paper_id": 202621357
    },
    {
      "context_text": "Data Curation: To generate data samples, we leverage the Amazon Reviews Dataset (Ni et al., 2019), which comprises 150 million reviews { 2 \"name\" : \"Roberto Battiti\", 3 \"input\" : \"Generate an abstract for the title \\\"Reactive and dynamic local search for max-clique: Engineering effective building…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Amazon Reviews Dataset"
      ],
      "dataset_descriptions": {
        "Amazon Reviews Dataset": "Used to generate data samples for personalized text generation, leveraging 150 million reviews to train models on user preferences and review content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the 'Amazon Reviews Dataset' as a source of data samples for generating personalized text. This dataset is specific and relevant to the topic of personalized text generation.",
      "processing_time": 65.41956782341003,
      "citing_paper_id": "271218187",
      "cited_paper_id": 202621357
    },
    {
      "context_text": "Email completion can greatly benefit from personalization (Trajanovski et al., 2021) as email tone varies significantly based on the recipient.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions personalization in email completion but does not specify any datasets. The context is too generic and lacks specific dataset names.",
      "processing_time": 56.89581608772278,
      "citing_paper_id": "271218187",
      "cited_paper_id": 233473617
    },
    {
      "context_text": "Email completion is a task that can significantly benefit from personalization (Trajanovski et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task that could benefit from personalization. No verifiable resources are named.",
      "processing_time": 56.489723682403564,
      "citing_paper_id": "271218187",
      "cited_paper_id": 233473617
    },
    {
      "context_text": "For fine-tuning experiments, we employed FLAN-T5 base (Longpre et al., 2023).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions FLAN-T5 base, which is a model, not a dataset. No datasets are explicitly mentioned in the citation context.",
      "processing_time": 57.35529088973999,
      "citing_paper_id": "271218187",
      "cited_paper_id": 256415991
    },
    {
      "context_text": "5 (size unknown) (Achiam et al., 2023) and LLaMA-2 3 (7B parameters) (Touvron et al., 2023) as LLMs.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions LLaMA-2 but does not indicate it is a dataset. It is referenced as a language model, which is excluded according to the instructions.",
      "processing_time": 57.93641996383667,
      "citing_paper_id": "271218187",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "The remaining emails were then organized by sender’s email address, selecting only those with a sending frequency of 10 to 200 emails, aligning with established methodologies (Salemi et al., 2023).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only describes a method for organizing emails by sender's frequency, which is not a named dataset.",
      "processing_time": 57.71598744392395,
      "citing_paper_id": "271218187",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "Personalizing the text generated from Large Language Models (LLMs) has recently attracted significant attention (Salemi et al., 2023; Richardson et al., 2023a,b; Mysore et al., 2023; Alhafni et al., 2024a; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent research on personalizing text generation from LLMs.",
      "processing_time": 56.88708972930908,
      "citing_paper_id": "271218187",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "Personalizing the text generated from Large Language Models (LLMs) has recently attracted significant attention (Salemi et al., 2023; Richardson et al., 2023a,b; Mysore et al., 2023; Alhafni et al., 2024a; Li et al., 2022).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to recent research on personalizing text generation from LLMs.",
      "processing_time": 56.88708972930908,
      "citing_paper_id": "271218187",
      "cited_paper_id": 267523283
    },
    {
      "context_text": "Salemi et al. (2023) introduce a benchmark for evaluating personalized LLMs using the RAG approach.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a benchmark for evaluating personalized LLMs, but does not specify a dataset name. The term 'benchmark' is excluded as it is primarily used for score comparison.",
      "processing_time": 58.80962514877319,
      "citing_paper_id": "271218187",
      "cited_paper_id": 258298303
    },
    {
      "context_text": "RAG architectures are increasingly adopted for personalized agents (Wang et al., 2024; Quidwai and Lagana, 2024), due to their ability to retrieve relevant passages to augment prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of RAG architectures for personalized agents. No verifiable resources are identified.",
      "processing_time": 56.97408676147461,
      "citing_paper_id": "271218187",
      "cited_paper_id": 267200117
    },
    {
      "context_text": "RAG architectures are increasingly adopted for personalized agents (Wang et al., 2024; Quidwai and Lagana, 2024), due to their ability to retrieve relevant passages to augment prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of RAG architectures for personalized agents. No verifiable resources are identified.",
      "processing_time": 56.97408676147461,
      "citing_paper_id": "271218187",
      "cited_paper_id": 268510706
    },
    {
      "context_text": "For example, popular chit-chat datasets such as Open-Subtitles (Vinyals & Le, 2015), Persona-Chat (Zhang et al., 2018) and Twitter (Sordoni et al., 2015) have tested the ability of sequence-to-sequence models that attend over the recent dialogue history, but do not attempt to recall long-term…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-Subtitles",
        "Persona-Chat",
        "Twitter"
      ],
      "dataset_descriptions": {
        "Open-Subtitles": "Used to test sequence-to-sequence models' ability to generate conversational responses, focusing on context sensitivity and recent dialogue history.",
        "Persona-Chat": "Used to evaluate models' performance in generating contextually relevant responses, emphasizing the importance of persona information in dialogues.",
        "Twitter": "Used to assess the effectiveness of sequence-to-sequence models in generating conversational responses, particularly in capturing recent dialogue history."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for testing sequence-to-sequence models in dialogue systems, which are directly relevant to personalized text generation.",
      "processing_time": 75.62583136558533,
      "citing_paper_id": "53218829",
      "cited_paper_id": 94285
    },
    {
      "context_text": "For example, popular chit-chat datasets such as Open-Subtitles (Vinyals & Le, 2015), Persona-Chat (Zhang et al., 2018) and Twitter (Sordoni et al., 2015) have tested the ability of sequence-to-sequence models that attend over the recent dialogue history, but do not attempt to recall long-term…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-Subtitles",
        "Persona-Chat",
        "Twitter"
      ],
      "dataset_descriptions": {
        "Open-Subtitles": "Used to test sequence-to-sequence models' ability to generate conversational responses, focusing on context sensitivity and recent dialogue history.",
        "Persona-Chat": "Used to evaluate models' performance in generating contextually relevant responses, emphasizing the importance of persona information in dialogues.",
        "Twitter": "Used to assess the effectiveness of sequence-to-sequence models in generating conversational responses, particularly in capturing recent dialogue history."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used for testing sequence-to-sequence models in dialogue systems, which are directly relevant to personalized text generation.",
      "processing_time": 75.62583136558533,
      "citing_paper_id": "53218829",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "For example, popular chit-chat datasets such as Open-Subtitles (Vinyals & Le, 2015), Persona-Chat (Zhang et al., 2018) and Twitter (Sordoni et al., 2015) have tested the ability of sequence-to-sequence models that attend over the recent dialogue history, but do not attempt to recall long-term knowledge beyond encoding it directly into the weights of the feed-forward network.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-Subtitles",
        "Persona-Chat",
        "Twitter"
      ],
      "dataset_descriptions": {
        "Open-Subtitles": "Used to test sequence-to-sequence models in generating conversational responses, focusing on context sensitivity without long-term knowledge recall.",
        "Persona-Chat": "Used to evaluate sequence-to-sequence models in generating personalized conversational responses, emphasizing recent dialogue history.",
        "Twitter": "Used to assess sequence-to-sequence models in generating conversational responses, focusing on context sensitivity without long-term knowledge recall."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to test sequence-to-sequence models in dialogue systems, which are relevant to personalized text generation.",
      "processing_time": 74.72587323188782,
      "citing_paper_id": "53218829",
      "cited_paper_id": 94285
    },
    {
      "context_text": "For example, popular chit-chat datasets such as Open-Subtitles (Vinyals & Le, 2015), Persona-Chat (Zhang et al., 2018) and Twitter (Sordoni et al., 2015) have tested the ability of sequence-to-sequence models that attend over the recent dialogue history, but do not attempt to recall long-term knowledge beyond encoding it directly into the weights of the feed-forward network.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-Subtitles",
        "Persona-Chat",
        "Twitter"
      ],
      "dataset_descriptions": {
        "Open-Subtitles": "Used to test sequence-to-sequence models in generating conversational responses, focusing on context sensitivity without long-term knowledge recall.",
        "Persona-Chat": "Used to evaluate sequence-to-sequence models in generating personalized conversational responses, emphasizing recent dialogue history.",
        "Twitter": "Used to assess sequence-to-sequence models in generating conversational responses, focusing on context sensitivity without long-term knowledge recall."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions specific datasets used to test sequence-to-sequence models in dialogue systems, which are relevant to personalized text generation.",
      "processing_time": 74.72587323188782,
      "citing_paper_id": "53218829",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "We design a set of architectures speciﬁcally for this goal that combine elements of Memory Network architectures (Sukhbaatar et al., 2015) to retrieve knowledge and read and condition on it, and Transformer architectures (Vaswani et al., 2017) to provide state-of-the-art text representations and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on combining Memory Network and Transformer architectures.",
      "processing_time": 56.652145862579346,
      "citing_paper_id": "53218829",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "We thus develop extensions of the Memory Network (Sukhbaatar et al., 2015) and Transformer (Vaswani et al., 2017) models that can (i) retrieve from a large memory relevant information conditioned on the dialogue history, (ii) carefully read and attend over the retrieved set of knowledge, and then…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.806938886642456,
      "citing_paper_id": "53218829",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "We compare Transform-ers against various baselines including a random baseline; an Information Retrieval (IR) baseline, which uses simple word overlap; and a Bag-of-Words Memory Network (Sukhbaatar et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing different models, including a random baseline, an IR baseline, and a Bag-of-Words Memory Network.",
      "processing_time": 59.171480655670166,
      "citing_paper_id": "53218829",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "Our work compares Memory Networks (Sukhbaatar et al., 2015) and Transformers which have been shown to be on-par or superior to RNN encoder-decoders (Vaswani et al., 2017), and develops an architecture that combines these approaches.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing architectures and developing a new combined approach.",
      "processing_time": 56.895763874053955,
      "citing_paper_id": "53218829",
      "cited_paper_id": 1399322
    },
    {
      "context_text": "The work of Dodge et al. (2016) employed Memory Networks to perform dialogue discussing movies in terms of recommendation and open-ended discussion from Reddit, conditioning on a structured knowledge base.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a structured knowledge base but does not specify a named dataset. The focus is on the method (Memory Networks) and the platform (Reddit) rather than a specific dataset.",
      "processing_time": 59.26997089385986,
      "citing_paper_id": "53218829",
      "cited_paper_id": 2239496
    },
    {
      "context_text": "We thus used exactly the same retriever that is commonly used for the Open-SQuAD dataset in Chen et al. (2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-SQuAD"
      ],
      "dataset_descriptions": {
        "Open-SQuAD": "Used to evaluate the performance of a retriever model in answering open-domain questions, focusing on the accuracy and efficiency of information retrieval from Wikipedia."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'Open-SQuAD' as a dataset used for experiments, which is a specific, verifiable resource.",
      "processing_time": 63.47574043273926,
      "citing_paper_id": "53218829",
      "cited_paper_id": 3618568
    },
    {
      "context_text": "…in SQuAD neural models have been developed that attend to a given paragraph from Wikipedia to answer questions (Rajpurkar et al., 2016), or Open-SQuAD which extends this to answering without being given the paragraph, instead performing retrieval over the entirety of Wikipedia (Chen et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SQuAD",
        "Open-SQuAD"
      ],
      "dataset_descriptions": {
        "SQuAD": "Used to develop neural models that answer questions based on given Wikipedia paragraphs, focusing on reading comprehension and information extraction.",
        "Open-SQuAD": "Extended to perform open-domain question answering by retrieving relevant information from the entire Wikipedia corpus, enhancing the scope of answerable questions."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions SQuAD and Open-SQuAD, which are specific datasets used for question-answering tasks. However, the context does not provide detailed usage descriptions for these datasets.",
      "processing_time": 72.006840467453,
      "citing_paper_id": "53218829",
      "cited_paper_id": 3618568
    },
    {
      "context_text": ", 2016), or Open-SQuAD which extends this to answering without being given the paragraph, instead performing retrieval over the entirety of Wikipedia (Chen et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Open-SQuAD"
      ],
      "dataset_descriptions": {
        "Open-SQuAD": "Described as an extension of SQuAD, used for open-domain question answering by retrieving information from the entire Wikipedia corpus."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'Open-SQuAD' as an extension of SQuAD, but does not provide specific usage details or research context. The citation is more about describing the dataset rather than its application.",
      "processing_time": 66.4380784034729,
      "citing_paper_id": "53218829",
      "cited_paper_id": 3618568
    },
    {
      "context_text": "The encoded candidates are ﬂattened into vectors using the normalization from Cer et al. (2018) to produce an attention prediction over the memory.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method for normalization. The context is about the process of flattening encoded candidates into vectors.",
      "processing_time": 56.831220865249634,
      "citing_paper_id": "53218829",
      "cited_paper_id": 4494896
    },
    {
      "context_text": "The current state-of-the-art approaches, sequence to sequence models of various kinds (Sutskever et al., 2014; Vinyals & Le, 2015; Serban et al., 2016; Vaswani et al., 2017) attempt to address some of these skills, but generally suffer from an inability to bring memory and knowledge to bear; as…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.65023493766785,
      "citing_paper_id": "53218829",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "Both Ghazvininejad et al. (2018) and Parthasarathi & Pineau (2018) use unstructured text instead, as we do: the former to discuss news articles using Wikipedia summaries as knowledge, and the latter to discuss local businesses in two-turn dialogues using Foursquare tips as knowledge.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of unstructured text, specifically Wikipedia summaries and Foursquare tips, but does not provide specific dataset names. These are described as knowledge sources rather than formal datasets.",
      "processing_time": 58.682087898254395,
      "citing_paper_id": "53218829",
      "cited_paper_id": 15442925
    },
    {
      "context_text": "Ghazvininejad et al. (2018) uses an extended Encoder-Decoder where the decoder is provided with an encoding of the context along with the external knowledge encoding.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method or model. The cited paper title confirms the focus on a neural conversation model rather than a dataset.",
      "processing_time": 57.32042598724365,
      "citing_paper_id": "53218829",
      "cited_paper_id": 15442925
    },
    {
      "context_text": "Zhou et al. (2018) also links Reddit to structured knowledge.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a link between Reddit and structured knowledge. No clear, verifiable dataset names are provided.",
      "processing_time": 55.88143563270569,
      "citing_paper_id": "53218829",
      "cited_paper_id": 51608183
    },
    {
      "context_text": "d performing retrieval over the entirety of Wikipedia (Chen et al., 2017). Recently, the QuAC dataset investigates similar themes, but as a sequence of questions and answers in dialogue form instead (Choi et al., 2018). In this work we do not address question answering, but focus on natural human dialogues which contain a diverse set of utterances, not just questions and answers. The closest work to ours lies in th",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QuAC"
      ],
      "dataset_descriptions": {
        "QuAC": "Used to explore question answering in dialogue form, focusing on sequences of questions and answers. This dataset is relevant for studying natural human dialogues and their structure in conversational systems."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the QuAC dataset, which is a specific, verifiable dataset used for question answering in dialogue form. The dataset is relevant to the topic of personalized text generation as it involves natural human dialogues.",
      "processing_time": 67.3930115699768,
      "citing_paper_id": "53218829",
      "cited_paper_id": 52057510
    },
    {
      "context_text": "Recently, the QuAC dataset investigates similar themes, but as a sequence of questions and answers in dialogue form instead (Choi et al., 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "QuAC"
      ],
      "dataset_descriptions": {
        "QuAC": "Used to investigate themes through dialogic question and answer sequences, focusing on the conversational context in personalized text generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the QuAC dataset, which is a specific, verifiable resource used for investigating themes through dialogic question and answer sequences.",
      "processing_time": 62.27787947654724,
      "citing_paper_id": "53218829",
      "cited_paper_id": 52057510
    },
    {
      "context_text": "Where noted, the Transformer is pretrained on Reddit data (Mazaré et al., 2018), and fine-tuned for our task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Reddit data' but does not provide a specific name or identifier for the dataset. It is a generic reference to data from Reddit, which is not specific enough to be included.",
      "processing_time": 58.99649357795715,
      "citing_paper_id": "53218829",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "Where noted, the Transformer is pretrained on Reddit data (Mazaré et al., 2018), and fine-tuned for our task.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Reddit data' but does not provide a specific name or identifier for the dataset. It is a generic reference to data from Reddit, which is not specific enough to be included.",
      "processing_time": 59.204110622406006,
      "citing_paper_id": "53218829",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "5-9B [28], and Pegasus [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models (PEGASUS and Yi). No verifiable resources are identified.",
      "processing_time": 56.44033408164978,
      "citing_paper_id": "275336716",
      "cited_paper_id": 209405420
    },
    {
      "context_text": "5-9B [28], and Pegasus [29].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models (PEGASUS and Yi). No verifiable resources are identified.",
      "processing_time": 56.44033408164978,
      "citing_paper_id": "275336716",
      "cited_paper_id": 268264158
    },
    {
      "context_text": "Existing approaches to automatic figure captioning have predominantly treated the task either as an image-to-text problem [8,21] or a text summarization task [11,4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches to figure captioning. No verifiable resources are identified.",
      "processing_time": 56.342841148376465,
      "citing_paper_id": "275336716",
      "cited_paper_id": 232139024
    },
    {
      "context_text": "Existing approaches to automatic figure captioning have predominantly treated the task either as an image-to-text problem [8,21] or a text summarization task [11,4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches to figure captioning. No verifiable resources are identified.",
      "processing_time": 56.342841148376465,
      "citing_paper_id": "275336716",
      "cited_paper_id": 237578994
    },
    {
      "context_text": "Existing approaches to automatic figure captioning have predominantly treated the task either as an image-to-text problem [8,21] or a text summarization task [11,4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general approaches to figure captioning. No verifiable resources are identified.",
      "processing_time": 56.342841148376465,
      "citing_paper_id": "275336716",
      "cited_paper_id": 268691658
    },
    {
      "context_text": "Many existing datasets [14,13,8,26], particularly those sourced from platforms like arXiv, contain captions that are incomplete, verbose, or poorly written.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets' generally and does not specify any particular dataset name. It refers to issues with captions in datasets sourced from platforms like arXiv, which is too generic.",
      "processing_time": 58.751441955566406,
      "citing_paper_id": "275336716",
      "cited_paper_id": 237578994
    },
    {
      "context_text": "To facilitate the generation of captions by neural networks, previous research developed a variety of datasets, such as FigureSeer [24], FigureQA [14], DVQA [13], and SciCap [8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "FigureSeer",
        "FigureQA",
        "DVQA",
        "SciCap"
      ],
      "dataset_descriptions": {
        "FigureSeer": "Used to generate captions for scientific figures, focusing on the accuracy and relevance of generated text to the figure content.",
        "FigureQA": "Applied to test the ability of models to answer questions about figures, emphasizing visual and textual understanding in scientific contexts.",
        "DVQA": "Utilized to evaluate the performance of models in answering questions about data visualizations, assessing both visual and logical reasoning capabilities.",
        "SciCap": "Employed to generate descriptive captions for scientific figures, specifically targeting the clarity and informativeness of the generated text."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions several datasets by name, which are used for generating captions for figures. These datasets are specific and have clear identifiers.",
      "processing_time": 79.77396965026855,
      "citing_paper_id": "275336716",
      "cited_paper_id": 237578994
    },
    {
      "context_text": "Next, we fine-tune LLaVA [17] on the constructed dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'constructed dataset' which is too generic and does not provide a specific, identifiable name.",
      "processing_time": 55.77615809440613,
      "citing_paper_id": "275336716",
      "cited_paper_id": 258179774
    },
    {
      "context_text": "To mitigate this issue, recent work [23] trained a classifier to select the best response generated by different reasoning models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for selecting responses from different reasoning models.",
      "processing_time": 55.55117726325989,
      "citing_paper_id": "275336716",
      "cited_paper_id": 258865893
    },
    {
      "context_text": "However, despite their versatility, individual LLMs exhibit distinct strengths and limitations due to differences in training data and architectural design [12].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to training data which are too generic.",
      "processing_time": 55.54798483848572,
      "citing_paper_id": "275336716",
      "cited_paper_id": 263830701
    },
    {
      "context_text": "Similar observations have been made in the natural image captioning task [3], where BLEU and ROUGE scores showed low correlation with human judgments (Kendall’s tau of approximately 0.3 for both metrics).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics (BLEU, ROUGE) which are excluded. The context focuses on the evaluation of image captions using these metrics.",
      "processing_time": 58.16749691963196,
      "citing_paper_id": "275336716",
      "cited_paper_id": 264305715
    },
    {
      "context_text": "LLMs have shown exceptional performance across a wide range of tasks, benefiting from their ability to comprehend instructions [1,28].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the performance of LLMs across various tasks. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.279160499572754,
      "citing_paper_id": "275336716",
      "cited_paper_id": 268264158
    },
    {
      "context_text": "Figure descriptions are generated by MiniCPM-V [27], which outperforms GPT-4V-1106 and Gemini-Pro [22] for OpenCompass benchmarks [5].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and benchmarks but does not refer to any specific datasets. The context focuses on model performance comparisons.",
      "processing_time": 56.12134027481079,
      "citing_paper_id": "275336716",
      "cited_paper_id": 271709626
    },
    {
      "context_text": "Figure descriptions are generated by MiniCPM-V [27], which outperforms GPT-4V-1106 and Gemini-Pro [22] for OpenCompass benchmarks [5].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models and benchmarks but does not refer to any specific datasets. The context focuses on model performance comparisons.",
      "processing_time": 56.12134027481079,
      "citing_paper_id": "275336716",
      "cited_paper_id": null
    },
    {
      "context_text": "We use a state-of-the-art sentiment classiﬁer (Hu et al., 2016a) which achieves 90% accuracy on the SST test set, to automatically evaluate the sentiment generation accuracy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SST"
      ],
      "dataset_descriptions": {
        "SST": "Used to evaluate the sentiment generation accuracy of a state-of-the-art sentiment classifier, focusing on the performance and reliability of the model."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions the SST test set, which is a well-known dataset for sentiment analysis. However, the full name 'Stanford Sentiment Treebank' is not explicitly mentioned in the context.",
      "processing_time": 65.80915904045105,
      "citing_paper_id": "20981275",
      "cited_paper_id": 801539
    },
    {
      "context_text": "We use a state-of-the-art sentiment classiﬁer (Hu et al., 2016a) which achieves 90% accuracy on the SST test set, to automatically evaluate the sentiment generation accuracy.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SST"
      ],
      "dataset_descriptions": {
        "SST": "Used to evaluate the sentiment generation accuracy of a state-of-the-art sentiment classifier, focusing on the performance and reliability of the model."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions the SST test set, which is a well-known dataset for sentiment analysis. However, the full name 'Stanford Sentiment Treebank' is not explicitly mentioned in the context.",
      "processing_time": 65.80915904045105,
      "citing_paper_id": "20981275",
      "cited_paper_id": 7663461
    },
    {
      "context_text": "…(e.g., logic rules or probabilistic structured models) on the interpretable latent code, to incorporate prior knowledge or human intentions (Hu et al., 2016a;b); or plug the disentangled generation model into dialog systems to generate natural language responses from structured dialog…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on incorporating prior knowledge or human intentions into disentangled generation models.",
      "processing_time": 57.37037253379822,
      "citing_paper_id": "20981275",
      "cited_paper_id": 801539
    },
    {
      "context_text": "…(e.g., logic rules or probabilistic structured models) on the interpretable latent code, to incorporate prior knowledge or human intentions (Hu et al., 2016a;b); or plug the disentangled generation model into dialog systems to generate natural language responses from structured dialog…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on incorporating prior knowledge or human intentions into disentangled generation models.",
      "processing_time": 57.37037253379822,
      "citing_paper_id": "20981275",
      "cited_paper_id": 7663461
    },
    {
      "context_text": "To alleviate the issue of noisy data and ensure robustness of model optimization, we incorporate a minimum entropy regularization term (Grandvalet et al., 2004; Reed et al., 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for handling noisy labels.",
      "processing_time": 54.856242418289185,
      "citing_paper_id": "20981275",
      "cited_paper_id": 2181703
    },
    {
      "context_text": "…semi-supervised training, another advantage of using discrimi-nators as learning signals for the generator, as compared to conventional conditional reconstruction based meth-ods (Wen et al., 2015; Kingma et al., 2014), is that discriminators of different attributes can be trained independently.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the advantages of using discriminators in semi-supervised learning.",
      "processing_time": 57.073567628860474,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "We see that our method consistently outperforms S-VAE on all datasets.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets' generically without specifying any particular dataset names. The cited paper title does not help in identifying specific datasets.",
      "processing_time": 56.79079842567444,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "Further incorporating the conditioned sentiment code of the generated samples, as in “Ours” and “S-VAE”, provides additional performance gains, indicating the advantages of conditional generation for automatic creation of labeled data.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and performance gains.",
      "processing_time": 54.72082209587097,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "Siddharth et al. (2017); Kingma et al. (2014) base on VAEs and obtain disentangled image representations with semi-supervised learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of VAEs and semi-supervised learning for disentangled image representations.",
      "processing_time": 58.297221183776855,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "In contrast, reconstruction based approaches typically require every instance of the training data to be labeled exhaustively with all target attributes (Wen et al., 2015), or to marginalize out any missing attributes (Kingma et al., 2014) which can be computationally expensive.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses general requirements for training data in reconstruction-based approaches.",
      "processing_time": 56.35252547264099,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "S-VAE uses the same protocol as our method to augment with the data generated by the S-VAE model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (S-VAE) and a general protocol, which do not qualify as datasets.",
      "processing_time": 58.28875994682312,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "Conditional generation in the context of VAEs (e.g., semi-supervised VAEs (Kingma et al., 2014)) is often learned by reconstructing observed examples given their feature code.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (semi-supervised VAEs).",
      "processing_time": 55.64794421195984,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "As an alternative to the discriminator based learning, semi-supervised VAEs (Kingma et al., 2014) minimize element-wise reconstruction error on observed examples and are applicable to discrete visibles.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (semi-supervised VAEs).",
      "processing_time": 55.46931314468384,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "S-VAE (Kingma et al., 2014) and our model are trained on the three sentiment datasets and generate 30K sentences, respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'three sentiment datasets' but does not provide specific names. The term 'sentiment datasets' is too generic and lacks specific identifiers.",
      "processing_time": 57.32594156265259,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "S-VAE learns to reconstruct observed sentences given attribute code, and no discrimi-nators are used.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (S-VAE) and its functionality. No verifiable resources are identified.",
      "processing_time": 56.94732356071472,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "Consistent with the above experiment, our model outperforms S-VAE.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison with a model (S-VAE).",
      "processing_time": 55.99738836288452,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "We compare with semi-supervised VAE (S-VAE) (Kingma et al., 2014), one of the few existing deep models capable of conditional text generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (S-VAE) for conditional text generation.",
      "processing_time": 56.15511131286621,
      "citing_paper_id": "20981275",
      "cited_paper_id": 6377199
    },
    {
      "context_text": "However, autoencoder frameworks (Sutskever et al., 2014) and recurrent neural network language models (Mikolov et al., 2010) do not apply to generic text generation from arbitrary hidden representations due to the unsmoothness of effective hidden codes (Bowman et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the limitations of certain neural network architectures for text generation.",
      "processing_time": 57.143595695495605,
      "citing_paper_id": "20981275",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "There is a surge of research interest in deep generative models (Hu et al., 2017), such as Variational Autoencoders (VAEs) (Kingma & Welling, 2013), Generative Adversarial Nets (GANs) (Goodfellow et al., 2014), and auto-regressive models (van den Oord et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.93094563484192,
      "citing_paper_id": "20981275",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "Dosovitskiy & Brox (2016); Taigman et al. (2017) use discriminators to measure high-level perceptual similarity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of discriminators in a methodological context.",
      "processing_time": 55.784250020980835,
      "citing_paper_id": "20981275",
      "cited_paper_id": 10756563
    },
    {
      "context_text": "The lexicon from (Wilson et al., 2005) contains 2700 words with sentiment labels.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "lexicon from (Wilson et al., 2005)"
      ],
      "dataset_descriptions": {
        "lexicon from (Wilson et al., 2005)": "Used for recognizing contextual polarity in phrase-level sentiment analysis, providing 2700 words with sentiment labels to enhance personalized text generation."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a lexicon with sentiment labels, which is a specific, verifiable resource. The lexicon is used for sentiment analysis, which is relevant to personalized text generation.",
      "processing_time": 68.50230312347412,
      "citing_paper_id": "20981275",
      "cited_paper_id": 11668878
    },
    {
      "context_text": "Despite their impressive advances in visual domain, such as image generation (Radford et al., 2015), learning interpretable image representations (Chen et al., 2016), and image editing (Zhu et al., 2016), applications to natural language generation have been relatively less studied.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advancements in the visual domain. No verifiable resources are identified.",
      "processing_time": 56.17348051071167,
      "citing_paper_id": "20981275",
      "cited_paper_id": 14924561
    },
    {
      "context_text": "To enhance the shopping experience for consumers in e-commerce, VTON (Han et al. 2018; Choi et al. 2021) tasks have become increasingly popular within the community.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks (VTON) which are methods or applications. No verifiable datasets are referenced.",
      "processing_time": 57.06183481216431,
      "citing_paper_id": "271244829",
      "cited_paper_id": 4532827
    },
    {
      "context_text": "To enhance the shopping experience for consumers in e-commerce, VTON (Han et al. 2018; Choi et al. 2021) tasks have become increasingly popular within the community.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only tasks (VTON) which are methods or applications. No verifiable datasets are referenced.",
      "processing_time": 57.06183481216431,
      "citing_paper_id": "271244829",
      "cited_paper_id": 232427801
    },
    {
      "context_text": "However, GAN-based methods (Choi et al. 2021; Han et al. 2018) often suffer from instability due to the min-max training objective, and they have limitations in preserving texture details and handling complex backgrounds.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses limitations of GAN-based methods. No verifiable resources are identified.",
      "processing_time": 56.44908165931702,
      "citing_paper_id": "271244829",
      "cited_paper_id": 4532827
    },
    {
      "context_text": "However, GAN-based methods (Choi et al. 2021; Han et al. 2018) often suffer from instability due to the min-max training objective, and they have limitations in preserving texture details and handling complex backgrounds.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses limitations of GAN-based methods. No verifiable resources are identified.",
      "processing_time": 56.44908165931702,
      "citing_paper_id": "271244829",
      "cited_paper_id": 20404279
    },
    {
      "context_text": "However, GAN-based methods (Choi et al. 2021; Han et al. 2018) often suffer from instability due to the min-max training objective, and they have limitations in preserving texture details and handling complex backgrounds.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses limitations of GAN-based methods. No verifiable resources are identified.",
      "processing_time": 56.44908165931702,
      "citing_paper_id": "271244829",
      "cited_paper_id": 232427801
    },
    {
      "context_text": "However, existing works primarily focus on virtual try-on (VTON) (Han et al. 2018; Choi et al. 2021; Kim et al. 2024; Morelli et al. 2023; Xu et al. 2024b; Yang et al. 2024) tasks for consumers, which involve given garments and fixed human conditions, lacking flexibility and editability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and methods related to virtual try-on. No verifiable resources are identified.",
      "processing_time": 56.505648612976074,
      "citing_paper_id": "271244829",
      "cited_paper_id": 4532827
    },
    {
      "context_text": "However, existing works primarily focus on virtual try-on (VTON) (Han et al. 2018; Choi et al. 2021; Kim et al. 2024; Morelli et al. 2023; Xu et al. 2024b; Yang et al. 2024) tasks for consumers, which involve given garments and fixed human conditions, lacking flexibility and editability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and methods related to virtual try-on. No verifiable resources are identified.",
      "processing_time": 56.505648612976074,
      "citing_paper_id": "271244829",
      "cited_paper_id": 232427801
    },
    {
      "context_text": "However, existing works primarily focus on virtual try-on (VTON) (Han et al. 2018; Choi et al. 2021; Kim et al. 2024; Morelli et al. 2023; Xu et al. 2024b; Yang et al. 2024) tasks for consumers, which involve given garments and fixed human conditions, lacking flexibility and editability.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tasks and methods related to virtual try-on. No verifiable resources are identified.",
      "processing_time": 56.505648612976074,
      "citing_paper_id": "271244829",
      "cited_paper_id": 265609458
    },
    {
      "context_text": "Early methods primarily relied on generative adversarial network (GAN) (Creswell et al. 2018), typically comprising a warping module to learn the semantic correspondence be-tween clothes and the human body, and a generator module to synthesize the warped clothes onto the person’s image.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of GANs and their components, which are not considered datasets.",
      "processing_time": 58.10041880607605,
      "citing_paper_id": "271244829",
      "cited_paper_id": 20404279
    },
    {
      "context_text": "Virtual Try-On Early virtual try-on (Dong et al. 2020; Jo and Park 2019; Lee et al. 2020; Liu et al. 2020; Choi et al. 2021) Recent research (Morelli et al. 2023; Gou et al. 2023; Zhu et al. 2023) have incorporated pre-trained diffusion models as priors for VTON tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers and methods. The focus is on the use of pre-trained diffusion models for virtual try-on tasks.",
      "processing_time": 58.33957815170288,
      "citing_paper_id": "271244829",
      "cited_paper_id": 198967908
    },
    {
      "context_text": "Virtual Try-On Early virtual try-on (Dong et al. 2020; Jo and Park 2019; Lee et al. 2020; Liu et al. 2020; Choi et al. 2021) Recent research (Morelli et al. 2023; Gou et al. 2023; Zhu et al. 2023) have incorporated pre-trained diffusion models as priors for VTON tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers and methods. The focus is on the use of pre-trained diffusion models for virtual try-on tasks.",
      "processing_time": 58.33957815170288,
      "citing_paper_id": "271244829",
      "cited_paper_id": 203594079
    },
    {
      "context_text": "Virtual Try-On Early virtual try-on (Dong et al. 2020; Jo and Park 2019; Lee et al. 2020; Liu et al. 2020; Choi et al. 2021) Recent research (Morelli et al. 2023; Gou et al. 2023; Zhu et al. 2023) have incorporated pre-trained diffusion models as priors for VTON tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers and methods. The focus is on the use of pre-trained diffusion models for virtual try-on tasks.",
      "processing_time": 58.33957815170288,
      "citing_paper_id": "271244829",
      "cited_paper_id": 232427801
    },
    {
      "context_text": "Virtual Try-On Early virtual try-on (Dong et al. 2020; Jo and Park 2019; Lee et al. 2020; Liu et al. 2020; Choi et al. 2021) Recent research (Morelli et al. 2023; Gou et al. 2023; Zhu et al. 2023) have incorporated pre-trained diffusion models as priors for VTON tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers and methods. The focus is on the use of pre-trained diffusion models for virtual try-on tasks.",
      "processing_time": 58.33957815170288,
      "citing_paper_id": "271244829",
      "cited_paper_id": 259164412
    },
    {
      "context_text": "Virtual Try-On Early virtual try-on (Dong et al. 2020; Jo and Park 2019; Lee et al. 2020; Liu et al. 2020; Choi et al. 2021) Recent research (Morelli et al. 2023; Gou et al. 2023; Zhu et al. 2023) have incorporated pre-trained diffusion models as priors for VTON tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to research papers and methods. The focus is on the use of pre-trained diffusion models for virtual try-on tasks.",
      "processing_time": 58.33957815170288,
      "citing_paper_id": "271244829",
      "cited_paper_id": 260866168
    },
    {
      "context_text": "TryOnDiffusion (Zhu et al. 2023) proposed an architecture with two parallel UNets and demonstrated the capability of diffusion-based virtual try-on by training on large-scale datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale datasets' but does not specify any particular dataset names. The term 'large-scale datasets' is too generic and lacks a specific identifier.",
      "processing_time": 57.91750764846802,
      "citing_paper_id": "271244829",
      "cited_paper_id": 259164412
    },
    {
      "context_text": "For example, LADI-VTON (Morelli et al. 2023) and DCI-VTON (Gou et al. 2023) explicitly warp clothes to align them with the human body, then use diffusion models to merge the clothes with the body.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the methodologies used for virtual try-on, which involves warping clothes and using diffusion models.",
      "processing_time": 58.26799488067627,
      "citing_paper_id": "271244829",
      "cited_paper_id": 260866168
    },
    {
      "context_text": "These methods (Kim et al. 2024; Zeng et al. 2024; Xu et al. 2024b) better retain the texture information of the input garments through a a r X i v : 2407 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 57.29251265525818,
      "citing_paper_id": "271244829",
      "cited_paper_id": 265506292
    },
    {
      "context_text": "These methods (Kim et al. 2024; Zeng et al. 2024; Xu et al. 2024b) better retain the texture information of the input garments through a a r X i v : 2407 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The titles of the cited papers also do not indicate the use of specific datasets.",
      "processing_time": 57.29251265525818,
      "citing_paper_id": "271244829",
      "cited_paper_id": 265609458
    },
    {
      "context_text": "StableVITON (Kim et al. 2024) introduces a zero-initialized cross-attention block to inject intermediate features of the spatial encoder into the UNet decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (StableVITON) and its components. The context is focused on describing the method's architecture and functionality.",
      "processing_time": 58.25930380821228,
      "citing_paper_id": "271244829",
      "cited_paper_id": 265609458
    },
    {
      "context_text": "Similarly, OOTDiffusion (Xu et al. 2024b) and IDM (Choi et al. 2024) utilize parallel UNets for garment feature extraction and enhance integration through self-attention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of parallel UNets and self-attention mechanisms.",
      "processing_time": 57.10669183731079,
      "citing_paper_id": "271244829",
      "cited_paper_id": 268296893
    },
    {
      "context_text": "While latent diffusion models (LDMs) (Rombach et al. 2022a; Wang et al. 2024; Shen et al. 2024; Wang et al. 2024) have been widely used for text-to-image (T2I) generation and editing tasks, the inaccuracy of natural language limits fine-grained image control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their applications. The context focuses on the limitations of natural language in controlling fine-grained image generation.",
      "processing_time": 57.50144648551941,
      "citing_paper_id": "271244829",
      "cited_paper_id": 270878382
    },
    {
      "context_text": "Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classiﬁcation (Howard and Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. No verifiable resources are identified.",
      "processing_time": 55.896570444107056,
      "citing_paper_id": "218973759",
      "cited_paper_id": 81026
    },
    {
      "context_text": "Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classiﬁcation (Howard and Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. No verifiable resources are identified.",
      "processing_time": 55.896570444107056,
      "citing_paper_id": "218973759",
      "cited_paper_id": 1272090
    },
    {
      "context_text": "Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classiﬁcation (Howard and Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. No verifiable resources are identified.",
      "processing_time": 55.896570444107056,
      "citing_paper_id": "218973759",
      "cited_paper_id": 31408841
    },
    {
      "context_text": "Language models are important components of systems for many downstream NLP tasks, such as machine translation (Och and Ney, 2004), speech recognition (Jelinek, 1976), handwriting recognition (Marti and Bunke, 2001), parsing (Choe and Charniak, 2016), and classiﬁcation (Howard and Ruder, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of language models. No verifiable resources are identified.",
      "processing_time": 55.896570444107056,
      "citing_paper_id": "218973759",
      "cited_paper_id": 41524799
    },
    {
      "context_text": "It was originally used in Schler et al. (2006).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not provide enough context to identify a specific dataset. The reference to 'it' is ambiguous and lacks details about the resource used.",
      "processing_time": 56.702022314071655,
      "citing_paper_id": "218973759",
      "cited_paper_id": 2075411
    },
    {
      "context_text": "Mikolov and Zweig (2012) adapted a recurrent neural network by using a vector of a distribution over topics as input to both the hidden layer and output layer.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting a recurrent neural network. No verifiable resources are identified.",
      "processing_time": 56.338857889175415,
      "citing_paper_id": "218973759",
      "cited_paper_id": 11383176
    },
    {
      "context_text": "Language modeling has been applied to a variety of domains including text from Wikipedia (Merity et al., 2016), children’s books (Hill et al., 2015), and newswire text (Mikolov et al., 2010).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several domains where language modeling has been applied but does not specify any named datasets. The domains mentioned are too generic and lack specific identifiers.",
      "processing_time": 57.16419291496277,
      "citing_paper_id": "218973759",
      "cited_paper_id": 16299141
    },
    {
      "context_text": "Although this has been shown to improve the performance of language models (Lynn et al., 2017), this demographic information is typically obtained via document metadata, which is not available for all document types, or for all users.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to demographic information obtained via document metadata.",
      "processing_time": 54.85570979118347,
      "citing_paper_id": "218973759",
      "cited_paper_id": 26192090
    },
    {
      "context_text": "Such metadata can be useful (Lynn et al., 2017), although it is not always available due to a limitation of the collected data, or privacy concerns of the user.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to metadata which is not a verifiable resource.",
      "processing_time": 55.408528566360474,
      "citing_paper_id": "218973759",
      "cited_paper_id": 26192090
    },
    {
      "context_text": "Lynn et al. (2017) used metadata such as age, gender, and personality to adapt their models for tasks such as sentiment analysis and sarcasm detection.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions using metadata for adapting models, but does not specify a named dataset. The context is about personalization in NLP tasks, which aligns with the research topic.",
      "processing_time": 58.43206810951233,
      "citing_paper_id": "218973759",
      "cited_paper_id": 26192090
    },
    {
      "context_text": "We further intend to consider transformers for personalization, which have shown state-of-the-art language modelling results (Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a reference to transformers and their performance in language modeling.",
      "processing_time": 55.67275071144104,
      "citing_paper_id": "218973759",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Radford et al. (2019) conditioned a transformer for text generation by priming it by exposing it to an input text, and then using the state of the transformer for text generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for text generation using a transformer model.",
      "processing_time": 55.51032638549805,
      "citing_paper_id": "218973759",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Previous research on taskoriented dialogue generation usually employed handcrafted generator to define the generation decision space with the handcrafted features or statistical models [35, 36, 37, 38, 39, 40, 41, 42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.83873200416565,
      "citing_paper_id": "6862403",
      "cited_paper_id": 53380
    },
    {
      "context_text": "are the related work and conclusion respectively. 2. Our Approach Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. The target of these work is that given an input sentence x, the RNN based model tries to generate an output sentence ythat can maximize the conditional probability of p(yjx). For the response genera",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only general references to RNN-based models and their application in generating responses. No verifiable resources are identified.",
      "processing_time": 57.885905265808105,
      "citing_paper_id": "6862403",
      "cited_paper_id": 94285
    },
    {
      "context_text": "nowledge bases that obtained from the Web to deal with the out-of-domain request. Recently, beneﬁt from the advantages of the sequence to sequence learning framework with neural networks [6], [7] and [19] had drawn inspiration from the neural machine translation [17, 18] and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence and a larger range of c",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches and methods. There are no clear identifiers for datasets in the text.",
      "processing_time": 56.66109228134155,
      "citing_paper_id": "6862403",
      "cited_paper_id": 94285
    },
    {
      "context_text": "[47] introduced a search-based system, namely IRIS, to generate dialogues using vector space model and then released the experimental corpus for research and development [48].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Movie-DiC"
      ],
      "dataset_descriptions": {
        "Movie-DiC": "Used to develop and evaluate dialogue generation systems, focusing on movie dialogues. The corpus supports research in personalized text generation by providing a rich source of conversational data."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the release of an experimental corpus for research and development, which aligns with the definition of a dataset. The title 'Movie-DiC: a Movie Dialogue Corpus for Research and Development' confirms it is a specific, verifiable dataset.",
      "processing_time": 69.47222208976746,
      "citing_paper_id": "6862403",
      "cited_paper_id": 244187
    },
    {
      "context_text": "The word2vec is trained with the SogouCS&CA corpus (2008 version) 2 , which is widely used for Chinese text analysis [30, 31].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SogouCS&CA corpus (2008 version)"
      ],
      "dataset_descriptions": {
        "SogouCS&CA corpus (2008 version)": "Used to train word2vec for Chinese text analysis, focusing on the construction of word vectors for natural language processing tasks."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the SogouCS&CA corpus, which is a specific dataset used for training word2vec for Chinese text analysis.",
      "processing_time": 67.37130403518677,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1003702
    },
    {
      "context_text": "Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (GRU, LSTM) which are excluded according to the instructions.",
      "processing_time": 56.59002685546875,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models (GRU, LSTM) which are excluded according to the instructions.",
      "processing_time": 56.59002685546875,
      "citing_paper_id": "6862403",
      "cited_paper_id": 261210189
    },
    {
      "context_text": "Further, [15] and [14] proposed a semantically conditioned LSTM to generate dialogue response and then compared it with an RNN encoder-decoder generator on multi-domain data to verify the ability of domain adaptation of the two generators.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'multi-domain data' but does not specify a named dataset. The context focuses on comparing models rather than using a specific dataset.",
      "processing_time": 57.146565198898315,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Here, T equals to the length (the number of words) of the input sentence and f is a nonlinear function which can be implemented as LSTM [14] or GRU [10].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to methods (LSTM and GRU).",
      "processing_time": 55.40051531791687,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Therefore, the hidden state at time t can be denoted as: Here, T equals to the length (the number of words) of the input sentence and f is a non-linear function which can be implemented as LSTM [6] or GRU [23].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (LSTM, GRU).",
      "processing_time": 55.07494592666626,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "Typically, the encoder and decoder are implemented by the GRU [10] or LSTM [14] based RNN.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (GRU and LSTM).",
      "processing_time": 54.77473187446594,
      "citing_paper_id": "6862403",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "the small size of personalized training data. Next, we will detail the proposed personalized response generation model. Typically, the encoder and decoder are implemented by the GRU [22, 23] or LSTM [24, 25, 26] based RNN. The encoder reads the input sentence word by word and outputs the hidden state of each word. These states are denoted as Hwhich is also called annotations. Here, h i represents the hidden",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the use of GRU and LSTM in RNNs for encoding input sentences, which are methods, not datasets.",
      "processing_time": 58.38969016075134,
      "citing_paper_id": "6862403",
      "cited_paper_id": 2118350
    },
    {
      "context_text": "Conversational robot usually plays the role of virtual companion or assistant of human [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only describes the general role of conversational robots.",
      "processing_time": 55.77813243865967,
      "citing_paper_id": "6862403",
      "cited_paper_id": 2254431
    },
    {
      "context_text": "• Task-oriented Dialogue Generation The most successful research on the task-oriented dialogue system is mainly based on the partially observed Markov decision process (POMDP) [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (POMDP).",
      "processing_time": 55.03535866737366,
      "citing_paper_id": "6862403",
      "cited_paper_id": 2364633
    },
    {
      "context_text": "[9] proposed an approach to automatically gathering dialogue data from similar speakers to improve the performance of personalized dialogue policy learning.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'dialogue data' but does not specify a named dataset. The context is about improving personalized dialogue policy learning, which is relevant to the topic.",
      "processing_time": 57.757546186447144,
      "citing_paper_id": "6862403",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "[22] proposed a statistical language generator which used a dynamic Bayesian networks to generate responses in dialogue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating responses in dialogue using dynamic Bayesian networks.",
      "processing_time": 55.98445510864258,
      "citing_paper_id": "6862403",
      "cited_paper_id": 6241225
    },
    {
      "context_text": "Recently, beneﬁt from the advantages of the sequence to sequence learning framework with neural networks [6], [7] and [19] had drawn inspiration from the neural machine translation [17, 18] and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach using RNN encoder-decoder for generating dialogues.",
      "processing_time": 56.42112326622009,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works using RNNs for generating sentences or responses.",
      "processing_time": 56.41642165184021,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Recently, a number of research works have been proposed to generate sentence or response based on the recurrent neural network (RNN) [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works using RNNs for generating sentences or responses.",
      "processing_time": 56.41642165184021,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "As [7] said: “Automatic evaluation of response generation is still an open problem.”.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about the challenges in automatic evaluation of response generation.",
      "processing_time": 56.1325957775116,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Note that the context vector c, which is generated from the encoder, is also used to initialize the first hidden state [6] or all of the hidden states [17] of the decoder to make sure that the decoder can be conditioned by the encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are not referenced for their datasets.",
      "processing_time": 55.49630093574524,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Personalized Response Generation Inspired by the RNN encoder-decoder framework, which is proposed by [17] and [6], in this paper, we proposed a personalized response generation approach for conversational systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the use of an RNN encoder-decoder framework for personalized response generation.",
      "processing_time": 57.71431517601013,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": ", hT } (1) Here, T equals to the length (the number of words) of the input sentence and f is a non-linear function which can be implemented as LSTM [6] or GRU [23].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (LSTM, GRU) which are excluded according to the rules.",
      "processing_time": 56.47712707519531,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Where, c can be implemented in many ways, such as [6] set c = hT .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for setting a variable in a neural network.",
      "processing_time": 55.724448919296265,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Recently, benefit from the advantages of the sequence to sequence learning framework with neural networks [6], [7] and [19] had drawn inspiration from the neural machine translation [17, 18] and proposed an RNN encoder-decoder based approach to generate dialogue by considering the last one sentence and a larger range of context respectively.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to neural network frameworks and approaches. No verifiable resources are identified.",
      "processing_time": 56.27067589759827,
      "citing_paper_id": "6862403",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "[24] proposed a dynamic attention mechanism based language model with topic reranking for conversation generation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for conversation generation.",
      "processing_time": 54.420926332473755,
      "citing_paper_id": "6862403",
      "cited_paper_id": 8466284
    },
    {
      "context_text": "[44] learned to generate paraphrases in dialogue through a factored language model that was training from the data collected by crowdsourcing.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'data collected by crowdsourcing' but does not specify a named dataset. The reference is too generic and lacks a clear identifier.",
      "processing_time": 56.99274826049805,
      "citing_paper_id": "6862403",
      "cited_paper_id": 14875383
    },
    {
      "context_text": "[23] proposed an end-to-end framework with grounded knowledge base for generating task-oriented conversations without slot filling.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The focus is on the proposed framework and its use of a knowledge base.",
      "processing_time": 56.989309549331665,
      "citing_paper_id": "6862403",
      "cited_paper_id": 15442925
    },
    {
      "context_text": "ponse generation model are as follow: The dimension of the hidden layer of the encoder and decoder is 1,024. The dimension of the word embedding is 500 which is obtained by using the word2vec toolkit [29]. The word2vec is trained with the SogouCS&amp;CA corpus (2008 version)2, which is widely used for Chinese text analysis [30, 31]. The details of the training corpus for training word embedding are sh",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SogouCS&CA corpus"
      ],
      "dataset_descriptions": {
        "SogouCS&CA corpus": "Used to train word embeddings for Chinese text analysis, specifically focusing on the 2008 version of the corpus."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the 'SogouCS&CA corpus' as a specific dataset used for training word embeddings, which is relevant to personalized text generation.",
      "processing_time": 65.46952438354492,
      "citing_paper_id": "6862403",
      "cited_paper_id": 16447573
    },
    {
      "context_text": "It can be applied to a large number of scenarios of human-computer interaction, such as question answering [1], negotiation [2, 3], e-commence [4], tutoring [5], etc. Recently, conversational system usually plays the role of virtual companion or assistant of hu-Email address: wnzhang@ir.hit.edu.cn…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of conversational systems. No verifiable resources are identified.",
      "processing_time": 56.25252914428711,
      "citing_paper_id": "6862403",
      "cited_paper_id": 18470744
    },
    {
      "context_text": "In addition, besides the conversation generation, capturing human’s personality is also important in personalized recommendation [8, 28, 29, 47, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of capturing human personality in personalized recommendation.",
      "processing_time": 55.875688314437866,
      "citing_paper_id": "6862403",
      "cited_paper_id": 20043644
    },
    {
      "context_text": "In addition, besides the conversation generation, capturing human’s personality is also important in personalized recommendation [8, 28, 29, 47, 50].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the importance of capturing human personality in personalized recommendation.",
      "processing_time": 55.875688314437866,
      "citing_paper_id": "6862403",
      "cited_paper_id": 21618040
    },
    {
      "context_text": "Where, eij = a(si−1, hi) is a feedforward neural network, which can be called as the alignment model or attention model [1, 7].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.51336050033569,
      "citing_paper_id": "6862403",
      "cited_paper_id": 51615798
    },
    {
      "context_text": "[4] proposed an example based approach to extend the input message and utilized a personal knowledge base for responses ranking in open domain conversation systems.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "personal knowledge base"
      ],
      "dataset_descriptions": {
        "personal knowledge base": "Used to rank responses in an open-domain conversation system, enhancing personalization through long-term memory storage and retrieval."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions a 'personal knowledge base' which is used for responses ranking in a dialogue system. This fits the criteria for a dataset as it is a specific, verifiable resource used in the research.",
      "processing_time": 65.41431641578674,
      "citing_paper_id": "6862403",
      "cited_paper_id": 206422013
    },
    {
      "context_text": "t has 26 responses in PchatbotW. This helps to establish dialogue assessment indicators at the discourse level. 3.4 Pre-trained Language Models We provide pre-trained language models including GloVe (Pennington et al., 2014), BPE (Sennrich et al.,2016),Fasttext(Bojanowski et al., 2017)andBERT(Devlin et al.,2018)basedonthe dataset. These pre-trained models will be released together with the data. 4 Experiments In this sec",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PchatbotW"
      ],
      "dataset_descriptions": {
        "PchatbotW": "Used to establish dialogue assessment indicators at the discourse level, focusing on the quality of responses in personalized text generation."
      },
      "confidence_score": 0.7,
      "reasoning": "The context mentions 'PchatbotW' which appears to be a dataset used for establishing dialogue assessment indicators. However, the citation does not provide enough detail about the dataset's structure or source.",
      "processing_time": 65.41623544692993,
      "citing_paper_id": "221969995",
      "cited_paper_id": 1957433
    },
    {
      "context_text": "Chen et al. [5] constructed the JD Customer Service Corpus including 435,005 dialogues based on customer service dialogues from JD.com.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "JD Customer Service Corpus"
      ],
      "dataset_descriptions": {
        "JD Customer Service Corpus": "Used to train models for dialogue generation, focusing on customer service interactions with a large set of 435,005 dialogues from JD.com."
      },
      "confidence_score": 1.0,
      "reasoning": "The citation mentions a specific dataset, 'JD Customer Service Corpus', which is used for dialogue generation research.",
      "processing_time": 64.11300945281982,
      "citing_paper_id": "221969995",
      "cited_paper_id": 4895052
    },
    {
      "context_text": "Other open-domain datasets are constructed from social media networks such as Twitter/Weibo/Douban (Ritter et al., 2010; Sordoni et al., 2015; Wang et al., 2013; Shang et al., 2015; Wu et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Twitter/Weibo/Douban' as sources for constructing open-domain datasets, but does not specify a particular dataset name. These are platforms, not datasets.",
      "processing_time": 58.44621229171753,
      "citing_paper_id": "221969995",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "Douban Conversation Corpus (Wu et al., 2017) 1,060,000 7,092,000 131,747,880 Chinese (session, response) pairs crawled from Douban.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Douban Conversation Corpus"
      ],
      "dataset_descriptions": {
        "Douban Conversation Corpus": "Used to train and evaluate multi-turn response selection models in retrieval-based chatbots, focusing on Chinese conversation pairs crawled from Douban."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Douban Conversation Corpus', which is used for multi-turn response selection in chatbots. The dataset is described as containing Chinese conversation pairs.",
      "processing_time": 65.90723609924316,
      "citing_paper_id": "221969995",
      "cited_paper_id": 5450801
    },
    {
      "context_text": ", 2015) and open-domain datasets (Ritter et al., 2010; Sordoni et al., 2015; Wu et al., 2017; Wang et al., 2013; Danescu-Niculescu-Mizil and Lee, 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions multiple papers but does not specify any particular datasets. The context suggests a discussion of various works in the field of chatbots and response selection, but no specific datasets are named.",
      "processing_time": 58.82987356185913,
      "citing_paper_id": "221969995",
      "cited_paper_id": 5450801
    },
    {
      "context_text": "a coherent persona by pre-defined attributes or profiles (Zhang et al., 2018; Mazaré et al., 2018; Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to personalizing dialogue agents. No clear, verifiable datasets are identified.",
      "processing_time": 56.536884784698486,
      "citing_paper_id": "221969995",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "a coherent persona by pre-defined attributes or profiles (Zhang et al., 2018; Mazaré et al., 2018; Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to personalizing dialogue agents. No clear, verifiable datasets are identified.",
      "processing_time": 56.536884784698486,
      "citing_paper_id": "221969995",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "Corresponding author endow personality to machine agents by given profiles (Zhang et al., 2018; Mazaré et al., 2018; Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to personalizing dialogue agents. No clear, verifiable datasets are identified.",
      "processing_time": 56.74247169494629,
      "citing_paper_id": "221969995",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Corresponding author endow personality to machine agents by given profiles (Zhang et al., 2018; Mazaré et al., 2018; Qian et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to personalizing dialogue agents. No clear, verifiable datasets are identified.",
      "processing_time": 56.74247169494629,
      "citing_paper_id": "221969995",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "PERSONA-CHAT (Zhang et al., 2018) 10,981 164,356 / English personalizing chit-chat dialogue corpus made by human.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PERSONA-CHAT"
      ],
      "dataset_descriptions": {
        "PERSONA-CHAT": "Used to personalize chit-chat dialogues, focusing on incorporating personal information into conversations. The dataset contains 10,981 dialogues with 164,356 utterances in English."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions PERSONA-CHAT, which is a specific dataset used for personalizing chit-chat dialogues. The dataset is described as containing a large number of dialogues in English.",
      "processing_time": 68.80197191238403,
      "citing_paper_id": "221969995",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "We believe such data have the potential to support several kinds of research questions, at least the following three: ( task only considers the interaction between the post and the response within a single turn [11, 23, 27, 31, 36], thus being naturally supported by our dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, identifiable dataset name.",
      "processing_time": 56.07424521446228,
      "citing_paper_id": "221969995",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "In recent years, researchers have constructed some open-domain datasets from social media, such as Twitter [21, 24], Weibo [23, 28], and Douban [30].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'open-domain datasets from social media' but does not specify exact dataset names. The sources (Twitter, Weibo, Douban) are mentioned, but not as specific datasets.",
      "processing_time": 59.07884120941162,
      "citing_paper_id": "221969995",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "(2) Generation-based models: Seq2Seq [15, 25]: Single-turn dialogue model that uses RNN-based encoder-decoder to generate responses; Speaker Model [12]: Single-turn personalized dialogue model that utilizes user IDs to learn user embeddings; Person-aWAE [4]: Single-turn personalized dialogue model…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing different models used for personalized dialogue generation.",
      "processing_time": 56.552563428878784,
      "citing_paper_id": "221969995",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "task only considers the interaction between the post and the response within a single turn [11, 23, 27, 31, 36], thus being naturally supported by our dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'our dataset' which is a generic placeholder and does not provide a specific, verifiable dataset name.",
      "processing_time": 56.12497615814209,
      "citing_paper_id": "221969995",
      "cited_paper_id": 9514751
    },
    {
      "context_text": "These datasets can be divided into domain-specific datasets [2, 14, 29] and open-domain datasets [7, 21, 24, 28, 30].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not provide specific names of datasets, only categories (domain-specific and open-domain). No specific, verifiable datasets are mentioned.",
      "processing_time": 56.73085880279541,
      "citing_paper_id": "221969995",
      "cited_paper_id": 10250499
    },
    {
      "context_text": "These domain-specific datasets can be used to build task-oriented dialogue systems [2, 29].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation does not specify any particular dataset names, only referring to 'domain-specific datasets' without providing specific identifiers.",
      "processing_time": 55.83247232437134,
      "citing_paper_id": "221969995",
      "cited_paper_id": 10250499
    },
    {
      "context_text": "As discussed in Vinyals and Le (2015), coherent personality is a key for chatbots to pass the Turing test.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a discussion about coherent personality in chatbots.",
      "processing_time": 55.30617570877075,
      "citing_paper_id": "221969995",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "We experiment with the following models 5 : (1) Retrieval-based models: Conv-KNRM [6]: Single-turn dialogue model that uses CNN to capture n-gram features; DAM [35]: Multi-turn dialogue model that takes the user’s dialogue history as context to construct multi-level text segment representations…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no clear identifiers for datasets within the text.",
      "processing_time": 56.20764136314392,
      "citing_paper_id": "221969995",
      "cited_paper_id": 33169397
    },
    {
      "context_text": "2 Benchmark Models We experiment with the following models5: (1) Retrieval-based models: Conv-KNRM [6]: Single-turn dialogue model that uses CNN to capture n-gram features; DAM [35]: Multi-turn dialogue model that takes the user’s dialogue history as context to construct multi-level text segment representations with stacked self-attention; IOI [26]: Multi-turn dialogue model that captures deep interactive matching features between response and utterance in the conversation context; RSM-DCK [9]: Knowledge enhanced multi-turn dialogue model that considers persona descriptions as external knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and their functionalities. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.724982500076294,
      "citing_paper_id": "221969995",
      "cited_paper_id": 51877568
    },
    {
      "context_text": "Reddit Corpus (Mazaré et al., 2018) 700,000,000 1,400,000,000 / English personalizing chit-chat dialogue corpus crawled from Reddit.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Reddit Corpus"
      ],
      "dataset_descriptions": {
        "Reddit Corpus": "Used to train personalized dialogue agents, focusing on chit-chat conversations. The dataset, crawled from Reddit, contains 700 million comments and 1.4 billion tokens, enabling the study of natural language interactions."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset, 'Reddit Corpus', which is used for personalizing chit-chat dialogue. The dataset is described as being crawled from Reddit and is used in the context of training personalized dialogue agents.",
      "processing_time": 68.87681031227112,
      "citing_paper_id": "221969995",
      "cited_paper_id": 52167799
    },
    {
      "context_text": "[0,10] (10,20] [20,30) (30,50] (50,70] (70,100] (100,150] (150,200] (200,500] (500, ) Length of Dialog History 0 1 2 3 4 5 6 7 8 Seq2Seq Speaker",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'dialogue responses' which is too generic.",
      "processing_time": 55.62279725074768,
      "citing_paper_id": "221969995",
      "cited_paper_id": 196211466
    },
    {
      "context_text": "So, recent studies start to explore generating diverse responses for a given input [10, 20, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general trend in research. No verifiable resources are identified.",
      "processing_time": 55.481255292892456,
      "citing_paper_id": "221969995",
      "cited_paper_id": 196211466
    },
    {
      "context_text": "[33], we apply heuristic methods to generate external knowledge.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only heuristic methods for generating external knowledge.",
      "processing_time": 54.69633078575134,
      "citing_paper_id": "221969995",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "As our dataset is significantly larger than other Chinese datasets, it is promising to learn a model with better performance [3, 16, 33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide specific names of datasets, only a general reference to 'Chinese datasets'. No clear, verifiable resource names are present.",
      "processing_time": 56.28315019607544,
      "citing_paper_id": "221969995",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "As our dataset is significantly larger than other Chinese datasets, it is promising to learn a model with better performance [3, 16, 33].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not provide specific names of datasets, only a general reference to 'Chinese datasets'. No clear, verifiable resource names are present.",
      "processing_time": 56.28315019607544,
      "citing_paper_id": "221969995",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "aWAE [4]: Single-turn personalized dialogue model that samples the personalized vector from a personalized Gaussianmixture distribution and uses it to guide the response generation;DialoGPT [33]: Large-scale dialogue model that concatenates the utterances as a long sequence and learns to generate the response.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (iWAE, DialoGPT) but does not reference any specific datasets. The context focuses on model architectures and their methodologies.",
      "processing_time": 57.2422194480896,
      "citing_paper_id": "221969995",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Many recent neural dialogue models demonstrate substantial gains on chatbots’ performances using large-scale dataset [3, 16, 33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'large-scale dataset' but does not specify a particular dataset name. The context is too generic and lacks specific identifiers.",
      "processing_time": 56.54781246185303,
      "citing_paper_id": "221969995",
      "cited_paper_id": 207869708
    },
    {
      "context_text": "Many recent neural dialogue models demonstrate substantial gains on chatbots’ performances using large-scale dataset [3, 16, 33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'large-scale dataset' but does not specify a particular dataset name. The context is too generic and lacks specific identifiers.",
      "processing_time": 56.54781246185303,
      "citing_paper_id": "221969995",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "[1,3] (3,5] [5,10) (10,20] (20,30] (30,40] (40,50] (50,100] (100, ) Number of Responses 0 2000 4000 6000 8000 10000",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not contain any specific, verifiable datasets or resources. It appears to be a histogram or chart description, which does not provide information about datasets.",
      "processing_time": 57.29746460914612,
      "citing_paper_id": "221969995",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "[1,3] (3,5] [5,10) (10,20] (20,30] (30,40] (40,50] (50,100] (100, ) Number of Responses 0 200000 400000 600000 800000 1000000 1200000 1400000 1600000",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided citation span does not contain any specific, verifiable datasets or resources. It appears to be a graphical representation of response counts.",
      "processing_time": 56.096271991729736,
      "citing_paper_id": "221969995",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Text prediction services have been applied for various applications, including text editor (Darragh et al., 1990), query autocompletion on search engine (Bast and Weber, 2006; Bar-Yossef and Kraus, 2011), mobile virtual keyboard (Hard et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of text prediction services. No verifiable resources are identified.",
      "processing_time": 56.0889036655426,
      "citing_paper_id": "233473617",
      "cited_paper_id": 9750936
    },
    {
      "context_text": "Text prediction services have been applied for various applications, including text editor (Darragh et al., 1990), query autocompletion on search engine (Bast and Weber, 2006; Bar-Yossef and Kraus, 2011), mobile virtual keyboard (Hard et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only applications of text prediction services. No verifiable resources are identified.",
      "processing_time": 56.0889036655426,
      "citing_paper_id": "233473617",
      "cited_paper_id": 53207681
    },
    {
      "context_text": "…Mnih and Hinton, 2009; Melis et al., 2018) rely on the most recent deep learning architectures, including large LSTMs (Hochreiter and Schmidhuber, 1997) or transformers (Vaswani et al., 2017), while prior approaches involve n-gram modeling (Kneser and Ney, 1995; James, 2000; Bickel et al., 2005).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.35944962501526,
      "citing_paper_id": "233473617",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "…Mnih and Hinton, 2009; Melis et al., 2018) rely on the most recent deep learning architectures, including large LSTMs (Hochreiter and Schmidhuber, 1997) or transformers (Vaswani et al., 2017), while prior approaches involve n-gram modeling (Kneser and Ney, 1995; James, 2000; Bickel et al., 2005).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.35944962501526,
      "citing_paper_id": "233473617",
      "cited_paper_id": 15102355
    },
    {
      "context_text": "…Mnih and Hinton, 2009; Melis et al., 2018) rely on the most recent deep learning architectures, including large LSTMs (Hochreiter and Schmidhuber, 1997) or transformers (Vaswani et al., 2017), while prior approaches involve n-gram modeling (Kneser and Ney, 1995; James, 2000; Bickel et al., 2005).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.35944962501526,
      "citing_paper_id": "233473617",
      "cited_paper_id": 17477012
    },
    {
      "context_text": "…to add useful additional signals such as preceding web interaction, linking pages, similar search queries or visitor interests of a page (White et al., 2009); previous sequence of utterances (Park et al., 2018; Zhang et al., 2018; Yoo et al., 2020) or re-lated text snippets (Ke et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It refers to methods and approaches in personalized text generation but does not cite any datasets by name.",
      "processing_time": 57.59716320037842,
      "citing_paper_id": "233473617",
      "cited_paper_id": 49411029
    },
    {
      "context_text": "In this work, we focus on the application of text prediction on production-level online communication tools, to help users compose emails (Chen et al., 2019; Microsoft Text Predictions, 2020), and in addition chat messages.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the application of text prediction in production-level online communication tools, specifically for composing emails and chat messages. However, no specific dataset names are mentioned.",
      "processing_time": 58.13458275794983,
      "citing_paper_id": "233473617",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "This approach is inspired by (Chen et al., 2019), as they showed that concatenating contextual signals into a single input string gave a comparable result to more complex methods encoding these signals separately 1 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about the methodology used in the cited paper, not a dataset.",
      "processing_time": 57.38284230232239,
      "citing_paper_id": "233473617",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "Text prediction services have been deployed across popular communication tools and platforms such as (Microsoft Text Predictions, 2020) or GMail Smart Compose (Chen et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions specific platforms using text prediction services but does not refer to any specific datasets.",
      "processing_time": 55.14291477203369,
      "citing_paper_id": "233473617",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "Recently prediction service is applied on communication tools for composing email and chat messages to improve user writing productivity (Kannan et al., 2016; Deb et al., 2019; Chen et al., 2019; Microsoft Text Predictions, 2020).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works and applications. No clear identifiers for datasets are present.",
      "processing_time": 56.57293677330017,
      "citing_paper_id": "233473617",
      "cited_paper_id": 173991095
    },
    {
      "context_text": "The few works that developed multi-turn ES chatbots rely on predefined templates and handcrafted rules (Zwaan et al., 2012), which suffer from limited generality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context focuses on the limitations of predefined templates and handcrafted rules in multi-turn ES chatbots.",
      "processing_time": 58.24096965789795,
      "citing_paper_id": "252780132",
      "cited_paper_id": 387296
    },
    {
      "context_text": ", 2002), ROUGE-L (RL) (Lin, 2004), METEOR (MET) (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (ROUGE-L, METEOR, CIDEr) but does not refer to any specific datasets. These are metrics, not datasets, and thus do not meet the criteria for inclusion.",
      "processing_time": 59.751171350479126,
      "citing_paper_id": "252780132",
      "cited_paper_id": 964287
    },
    {
      "context_text": "To evaluate the generation quality, we adopt the following metrics: perplexity (PPL), BLEU-1/2/3/4 (B-1/2/3/4) (Papineni et al., 2002), ROUGE-L (RL) (Lin, 2004), METEOR (MET) (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al., 2015).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions several evaluation metrics but does not refer to any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 55.94428896903992,
      "citing_paper_id": "252780132",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "To evaluate the generation quality, we adopt the following metrics: perplexity (PPL), BLEU-1/2/3/4 (B-1/2/3/4) (Papineni et al., 2002), ROUGE-L (RL) (Lin, 2004), METEOR (MET) (Lavie and Agarwal, 2007), and CIDEr (Vedantam et al.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span mentions several metrics but does not reference any specific datasets. Metrics are excluded according to the instructions.",
      "processing_time": 55.94111227989197,
      "citing_paper_id": "252780132",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "For user state modeling, MultiESC captures the user’s subtle emotion expressed in the context by incorporating external knowledge from the NRC VAD lexicon (Mohammad, 2018).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "NRC VAD lexicon"
      ],
      "dataset_descriptions": {
        "NRC VAD lexicon": "Used to incorporate external knowledge for capturing subtle user emotions in the MultiESC model, enhancing the representation of emotional states through valence, arousal, and dominance scores."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'NRC VAD lexicon' as an external knowledge source used in the MultiESC model for capturing user emotions. The lexicon is a specific, verifiable resource.",
      "processing_time": 67.84640574455261,
      "citing_paper_id": "252780132",
      "cited_paper_id": 44090948
    },
    {
      "context_text": "Frequently, people deal with the distress by seeking Emotional Support (ES) from social interactions (Langford et al., 1997; Greene, 2003).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to general concepts and findings from the cited papers.",
      "processing_time": 56.186267614364624,
      "citing_paper_id": "252780132",
      "cited_paper_id": 44989068
    },
    {
      "context_text": "Frequently, people deal with the distress by seeking Emotional Support (ES) from social interactions (Langford et al., 1997; Greene, 2003).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to general concepts and findings from the cited papers.",
      "processing_time": 56.186267614364624,
      "citing_paper_id": "252780132",
      "cited_paper_id": 142282585
    },
    {
      "context_text": "Since early ES datasets were mainly composed of single-turn conversations (Medeiros and Bosse, 2018; Sharma et al., 2020), most research on de-",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to early ES datasets generically without naming them.",
      "processing_time": 56.43969702720642,
      "citing_paper_id": "252780132",
      "cited_paper_id": 49397755
    },
    {
      "context_text": "Thus, most of the existing research on ESC also only considers single-turn interactions with the user (Medeiros and Bosse, 2018; Sharma et al., 2020, 2021), which is over-simplified and has limited support effects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general criticism of existing research. No verifiable resources are identified.",
      "processing_time": 55.986796140670776,
      "citing_paper_id": "252780132",
      "cited_paper_id": 49397755
    },
    {
      "context_text": "by crawling post-response pairs from online forums, they only contain single-turn conversations (Medeiros and Bosse, 2018; Sharma et al., 2020).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generic data collection methods.",
      "processing_time": 55.538877964019775,
      "citing_paper_id": "252780132",
      "cited_paper_id": 49397755
    },
    {
      "context_text": "cal literature, particular procedures and strategies are indispensable for effective emotional support (Greene, 2003; Hill, 2009).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It refers to general procedures and strategies for emotional support, which are not verifiable resources.",
      "processing_time": 57.832820415496826,
      "citing_paper_id": "252780132",
      "cited_paper_id": 142282585
    },
    {
      "context_text": "cal literature, particular procedures and strategies are indispensable for effective emotional support (Greene, 2003; Hill, 2009).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, models, or methods. It refers to general procedures and strategies for emotional support, which are not verifiable resources.",
      "processing_time": 57.832820415496826,
      "citing_paper_id": "252780132",
      "cited_paper_id": 143336295
    },
    {
      "context_text": "lookahead heuristic, MultiESC tends to proactively explore the user’s situation at the beginning stage of the conversation instead of directly comforting the user, which aligns with the suggested procedure for providing emotional support (Hill, 2009).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for providing emotional support.",
      "processing_time": 55.11200308799744,
      "citing_paper_id": "252780132",
      "cited_paper_id": 143336295
    },
    {
      "context_text": "Empathetic Response Generation (ERG) (Rashkin et al., 2019) is a research area closely related to ESC, as being empathetic is a crucial ability for providing emo-",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'Empathetic Response Generation' and refers to a dataset, but does not provide a specific name. The title of the cited paper suggests the existence of a new benchmark and dataset, but the context does not explicitly mention the dataset name.",
      "processing_time": 60.80715847015381,
      "citing_paper_id": "252780132",
      "cited_paper_id": 195069365
    },
    {
      "context_text": "Empathetic Response Generation (ERG) (Rashkin et al., 2019) is a research area closely related to ESC, as being empathetic is a crucial ability for providing emotional support (Greene, 2003; Pérez-Rosas et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'Empathetic Response Generation' and 'ESC', but does not specify any dataset names. The context is about the research area and its importance, not the use of a specific dataset.",
      "processing_time": 59.020183086395264,
      "citing_paper_id": "252780132",
      "cited_paper_id": 195069365
    },
    {
      "context_text": "However, ERG does not has the explicit goal of proactively soothing the user’s negative emotion.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, and the cited paper title suggests a new benchmark and dataset, but no specific name is provided in the context.",
      "processing_time": 56.57133913040161,
      "citing_paper_id": "252780132",
      "cited_paper_id": 195069365
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.529043197631836,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et…",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.529043197631836,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "Another finding is that the three empathetic generators (i.e., MoEL, MIME, and EmpDG) achieve significantly worse perplexity and CIDEr scores than the other models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context focuses on comparing the performance of empathetic dialogue generators.",
      "processing_time": 57.01696419715881,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Another finding is that the three empathetic generators (i.e., MoEL, MIME, and EmpDG) achieve significantly worse perplexity and CIDEr scores than the other models.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their performance metrics. The context focuses on comparing the performance of empathetic dialogue generators.",
      "processing_time": 57.01696419715881,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "Prior works on emotion-related dialogue tasks mainly detect the user’s coarse-grained emotion type to enhance dialogue generation (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prior works on emotion-related dialogue tasks. No verifiable resources are identified.",
      "processing_time": 56.35767698287964,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Prior works on emotion-related dialogue tasks mainly detect the user’s coarse-grained emotion type to enhance dialogue generation (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020a).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only prior works on emotion-related dialogue tasks. No verifiable resources are identified.",
      "processing_time": 56.35767698287964,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "As shown in Table 2, we can see that the advantage of MultiESC over MoEL is substantial in all the metrics.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (MultiESC and MoEL).",
      "processing_time": 55.764780044555664,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is about comparing different empathetic response generators.",
      "processing_time": 56.07190465927124,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "Our baselines include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate state-of-the-art empathetic dialogue generation models, focusing on generating empathetic responses in conversations."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating state-of-the-art methods in empathetic dialogue generation. No other specific datasets are mentioned.",
      "processing_time": 63.46240949630737,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "MIME (Majumder et al., 2020) follows the architecture of MoEL and adds extra mechanisms to combine the results from different decoders.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions MIME, which is a method or model, not a dataset. There are no specific datasets mentioned in the citation span.",
      "processing_time": 56.387343406677246,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Instead, it only reactively generates responses that are consistent with the user’s emotion (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020a; Zheng et al., 2021; Wang et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for generating empathetic responses.",
      "processing_time": 55.29575848579407,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "Instead, it only reactively generates responses that are consistent with the user’s emotion (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020a; Zheng et al., 2021; Wang et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for generating empathetic responses.",
      "processing_time": 55.29575848579407,
      "citing_paper_id": "252780132",
      "cited_paper_id": 238582998
    },
    {
      "context_text": "Instead, it only reactively generates responses that are consistent with the user’s emotion (Lin et al., 2019; Majumder et al., 2020; Li et al., 2020a; Zheng et al., 2021; Wang et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods or approaches for generating empathetic responses.",
      "processing_time": 55.29575848579407,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "MoEL (Lin et al., 2019) adopts several decoders focusing on different types of emotional utterances, whose outputs are combined to generate the final utterances.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (MoEL) and its approach to generating emotional utterances.",
      "processing_time": 56.11777210235596,
      "citing_paper_id": "252780132",
      "cited_paper_id": 201124425
    },
    {
      "context_text": "(3)\nPrevious research on dialogue strategy prediction generally followed this history-based scheme (Zhou et al., 2019; Joshi et al., 2021; Dutt et al., 2021), though they may vary in their methods of obtaining the representations of Ht and Ut.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches in dialogue strategy prediction.",
      "processing_time": 55.04064130783081,
      "citing_paper_id": "252780132",
      "cited_paper_id": 203593798
    },
    {
      "context_text": "(3)\nPrevious research on dialogue strategy prediction generally followed this history-based scheme (Zhou et al., 2019; Joshi et al., 2021; Dutt et al., 2021), though they may vary in their methods of obtaining the representations of Ht and Ut.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general methods and approaches in dialogue strategy prediction.",
      "processing_time": 55.04064130783081,
      "citing_paper_id": "252780132",
      "cited_paper_id": 231709784
    },
    {
      "context_text": "For reference, BlenderBot-Joint, DialoGPT-Joint, and GLHG have 90M, 117M, and 92M parameters, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model sizes. No verifiable resources are identified.",
      "processing_time": 54.85045409202576,
      "citing_paper_id": "252780132",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "For reference, BlenderBot-Joint, DialoGPT-Joint, and GLHG have 90M, 117M, and 92M parameters, respectively.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only model sizes. No verifiable resources are identified.",
      "processing_time": 54.85045409202576,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "It also outperforms BlenderBotJoint in the overall supporting effects, though relatively inferior in terms of fluency, probably because the backbone of BlenderBot-Joint is extensively pre-trained on large-scale dialogue corpora (Roller et al., 2021).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large-scale dialogue corpora' but does not provide a specific name or identifier. The reference to BlenderBot-Joint is about a model, not a dataset.",
      "processing_time": 58.07800793647766,
      "citing_paper_id": "252780132",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "DialoGPT-Joint and BlenderBot-Joint (Liu et al., 2021) are developed on the backbones of DialoGPT (Zhang et al., 2020) and BlenderBot (Roller et al., 2021), respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models (DialoGPT and BlenderBot) but does not refer to any specific datasets. The context is about the development of chatbots using these models.",
      "processing_time": 58.072941303253174,
      "citing_paper_id": "252780132",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "We compare MultiESC with the three baselinses capable of strategy planning (i.e., DialoGPT-Joint, BlenderBot-Joint, and MISC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods used for comparison. The context is about comparing different chatbot models, not using datasets.",
      "processing_time": 57.18237328529358,
      "citing_paper_id": "252780132",
      "cited_paper_id": 216562425
    },
    {
      "context_text": "We compare MultiESC with the three baselinses capable of strategy planning (i.e., DialoGPT-Joint, BlenderBot-Joint, and MISC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods used for comparison. The context is about comparing different chatbot models, not using datasets.",
      "processing_time": 57.18237328529358,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "We compare MultiESC with the three baselinses capable of strategy planning (i.e., DialoGPT-Joint, BlenderBot-Joint, and MISC).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods used for comparison. The context is about comparing different chatbot models, not using datasets.",
      "processing_time": 57.18237328529358,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "MISC (Tu et al., 2022) enhances context encoding with commonsense knowledge and uses the predicted strategy distribution to guide the emotional support dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MISC' which is a method/model, not a dataset. No specific dataset is mentioned or used according to the citation context.",
      "processing_time": 57.01205611228943,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "MISC (Tu et al., 2022) enhances context encoding with commonsense knowledge and uses the predicted strategy distribution to guide the emotional support dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'MISC' which is a method/model, not a dataset. No specific dataset is mentioned or used according to the citation context.",
      "processing_time": 57.01205611228943,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "…include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate empathetic response generators and state-of-the-art methods in emotional support conversations, focusing on generating empathetic and contextually appropriate responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating empathetic response generators and state-of-the-art methods in emotional support conversations.",
      "processing_time": 64.22718691825867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "…include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate empathetic response generators and state-of-the-art methods in emotional support conversations, focusing on generating empathetic and contextually appropriate responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating empathetic response generators and state-of-the-art methods in emotional support conversations.",
      "processing_time": 64.22718691825867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "…include three empathetic response generators: MoEL (Lin et al., 2019), MIME (Majumder et al., 2020), and EmpDG (Li et al., 2020a); and four state-of-the-art methods on the ESCONV dataset: DialoGPT-Joint, BlenderBot-Joint (Liu et al., 2021), MISC (Tu et al., 2022), and GLHG (Peng et al., 2022).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ESCONV"
      ],
      "dataset_descriptions": {
        "ESCONV": "Used to evaluate empathetic response generators and state-of-the-art methods in emotional support conversations, focusing on generating empathetic and contextually appropriate responses."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the ESCONV dataset, which is used for evaluating empathetic response generators and state-of-the-art methods in emotional support conversations.",
      "processing_time": 64.22718691825867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "Following Liu et al. (2021), Peng et al. (2022) and Tu et al. (2022) recently explored data-driven multi-turn ESC systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works exploring data-driven multi-turn emotional support conversation systems.",
      "processing_time": 56.223374128341675,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "Following Liu et al. (2021), Peng et al. (2022) and Tu et al. (2022) recently explored data-driven multi-turn ESC systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works exploring data-driven multi-turn emotional support conversation systems.",
      "processing_time": 56.223374128341675,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "Following Liu et al. (2021), Peng et al. (2022) and Tu et al. (2022) recently explored data-driven multi-turn ESC systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works exploring data-driven multi-turn emotional support conversation systems.",
      "processing_time": 56.223374128341675,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "Since the codes of MISC and GLHG were not released, we directly refer to the results reported in their original papers.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to results from other papers. No verifiable resources are identified.",
      "processing_time": 56.03098678588867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "Since the codes of MISC and GLHG were not released, we directly refer to the results reported in their original papers.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to results from other papers. No verifiable resources are identified.",
      "processing_time": 56.03098678588867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "Since the codes of MISC and GLHG were not released, we directly refer to the results reported in their original papers.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to results from other papers. No verifiable resources are identified.",
      "processing_time": 56.03098678588867,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "Tu et al. (2022) proposed to enhance context encoding with commonsense knowledge and use the predicted strategy distribution to guide response generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing context encoding with commonsense knowledge.",
      "processing_time": 55.303242921829224,
      "citing_paper_id": "252780132",
      "cited_paper_id": 221761251
    },
    {
      "context_text": "Tu et al. (2022) proposed to enhance context encoding with commonsense knowledge and use the predicted strategy distribution to guide response generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for enhancing context encoding with commonsense knowledge.",
      "processing_time": 55.303242921829224,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "Previous research on dialogue strategy prediction generally followed this history-based scheme (Zhou et al., 2019; Joshi et al., 2021; Dutt et al., 2021), though they may vary in their methods of obtaining the representations of Ht and Ut.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to previous research works and their methods.",
      "processing_time": 54.97721457481384,
      "citing_paper_id": "252780132",
      "cited_paper_id": 231709784
    },
    {
      "context_text": "veloping ESC systems only considered the simplified scenario of single-turn interactions with the user (Sharma et al., 2021; Hosseini and Caragea, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.181262254714966,
      "citing_paper_id": "252780132",
      "cited_paper_id": 233238086
    },
    {
      "context_text": "AdamW (Loshchilov and Hutter, 2018) is used as optimizer; its initial learning rate is set to be 5×10−5 and adaptively decays during training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only an optimizer method (AdamW).",
      "processing_time": 54.584314823150635,
      "citing_paper_id": "252780132",
      "cited_paper_id": 235294326
    },
    {
      "context_text": "AdamW (Loshchilov and Hutter, 2018) is used as optimizer; its initial learning rate is set to be 5×10−5 and adaptively decays during training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only an optimizer method (AdamW).",
      "processing_time": 54.584314823150635,
      "citing_paper_id": "252780132",
      "cited_paper_id": null
    },
    {
      "context_text": "EmpDG (Li et al., 2020a) learns how to generate responses consistent with the user’s emotion via an adversarial learning framework.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'EmpDG' but does not provide enough information to determine if it is a dataset, method, or other resource. The title suggests it is a method for empathetic dialogue generation.",
      "processing_time": 58.5385901927948,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "The dominance dimension is not considered here as it is less relevant for capturing emotion intensity (Zhong et al., 2019; Li et al., 2020b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to previous research findings about the dominance dimension in emotion intensity.",
      "processing_time": 56.596039056777954,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": "3The dominance dimension is not considered here as it is less relevant for capturing emotion intensity (Zhong et al., 2019; Li et al., 2020b).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only refers to previous research findings about the dominance dimension in emotion intensity.",
      "processing_time": 56.69820165634155,
      "citing_paper_id": "252780132",
      "cited_paper_id": 245537585
    },
    {
      "context_text": ", 2021), MISC (Tu et al., 2022), and GLHG (Peng et al.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MISC"
      ],
      "dataset_descriptions": {
        "MISC": "Used to train a mixed strategy-aware model for emotional support conversations, integrating COMET to enhance conversational strategies and emotional responses."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'MISC' and 'GLHG', but only 'MISC' is a plausible dataset name. 'GLHG' is likely a method or model, not a dataset.",
      "processing_time": 64.56960773468018,
      "citing_paper_id": "252780132",
      "cited_paper_id": 247748640
    },
    {
      "context_text": "GLHG (Peng et al., 2022) adopts a graph neural network to model the relationships between the user’s emotion causes, intentions and the dialogue history for emotional support dialogue generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (graph neural network) and a research application (emotional support dialogue generation).",
      "processing_time": 56.54664874076843,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "Peng et al. (2022) proposed a hierarchical graph network to capture both the global context and the local user intention.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (hierarchical graph network).",
      "processing_time": 54.61099410057068,
      "citing_paper_id": "252780132",
      "cited_paper_id": 248406141
    },
    {
      "context_text": "See et al. (2017) further improved the performance on ab-stractive summarization of articles by introducing the ability to copy words from the source article (Gulcehre et al., 2016) using a pointer network (Vinyals et al., 2015), in addition to generating new words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the improvement of abstractive summarization techniques.",
      "processing_time": 56.10878300666809,
      "citing_paper_id": "44160625",
      "cited_paper_id": 969555
    },
    {
      "context_text": "However, the selection of sentences is based on topic signatures as described by Lin and Hovy (2000) and Conroy et al. (2006) instead of word frequencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or approaches for selecting sentences based on topic signatures.",
      "processing_time": 54.97141766548157,
      "citing_paper_id": "44160625",
      "cited_paper_id": 7096616
    },
    {
      "context_text": "However, the selection of sentences is based on topic signatures as described by Lin and Hovy (2000) and Conroy et al. (2006) instead of word frequencies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods or approaches for selecting sentences based on topic signatures.",
      "processing_time": 54.97141766548157,
      "citing_paper_id": "44160625",
      "cited_paper_id": 8598694
    },
    {
      "context_text": "Conroy et al. (2006) further extended the method for query-based multi-document summarization by considering sentence overlap with both query terms and topic signature words.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for query-based multi-document summarization.",
      "processing_time": 54.96704888343811,
      "citing_paper_id": "44160625",
      "cited_paper_id": 7096616
    },
    {
      "context_text": "Lin and Hovy (2000) proposed the idea of extracting topic-based signature terms for summariza-",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting topic-based signature terms.",
      "processing_time": 54.69548773765564,
      "citing_paper_id": "44160625",
      "cited_paper_id": 8598694
    },
    {
      "context_text": "Lin and Hovy (2000) proposed the idea of extracting topic-based signature terms for summarization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for extracting topic-based signature terms.",
      "processing_time": 54.686309814453125,
      "citing_paper_id": "44160625",
      "cited_paper_id": 8598694
    },
    {
      "context_text": "Early works on abstractive summarization were focused on sentence compression based approaches (Filip-pova, 2010; Berg-Kirkpatrick et al., 2011; Baner-jee et al., 2015) which connected fragments from multiple sentences to generate novel sentences for the summary or template based approaches that…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches to abstractive summarization. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.75462865829468,
      "citing_paper_id": "44160625",
      "cited_paper_id": 15467396
    },
    {
      "context_text": "Extractive summarization uses sentence-level features (Yang et al., 2017) that have been leveraged for producing query-focused or topic-based summaries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for extractive summarization.",
      "processing_time": 54.73131251335144,
      "citing_paper_id": "44160625",
      "cited_paper_id": 15827373
    },
    {
      "context_text": "One of those demographics is women, whose biographies make up only 18.58% of English Wikipedia’s biographies (Graells-Garrido et al., 2015).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "English Wikipedia’s biographies"
      ],
      "dataset_descriptions": {
        "English Wikipedia’s biographies": "Used to quantify gender bias in Wikipedia content, specifically analyzing the percentage of biographies dedicated to women. The dataset provides insights into underrepresentation issues."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions a specific statistic about the representation of women's biographies on English Wikipedia, which suggests the use of a dataset or survey data.",
      "processing_time": 64.34924244880676,
      "citing_paper_id": "229348988",
      "cited_paper_id": 1082360
    },
    {
      "context_text": "58% of English Wikipedia’s biographies (Graells-Garrido et al., 2015).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'English Wikipedia’s biographies' but does not specify a dataset name. The reference is too generic and lacks a specific, identifiable dataset.",
      "processing_time": 56.73803377151489,
      "citing_paper_id": "229348988",
      "cited_paper_id": 1082360
    },
    {
      "context_text": "This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and applications. The cited paper titles do not introduce any datasets either.",
      "processing_time": 55.85374164581299,
      "citing_paper_id": "229348988",
      "cited_paper_id": 1137329
    },
    {
      "context_text": "This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and applications. The cited paper titles do not introduce any datasets either.",
      "processing_time": 55.85374164581299,
      "citing_paper_id": "229348988",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and applications. The cited paper titles do not introduce any datasets either.",
      "processing_time": 55.85374164581299,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and applications. The cited paper titles do not introduce any datasets either.",
      "processing_time": 55.85374164581299,
      "citing_paper_id": "229348988",
      "cited_paper_id": 21850704
    },
    {
      "context_text": "This includes REINFORCE (Williams, 1992a) for Machine translation (MT) Ranzato et al. (2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al. (2016b), Dialogue Generation Li et al. (2016b), and Video Captioning (Pasunuru & Bansal, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and applications. The cited paper titles do not introduce any datasets either.",
      "processing_time": 55.85374164581299,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.633326292037964,
      "citing_paper_id": "229348988",
      "cited_paper_id": 2642042
    },
    {
      "context_text": "Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.18 There has been a recent surge of interest in these types of models across a variety of fields.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.633326292037964,
      "citing_paper_id": "229348988",
      "cited_paper_id": 207596505
    },
    {
      "context_text": "Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to Energy-Based Models (EBMs) and their historical significance. No verifiable datasets are named or described.",
      "processing_time": 57.1030535697937,
      "citing_paper_id": "229348988",
      "cited_paper_id": 2642042
    },
    {
      "context_text": "Energy Based Models for Text Energy-Based Models (EBMs) (Hinton, 2002; LeCun et al., 2006; Ranzato et al., 2007) are learning frameworks that attracted a lot of attention several decades ago.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only references to Energy-Based Models (EBMs) and their historical significance. No verifiable datasets are named or described.",
      "processing_time": 57.1030535697937,
      "citing_paper_id": "229348988",
      "cited_paper_id": 207596505
    },
    {
      "context_text": "So additionally, we report SelfBLEU-3,4,5 (Zhu et al., 2018) to measure repetitions at a distributional level across the whole set of generated samples, and also provide a token/type frequency analysis (see Fig.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and methods. The context focuses on evaluating generated text using SelfBLEU and token/type frequency analysis.",
      "processing_time": 56.82335925102234,
      "citing_paper_id": "229348988",
      "cited_paper_id": 3636178
    },
    {
      "context_text": "As metrics, we use sentiment class expectation Eφ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section §3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context focuses on evaluation metrics rather than data sources.",
      "processing_time": 55.61915922164917,
      "citing_paper_id": "229348988",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "As metrics, we use sentiment class expectation Eφ(x), the perplexity according to an external GPT2 small architecture as in (Li et al., 2018), and the diversity metrics introduced in section §3.1.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and models. The context is focused on evaluation metrics rather than data sources.",
      "processing_time": 55.61008930206299,
      "citing_paper_id": "229348988",
      "cited_paper_id": 4937880
    },
    {
      "context_text": "Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & Hernández-Lobato (2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.331823110580444,
      "citing_paper_id": "229348988",
      "cited_paper_id": 6093112
    },
    {
      "context_text": "Continuous approximation using the Gumbel Softmax was developed for the training of Variational Autoencoders but several works have implemented it for natural language generation Shetty et al. (2017); Chu & Liu (2019); Kusner & Hernández-Lobato (2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.331823110580444,
      "citing_paper_id": "229348988",
      "cited_paper_id": 59413781
    },
    {
      "context_text": "On line 3, we then use SNIS (Self Normalized Importance Sampling) (Kim & Bengio, 2016; Parshakova et al., 2019a) to estimate µ(λ) .= Ex∼pφ(x).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on the estimation technique SNIS.",
      "processing_time": 55.86430788040161,
      "citing_paper_id": "229348988",
      "cited_paper_id": 8070055
    },
    {
      "context_text": "…Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (“degeneration”): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concern about degeneration in controlled text generation.",
      "processing_time": 55.01822590827942,
      "citing_paper_id": "229348988",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "…Controlled Text Generation When using such approaches, one needs to take care of not forgetting too much of the original LM policy (“degeneration”): Liu et al. (2016a) noted that such optimization may produce adversarial examples that improve the average reward without an actual increase in…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a concern about degeneration in controlled text generation.",
      "processing_time": 55.01822590827942,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to “degeneration”, producing poor examples that improve the average reward but forgo coherence and fluency.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in dialogue system evaluation.",
      "processing_time": 54.86243009567261,
      "citing_paper_id": "229348988",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "However, such an optimization process is not infallible; Liu et al. (2016a) noted that it often leads to “degeneration”, producing poor examples that improve the average reward but forgo coherence and fluency.",
      "catation_intent": "findings",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological issue in dialogue system evaluation.",
      "processing_time": 54.86243009567261,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in evaluating dialogue systems.",
      "processing_time": 54.581262826919556,
      "citing_paper_id": "229348988",
      "cited_paper_id": 9197196
    },
    {
      "context_text": "Liu et al. (2016a), however, show that defining a combination reward accounting for text fluency is highly non-trivial and the results of directly optimizing it cannot be fully trusted.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses challenges in evaluating dialogue systems.",
      "processing_time": 54.581262826919556,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "…models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and metrics. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.01911115646362,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "…models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and metrics. The cited papers' titles do not provide additional dataset information.",
      "processing_time": 56.01911115646362,
      "citing_paper_id": "229348988",
      "cited_paper_id": 199465680
    },
    {
      "context_text": "With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and metrics. The context focuses on optimization techniques and rewards in machine translation and summarization.",
      "processing_time": 56.4807288646698,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and metrics. The context focuses on reinforcement learning and optimizing autoregressive models using task-specific rewards.",
      "processing_time": 56.905092000961304,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "With respect to rewards, some approaches for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017) directly optimize end task rewards such as BLEU and ROUGE at training time to compensate for the mismatch between the perplexity-based training of the initial model and the evaluation metrics used at test time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only optimization methods and evaluation metrics. The context focuses on the use of end-task rewards in training models for Machine Translation and Summarization.",
      "processing_time": 57.77391481399536,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on combining NLL loss with reward maximization in a mixed training objective.",
      "processing_time": 57.05582523345947,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "Wu et al. (2016); Paulus et al. (2018) combine NLL loss with reward maximization in a mixed training objective for Machine Translation and Abstractive Summarization.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on combining NLL loss with reward maximization in a mixed training objective.",
      "processing_time": 57.05582523345947,
      "citing_paper_id": "229348988",
      "cited_paper_id": 21850704
    },
    {
      "context_text": "Prior work, taking inspiration from Reinforcement Learning (RL), has aimed at inducing autoregressive models to optimize global objectives using task specific rewards such as BLEU and ROUGE for Machine Translation and Summarization (Ranzato et al., 2016; Bahdanau et al., 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and metrics. The focus is on the use of reinforcement learning techniques to optimize autoregressive models.",
      "processing_time": 56.666452407836914,
      "citing_paper_id": "229348988",
      "cited_paper_id": 14096841
    },
    {
      "context_text": "Jaques et al. (2017); Ziegler et al. (2019) propose a conservative fine-tuning approach moderated by a KL penalty between the trained policy and the original LM, discouraging large deviations.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fine-tuning sequence generation models.",
      "processing_time": 54.44004273414612,
      "citing_paper_id": "229348988",
      "cited_paper_id": 15636415
    },
    {
      "context_text": "Jaques et al. (2017; 2019) propose a conservative fine-tuning approach with a KL penalty between the trained policy and the original auto-regressive model.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fine-tuning sequence generation models.",
      "processing_time": 54.43736243247986,
      "citing_paper_id": "229348988",
      "cited_paper_id": 15636415
    },
    {
      "context_text": "Note: Csiszár & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and generalizations. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.46841239929199,
      "citing_paper_id": "229348988",
      "cited_paper_id": 18053591
    },
    {
      "context_text": "Note: Csiszár & Shields (2004) assume a finite X here, but generalizations to infinite (countable and/or continuous) X spaces are possible, see (Csiszar, 1975).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical concepts and generalizations.",
      "processing_time": 53.183769941329956,
      "citing_paper_id": "229348988",
      "cited_paper_id": 18053591
    },
    {
      "context_text": "As a proxy to perplexity, Holtzman et al. (2018) design hand-crafted rewards using a set of discriminators to ensure the quality of generated text in open-ended text generation.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method involving hand-crafted rewards and discriminators.",
      "processing_time": 54.58045172691345,
      "citing_paper_id": "229348988",
      "cited_paper_id": 21731209
    },
    {
      "context_text": "These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.05867958068848,
      "citing_paper_id": "229348988",
      "cited_paper_id": 21731209
    },
    {
      "context_text": "These techniques usually referred to as weighted decoding Holtzman et al. (2018); See et al. (2019) this however still requires a heavy search procedure and this biased estimation of sequences that satisfy the global constraint compromises fluency and coherence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only techniques and methods. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.05867958068848,
      "citing_paper_id": "229348988",
      "cited_paper_id": 67855999
    },
    {
      "context_text": "(2016), actor critic for Abstractive Summarization (Paulus et al., 2018), Imageto-Text Liu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is too vague to infer any dataset usage.",
      "processing_time": 55.45455551147461,
      "citing_paper_id": "229348988",
      "cited_paper_id": 21850704
    },
    {
      "context_text": "There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on distributional biases. No verifiable resources are named.",
      "processing_time": 55.77154541015625,
      "citing_paper_id": "229348988",
      "cited_paper_id": 52179151
    },
    {
      "context_text": "There has been a large body of work analysing these distributional biases (Blodgett et al., 2020; Stanovsky et al., 2019; Prates et al., 2020; Sheng et al., 2019a; Brown et al., 2020b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies on distributional biases. No verifiable resources are named.",
      "processing_time": 55.77154541015625,
      "citing_paper_id": "229348988",
      "cited_paper_id": 173991101
    },
    {
      "context_text": "However, the shortcomings in terms of sample diversity, of optimization techniques when training generative models for text, has recently been documented in (Caccia et al., 2020).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions shortcomings in sample diversity and optimization techniques in generative models for text, but does not specify any datasets.",
      "processing_time": 54.6314914226532,
      "citing_paper_id": "229348988",
      "cited_paper_id": 53208122
    },
    {
      "context_text": "3One possible sampling approach here would be to employ MCMC techniques, such as Metropolis-\nHastings (Robert & Casella, 2005).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Metropolis-Hastings) which is not a dataset.",
      "processing_time": 55.25982427597046,
      "citing_paper_id": "229348988",
      "cited_paper_id": 59843537
    },
    {
      "context_text": "We follow Csiszár & Shields (2004) on this question, a problem that is at the core of the field of Information Geometry (Nielsen, 2018; Amari & Nagaoka, 2000).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to theoretical works and concepts in Information Geometry.",
      "processing_time": 54.61855697631836,
      "citing_paper_id": "229348988",
      "cited_paper_id": 116976027
    },
    {
      "context_text": "According to (Csiszár, 1996), the Generalized MaxEnt of sections §2.1 and §2.2 has the “Transitivity property”.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical property of the Generalized MaxEnt method.",
      "processing_time": 54.840224504470825,
      "citing_paper_id": "229348988",
      "cited_paper_id": 118126338
    },
    {
      "context_text": "(Csiszár, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is focused on a proof and the application of information geometry.",
      "processing_time": 56.02015256881714,
      "citing_paper_id": "229348988",
      "cited_paper_id": 118126338
    },
    {
      "context_text": "According to (Csiszár, 1996), the Generalized MaxEnt of sections §2.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical concept (Generalized MaxEnt). There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.308318853378296,
      "citing_paper_id": "229348988",
      "cited_paper_id": 118126338
    },
    {
      "context_text": "(Csiszár, 1996) gives only a minimal proof sketch, but it is instructive to provide the details, as we do now, because the proof is a neat illustration of the power of information geometry for problems of the kind we consider.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It is focused on the proof and the application of information geometry.",
      "processing_time": 55.947916984558105,
      "citing_paper_id": "229348988",
      "cited_paper_id": 118126338
    },
    {
      "context_text": "9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to estimating diversity metrics. No clear, verifiable dataset names are present.",
      "processing_time": 56.10268211364746,
      "citing_paper_id": "229348988",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "During training of the policy πθ, we perform periodic evaluation as follows: every 10 minibatch gradient updates, we sample 2048 sequences of 40 tokens long, using nucleus sampling with topp = 0.9 (Holtzman et al., 2020) and estimate diversity metrics on these samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for evaluating a policy during training, including sampling sequences and estimating diversity metrics.",
      "processing_time": 56.65682005882263,
      "citing_paper_id": "229348988",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "Neural language models, such as GPT-2/3 (Radford et al., 2019; Brown et al., 2020a), pretrained on huge amounts of text, have become pre-eminent in NLP, producing texts of unprecedented quality.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. No verifiable resources are identified.",
      "processing_time": 55.476555585861206,
      "citing_paper_id": "229348988",
      "cited_paper_id": 160025533
    },
    {
      "context_text": ", 2017), or hand crafted rewards (Li et al., 2016b; Tambwekar et al., 2019) to improve certain a priori desirable features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on using reward shaping to improve desirable features in neural story plot generation.",
      "processing_time": 56.650538206100464,
      "citing_paper_id": "229348988",
      "cited_paper_id": 199465680
    },
    {
      "context_text": "Some others use heuristic rewards as in (Li et al., 2016b; Tambwekar et al., 2019), in order to improve certain a priori desirable features of generated stories or dialogues.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or approaches. The context focuses on the use of heuristic rewards to improve story or dialogue generation.",
      "processing_time": 56.53520393371582,
      "citing_paper_id": "229348988",
      "cited_paper_id": 199465680
    },
    {
      "context_text": "(2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating non-biased continuations using adversarial triggers.",
      "processing_time": 55.13462519645691,
      "citing_paper_id": "229348988",
      "cited_paper_id": 201698258
    },
    {
      "context_text": "Sheng et al. (2020) introduce a method relying on adversarial triggers (Wallace et al., 2019); this method does not de-bias the whole distribution but only obtains non-biased continuations of given prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method but does not reference any specific datasets. The focus is on the method's capability to generate non-biased continuations of given prompts.",
      "processing_time": 56.52788829803467,
      "citing_paper_id": "229348988",
      "cited_paper_id": 201698258
    },
    {
      "context_text": "Here we compare GDC to other baselines, namely Plug and Play (PPLM) (Dathathri et al., 2020) and CTRL (Keskar et al., 2019) for sentiment control.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on comparing GDC to PPLM and CTRL for sentiment control.",
      "processing_time": 57.0084924697876,
      "citing_paper_id": "229348988",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "We use the same sampling parameters across all approaches by setting the temperature T = 1.0, using top-k sampling with k = 10, and removing the repetition penalty used in CTRL (Keskar et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model parameters and methods.",
      "processing_time": 54.52378010749817,
      "citing_paper_id": "229348988",
      "cited_paper_id": 202573071
    },
    {
      "context_text": "We implement GDC and all baselines using the PyTorch framework (Paszke et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using the PyTorch framework but does not reference any specific datasets. PyTorch is a software toolkit, not a dataset.",
      "processing_time": 56.82928395271301,
      "citing_paper_id": "229348988",
      "cited_paper_id": 202786778
    },
    {
      "context_text": "P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .= ∑ x P (x) is the partition function of P .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the concept of Energy-Based Models and their normalization.",
      "processing_time": 55.3753445148468,
      "citing_paper_id": "229348988",
      "cited_paper_id": 207596505
    },
    {
      "context_text": "P (x) is an unnormalized distribution, aka an Energy-Based Model (EBM) (Hinton, 2002; LeCun et al., 2006; Bakhtin et al., 2020), of which p(x) = 1/Z P (x) is the normalized version, where Z .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about energy-based models and their normalization.",
      "processing_time": 55.9076247215271,
      "citing_paper_id": "229348988",
      "cited_paper_id": 207596505
    },
    {
      "context_text": "Tu et al. (2020) propose an energy-based method to perform inference networks from pretrained Non-Autoregressive Machine Translation models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for inference networks from pretrained Non-Autoregressive Machine Translation models.",
      "processing_time": 55.972349643707275,
      "citing_paper_id": "229348988",
      "cited_paper_id": 218486908
    },
    {
      "context_text": "Those LMs - dubbed “Stochastic Parrots” in (Bender et al., 2021) - tend to encode hegemonic biases that are harmful to marginalized populations.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the issue of biases in large language models.",
      "processing_time": 55.24465465545654,
      "citing_paper_id": "229348988",
      "cited_paper_id": 262580630
    },
    {
      "context_text": "The early work on ”Whole sentence exponential models” by (Rosenfeld et al., 2001) — which only came to our attention when preparing the final version of this paper — can be considered as a form of EBM over texts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The context focuses on describing a form of EBM over texts, which is not a dataset.",
      "processing_time": 57.47374224662781,
      "citing_paper_id": "229348988",
      "cited_paper_id": 262695955
    },
    {
      "context_text": "We start by sampling 1M sequences from GPT-2 small — a process that took us roughly 48 hours — and keeping only the ones containing “amazing” (this filtration process can be seen as a variant of rejection sampling (Casella et al., 2004)).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a process of sampling sequences from GPT-2, which is a model, not a dataset.",
      "processing_time": 57.26279330253601,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or approaches. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.377291202545166,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "Some early NLP-related EBM research is concerned with neural-based sequence labelling problems (e.g. tagging) exploiting the global sequence (Andor et al., 2016; Belanger & McCallum, 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research areas and methods.",
      "processing_time": 54.66193914413452,
      "citing_paper_id": "229348988",
      "cited_paper_id": null
    },
    {
      "context_text": "(2) It can be shown that this epsilon matching in (2) is equivalent to the Denoising Score Matching (DSM [14, 28]) with different parameterization: where s θ ˚ p x t , t q » ´ t ´ t 0 1 ´ ¯ α “ ´ ?",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Denoising Score Matching). There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.521403074264526,
      "citing_paper_id": "265608824",
      "cited_paper_id": 1152227
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": 14888175
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": 219955663
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": 227209335
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": 261081082
    },
    {
      "context_text": "The evolution of diffusion models [12, 26, 29] has significantly advanced Text-to-Image (T2I) generation, notably when paired with extensive text-image datasets [3, 23].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "LAION-5B"
      ],
      "dataset_descriptions": {
        "LAION-5B": "Used to train next-generation text-to-image models, focusing on the evolution of diffusion models and their ability to generate high-quality images from textual descriptions."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions 'extensive text-image datasets' but does not specify any particular dataset names. However, one of the cited papers (LAION-5B) is a well-known text-image dataset, which is likely the dataset referred to in the context.",
      "processing_time": 68.76245450973511,
      "citing_paper_id": "265608824",
      "cited_paper_id": null
    },
    {
      "context_text": "To accelerate sampling, DDIM [27] further proposes another sampling method as follows: x t ´ 1 “ ?",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (DDIM) for accelerating sampling in diffusion models.",
      "processing_time": 55.64839053153992,
      "citing_paper_id": "265608824",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "We use CLIP [20] for automatic metrics.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions CLIP but does not indicate it is used as a dataset. CLIP is a model, not a dataset, and thus should not be included.",
      "processing_time": 57.065632820129395,
      "citing_paper_id": "265608824",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Out of these 24 videos, 13 are sourced from the DAVIS dataset [19], 10 from the WebVid dataset [1], and 1 video is obtained from LAMP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS",
        "WebVid",
        "LAMP"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to source 13 videos for the study, focusing on video content for end-to-end retrieval and generation tasks.",
        "WebVid": "Used to source 10 videos for the study, focusing on video content for end-to-end retrieval and generation tasks.",
        "LAMP": "Used to source 1 video for the study, focusing on motion pattern learning for few-shot-based video generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets: DAVIS, WebVid, and LAMP. These are used as sources for video content in the research.",
      "processing_time": 74.9485547542572,
      "citing_paper_id": "265608824",
      "cited_paper_id": 232478955
    },
    {
      "context_text": "Out of these 24 videos, 13 are sourced from the DAVIS dataset [19], 10 from the WebVid dataset [1], and 1 video is obtained from LAMP [33].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "DAVIS",
        "WebVid",
        "LAMP"
      ],
      "dataset_descriptions": {
        "DAVIS": "Used to source 13 videos for the study, focusing on video content for end-to-end retrieval and generation tasks.",
        "WebVid": "Used to source 10 videos for the study, focusing on video content for end-to-end retrieval and generation tasks.",
        "LAMP": "Used to source 1 video for the study, focusing on motion pattern learning for few-shot-based video generation."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions three specific datasets: DAVIS, WebVid, and LAMP. These are used as sources for video content in the research.",
      "processing_time": 74.9485547542572,
      "citing_paper_id": "265608824",
      "cited_paper_id": 264172280
    },
    {
      "context_text": "This scenario, an exceedingly rare event in real-world videos, is highly improbable within standard training video datasets [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset names, only refers to 'standard training video datasets' generically.",
      "processing_time": 55.30214190483093,
      "citing_paper_id": "265608824",
      "cited_paper_id": 232478955
    },
    {
      "context_text": "Addressing this, we tackle the challenge of Motion Customiza-tion [35]—adapting pre-trained Video Diffusion Models (VDM) to produce motion-specific videos in different contexts, while maintaining the same motion patterns of target subjects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of Motion Customization and the use of Video Diffusion Models. No verifiable resources are identified.",
      "processing_time": 57.105037450790405,
      "citing_paper_id": "265608824",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Addressing this, we tackle the challenge of Motion Customiza-tion [35]—adapting pre-trained Video Diffusion Models (VDM) to produce motion-specific videos in different contexts, while maintaining the same motion patterns of target subjects.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the concept of Motion Customization and the use of Video Diffusion Models. No verifiable resources are identified.",
      "processing_time": 57.105037450790405,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263909602
    },
    {
      "context_text": "Typically, U-Net generative modules integrate temporal attention blocks after spatial attentions [11].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (U-Net generative modules).",
      "processing_time": 55.478729009628296,
      "citing_paper_id": "265608824",
      "cited_paper_id": 248006185
    },
    {
      "context_text": "Moreover, 2D convolution layers are inflated to 3D convolution layers by altering kernels [11].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological change from 2D to 3D convolution layers.",
      "processing_time": 55.14422917366028,
      "citing_paper_id": "265608824",
      "cited_paper_id": 248006185
    },
    {
      "context_text": "Video diffusion models [11, 13, 34] further attempt to model the video data distribution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The term 'video data' is too generic and lacks a specific identifier.",
      "processing_time": 56.7409393787384,
      "citing_paper_id": "265608824",
      "cited_paper_id": 248006185
    },
    {
      "context_text": "Video diffusion models [11, 13, 34] further attempt to model the video data distribution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The term 'video data' is too generic and lacks a specific identifier.",
      "processing_time": 56.7409393787384,
      "citing_paper_id": "265608824",
      "cited_paper_id": 252715883
    },
    {
      "context_text": "Video diffusion models [11, 13, 34] further attempt to model the video data distribution.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The term 'video data' is too generic and lacks a specific identifier.",
      "processing_time": 56.7409393787384,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263151295
    },
    {
      "context_text": "This observation stems from the premise that integrating the proposed motion distillation objective (17) may autonomously and accurately embed motion information within temporal attention layers [11, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method or model. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.94576287269592,
      "citing_paper_id": "265608824",
      "cited_paper_id": 248006185
    },
    {
      "context_text": "This observation stems from the premise that integrating the proposed motion distillation objective (17) may autonomously and accurately embed motion information within temporal attention layers [11, 13].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only a method or model. The cited papers' titles also do not provide additional context about datasets.",
      "processing_time": 56.94576287269592,
      "citing_paper_id": "265608824",
      "cited_paper_id": 252715883
    },
    {
      "context_text": "While cascaded diffusion pipelines [2, 9, 13, 25, 31, 34, 36] have extended this success to Text-to-Video (T2V) generation, current models lack the ability to replicate specific motions or generate diverse variations of the same motion with distinct visual attributes and backgrounds.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses limitations of current models in text-to-video generation.",
      "processing_time": 55.53387117385864,
      "citing_paper_id": "265608824",
      "cited_paper_id": 252595919
    },
    {
      "context_text": "While cascaded diffusion pipelines [2, 9, 13, 25, 31, 34, 36] have extended this success to Text-to-Video (T2V) generation, current models lack the ability to replicate specific motions or generate diverse variations of the same motion with distinct visual attributes and backgrounds.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses limitations of current models in text-to-video generation.",
      "processing_time": 55.53387117385864,
      "citing_paper_id": "265608824",
      "cited_paper_id": 252715883
    },
    {
      "context_text": "While cascaded diffusion pipelines [2, 9, 13, 25, 31, 34, 36] have extended this success to Text-to-Video (T2V) generation, current models lack the ability to replicate specific motions or generate diverse variations of the same motion with distinct visual attributes and backgrounds.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only discusses limitations of current models in text-to-video generation.",
      "processing_time": 55.53387117385864,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263151295
    },
    {
      "context_text": "In exploring video generative tasks through diffusion models, two primary approaches have emerged: leveraging foundational Video Diffusion Models (VDMs) [7, 16, 30, 35] or pre-trained Text-to-Image (T2I) models [4, 10, 15, 32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.569236755371094,
      "citing_paper_id": "265608824",
      "cited_paper_id": 257663916
    },
    {
      "context_text": "In exploring video generative tasks through diffusion models, two primary approaches have emerged: leveraging foundational Video Diffusion Models (VDMs) [7, 16, 30, 35] or pre-trained Text-to-Image (T2I) models [4, 10, 15, 32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.569236755371094,
      "citing_paper_id": "265608824",
      "cited_paper_id": 259991741
    },
    {
      "context_text": "In exploring video generative tasks through diffusion models, two primary approaches have emerged: leveraging foundational Video Diffusion Models (VDMs) [7, 16, 30, 35] or pre-trained Text-to-Image (T2I) models [4, 10, 15, 32].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only approaches and models. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 56.569236755371094,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263909602
    },
    {
      "context_text": "Given a few subject images for reference, appearance customization [8, 17, 21, 22, 24, 32] in generative models aims to fine-tune models to generate subject images in diverse contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of appearance customization in generative models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 57.59930753707886,
      "citing_paper_id": "265608824",
      "cited_paper_id": 258041269
    },
    {
      "context_text": "Given a few subject images for reference, appearance customization [8, 17, 21, 22, 24, 32] in generative models aims to fine-tune models to generate subject images in diverse contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of appearance customization in generative models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 57.59930753707886,
      "citing_paper_id": "265608824",
      "cited_paper_id": 259847576
    },
    {
      "context_text": "Given a few subject images for reference, appearance customization [8, 17, 21, 22, 24, 32] in generative models aims to fine-tune models to generate subject images in diverse contexts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of appearance customization in generative models. The cited papers' titles do not provide additional context about datasets.",
      "processing_time": 57.59930753707886,
      "citing_paper_id": "265608824",
      "cited_paper_id": 260091569
    },
    {
      "context_text": "Control-A-Video (CAV) [5] introduces a controllable T2V diffusion model utilizing con-trol signals and a first-frame conditioning strategy.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Control-A-Video) and its components. The citation is focused on describing the method rather than a dataset.",
      "processing_time": 57.795101165771484,
      "citing_paper_id": "265608824",
      "cited_paper_id": 258841645
    },
    {
      "context_text": "This capability not only enables leveraging the distilled motion but also offers prospects for further contextual editing.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is a general statement about capabilities and prospects.",
      "processing_time": 55.87487745285034,
      "citing_paper_id": "265608824",
      "cited_paper_id": 258960192
    },
    {
      "context_text": "Di-verging from traditional generative customization frameworks [21, 22, 32, 35] that rely on text prompts that “faith-fully” describe the input image or video during model fine-tuning, our framework purposedly employs “unfaithful” text prompts during the training phase.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a divergence from traditional generative customization frameworks. No verifiable resources are identified.",
      "processing_time": 56.28836441040039,
      "citing_paper_id": "265608824",
      "cited_paper_id": 259847576
    },
    {
      "context_text": "Di-verging from traditional generative customization frameworks [21, 22, 32, 35] that rely on text prompts that “faith-fully” describe the input image or video during model fine-tuning, our framework purposedly employs “unfaithful” text prompts during the training phase.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a divergence from traditional generative customization frameworks. No verifiable resources are identified.",
      "processing_time": 56.28836441040039,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263909602
    },
    {
      "context_text": "In our experiments, we choose Show-1 [34] as our VDM backbone and its publicly available pre-trained weights.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Show-1' but it is described as a VDM backbone with pre-trained weights, which indicates it is a model or method rather than a dataset.",
      "processing_time": 57.91091203689575,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263151295
    },
    {
      "context_text": "For instance, [34] initially generates a low-resolution video with strong text-video correlation, further enhancing its resolution via temporal interpolation and spatial super-resolution modules.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating videos from text.",
      "processing_time": 55.38004779815674,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263151295
    },
    {
      "context_text": "Notably, while closely aligned with our framework, Motion Director [35] lacks available code at the time of our research.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or tool called 'Motion Director'. There are no verifiable resources or datasets mentioned.",
      "processing_time": 57.031049966812134,
      "citing_paper_id": "265608824",
      "cited_paper_id": 263909602
    },
    {
      "context_text": "In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works addressing similar problems in dialogue response generation.",
      "processing_time": 56.15549182891846,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1415790
    },
    {
      "context_text": "In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works addressing similar problems in dialogue response generation.",
      "processing_time": 56.15549182891846,
      "citing_paper_id": "67855999",
      "cited_paper_id": 52159416
    },
    {
      "context_text": "…use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in dialogue response generation.",
      "processing_time": 55.62465143203735,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1415790
    },
    {
      "context_text": "…use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in dialogue response generation.",
      "processing_time": 55.62465143203735,
      "citing_paper_id": "67855999",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "…use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in dialogue response generation.",
      "processing_time": 55.62465143203735,
      "citing_paper_id": "67855999",
      "cited_paper_id": 9346726
    },
    {
      "context_text": "…use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches in dialogue response generation.",
      "processing_time": 55.62465143203735,
      "citing_paper_id": "67855999",
      "cited_paper_id": 14635535
    },
    {
      "context_text": "Well known problems, such as the genericness and repetitiveness of responses (Serban et al., 2016a), remain without a de facto solution.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general problems in dialogue generation.",
      "processing_time": 54.97901225090027,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1857011
    },
    {
      "context_text": "Nevertheless, a number of studies only use automated metrics, with no human study at all (Lowe et al., 2015; Serban et al., 2016b; Parthasarathi and Pineau, 2018).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to studies using automated metrics.",
      "processing_time": 55.28492498397827,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1857011
    },
    {
      "context_text": ", 2017b) and learning a scoring model (Lowe et al., 2017), but their value is still unclear.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or models. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 57.13946342468262,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "There are more recent attempts to ﬁnd better automatic approaches, such as adversarial evaluation (Li et al., 2017b) and learning a scoring model (Lowe et al., 2017), but their value is still unclear.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.13225483894348,
      "citing_paper_id": "67855999",
      "cited_paper_id": 1880070
    },
    {
      "context_text": "There are more recent attempts to ﬁnd better automatic approaches, such as adversarial evaluation (Li et al., 2017b) and learning a scoring model (Lowe et al., 2017), but their value is still unclear.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 57.13225483894348,
      "citing_paper_id": "67855999",
      "cited_paper_id": 14635535
    },
    {
      "context_text": "Dialogue Dialogue evaluation is relatively well understood in goal-oriented tasks, where automated approaches can be coded by measuring task completion (Bordes et al., 2017; El Asri et al., 2017; Hastie, 2012; Henderson et al., 2014; Wen et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses dialogue evaluation in goal-oriented tasks, mentioning automated approaches for measuring task completion. No specific datasets are named.",
      "processing_time": 56.42293858528137,
      "citing_paper_id": "67855999",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "Dialogue Dialogue evaluation is relatively well understood in goal-oriented tasks, where automated approaches can be coded by measuring task completion (Bordes et al., 2017; El Asri et al., 2017; Hastie, 2012; Henderson et al., 2014; Wen et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context discusses dialogue evaluation in goal-oriented tasks, mentioning automated approaches for measuring task completion. No specific datasets are named.",
      "processing_time": 56.42293858528137,
      "citing_paper_id": "67855999",
      "cited_paper_id": 60964931
    },
    {
      "context_text": "Dialogue Dialogue evaluation is relatively well understood in goal-oriented tasks, where automated approaches can be coded by measuring task completion (Hastie, 2012; El Asri et al., 2017; Henderson et al., 2014; Wen et al., 2017; Bordes et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to goal-oriented dialogue evaluation methods.",
      "processing_time": 55.25057053565979,
      "citing_paper_id": "67855999",
      "cited_paper_id": 2129889
    },
    {
      "context_text": "Other works do use human evaluations (Vinyals and Le, 2015; Venkatesh et al., 2017; Li et al., 2016b,a; Zhang et al., 2018b; Di-nan et al., 2018), typically reporting just one type of judgment (either quality or appropriateness) either using a Likert scale or pairwise comparisons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods of human evaluation. No dataset names are present in the citation span.",
      "processing_time": 56.576741218566895,
      "citing_paper_id": "67855999",
      "cited_paper_id": 3147007
    },
    {
      "context_text": "Other works do use human evaluations (Vinyals and Le, 2015; Venkatesh et al., 2017; Li et al., 2016b,a; Zhang et al., 2018b; Di-nan et al., 2018), typically reporting just one type of judgment (either quality or appropriateness) either using a Likert scale or pairwise comparisons.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods of human evaluation. No dataset names are present in the citation span.",
      "processing_time": 56.576741218566895,
      "citing_paper_id": "67855999",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "Conducting experiments on the PersonaChat task (Zhang et al., 2018b), we obtain signiﬁcantly higher engaging-ness scores than the baseline by optimizing con-trol of repetition, speciﬁcity and question-asking over multiple turns.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to evaluate dialogue systems, focusing on optimizing control of repetition, specificity, and question-asking over multiple turns to enhance engagingness."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the PersonaChat task, which is a specific dataset used for evaluating dialogue systems. The dataset is used to optimize control of repetition, specificity, and question-asking in multi-turn dialogues.",
      "processing_time": 66.15658259391785,
      "citing_paper_id": "67855999",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "PersonaChat (Zhang et al., 2018b) is a chitchat dialogue task involving two participants (two humans or a human and a bot).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PersonaChat"
      ],
      "dataset_descriptions": {
        "PersonaChat": "Used to train and evaluate dialogue agents in a chitchat setting, focusing on interactions between two participants (human-human or human-bot) to enhance conversational skills."
      },
      "confidence_score": 1.0,
      "reasoning": "PersonaChat is a specific dataset used for training and evaluating dialogue agents in a chitchat setting, involving interactions between two participants.",
      "processing_time": 63.89564108848572,
      "citing_paper_id": "67855999",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works addressing similar problems in dialog generation.",
      "processing_time": 55.63657331466675,
      "citing_paper_id": "67855999",
      "cited_paper_id": 9346726
    },
    {
      "context_text": "In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (repetition, genericness and unrelated output), in the context of single-turn response generation (Li et al., 2017a; Shen et al., 2017; Xing et al., 2017; Wang et al., 2017; Zhang et al., 2018a; Zhou et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to works addressing similar problems in dialog generation.",
      "processing_time": 55.63657331466675,
      "citing_paper_id": "67855999",
      "cited_paper_id": 14635535
    },
    {
      "context_text": "Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works on controllable neural text generation.",
      "processing_time": 56.45448708534241,
      "citing_paper_id": "67855999",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works on controllable neural text generation.",
      "processing_time": 56.45448708534241,
      "citing_paper_id": "67855999",
      "cited_paper_id": 22716243
    },
    {
      "context_text": "…generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Peng et al., 2018; Fan et al., 2018; Kikuchi et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for controlling aspects of RNN-based natural language generation.",
      "processing_time": 57.39645028114319,
      "citing_paper_id": "67855999",
      "cited_paper_id": 20981275
    },
    {
      "context_text": "…generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Peng et al., 2018; Fan et al., 2018; Kikuchi et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and approaches for controlling aspects of RNN-based natural language generation.",
      "processing_time": 57.39645028114319,
      "citing_paper_id": "67855999",
      "cited_paper_id": 22716243
    },
    {
      "context_text": "Given these constraints, we study two con-trol methods: conditional training (Fan et al., 2018; Kikuchi et al., 2016; Peng et al., 2018) and weighted decoding (Ghazvininejad et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only control methods for text generation. No verifiable resources are identified.",
      "processing_time": 56.60281181335449,
      "citing_paper_id": "67855999",
      "cited_paper_id": 22716243
    },
    {
      "context_text": "Conditional Training (Fan et al., 2018; Kikuchi et al., 2016; Peng et al., 2018) is a method to learn a sequence-to-sequence model P (y|x, z), where z is a discrete control variable.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on describing a method called Conditional Training, which is not a dataset.",
      "processing_time": 57.635035276412964,
      "citing_paper_id": "67855999",
      "cited_paper_id": 22716243
    },
    {
      "context_text": "Conditional Training (Fan et al., 2018; Kikuchi et al., 2016; Peng et al., 2018) is a method to learn a sequence-to-sequence model P ( y | x, z ) , where z is a discrete control variable .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is focused on describing a method called Conditional Training.",
      "processing_time": 57.079925775527954,
      "citing_paper_id": "67855999",
      "cited_paper_id": 22716243
    },
    {
      "context_text": "This method of controlling response-relatedness is similar to that described in (Baheti et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for generating responses in neural conversation models.",
      "processing_time": 56.05473875999451,
      "citing_paper_id": "67855999",
      "cited_paper_id": 52159416
    },
    {
      "context_text": "6 As in the ConvAI2 challenge, each of our 28 model configurations was evaluated by over 100 crowdworkers, and the results were adjusted for annotator variance via a Bayesian calibration (Kulikov et al., 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the ConvAI2 challenge but does not specify a dataset. It refers to a method for evaluating model configurations using crowdworkers and Bayesian calibration.",
      "processing_time": 57.40034794807434,
      "citing_paper_id": "67855999",
      "cited_paper_id": 53297919
    },
    {
      "context_text": "weighted average of the GloVe embeddings of the words in s, with the first principal component projected out; for full details, see Arora et al. (2017). This method of controlling response-relatedness is similar to that described in (Baheti et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods (GloVe embeddings and a technique from Arora et al. 2017).",
      "processing_time": 57.60380935668945,
      "citing_paper_id": "67855999",
      "cited_paper_id": 64908139
    },
    {
      "context_text": "Here the sentence embedding sent emb ( s ) for an utter-ance s is a weighted average of the GloVe embeddings of the words in s , with the ﬁrst principal component projected out; for full details, see Arora et al. (2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for generating sentence embeddings using GloVe. GloVe itself is a model, not a dataset.",
      "processing_time": 57.24352288246155,
      "citing_paper_id": "67855999",
      "cited_paper_id": 64908139
    },
    {
      "context_text": "Because each mobile This work was supported in part by Ministry of Science and Technology (MOST), R.O.C., under Contract 104-2218-E-002-021-MY2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only acknowledges funding support.",
      "processing_time": 55.89060091972351,
      "citing_paper_id": "15595142",
      "cited_paper_id": 1372314
    },
    {
      "context_text": "The lack of a natural strategy to model long-range dependencies [40]–[42] and the potentially error-prone back-off behavior [43] limit the performance of N-gram-based LMs.",
      "catation_intent": "research work",
      "resource_type": "limitation",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of N-gram-based language models.",
      "processing_time": 56.04279279708862,
      "citing_paper_id": "15595142",
      "cited_paper_id": 2672293
    },
    {
      "context_text": "receive different results based on their specific interests and backgrounds [1]–[4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general concept of personalized query expansion. No verifiable resources are identified.",
      "processing_time": 56.829808950424194,
      "citing_paper_id": "15595142",
      "cited_paper_id": 3344488
    },
    {
      "context_text": "If necessary, users manually correct inaccurate transcriptions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'users manually correct inaccurate transcriptions.' There are no multi-word proper nouns, acronyms, or hyphenated names with digits that could indicate a specific dataset.",
      "processing_time": 60.28651213645935,
      "citing_paper_id": "15595142",
      "cited_paper_id": 3344840
    },
    {
      "context_text": "If necessary, users manually correct inaccurate transcriptions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'users manually correct inaccurate transcriptions.' There are no multi-word proper nouns, acronyms, or hyphenated names with digits that could indicate a specific dataset.",
      "processing_time": 60.28651213645935,
      "citing_paper_id": "15595142",
      "cited_paper_id": 17048224
    },
    {
      "context_text": "If necessary, users manually correct inaccurate transcriptions.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a general reference to 'users manually correct inaccurate transcriptions.' There are no multi-word proper nouns, acronyms, or hyphenated names with digits that could indicate a specific dataset.",
      "processing_time": 60.28651213645935,
      "citing_paper_id": "15595142",
      "cited_paper_id": 221275765
    },
    {
      "context_text": "As mentioned in previous work [50], [54], adding additional auxiliary features to augment the input 1-of-N encoding features is relatively easy and helpful.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general statement about augmenting input features.",
      "processing_time": 55.219165086746216,
      "citing_paper_id": "15595142",
      "cited_paper_id": 10619968
    },
    {
      "context_text": "In this paper, we propose a general framework for per-sonalizing recurrent neural network-based language models (RNNLMs) using text posted by many individual users and their friend relationships collected from social networks.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes a general framework for personalizing RNNLMs using user-generated text and friend relationships from social networks.",
      "processing_time": 58.31585741043091,
      "citing_paper_id": "15595142",
      "cited_paper_id": 10986188
    },
    {
      "context_text": "and the language model will help in choosing the possible paths to form the n-best lists [64].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to a language model and n-best lists.",
      "processing_time": 55.95714616775513,
      "citing_paper_id": "15595142",
      "cited_paper_id": 12568952
    },
    {
      "context_text": "Language learning systems provide learners with tailored learning materials to make language learning more effective and attractive [8], [9].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general references to language learning systems and personalized training. No verifiable resources are identified.",
      "processing_time": 56.60078263282776,
      "citing_paper_id": "15595142",
      "cited_paper_id": 13441906
    },
    {
      "context_text": "The MIT movie browser [32], [33] used Amazon’s Mechanical Turk, the most well-known crowdsourcing platform, to build",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'MIT movie browser' which could be a dataset, but it lacks specific details about its use in the research. The citation does not provide enough information to confidently label it as a dataset.",
      "processing_time": 58.09196209907532,
      "citing_paper_id": "15595142",
      "cited_paper_id": 15085986
    },
    {
      "context_text": "The aim of LM personalization here is different from that of LM adaptation which has been studied for decades [13]–[20].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to language model adaptation studies.",
      "processing_time": 55.3414740562439,
      "citing_paper_id": "15595142",
      "cited_paper_id": 31924166
    },
    {
      "context_text": "Today, when users enter search terms into the search engine, users receive different results based on their speciﬁc interests and backgrounds [1]– [4].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about search results varying based on user interests and backgrounds.",
      "processing_time": 56.13027381896973,
      "citing_paper_id": "15595142",
      "cited_paper_id": 62586139
    },
    {
      "context_text": "As social media blossoms today, and given the fact that each user is a part of the social network, we can take advantage of the huge quantities of texts left on the network by large numbers of users with known relationships.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets or resources. It only discusses the general availability of user-generated content on social media.",
      "processing_time": 56.36797046661377,
      "citing_paper_id": "15595142",
      "cited_paper_id": null
    },
    {
      "context_text": "During testing, given a new user, his or her characteristic feature is extracted to augment the 1-of-N word encoding, with which the universal RNNLM is used.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It describes a method for extracting user features and using them with a universal RNNLM.",
      "processing_time": 56.986146688461304,
      "citing_paper_id": "15595142",
      "cited_paper_id": null
    },
    {
      "context_text": "To generate personalized outputs tailored to specific individuals, we utilize a pretrained face recognition network [24, 25] as an additional concept head.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a pretrained face recognition network. The context focuses on the use of a method (face recognition network) rather than a dataset.",
      "processing_time": 57.91832900047302,
      "citing_paper_id": "268553866",
      "cited_paper_id": 8923541
    },
    {
      "context_text": "To recognize user-specific individuals in images, we employ a pretrained face detector [24] and face recognition model [25].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions using a pretrained face detector and face recognition model but does not refer to any specific dataset. The cited paper titles do not introduce a dataset either.",
      "processing_time": 57.194443464279175,
      "citing_paper_id": "268553866",
      "cited_paper_id": 8923541
    },
    {
      "context_text": "Another line of work focuses on personalizing image captioning models [20, 62, 71, 81, 91].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a line of work focusing on personalizing image captioning models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.09324264526367,
      "citing_paper_id": "268553866",
      "cited_paper_id": 17618504
    },
    {
      "context_text": "Another line of work focuses on personalizing image captioning models [20, 62, 71, 81, 91].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a line of work focusing on personalizing image captioning models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.09324264526367,
      "citing_paper_id": "268553866",
      "cited_paper_id": 53022581
    },
    {
      "context_text": "Another line of work focuses on personalizing image captioning models [20, 62, 71, 81, 91].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a line of work focusing on personalizing image captioning models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.09324264526367,
      "citing_paper_id": "268553866",
      "cited_paper_id": 196202891
    },
    {
      "context_text": "Another line of work focuses on personalizing image captioning models [20, 62, 71, 81, 91].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a line of work focusing on personalizing image captioning models. The cited paper titles do not provide additional context about datasets.",
      "processing_time": 58.09324264526367,
      "citing_paper_id": "268553866",
      "cited_paper_id": 266149540
    },
    {
      "context_text": "For computing our sentence similarity metric, we utilize a BERT [26] sentence trans-former, taken from the SentenceTransformer library [67].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a BERT sentence transformer from the SentenceTransformer library, which is a method or tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.354010820388794,
      "citing_paper_id": "268553866",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For computing our sentence similarity metric, we utilize a BERT [26] sentence trans-former, taken from the SentenceTransformer library [67].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the use of a BERT sentence transformer from the SentenceTransformer library, which is a method or tool, not a dataset. No specific dataset is mentioned.",
      "processing_time": 57.354010820388794,
      "citing_paper_id": "268553866",
      "cited_paper_id": 201646309
    },
    {
      "context_text": "For the optimization process, we use AdamW [55] with a constant learning rate of 1 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (AdamW) for optimization.",
      "processing_time": 54.9926335811615,
      "citing_paper_id": "268553866",
      "cited_paper_id": 53592270
    },
    {
      "context_text": "In the context of language models, several approaches incorporate hypernetworks [34] to predict edits for specific inputs [23, 58, 59] or perform parameter-efficient model tuning [37, 52, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.433618783950806,
      "citing_paper_id": "268553866",
      "cited_paper_id": 208981547
    },
    {
      "context_text": "In the context of language models, several approaches incorporate hypernetworks [34] to predict edits for specific inputs [23, 58, 59] or perform parameter-efficient model tuning [37, 52, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.433618783950806,
      "citing_paper_id": "268553866",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "In the context of language models, several approaches incorporate hypernetworks [34] to predict edits for specific inputs [23, 58, 59] or perform parameter-efficient model tuning [37, 52, 56, 57].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.433618783950806,
      "citing_paper_id": "268553866",
      "cited_paper_id": 249642147
    },
    {
      "context_text": "[50], is a VLM model that is built around three main components: (1) a pretrained ViT-L/14 [28] vision encoder, (2) a pretrained language model [19], and (3) a trainable Querying Transformer (Q-Former) model tasked with bridging the vision-language modality gap.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and components. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.26057195663452,
      "citing_paper_id": "268553866",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "First, for the text-to-image similarity measure, we use CLIP ViT L/14 from OpenAI [28, 65] with an input resolution of 336 × 336 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about models and methodologies, not datasets.",
      "processing_time": 56.023836851119995,
      "citing_paper_id": "268553866",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "First, for the text-to-image similarity measure, we use CLIP ViT L/14 from OpenAI [28, 65] with an input resolution of 336 × 336 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about models and methodologies, not datasets.",
      "processing_time": 56.023836851119995,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We use CLIP-ViT H/14 [28, 65] as the vision encoder and MPT-1b-RedPajama-200b [75] as the language model.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (CLIP-ViT H/14 and MPT-1b-RedPajama-200b) but does not refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.45527648925781,
      "citing_paper_id": "268553866",
      "cited_paper_id": 225039882
    },
    {
      "context_text": "We use CLIP-ViT H/14 [28, 65] as the vision encoder and MPT-1b-RedPajama-200b [75] as the language model.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (CLIP-ViT H/14 and MPT-1b-RedPajama-200b) but does not refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.45527648925781,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We use CLIP-ViT H/14 [28, 65] as the vision encoder and MPT-1b-RedPajama-200b [75] as the language model.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models (CLIP-ViT H/14 and MPT-1b-RedPajama-200b) but does not refer to any specific datasets. The cited paper titles do not provide additional context to identify datasets.",
      "processing_time": 60.45527648925781,
      "citing_paper_id": "268553866",
      "cited_paper_id": null
    },
    {
      "context_text": "As discussed in the original CLIP paper [65], these challenges can be mitigated using linear heads.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (linear heads) to mitigate challenges. The context is about a methodological approach rather than a dataset.",
      "processing_time": 57.16359829902649,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Similar to BLIP, LLaVA [54] seeks to connect a fixed vision encoder with a fixed language model, in this case, CLIP ViT-L/14 [65] and Vicuna [18] models, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.23869013786316,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Similar to BLIP, LLaVA [54] seeks to connect a fixed vision encoder with a fixed language model, in this case, CLIP ViT-L/14 [65] and Vicuna [18] models, respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.23869013786316,
      "citing_paper_id": "268553866",
      "cited_paper_id": null
    },
    {
      "context_text": "For recognizing objects, we consider state-of-the-art large-scale vision models tailored for zero-shot classification and retrieval tasks, employing the recent DFN5B CLIP-ViT H/14 model [29, 65], implemented in the transformers library [83].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and libraries but does not refer to any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.41719603538513,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "For recognizing objects, we consider state-of-the-art large-scale vision models tailored for zero-shot classification and retrieval tasks, employing the recent DFN5B CLIP-ViT H/14 model [29, 65], implemented in the transformers library [83].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and libraries but does not refer to any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.41719603538513,
      "citing_paper_id": "268553866",
      "cited_paper_id": 263310452
    },
    {
      "context_text": "LLaVA [53, 54] and MiniGPT-4 [94] employ instruction-tuned language models [22, 61, 82] and extract visual features from a pretrained visual encoder (e.g., CLIP [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.23094367980957,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "LLaVA [53, 54] and MiniGPT-4 [94] employ instruction-tuned language models [22, 61, 82] and extract visual features from a pretrained visual encoder (e.g., CLIP [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.23094367980957,
      "citing_paper_id": "268553866",
      "cited_paper_id": 237416585
    },
    {
      "context_text": "LLaVA [53, 54] and MiniGPT-4 [94] employ instruction-tuned language models [22, 61, 82] and extract visual features from a pretrained visual encoder (e.g., CLIP [65]).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions models and methods but does not reference any specific datasets. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 56.23094367980957,
      "citing_paper_id": "268553866",
      "cited_paper_id": 263672058
    },
    {
      "context_text": "We then explore the embedding space of the DFN5B CLIP-ViT H/14 model [29, 65], used as our base feature extractor, showing that it too is not expressive enough to be used directly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a model (DFN5B CLIP-ViT H/14) and discusses its limitations as a feature extractor.",
      "processing_time": 58.69138312339783,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "We then explore the embedding space of the DFN5B CLIP-ViT H/14 model [29, 65], used as our base feature extractor, showing that it too is not expressive enough to be used directly.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It refers to a model (DFN5B CLIP-ViT H/14) and discusses its limitations as a feature extractor.",
      "processing_time": 58.69138312339783,
      "citing_paper_id": "268553866",
      "cited_paper_id": 263310452
    },
    {
      "context_text": "Specifically, for identifying user-specific objects, we choose to employ a simple linear classifier trained over embeddings extracted from a pre-trained CLIP model [29, 65].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a pre-trained CLIP model, which is a method/model, not a dataset.",
      "processing_time": 57.29882860183716,
      "citing_paper_id": "268553866",
      "cited_paper_id": 231591445
    },
    {
      "context_text": "Specifically, for identifying user-specific objects, we choose to employ a simple linear classifier trained over embeddings extracted from a pre-trained CLIP model [29, 65].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a pre-trained CLIP model, which is a method/model, not a dataset.",
      "processing_time": 57.29882860183716,
      "citing_paper_id": "268553866",
      "cited_paper_id": 263310452
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": 254017562
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257557484
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": 259847716
    },
    {
      "context_text": "Personalization has also been heavily studied in the context of image generation [2, 6, 30, 31, 45, 49, 60, 66, 69, 78, 80, 86].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only that personalization has been studied in the context of image generation. No verifiable resources are named.",
      "processing_time": 57.111249923706055,
      "citing_paper_id": "268553866",
      "cited_paper_id": null
    },
    {
      "context_text": "Most relevant to our work are inversion-based approaches [30] where embeddings are optimized to capture the target concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach. The context is about inversion-based approaches and optimizing embeddings, which are methods, not datasets.",
      "processing_time": 57.329046964645386,
      "citing_paper_id": "268553866",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "For the identifier, we follow DreamBooth [69] and use an existing, uncommon word when personalizing outputs for objects and use a short name when personalizing individuals.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (DreamBooth) for personalizing outputs in text-to-image generation.",
      "processing_time": 56.237576961517334,
      "citing_paper_id": "268553866",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Therefore, for personalizing outputs over objects, we follow the convention used for text-to-image personalization methods and set the concept identifier to “sks”, introduced in [69].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method or convention for setting a concept identifier.",
      "processing_time": 55.09773111343384,
      "citing_paper_id": "268553866",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "One particular area of interest is enabling a large set of edits within a single model [35, 57].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general area of interest in enabling edits within a single model.",
      "processing_time": 55.30101943016052,
      "citing_paper_id": "268553866",
      "cited_paper_id": 253735429
    },
    {
      "context_text": "[35] introduce a codebook within the language model’s intermediate feature space, storing previously learned edits.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method involving a codebook within a language model's feature space.",
      "processing_time": 55.53025245666504,
      "citing_paper_id": "268553866",
      "cited_paper_id": 253735429
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257219775
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257404891
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 258547300
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 263875678
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 266374640
    },
    {
      "context_text": "The recent remarkable progress of large language models (LLMs) [15, 18, 19, 74, 77, 77], has spurred efforts to equip them with the ability to reason over visual content [1, 3, 8, 38, 48, 51, 63, 73, 84, 87, 90, 94].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to large language models and their capabilities in reasoning over visual content.",
      "processing_time": 55.64205050468445,
      "citing_paper_id": "268553866",
      "cited_paper_id": null
    },
    {
      "context_text": "Similar model editing techniques have been explored for generative image models [5, 11, 32, 44, 76] and multi-modal learning [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model editing techniques in generative image models and multi-modal learning. No verifiable resources are identified.",
      "processing_time": 57.082120180130005,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257495777
    },
    {
      "context_text": "Similar model editing techniques have been explored for generative image models [5, 11, 32, 44, 76] and multi-modal learning [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model editing techniques in generative image models and multi-modal learning. No verifiable resources are identified.",
      "processing_time": 57.082120180130005,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257687839
    },
    {
      "context_text": "Similar model editing techniques have been explored for generative image models [5, 11, 32, 44, 76] and multi-modal learning [17].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to model editing techniques in generative image models and multi-modal learning. No verifiable resources are identified.",
      "processing_time": 57.082120180130005,
      "citing_paper_id": "268553866",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "Personalization has been explored for a range of tasks including recommendation systems [4, 13] and object retrieval [10, 21, 43, 70, 88].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks where personalization has been explored. No verifiable resources are named.",
      "processing_time": 56.02722501754761,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257766776
    },
    {
      "context_text": "Personalization has been explored for a range of tasks including recommendation systems [4, 13] and object retrieval [10, 21, 43, 70, 88].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks where personalization has been explored. No verifiable resources are named.",
      "processing_time": 56.02722501754761,
      "citing_paper_id": "268553866",
      "cited_paper_id": 264128238
    },
    {
      "context_text": "Large language models (LLMs) [93] have transformed human-computer interaction, offering users intuitive interfaces for interacting with textual information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the impact of large language models on human-computer interaction.",
      "processing_time": 55.10819458961487,
      "citing_paper_id": "268553866",
      "cited_paper_id": 257900969
    },
    {
      "context_text": "This behavior was also previously observed in text-to-image personalized techniques [2, 76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a behavior observed in text-to-image personalized techniques.",
      "processing_time": 54.66850662231445,
      "citing_paper_id": "268553866",
      "cited_paper_id": 258436985
    },
    {
      "context_text": "[81] employed a transformer to fuse visual features and text features encoding user-specific keywords.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for fusing visual and text features. No verifiable resources are identified.",
      "processing_time": 56.01282072067261,
      "citing_paper_id": "268553866",
      "cited_paper_id": 266149540
    },
    {
      "context_text": "Recently, Retrieval-Augmented Generation (RAG) has also emerged as an alternative approach for injecting knowledge into LLMs [33, 47, 79].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Retrieval-Augmented Generation).",
      "processing_time": 55.05879783630371,
      "citing_paper_id": "268553866",
      "cited_paper_id": 266359151
    },
    {
      "context_text": "Recently, VLMs have been adopted for guiding various downstream tasks such as reinforcement learning [16] and image generation [14, 68].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of vision-language models (VLMs) for reinforcement learning and image generation. No verifiable datasets are named.",
      "processing_time": 57.70237708091736,
      "citing_paper_id": "268553866",
      "cited_paper_id": 266359854
    },
    {
      "context_text": "Recently, VLMs have been adopted for guiding various downstream tasks such as reinforcement learning [16] and image generation [14, 68].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only the use of vision-language models (VLMs) for reinforcement learning and image generation. No verifiable datasets are named.",
      "processing_time": 57.70237708091736,
      "citing_paper_id": "268553866",
      "cited_paper_id": 267412750
    },
    {
      "context_text": "6 with Vicuna-7B [18] as the language model.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a language model. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.56789779663086,
      "citing_paper_id": "268553866",
      "cited_paper_id": null
    },
    {
      "context_text": "Dialogue systems can be classiﬁed into two classes: open domain dialogue systems [15, 5, 18, 12, 13] and task-oriented dialogue systems [11, 27, 24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general classifications of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.77093052864075,
      "citing_paper_id": "2963092",
      "cited_paper_id": 739696
    },
    {
      "context_text": "Dialogue systems can be classiﬁed into two classes: open domain dialogue systems [15, 5, 18, 12, 13] and task-oriented dialogue systems [11, 27, 24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general classifications of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.77093052864075,
      "citing_paper_id": "2963092",
      "cited_paper_id": 780171
    },
    {
      "context_text": "Dialogue systems can be classiﬁed into two classes: open domain dialogue systems [15, 5, 18, 12, 13] and task-oriented dialogue systems [11, 27, 24, 25, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general classifications of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.77093052864075,
      "citing_paper_id": "2963092",
      "cited_paper_id": 62561424
    },
    {
      "context_text": "Personalized dialogue systems could be categorized into rule-based dialogue systems [22, 10, 1] and learning-based dialogue systems [4, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.558921098709106,
      "citing_paper_id": "2963092",
      "cited_paper_id": 2411932
    },
    {
      "context_text": "Personalized dialogue systems could be categorized into rule-based dialogue systems [22, 10, 1] and learning-based dialogue systems [4, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.558921098709106,
      "citing_paper_id": "2963092",
      "cited_paper_id": 35725744
    },
    {
      "context_text": "Personalized dialogue systems could be categorized into rule-based dialogue systems [22, 10, 1] and learning-based dialogue systems [4, 8].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only categories of dialogue systems. No verifiable resources are identified.",
      "processing_time": 55.558921098709106,
      "citing_paper_id": "2963092",
      "cited_paper_id": 206422013
    },
    {
      "context_text": "For rule-based personalized dialogue systems, Thompson et al. [22] propose an interactive system where users can choose a place via an interactive conversational process and the system could learn user preference to improve future conversations.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system for personalized dialogue. No clear, verifiable dataset names are present.",
      "processing_time": 56.665292501449585,
      "citing_paper_id": "2963092",
      "cited_paper_id": 2411932
    },
    {
      "context_text": "Dialogue systems can be classified into two classes: open domain dialogue systems (Ritter, Cherry, and Dolan 2011; Galley et al. 2015; Serban et al. 2015; Li et al. 2016b; Mou et al. 2016) and task-oriented dialogue systems (Levin, Pieraccini, and Eckert 1997; Young et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and classifications of dialogue systems.",
      "processing_time": 55.34331488609314,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5165773
    },
    {
      "context_text": "Dialogue systems can be classified into two classes: open domain dialogue systems (Ritter, Cherry, and Dolan 2011; Galley et al. 2015; Serban et al. 2015; Li et al. 2016b; Mou et al. 2016) and task-oriented dialogue systems (Levin, Pieraccini, and Eckert 1997; Young et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers and classifications of dialogue systems.",
      "processing_time": 55.34331488609314,
      "citing_paper_id": "2963092",
      "cited_paper_id": 6078795
    },
    {
      "context_text": "2015) and learning-based dialogue systems (Casanueva et al. 2015; Genevay and Laroche 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to learning-based dialogue systems and knowledge transfer. No verifiable resources are identified.",
      "processing_time": 56.64258170127869,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "2015) and learning-based dialogue systems (Casanueva et al. 2015; Genevay and Laroche 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to learning-based dialogue systems and knowledge transfer. No verifiable resources are identified.",
      "processing_time": 56.64258170127869,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Moreover, similar to (Casanueva et al. 2015), the differences between selected source users and the target user will deteriorate the performance.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to differences between users which is too generic.",
      "processing_time": 55.54405689239502,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "Specifically, for each model, we report the mean and standard deviation of averaged reward (Genevay and Laroche 2016), averaged success rate (Casanueva et al. 2015) and averaged dialogue length over all possible target users in 5 trials with different random seeds.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods. The context focuses on reporting performance metrics across multiple trials.",
      "processing_time": 56.25993800163269,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "Some works (Casanueva et al. 2015; Genevay and Laroche 2016) have been proposed to transfer dialogue knowledge among similar users, but they did not model the difference among users, which might harm the performance in the target domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the transfer of dialogue knowledge among users but does not mention any specific datasets. The focus is on the methodology and limitations of previous works.",
      "processing_time": 57.062748670578,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "Some works (Casanueva et al. 2015; Genevay and Laroche 2016) have been proposed to transfer dialogue knowledge among similar users, but they did not model the difference among users, which might harm the performance in the target domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses the transfer of dialogue knowledge among users but does not mention any specific datasets. The focus is on the methodology and limitations of previous works.",
      "processing_time": 57.062748670578,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Sim (Casanueva et al. 2015): The dialogue system is trained with the data from both target user and the most similar user in the source domain.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'data' but does not specify a named dataset. It refers to data from target and similar users, which is too generic.",
      "processing_time": 57.08570837974548,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "(Casanueva et al. 2015) propose to initialize personalized dialogue systems for a target user with data from similar users in the source domain to improve the performance for the target user.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'data from similar users' but does not specify a named dataset. The reference is too generic and lacks a specific, identifiable dataset name.",
      "processing_time": 57.267021894454956,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5736825
    },
    {
      "context_text": "Transfer learning (Taylor and Stone 2009; Pan and Yang 2010; Tan et al. 2014; 2015; Wei, Zheng, and Yang 2016) has been applied to other tasks in dialogue systems.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only transfer learning methods and applications in dialogue systems.",
      "processing_time": 55.13340616226196,
      "citing_paper_id": "2963092",
      "cited_paper_id": 5972669
    },
    {
      "context_text": "(Zhang et al. 2017) study the personalized response generation problem in single-turn open-domain dialogue system, which does not model the multi-turn dialogue context or the dialogue state.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses personalized response generation but does not mention any specific datasets. The focus is on the methodological approach rather than a particular dataset.",
      "processing_time": 56.60495209693909,
      "citing_paper_id": "2963092",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "Transfer learning [21, 14, 19, 20, 23] has been applied to other tasks in dialogue systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only transfer learning in dialogue systems. No verifiable resources are identified.",
      "processing_time": 56.292778968811035,
      "citing_paper_id": "2963092",
      "cited_paper_id": 10014803
    },
    {
      "context_text": "…algorithm, “PriorSim” [6] in which for each target user, the policy from the most similar user in the source domain is used as a prior, “PriorAll” [6] in which for each target user, the dialogue policy trained on all the users in the source domain is used as a prior, and “All” where the policy is…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and policies. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 56.42817568778992,
      "citing_paper_id": "2963092",
      "cited_paper_id": 11193872
    },
    {
      "context_text": "Gasic et al. [6] uses transfer learning to extend a dialogue system to include a previously unseen concept.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (transfer learning) applied to a dialogue system.",
      "processing_time": 55.64056396484375,
      "citing_paper_id": "2963092",
      "cited_paper_id": 11193872
    },
    {
      "context_text": "…user in the source domain, “Bandit” [8] in which for each target user, the most useful source user is identiﬁed by a bandit algorithm, “PriorSim” [6] in which for each target user, the policy from the most similar user in the source domain is used as a prior, “PriorAll” [6] in which for each…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods or approaches. There are no clear identifiers for datasets.",
      "processing_time": 55.956815242767334,
      "citing_paper_id": "2963092",
      "cited_paper_id": 11193872
    },
    {
      "context_text": "(Gasic et al. 2014) propose an incremental scheme to adapt an existing dialogue management system to an extended domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for adapting dialogue management systems.",
      "processing_time": 55.11116886138916,
      "citing_paper_id": "2963092",
      "cited_paper_id": 12489276
    },
    {
      "context_text": "In argumentation agents, there are some works [9, 17, 16] which study personalized dialogue system.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to works studying personalized dialogue systems.",
      "processing_time": 55.46900010108948,
      "citing_paper_id": "2963092",
      "cited_paper_id": 13214001
    },
    {
      "context_text": "In argumentation agents, there are some works [9, 17, 16] which study personalized dialogue system.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general references to works studying personalized dialogue systems.",
      "processing_time": 55.46900010108948,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15788992
    },
    {
      "context_text": "Genevay and Laroche [8] propose to select and transfer an optimized policy from source users to a target user by using a multi-armed stochastic bandit algorithm which does not require a predeﬁned user similarity measure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for user adaptation in spoken dialogue systems.",
      "processing_time": 54.613994121551514,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Bandit (Genevay and Laroche 2016): For each target user, the most useful source user is identified by a bandit algorithm.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It describes a method for identifying useful source users using a bandit algorithm.",
      "processing_time": 56.332767963409424,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Genevay and Laroche (Genevay and Laroche 2016) propose to select and transfer an optimized policy from source users to a target user by using a multi-armed stochastic bandit algorithm which does not require a predefined user similarity measure.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses a method for user adaptation in spoken dialogue systems using a multi-armed stochastic bandit algorithm, but does not mention any specific datasets.",
      "processing_time": 57.053812742233276,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Specifically, for each model, we report the mean and standard deviation of averaged reward (Genevay and Laroche 2016), averaged success rate (Casanueva et al.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and methods. The context is focused on reporting performance metrics rather than using a particular dataset.",
      "processing_time": 56.9084415435791,
      "citing_paper_id": "2963092",
      "cited_paper_id": 15497443
    },
    {
      "context_text": "Personalization frameworks proposed in [10, 1] extract and utilize user-related facts (triples), and then generate responses by applying predeﬁned templates to these facts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using user-related facts (triples) for personalization. No clear, verifiable dataset names are provided.",
      "processing_time": 57.937556982040405,
      "citing_paper_id": "2963092",
      "cited_paper_id": 35725744
    },
    {
      "context_text": "Personalization frameworks proposed in [10, 1] extract and utilize user-related facts (triples), and then generate responses by applying predeﬁned templates to these facts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using user-related facts (triples) for personalization. No clear, verifiable dataset names are provided.",
      "processing_time": 57.937556982040405,
      "citing_paper_id": "2963092",
      "cited_paper_id": 206422013
    },
    {
      "context_text": "We adopt an online stochastic gradient descent algorithm (Bottou 2010) with a learning rate 0.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (stochastic gradient descent).",
      "processing_time": 54.970967531204224,
      "citing_paper_id": "2963092",
      "cited_paper_id": 115963355
    },
    {
      "context_text": "We use the value iteration method (Bellman 1957) to learn both the general and personal Q-functions.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (value iteration) but does not reference any specific dataset. The context is about learning Q-functions, which is a methodological approach.",
      "processing_time": 56.94313287734985,
      "citing_paper_id": "2963092",
      "cited_paper_id": 123329493
    },
    {
      "context_text": "For each task, we jointly learn a 60,000 sub-word units with Byte-Pair Encoding (Sennrich et al., 2016) between source and target languages.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Byte-Pair Encoding (BPE) as a method for learning sub-word units, but does not reference any specific dataset.",
      "processing_time": 56.371506214141846,
      "citing_paper_id": "146808476",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "…et al., 2006; Collobert & Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) and sentence-level representations (Kiros et al., 2015; Logeswaran & Lee, 2018; Le & Mikolov, 2014), as well as context sensitive features from the NMT model (McCann et al., 2017) and ELMo (Peters et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 55.92976117134094,
      "citing_paper_id": "146808476",
      "cited_paper_id": 2407601
    },
    {
      "context_text": "…et al., 2006; Collobert & Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) and sentence-level representations (Kiros et al., 2015; Logeswaran & Lee, 2018; Le & Mikolov, 2014), as well as context sensitive features from the NMT model (McCann et al., 2017) and ELMo (Peters et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No dataset names are present in the text.",
      "processing_time": 55.92976117134094,
      "citing_paper_id": "146808476",
      "cited_paper_id": 9447219
    },
    {
      "context_text": "…and features to the downstream tasks, which includes word-level representations (Brown et al., 1992; Ando & Zhang, 2005; Blitzer et al., 2006; Collobert & Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) and sentence-level representations (Kiros et al., 2015; Logeswaran & Lee,…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models for generating word-level and sentence-level representations.",
      "processing_time": 55.93256735801697,
      "citing_paper_id": "146808476",
      "cited_paper_id": 2617020
    },
    {
      "context_text": "For NMT, we mainly investigate the zero-resource (unsupervised) setting, as unsupervised NMT has become a challenging task in recent years (Artetxe et al., 2017; Lample et al., 2017; 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only unsupervised NMT as a research area. The cited papers do not provide additional dataset names.",
      "processing_time": 56.929983377456665,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3518190
    },
    {
      "context_text": "For NMT, we mainly investigate the zero-resource (unsupervised) setting, as unsupervised NMT has become a challenging task in recent years (Artetxe et al., 2017; Lample et al., 2017; 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only unsupervised NMT as a research area. The cited papers do not provide additional dataset names.",
      "processing_time": 56.929983377456665,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5033497
    },
    {
      "context_text": "…NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for neural machine translation and text summarization.",
      "processing_time": 55.9369330406189,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3518190
    },
    {
      "context_text": "…NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various methods and models for neural machine translation and text summarization.",
      "processing_time": 55.9369330406189,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5033497
    },
    {
      "context_text": "Different from Artetxe et al. (2017); Lample et al. (2017; 2018); Leng et al. (2019), we just use back-translation to generate pseudo bilingual data for training, without using denoising auto-encoder6.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses a method (back-translation) and contrasts it with other methods.",
      "processing_time": 56.68237090110779,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3518190
    },
    {
      "context_text": ", 2017) is a challenging task in artificial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications of language generation and related works. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.435689210891724,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3518190
    },
    {
      "context_text": ", 2017) is a challenging task in artificial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only various applications of language generation and related works. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 57.435689210891724,
      "citing_paper_id": "146808476",
      "cited_paper_id": 44172616
    },
    {
      "context_text": "…2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017; Fedus et al., 2018) and conversational response generation (Shang et al., 2015; Vinyals…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and papers. There are no clear identifiers for datasets.",
      "processing_time": 56.25906324386597,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Sequence to sequence learning (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The context is about sequence-to-sequence learning and neural machine translation, which are methods, not datasets.",
      "processing_time": 58.191325426101685,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Sequence to sequence learning (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The context is about sequence-to-sequence learning and neural machine translation, which are methods, not datasets.",
      "processing_time": 58.191325426101685,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "Sequence to sequence learning (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The context is about sequence-to-sequence learning and neural machine translation, which are methods, not datasets.",
      "processing_time": 58.191325426101685,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "As shown in Figure 3, MASS outperforms the baseline models that are trained only on 8 The popular encoder-decoder based model structures (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017) all adopt residual connection (He et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.52615427970886,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "…2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017; Fedus et al., 2018) and conversational response generation (Shang et al., 2015;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and papers. There are no clear identifiers for datasets.",
      "processing_time": 56.086556911468506,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "…some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. No verifiable resources are identified.",
      "processing_time": 55.60689616203308,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "…some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. No verifiable resources are identified.",
      "processing_time": 55.60689616203308,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "…some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. No verifiable resources are identified.",
      "processing_time": 55.60689616203308,
      "citing_paper_id": "146808476",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and models. No verifiable resources are identified.",
      "processing_time": 55.60689616203308,
      "citing_paper_id": "146808476",
      "cited_paper_id": 18318429
    },
    {
      "context_text": "…intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers on neural machine translation. No verifiable resources are identified.",
      "processing_time": 56.42459440231323,
      "citing_paper_id": "146808476",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Different from Artetxe et al. (2017); Lample et al. (2017; 2018); Leng et al. (2019), we just use back-translation to generate pseudo bilingual data for training, without using denoising auto-encoder 6 .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'pseudo bilingual data' generated through back-translation, but does not specify a named dataset. The citation is focused on methodology rather than a specific dataset.",
      "processing_time": 57.62279415130615,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5033497
    },
    {
      "context_text": "For unsupervised NMT, we directly ﬁne-tune the pre-trained model on monolingual data with back-translation loss (Lample et al., 2018), instead of using additional denoising auto-encoder loss as in Lample et al. (2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'monolingual data' but does not specify a named dataset. The citation is focused on the methodology rather than a specific dataset.",
      "processing_time": 56.6659038066864,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5033497
    },
    {
      "context_text": "On all the 6 translation directions, our method outperforms all of the previous results, including the meth-ods without pre-training (Lample et al., 2018) and with pre-training (Lample & Conneau, 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and results. The context focuses on performance comparisons between different methods.",
      "processing_time": 55.89280962944031,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5033497
    },
    {
      "context_text": "…2017; Vaswani et al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.41219878196716,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "…language generation aims to generate natural language sentences conditioned on some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017)…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and methods. No verifiable resources are identified.",
      "processing_time": 55.5939838886261,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "…2017; Vaswani et al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 57.54276251792908,
      "citing_paper_id": "146808476",
      "cited_paper_id": 5590763
    },
    {
      "context_text": "During fine-tuning, we use Adam optimizer (Kingma & Ba, 2015) with initial learning rate 10−4, and the batch size is set as 2000 tokens for each GPU.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the Adam optimizer and training parameters.",
      "processing_time": 54.68793249130249,
      "citing_paper_id": "146808476",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "We use Adam optimizer (Kingma & Ba, 2015) with a learning rate of 10−4 for the pre-training.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the Adam optimizer, which is a method, not a dataset. No datasets are mentioned in the citation.",
      "processing_time": 55.87295341491699,
      "citing_paper_id": "146808476",
      "cited_paper_id": 6628106
    },
    {
      "context_text": "Fine-tuning approaches mainly pre-train a model on language modeling objective and then ﬁne-tune the model on the downstream tasks with supervised data (Dai & Le, 2015; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on fine-tuning approaches and pre-training models.",
      "processing_time": 56.62636375427246,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7138078
    },
    {
      "context_text": "Dai & Le (2015); Ra-machandran et al. (2016) leverage a language model or auto-encoder to pre-train the encoder and decoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods (language model, auto-encoder).",
      "processing_time": 55.29963541030884,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7138078
    },
    {
      "context_text": "Correspondence to: Tao Qin < tao-qin@microsoft.com > . while pre-training has plenty of data (Girshick et al., 2014; Szegedy et al., 2015; Ouyang et al., 2015; Dai & Le, 2015; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to pre-training methods and models. No verifiable resources are identified.",
      "processing_time": 56.158796310424805,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7138078
    },
    {
      "context_text": "Correspondence to: Tao Qin < tao-qin@microsoft.com > . while pre-training has plenty of data (Girshick et al., 2014; Szegedy et al., 2015; Ouyang et al., 2015; Dai & Le, 2015; Howard & Ruder, 2018; Radford et al., 2018; Devlin et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to pre-training methods and models. No verifiable resources are identified.",
      "processing_time": 56.158796310424805,
      "citing_paper_id": "146808476",
      "cited_paper_id": 206592484
    },
    {
      "context_text": "Experimental Setting Conversational response generation generates a ﬂexible response for the conversation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of conversational response generation. No verifiable resources are identified.",
      "processing_time": 56.149373054504395,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "Experimental Setting Conversational response generation generates a ﬂexible response for the conversation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of conversational response generation. No verifiable resources are identified.",
      "processing_time": 56.149373054504395,
      "citing_paper_id": "146808476",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017; Fedus et al., 2018) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and tasks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.986546993255615,
      "citing_paper_id": "146808476",
      "cited_paper_id": 7356547
    },
    {
      "context_text": "…et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017; Fedus et al., 2018) and conversational response generation (Shang et al., 2015; Vinyals & Le, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only research areas and tasks. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.986546993255615,
      "citing_paper_id": "146808476",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…mainly leverage pre-training to provide language representations and features to the downstream tasks, which includes word-level representations (Brown et al., 1992; Ando & Zhang, 2005; Blitzer et al., 2006; Collobert & Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models for pre-training language representations.",
      "processing_time": 55.52844429016113,
      "citing_paper_id": "146808476",
      "cited_paper_id": 10986188
    },
    {
      "context_text": "…al., 2017) is a challenging task in artiﬁcial intelligence, and covers a variety of language generation applications such as NMT (Cho et al., 2014; Bahdanau et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various papers on neural machine translation. No verifiable resources are identified.",
      "processing_time": 56.34664845466614,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "…pairs: English-French and English-German, and outperform the previous unsupervised NMT method (Lam-ple & Conneau, 2019) by more than 4 points on English-French and 1 point on French-English in terms of BLEU score, and even beating the early attention-based supervised model (Bahdanau et al., 2015b).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and performance metrics. The context focuses on comparing different neural machine translation methods.",
      "processing_time": 56.3447847366333,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "…aims to generate natural language sentences conditioned on some inputs, including tasks like neural machine translation (NMT) (Cho et al., 2014; Bahdanau et al., 2015a; Vaswani et al., 2017), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017) and…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and methods. No verifiable resources are identified.",
      "processing_time": 55.36827278137207,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "Attention mechanism (Bahdanau et al., 2015a) is further introduced between the encoder and decoder to ﬁnd which source representation to focus on when predicting the current token.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention mechanism).",
      "processing_time": 54.41646146774292,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "…language understanding tasks such as sentiment classiﬁcation (Socher et al., 2013), natural language inference (Bowman et al., 2015), named entity recognition (Tjong Kim Sang & De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016), which usually have limited supervised data.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SQuAD"
      ],
      "dataset_descriptions": {
        "SQuAD": "Used to illustrate the challenge of limited supervised data in question answering tasks, emphasizing the need for robust models in natural language processing."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions SQuAD, which is a well-known dataset for question answering. It is used to highlight the challenge of limited supervised data in various NLP tasks.",
      "processing_time": 63.636385440826416,
      "citing_paper_id": "146808476",
      "cited_paper_id": 11816014
    },
    {
      "context_text": "We report the results with perplexity (PPL) following Vinyals & Le (2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (perplexity) and a reference to a method/paper. No verifiable resources are identified.",
      "processing_time": 57.46840047836304,
      "citing_paper_id": "146808476",
      "cited_paper_id": 12300158
    },
    {
      "context_text": "…to provide language representations and features to the downstream tasks, which includes word-level representations (Brown et al., 1992; Ando & Zhang, 2005; Blitzer et al., 2006; Collobert & Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) and sentence-level representations…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only references methods and models for language representations.",
      "processing_time": 55.71174645423889,
      "citing_paper_id": "146808476",
      "cited_paper_id": 13650160
    },
    {
      "context_text": "uage Generation Method Setting en - fr fr - en en - de de - en en - ro ro - en Artetxe et al. (2017) 2-layer RNN 15.13 15.56 6.89 10.16 - - Lample et al. (2017) 3-layer RNN 15.05 14.31 9.75 13.33 - - Yang et al. (2018) 4-layer Transformer 16.97 15.58 10.86 14.62 - - Lample et al. (2018) 4-layer Transformer 25.14 24.18 17.16 21.00 21.18 19.44 MASS 4-layer Transformer 27.41 27.09 18.21 23.37 22.37 20.74 XLM (Lample &",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and performance metrics. No verifiable resources are identified.",
      "processing_time": 55.70763921737671,
      "citing_paper_id": "146808476",
      "cited_paper_id": 13748556
    },
    {
      "context_text": "Zhang & Zong (2016) designed a sentence reordering task for pre-training, but only for the encoder part of the encoder-decoder model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for pre-training the encoder part of an encoder-decoder model.",
      "processing_time": 56.11038303375244,
      "citing_paper_id": "146808476",
      "cited_paper_id": 17667087
    },
    {
      "context_text": "…Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017; Fedus et al., 2018) and conversational…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research areas and papers. No verifiable resources are identified.",
      "processing_time": 55.90594673156738,
      "citing_paper_id": "146808476",
      "cited_paper_id": 18318429
    },
    {
      "context_text": "…Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring et al., 2017), question answering (Yuan et al., 2017;…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various research works and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 56.78504800796509,
      "citing_paper_id": "146808476",
      "cited_paper_id": 44172616
    },
    {
      "context_text": "…et al., 2015a; Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Tan et al., 2019; Artetxe et al., 2017; Lample et al., 2017; 2018; He et al., 2018; Hassan et al., 2018; Song et al., 2018; Shen et al., 2018), text summarization (Ayana et al., 2016; Suzuki & Nagata, 2017; Gehring…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various research works. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 57.38308668136597,
      "citing_paper_id": "146808476",
      "cited_paper_id": 54088698
    },
    {
      "context_text": "For example, in computer vision, models are usually pre-trained on the large scale ImageNet dataset and then ﬁne-tuned on downstream tasks like object detection (Szegedy et al., 2015; Ouyang et al., 2015) or image segmentation (Girshick et al., 2014).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ImageNet"
      ],
      "dataset_descriptions": {
        "ImageNet": "Used for pre-training models in computer vision, specifically for tasks like object detection and image segmentation, but not directly for personalized text generation."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions the ImageNet dataset, which is a large-scale dataset commonly used for pre-training models in computer vision. However, the context does not specify its use in personalized text generation.",
      "processing_time": 64.95579743385315,
      "citing_paper_id": "146808476",
      "cited_paper_id": 206592484
    },
    {
      "context_text": "We ﬁne-tune the pre-trained model on text summarization task with different scales (10K, 100K, 1M and 3.8M) of training data from the Gigaword corpus (Graff et al., 2003) 9 , which consists of a total of 3.8M article-title pairs in English.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Gigaword corpus"
      ],
      "dataset_descriptions": {
        "Gigaword corpus": "Used to fine-tune a pre-trained model on text summarization tasks, specifically evaluating performance across different scales of training data (10K, 100K, 1M, and 3.8M article-title pairs)."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the Gigaword corpus, which is a well-known dataset used for text summarization tasks. The dataset is used to fine-tune a pre-trained model on different scales of training data.",
      "processing_time": 70.3209867477417,
      "citing_paper_id": "146808476",
      "cited_paper_id": null
    },
    {
      "context_text": "We also compare our results to the standard MPC (Bar-Yossef and Kraus, 2011).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method or approach called 'MPC'. The context is about comparing results, but no dataset is explicitly named.",
      "processing_time": 57.20579695701599,
      "citing_paper_id": "5045941",
      "cited_paper_id": 1623913
    },
    {
      "context_text": "In addition, we evaluate the systems on a second real-world dataset from a production search engine in the biomedical domain, PubMed (Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018), that was created in the same manner.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubMed"
      ],
      "dataset_descriptions": {
        "PubMed": "Used to evaluate systems in the biomedical domain, focusing on real-world performance in a production search engine setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PubMed' as a dataset used for evaluating systems in the biomedical domain. It is a specific, verifiable resource with clear provenance.",
      "processing_time": 62.206687688827515,
      "citing_paper_id": "5045941",
      "cited_paper_id": 8221103
    },
    {
      "context_text": "In addition, we evaluate the systems on a second real-world dataset from a production search engine in the biomedical domain, PubMed (Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018), that was created in the same manner.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PubMed"
      ],
      "dataset_descriptions": {
        "PubMed": "Used to evaluate systems in the biomedical domain, focusing on real-world performance in a production search engine setting."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'PubMed' as a dataset used for evaluating systems in the biomedical domain. It is a specific, verifiable resource with clear provenance.",
      "processing_time": 62.206687688827515,
      "citing_paper_id": "5045941",
      "cited_paper_id": 28572328
    },
    {
      "context_text": "was previously observed (Jozefowicz et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a previous observation about recurrent network architectures.",
      "processing_time": 55.056249380111694,
      "citing_paper_id": "5045941",
      "cited_paper_id": 9668607
    },
    {
      "context_text": "In preliminary experiments, we tried various forms of RNNs: vanilla RNNs, GRUs and LSTMs. GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only neural network architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.813889503479004,
      "citing_paper_id": "5045941",
      "cited_paper_id": 9668607
    },
    {
      "context_text": "In preliminary experiments, we tried various forms of RNNs: vanilla RNNs, GRUs and LSTMs. GRUs performed similarly to LSTM with a smaller computational complexity due to fewer parameters to learn as was previously observed (Jozefowicz et al., 2015).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only neural network architectures. The cited papers do not provide additional context to identify datasets.",
      "processing_time": 55.813889503479004,
      "citing_paper_id": "5045941",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "Each GRU cell is activated with ReLu(x) = x+ and gradients are clipped to a norm of 0.5 to avoid gradient exploding problems.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only technical details about GRU cells and gradient clipping.",
      "processing_time": 54.730398654937744,
      "citing_paper_id": "5045941",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "Both gated recurrent units (GRU) (Cho et al., 2014) and long-short term memory cells (LSTMs) solve this limitation — albeit with a different approach — and are increasingly used.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural network architectures (GRU and LSTM).",
      "processing_time": 54.72671055793762,
      "citing_paper_id": "5045941",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "Although GRUs have less expressive power than LSTMs, their smaller number of parameters to train allowed them to better converge than all LSTM models we tested, including that of (Park and Chiba, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between GRUs and LSTMs. No verifiable resources are identified.",
      "processing_time": 56.10596537590027,
      "citing_paper_id": "5045941",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "We feed this input vector into 2 layers of 1024 GRUs2, each followed by a dropout layer (with a dropout rate of 50",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only a neural network architecture. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.67199921607971,
      "citing_paper_id": "5045941",
      "cited_paper_id": 11336213
    },
    {
      "context_text": "search (Lankinen et al., 2016), where systems are typically less used, with a wider range of possible queries.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to systems and queries. No verifiable resources are identified.",
      "processing_time": 55.670109033584595,
      "citing_paper_id": "5045941",
      "cited_paper_id": 13113319
    },
    {
      "context_text": "This becomes a bigger problem in academic search (Lankinen et al., 2016), where systems are typically less used, with a wider range of possible queries.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It discusses a general issue in academic search systems.",
      "processing_time": 55.507344484329224,
      "citing_paper_id": "5045941",
      "cited_paper_id": 13113319
    },
    {
      "context_text": "By appending these results to MPC’s and re-ranking the list with LambdaMART (Burges, 2010) in another step as suggested in previous work (Mitra and Craswell, 2015), they achieve state-of-the-art performance in neural query auto completion at the cost of a higher complexity and more computation time.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The focus is on the technique of re-ranking lists using LambdaMART for neural query auto-completion.",
      "processing_time": 57.35754871368408,
      "citing_paper_id": "5045941",
      "cited_paper_id": 15960087
    },
    {
      "context_text": "…the field has recently started to shift towards deep learningbased models, which can be categorized into two main classes: semantic models (using Convolutional Neural Nets, or CNNs) (Mitra and Craswell, 2015) and language models (using Recurrent Neural Nets, or RNNs) (Park and Chiba, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.037001609802246,
      "citing_paper_id": "5045941",
      "cited_paper_id": 15960087
    },
    {
      "context_text": "While QAC has been well studied, the field has recently started to shift towards deep learningbased models, which can be categorized into two main classes: semantic models (using Convolutional Neural Nets, or CNNs) (Mitra and Craswell, 2015) and language models (using Recurrent Neural Nets, or RNNs) (Park and Chiba, 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on categorizing deep learning models for query auto-completion.",
      "processing_time": 56.30754995346069,
      "citing_paper_id": "5045941",
      "cited_paper_id": 15960087
    },
    {
      "context_text": "Recent advances in deep learning, particularly in semantic modeling (Mitra and Craswell, 2015) and neural language modeling (Park and Chiba, 2017) showed promising results for predicting rare queries.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research papers and their contributions to semantic modeling and neural language modeling.",
      "processing_time": 55.729499101638794,
      "citing_paper_id": "5045941",
      "cited_paper_id": 15960087
    },
    {
      "context_text": "Still, these preliminary approaches have yet to integrate standards in QAC, e.g. query personalization (Koutrika and Ioannidis, 2005; Margaris et al., 2018) and time sensitivity (Cai et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to query personalization and time sensitivity in QAC systems.",
      "processing_time": 55.503328800201416,
      "citing_paper_id": "5045941",
      "cited_paper_id": 20277316
    },
    {
      "context_text": "query personalization (Koutrika and Ioannidis, 2005; Margaris et al., 2018) and time sensitivity (Cai et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts of query personalization and time sensitivity.",
      "processing_time": 54.915751934051514,
      "citing_paper_id": "5045941",
      "cited_paper_id": 20277316
    },
    {
      "context_text": "In practice, most search engines today use query auto completion (QAC) systems, consisting of suggesting queries as users type in the search box (Fiorini et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general concept of query auto completion systems.",
      "processing_time": 54.589252948760986,
      "citing_paper_id": "5045941",
      "cited_paper_id": 28572328
    },
    {
      "context_text": "The AOL query logs (Pass et al., 2006) are commonly used to evaluate the quality of QAC systems.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AOL query logs"
      ],
      "dataset_descriptions": {
        "AOL query logs": "Used to evaluate the quality of QAC systems, focusing on the effectiveness of query auto-completion and correction mechanisms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the AOL query logs, which is a specific dataset used for evaluating QAC systems. The dataset is clearly identified and its usage is described.",
      "processing_time": 63.40596270561218,
      "citing_paper_id": "5045941",
      "cited_paper_id": 28918994
    },
    {
      "context_text": "Predicting the next characters or words following a prefix has had multiple uses from helping handicapped people (Swiffin et al., 1987) to, more recently, helping search engine users (Cai et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of predictive techniques.",
      "processing_time": 54.37570118904114,
      "citing_paper_id": "5045941",
      "cited_paper_id": 61246569
    },
    {
      "context_text": "Predicting the next characters or words following a prefix has had multiple uses from helping handicapped people (Swiffin et al., 1987) to, more recently, helping search engine users (Cai et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of predictive text generation.",
      "processing_time": 54.42160224914551,
      "citing_paper_id": "5045941",
      "cited_paper_id": 61246569
    },
    {
      "context_text": "Rendle et al. (2009); Quadrana et al. (2018); Ueda et al. (2011) explore similar schemes for personalized recommendation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works exploring personalized recommendation schemes.",
      "processing_time": 55.11123037338257,
      "citing_paper_id": "202120896",
      "cited_paper_id": 3518497
    },
    {
      "context_text": "Rendle et al. (2009); Quadrana et al. (2018); Ueda et al. (2011) explore similar schemes for personalized recommendation.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other works exploring personalized recommendation schemes.",
      "processing_time": 55.11123037338257,
      "citing_paper_id": "202120896",
      "cited_paper_id": 113580234
    },
    {
      "context_text": "The Prior Name model achieves the best UMA and MRR by a large margin, revealing that prior recipe names are strong signals for personalization.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model and evaluation metrics. The context is focused on the performance of a model rather than the use of a dataset.",
      "processing_time": 57.36231732368469,
      "citing_paper_id": "202120896",
      "cited_paper_id": 5232546
    },
    {
      "context_text": "We measure user matching accuracy (UMA)—the proportion where the gold user is ranked highest—and Mean Reciprocal Rank (MRR) (Radev et al., 2002) of the gold user.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics (UMA and MRR).",
      "processing_time": 54.87427115440369,
      "citing_paper_id": "202120896",
      "cited_paper_id": 5232546
    },
    {
      "context_text": "Kiddon et al. (2015); Bosselut et al. (2018b) model recipes as a structured collection of ingredient entities acted upon by cooking actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods for handling recipe data. No clear, verifiable dataset names are provided.",
      "processing_time": 56.35119032859802,
      "citing_paper_id": "202120896",
      "cited_paper_id": 6736944
    },
    {
      "context_text": "Kiddon et al. (2015); Bosselut et al. (2018b) model recipes as a structured collection of ingredient entities acted upon by cooking actions.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods for handling recipe data. No clear, verifiable dataset names are provided.",
      "processing_time": 56.35119032859802,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13700629
    },
    {
      "context_text": "We initially adapted the Neural Checklist Model of Kiddon et al. (2016) as a baseline; however, we ultimately use a simple Encoder-Decoder baseline with ingredient attention ( Enc-Dec ), which provides comparable performance and lower complexity.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is about adapting a model for a baseline, not using a dataset.",
      "processing_time": 56.57866168022156,
      "citing_paper_id": "202120896",
      "cited_paper_id": 9818013
    },
    {
      "context_text": "These users with limited knowledge cannot rely on existing recipe generation approaches that focus on creating coherent recipes given all ingredients and a recipe name (Kiddon et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach for generating coherent recipes.",
      "processing_time": 54.70915246009827,
      "citing_paper_id": "202120896",
      "cited_paper_id": 9818013
    },
    {
      "context_text": "Kiddon et al. (2016) imposes a ‘checklist’ attention constraint emphasizing hitherto unused ingredients during generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model. The title 'Globally Coherent Text Generation with Neural Checklist Models' confirms that the focus is on a method rather than a dataset.",
      "processing_time": 58.63529109954834,
      "citing_paper_id": "202120896",
      "cited_paper_id": 9818013
    },
    {
      "context_text": "We manually construct a list of 58 cooking techniques from 384 cooking actions collected by Bosselut et al. (2018b); the most common techniques ( bake , combine , pour , boil ) account for 36.5% of technique mentions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It refers to a list of cooking techniques and actions, but these are described as manually constructed and not linked to a named, publicly accessible dataset.",
      "processing_time": 58.29189157485962,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13700629
    },
    {
      "context_text": "We manually construct a list of 58 cooking techniques from 384 cooking actions collected by Bosselut et al. (2018b); the most common techniques (bake, combine, pour, boil) account for 36.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'cooking actions collected by Bosselut et al. (2018b)', but does not specify a dataset name. The reference appears to be to a method or data collection process rather than a named, verifiable dataset.",
      "processing_time": 60.150853633880615,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13700629
    },
    {
      "context_text": "We use the neural scoring model from Bosselut et al. (2018a) to measure recipe-level coherence for each generated recipe.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a neural scoring model but does not refer to a specific dataset. The model is used to measure recipe-level coherence in generated recipes.",
      "processing_time": 55.9868221282959,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13700629
    },
    {
      "context_text": "Large-scale transformer-based language models have shown surprising expressivity and fluency in creative and conditional long-text generation (Vaswani et al., 2017; Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities in text generation.",
      "processing_time": 54.287440061569214,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Large-scale transformer-based language models have shown surprising expressivity and fluency in creative and conditional long-text generation (Vaswani et al., 2017; Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities in text generation.",
      "processing_time": 54.287440061569214,
      "citing_paper_id": "202120896",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "To summarize, our main contributions are as follows: 2 Related Work Large-scale transformer-based language models have shown surprising expressivity and ﬂuency in creative and conditional long-text generation (Vaswani et al., 2017; Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to transformer-based language models and their capabilities in text generation.",
      "processing_time": 55.47380781173706,
      "citing_paper_id": "202120896",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "To summarize, our main contributions are as follows: 2 Related Work Large-scale transformer-based language models have shown surprising expressivity and ﬂuency in creative and conditional long-text generation (Vaswani et al., 2017; Radford et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to transformer-based language models and their capabilities in text generation.",
      "processing_time": 55.47380781173706,
      "citing_paper_id": "202120896",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Recipe generation belongs to the ﬁeld of data-to-text natural language generation (Gatt and Krahmer, 2018), which sees other applications in automated journalism (Lepp¨anen et al., 2017), question-answering (Agrawal et al., 2017), and abstractive summarization (Paulus et al., 2018), among others.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of data-to-text natural language generation. No verifiable resources are identified.",
      "processing_time": 56.18359088897705,
      "citing_paper_id": "202120896",
      "cited_paper_id": 29213380
    },
    {
      "context_text": "Recipe generation belongs to the field of datato-text natural language generation (Gatt and Krahmer, 2018), which sees other applications in automated journalism (Leppänen et al., 2017), question-answering (Agrawal et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of data-to-text natural language generation.",
      "processing_time": 54.86477446556091,
      "citing_paper_id": "202120896",
      "cited_paper_id": 29213380
    },
    {
      "context_text": "We order reviews by timestamp, keeping the most recent review for each user as the test set, the second most recent for validation, and the remainder for training (sequential leave-one-out evaluation (Kang and McAuley, 2018)).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, named datasets. It describes a method for splitting a dataset into training, validation, and test sets, but does not name the dataset itself.",
      "processing_time": 57.23829412460327,
      "citing_paper_id": "202120896",
      "cited_paper_id": 52127932
    },
    {
      "context_text": "We decode recipe text via top-k sampling (Radford et al., 2019), finding k = 3 to produce satisfactory results.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (top-k sampling) and a parameter value (k = 3).",
      "processing_time": 55.68414616584778,
      "citing_paper_id": "202120896",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Our method builds upon Stable Diffusion XL [20], comprising three core components: a Variational AutoEncoder (VAE) denoted as ξ ( · ) , a conditional U-Net [25] represented by ϵ θ ( · ) , and two pre-trained text encoders [22] denoted as Θ 1 ( · ) and Θ 2 ( · ) .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on describing the components of the method, which are not datasets.",
      "processing_time": 56.45217823982239,
      "citing_paper_id": "268531420",
      "cited_paper_id": 3719281
    },
    {
      "context_text": "For image encoding, we utilize the OpenCLIP ViT-H/14 [37] and the backbone of ArcFace [10] with a batch size of 4 per GPU.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of OpenCLIP and ArcFace for image encoding.",
      "processing_time": 56.309576749801636,
      "citing_paper_id": "268531420",
      "cited_paper_id": 6576859
    },
    {
      "context_text": "Specifically, we measure identity fidelity, utilizing metrics such as M FaceNet (measured by FaceNet [28]) and CLIP-I [7].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions metrics but does not refer to any specific datasets. The cited papers are methods/models, not datasets.",
      "processing_time": 55.20253849029541,
      "citing_paper_id": "268531420",
      "cited_paper_id": 206592766
    },
    {
      "context_text": "Current research endeavors aim to further enhance these models along multiple fronts, including the utilization of high-quality and large-scale datasets [29, 30], refinements to foundational architectures [3, 24, 27], and advancements in controllability [11,26,40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not mention any specific datasets by name, only referring to 'high-quality and large-scale datasets' in a generic manner. No specific dataset names are provided in the context or titles.",
      "processing_time": 58.46097922325134,
      "citing_paper_id": "268531420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Current research endeavors aim to further enhance these models along multiple fronts, including the utilization of high-quality and large-scale datasets [29, 30], refinements to foundational architectures [3, 24, 27], and advancements in controllability [11,26,40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not mention any specific datasets by name, only referring to 'high-quality and large-scale datasets' in a generic manner. No specific dataset names are provided in the context or titles.",
      "processing_time": 58.46097922325134,
      "citing_paper_id": "268531420",
      "cited_paper_id": 252917726
    },
    {
      "context_text": "Current research endeavors aim to further enhance these models along multiple fronts, including the utilization of high-quality and large-scale datasets [29, 30], refinements to foundational architectures [3, 24, 27], and advancements in controllability [11,26,40].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation context does not mention any specific datasets by name, only referring to 'high-quality and large-scale datasets' in a generic manner. No specific dataset names are provided in the context or titles.",
      "processing_time": 58.46097922325134,
      "citing_paper_id": "268531420",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Initially, tuning-based methods , exemplified by DreamBooth [26] and Textual Inversion [7], employ images of the same identity (ID) to fine-tune the model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'DreamBooth' and 'Textual Inversion' as examples of tuning-based methods, but does not refer to them as datasets. They are cited as methods or models.",
      "processing_time": 57.82432150840759,
      "citing_paper_id": "268531420",
      "cited_paper_id": 251800180
    },
    {
      "context_text": "Human photo synthesis [17,34] has experienced notable advancements, particularly with the introduction of large text-to-image diffusion models such as Stable Diffusion (SD) [24], Imagen [27], and DALL-E 3 [3].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.349531173706055,
      "citing_paper_id": "268531420",
      "cited_paper_id": 256615209
    },
    {
      "context_text": "Text-to-image diffusion models, such as those explored in [3, 12, 24, 27, 38, 40], have garnered significant attention due to their impressive image generation capabilities.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and their capabilities. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.75577116012573,
      "citing_paper_id": "268531420",
      "cited_paper_id": 256827727
    },
    {
      "context_text": "Consequently, recent advancements in tuning-free methods [5,6,8,15,16,18,32,35,36] have been introduced to streamline the generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in tuning-free methods. No verifiable resources are identified.",
      "processing_time": 55.41039776802063,
      "citing_paper_id": "268531420",
      "cited_paper_id": 257364757
    },
    {
      "context_text": "Consequently, recent advancements in tuning-free methods [5,6,8,15,16,18,32,35,36] have been introduced to streamline the generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in tuning-free methods. No verifiable resources are identified.",
      "processing_time": 55.41039776802063,
      "citing_paper_id": "268531420",
      "cited_paper_id": 259951373
    },
    {
      "context_text": "Consequently, recent advancements in tuning-free methods [5,6,8,15,16,18,32,35,36] have been introduced to streamline the generation process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only advancements in tuning-free methods. No verifiable resources are identified.",
      "processing_time": 55.41039776802063,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "Cao et al. [4] utilized a mutual self-attention approach to achieve consistent image generation and non-rigid image editing, wherein the key and value of the synthesis image were replaced with those of the reference image within the self-attention layers of the diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for image generation and editing.",
      "processing_time": 54.36275315284729,
      "citing_paper_id": "268531420",
      "cited_paper_id": 258179432
    },
    {
      "context_text": "As explored in previous studies [4, 13, 33], the features in the self attention layers play a crucial role in consistency image generation (across-frame in text-to-video works), which indicates these features provide a refined and detailed semantics information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the role of self-attention layers in image generation.",
      "processing_time": 55.06665802001953,
      "citing_paper_id": "268531420",
      "cited_paper_id": 258179432
    },
    {
      "context_text": "Wang et al. [35] and Avrahami et al. [2] exploited attention maps within the cross-attention layers to guide the optimization process towards disentangling learned concepts in personalized generation tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context focuses on the use of attention maps in cross-attention layers for disentangling learned concepts in personalized generation tasks.",
      "processing_time": 57.926647663116455,
      "citing_paper_id": "268531420",
      "cited_paper_id": 258888228
    },
    {
      "context_text": "As highlighted in [36], features extracted by the face recognition backbone are adept at capturing the characteristics associated with human facial features within identity-preserved personalization tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model for personalization tasks.",
      "processing_time": 54.5618896484375,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "According to the stylization results, the IP-Adapter [36] and IP-Adapter-Face fails to depict the desired style in the prompt and the style of generation results always obey the tone in the reference image.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and their performance. The citation is about evaluating the performance of IP-Adapter and IP-Adapter-Face, which are methods, not datasets.",
      "processing_time": 57.94424510002136,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "Diverging from conventional methods [5,16,36] that utilize text-image pairs for training, we opt to exclude the text prompt input and de-activate cross-attention modules for text embeddings within the U-Net model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach. The context focuses on the exclusion of text prompts and de-activation of cross-attention modules.",
      "processing_time": 56.81599140167236,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "The second type, demonstrated by IP-Adapter [36], directly injects ID information into the U-Net of the diffusion model through an additional trainable cross-attention module.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IP-Adapter) and its integration into a diffusion model.",
      "processing_time": 55.5746808052063,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "On the other hand, some studies [5, 32, 36] directly integrate identity information into the U-Net of the diffusion model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the integration of identity information into a model architecture.",
      "processing_time": 54.64938735961914,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "IP-Adapter [36] distinguishes itself by incorporating a additional cross-attention layer for each existing cross-attention layer within the original UNet model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (IP-Adapter) and its integration into a model (UNet).",
      "processing_time": 55.95306897163391,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "While IP-Adapter [36] and IP-Adapter-Face [36] demonstrates relatively fewer artifacts, its semantic consistency fall short.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context focuses on comparing the performance of IP-Adapter and IP-Adapter-Face.",
      "processing_time": 56.65107536315918,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "As elucidated in [36], the features extracted by the CLIP image encoder are instrumental in capturing the structural information pertinent to the identity face within identity-preserved personalization tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of CLIP image encoder features for identity-preserved personalization tasks.",
      "processing_time": 55.74911689758301,
      "citing_paper_id": "268531420",
      "cited_paper_id": 260886966
    },
    {
      "context_text": "Similarly, Shi et al. [31] proposed a method termed reference attention, enabling consistent multi-view generation of target objects by concatenating the key and value features between the condition signal and the synthesis image in the self-attention layers.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method called 'reference attention'. No dataset names are present in the citation context.",
      "processing_time": 55.90265202522278,
      "citing_paper_id": "268531420",
      "cited_paper_id": 264436559
    },
    {
      "context_text": "Hertz et al. [9] employed a shared attention mechanism, concatenating and applying an AdaIN module on the key and value between reference and synthesis images within the self-attention layer to ensure style-consistent image generation using a reference style.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for style-consistent image generation. No verifiable resources are identified.",
      "processing_time": 55.779298543930054,
      "citing_paper_id": "268531420",
      "cited_paper_id": 265608730
    },
    {
      "context_text": "…refine semantic control, we incorporate text features into the identity feature within the cross-attention layers using the following formulation: where [9], we propose an adaptive mean normalization (AdaIN-mean) operation to further align the style of the synthesis image with the style prompts.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for style alignment in image generation.",
      "processing_time": 54.3109405040741,
      "citing_paper_id": "268531420",
      "cited_paper_id": 265608730
    },
    {
      "context_text": "On one hand, methods [1, 16, 19, 35] incorporate text information alongside identity details within the text embedding space of the text encoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods that incorporate text information and identity details. No verifiable resources are identified.",
      "processing_time": 55.76989507675171,
      "citing_paper_id": "268531420",
      "cited_paper_id": 266163204
    },
    {
      "context_text": "GPT-2 uses Byte Pair Encoding (BPE) [44] for vocabulary construction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Byte Pair Encoding) used for vocabulary construction in GPT-2.",
      "processing_time": 55.94193625450134,
      "citing_paper_id": "246863587",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "GPT-2 uses Byte Pair Encoding (BPE) [46] for vocabulary construction.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (Byte Pair Encoding) used in GPT-2.",
      "processing_time": 55.209070920944214,
      "citing_paper_id": "246863587",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "For our model PEPLER-D, the number of input features is also set to 20 BPE tokens.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model configuration detail. No verifiable resources are identified.",
      "processing_time": 55.02382969856262,
      "citing_paper_id": "246863587",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "In total, there are 50,257 BPE tokens in GPT-2.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to BPE tokens in GPT-2, which is a model, not a dataset.",
      "processing_time": 56.87070274353027,
      "citing_paper_id": "246863587",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "For fair comparison, we apply BPE to all the models, and set the length of explanations to 20 BPE tokens.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (BPE) applied to models. No verifiable resources are identified.",
      "processing_time": 55.85283660888672,
      "citing_paper_id": "246863587",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "3 Compared Methods We introduce four state-of-the-art baselines, which are based on representative language models, including BERT [15], Transformer [53], GRU [14] and LSTM [22], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (BERT, Transformer, GRU, LSTM) but does not refer to any specific datasets. The citation is focused on describing the methods used as baselines.",
      "processing_time": 57.66408324241638,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "3 Compared Methods We introduce four state-of-the-art baselines, which are based on representative language models, including BERT [15], Transformer [53], GRU [14] and LSTM [22], respectively.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several models (BERT, Transformer, GRU, LSTM) but does not refer to any specific datasets. The citation is focused on describing the methods used as baselines.",
      "processing_time": 57.66408324241638,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Transformer [53] consists of L identical layers.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a model architecture (Transformer).",
      "processing_time": 54.25521993637085,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "• PErsonalized Transformer for Explainable Recommendation (PETER) [29] is a small unpretrained Transformer [53] particularly designed for explanation generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (PETER) and a reference to the Transformer architecture. No verifiable resources are identified.",
      "processing_time": 56.5142183303833,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "1), as well as the advancement of natural language generation techniques, such as Recurrent Neural Networks (RNN), Transformer [53] and pre-trained language models [15, 17, 42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. Therefore, no datasets can be extracted.",
      "processing_time": 55.38525104522705,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "1), as well as the advancement of natural language generation techniques, such as Recurrent Neural Networks (RNN), Transformer [53] and pre-trained language models [15, 17, 42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. Therefore, no datasets can be extracted.",
      "processing_time": 55.38525104522705,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "1), as well as the advancement of natural language generation techniques, such as Recurrent Neural Networks (RNN), Transformer [53] and pre-trained language models [15, 17, 42].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. Therefore, no datasets can be extracted.",
      "processing_time": 55.38525104522705,
      "citing_paper_id": "246863587",
      "cited_paper_id": 147704286
    },
    {
      "context_text": "2 Transformer and Pre-trained Models Transformer [53] was first brought to the domain of machine translation with the architecture of encoder-decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and architectures. The citation is about the introduction of the Transformer model in machine translation.",
      "processing_time": 55.90977883338928,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Without bells andwhistles, ourmethod consistently achieves the best performance against strong baselines (built on top of LSTM [22], GRU [14], Transformer [53] or BERT [15]) in terms of both text quality and explainability on three datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'three datasets' but does not provide specific names. The context focuses on comparing model performance across different architectures.",
      "processing_time": 55.753746032714844,
      "citing_paper_id": "246863587",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "Without bells andwhistles, ourmethod consistently achieves the best performance against strong baselines (built on top of LSTM [22], GRU [14], Transformer [53] or BERT [15]) in terms of both text quality and explainability on three datasets.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation mentions 'three datasets' but does not provide specific names. The context focuses on comparing model performance across different architectures.",
      "processing_time": 55.753746032714844,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "The former investigates how people perceive different styles of explanation [8, 9, 20], while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.80823373794556,
      "citing_paper_id": "246863587",
      "cited_paper_id": 17471203
    },
    {
      "context_text": "The former investigates how people perceive different styles of explanation [8, 9, 20], while the latter provides explanations by designing new explainable recommendation algorithms, to which our work is more related.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general concepts and approaches. The cited papers' titles do not provide additional information about datasets.",
      "processing_time": 56.80823373794556,
      "citing_paper_id": "246863587",
      "cited_paper_id": 18712907
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation [16, 51], tip generation [30, 31] and explanation generation [13, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of a strategy. No dataset names are provided.",
      "processing_time": 55.74574947357178,
      "citing_paper_id": "246863587",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "• Attribute-to-Sequence (Att2Seq) [16] is a review generation approach with a two-layer LSTM [22].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Att2Seq) and a neural network architecture (LSTM).",
      "processing_time": 56.32896709442139,
      "citing_paper_id": "246863587",
      "cited_paper_id": 17865105
    },
    {
      "context_text": "We first examine those that map IDs onto item features, i.e., ACMLM and PEPLER-D.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is focused on ID mapping techniques, which are not datasets.",
      "processing_time": 56.613001108169556,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "• Aspect Conditional Masked Language Model (ACMLM) [42] is a fine-tuned BERT [16], where an attention layer is introduced to encode the features for both the user and the item.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ACMLM) and a model (BERT).",
      "processing_time": 55.82661771774292,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "We adopt the code of ACMLM, and implement the other baselines (i.e., NRT, Att2Seq and PETER) by ourselves.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.196924448013306,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Typical Example Promptless Fine-tuning Tuned N/A BERT [15] Tuning-free Prompting Frozen None GPT-3 [4] Fixed-LM Prompt Tuning Frozen Tuned Prefix-Tuning [32] Fixed-prompt LM Tuning Tuned None Our PEPLER-D Prompt+LM Fine-tuning Tuned Tuned P-Tuning [37] Sequential Tuning: Fixed-LM Prompt Tuning → Prompt+LM Fine-tuning Tuned Tuned Twice Our PEPLER",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria for inclusion.",
      "processing_time": 56.48155212402344,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Our PEPLER-D consistently and significantly outperforms ACMLM on the three datasets in terms of text quality measured by BLEU and ROUGE.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'three datasets' but does not provide specific names. It also mentions BLEU and ROUGE, which are metrics, not datasets.",
      "processing_time": 56.827922344207764,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "With the two types of masking mechanism, there are also two corresponding pre-training objectives: cloze task, which is formally termed Masked Language Model (MLM) [16], for bidirectional\nJ. ACM, Vol. 37, No. 4, Article 111.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Masked Language Model) used in the BERT model.",
      "processing_time": 55.61285662651062,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "This may explain why ACMLM produces diverse sentences (high USR) and features (low DIV).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses model performance metrics.",
      "processing_time": 54.09571123123169,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Under the paradigm of pretraining plus fine-tuning, Transformer’s effectiveness has been confirmed on a wide range of natural language understanding tasks [15, 42], such as commonsense reasoning and question answering.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the effectiveness of Transformer models on various NLU tasks.",
      "processing_time": 55.02264428138733,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Later works [15, 36] show that it remains effective, even when the encoder or the decoder is removed, reducing nearly half of model parameters.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or methods. The context is about the effectiveness of a model architecture.",
      "processing_time": 55.606465339660645,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Only two methods, i.e., ACMLM and our PEPLER, successfully capture this key feature.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 55.29462265968323,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "As we can see, the text quality of these methods are largely improved compared with those that convert IDs into item features (i.e., ACMLM and PEPLER-D), because the conversion process may lose certain information of IDs, e.g., identification.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. There are no clear identifiers for datasets in the text.",
      "processing_time": 55.63049077987671,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "2 Discrete Prompt Learning Pre-trained language models, such as BERT [15] and GPT-2 [43], were trained on a large amount of word tokens, which are inherently in a different semantic space as ID tokens, but IDs (e.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-trained language models. No verifiable resources are identified.",
      "processing_time": 55.138469219207764,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "For example, in bidirectional language models such as BERT [15],M is a zero matrix that allows all tokens in the sequence to attend to each other.",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (BERT). No dataset names are present in the citation span.",
      "processing_time": 55.618821144104004,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "However, ACMLM’s explanation is not even readable, because it is just a bunch of unordered random words.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only criticizes the readability of ACMLM's explanations.",
      "processing_time": 56.25751495361328,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "Also, we notice that the performance gap between our PEPLER-D and ACMLM (a fine-tuned BERT) is extremely large, because the latter’s generation is achieved by predicting masked tokens, which is quite different from conventional auto-regressive generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on comparing the performance of two models, PEPLER-D and ACMLM, rather than using a specific dataset.",
      "processing_time": 58.866334676742554,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "• Aspect Conditional Masked Language Model (ACMLM) [40] is a fine-tuned BERT [15], where an attention layer is introduced to encode the features for both the user and the item.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ACMLM) and a model (BERT).",
      "processing_time": 55.61745285987854,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "With the two types of masking mechanism, there are also two corresponding pre-training objectives: cloze task, which is formally termed Masked Language Model (MLM) [15], for bidirectional language models, and auto-regressive generation for unidirectional language models.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only pre-training objectives and mechanisms. The cited paper title is about BERT, which is a method, not a dataset.",
      "processing_time": 57.45867872238159,
      "citing_paper_id": "246863587",
      "cited_paper_id": 52967399
    },
    {
      "context_text": "3 Personalized Natural Language Generation Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation [6, 26, 29], review summarization [23], and dialog systems [59, 61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications of personalized natural language generation.",
      "processing_time": 55.2139196395874,
      "citing_paper_id": "246863587",
      "cited_paper_id": 69778590
    },
    {
      "context_text": "3 Personalized Natural Language Generation Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation [6, 26, 29], review summarization [23], and dialog systems [59, 61].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general tasks and applications of personalized natural language generation.",
      "processing_time": 55.2139196395874,
      "citing_paper_id": "246863587",
      "cited_paper_id": 204874165
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation [17, 53], tip generation [31, 32] and explanation generation [14, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of personalized text generation. No dataset names are provided in the context or titles.",
      "processing_time": 56.52814722061157,
      "citing_paper_id": "246863587",
      "cited_paper_id": 70350032
    },
    {
      "context_text": "This strategy can be found in many applications, such as review generation [17, 53], tip generation [31, 32] and explanation generation [14, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general applications of personalized text generation. No dataset names are provided in the context or titles.",
      "processing_time": 56.52814722061157,
      "citing_paper_id": "246863587",
      "cited_paper_id": 86471548
    },
    {
      "context_text": "There exist various types of explanation style, such as pre-defined templates [25, 51, 64], item features [21, 56], ranked text [5, 12, 28], image visualizations [10], knowledge graph paths [1, 19, 58, 59], and reasoning rules [7, 48, 67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of explanation styles. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.56053972244263,
      "citing_paper_id": "246863587",
      "cited_paper_id": 197640939
    },
    {
      "context_text": "There exist various types of explanation style, such as pre-defined templates [25, 51, 64], item features [21, 56], ranked text [5, 12, 28], image visualizations [10], knowledge graph paths [1, 19, 58, 59], and reasoning rules [7, 48, 67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of explanation styles. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.56053972244263,
      "citing_paper_id": "246863587",
      "cited_paper_id": 231986430
    },
    {
      "context_text": "There exist various types of explanation style, such as pre-defined templates [25, 51, 64], item features [21, 56], ranked text [5, 12, 28], image visualizations [10], knowledge graph paths [1, 19, 58, 59], and reasoning rules [7, 48, 67].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only types of explanation styles. The cited paper titles do not provide additional context to identify specific datasets.",
      "processing_time": 56.56053972244263,
      "citing_paper_id": "246863587",
      "cited_paper_id": 237278204
    },
    {
      "context_text": "There is a variety of explanation style, such as pre-defined templates [24, 49, 60], highlighted image regions [10] and automatically generated sentences [6, 26, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to various styles of explanations, including pre-defined templates, highlighted image regions, and automatically generated sentences.",
      "processing_time": 57.80942964553833,
      "citing_paper_id": "246863587",
      "cited_paper_id": 204874165
    },
    {
      "context_text": "In the case of explainable recommendation, users may value more an explanation that justifies a recommendation’s advantage on certain item features [6, 26].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of generating natural language explanations for recommendations.",
      "processing_time": 55.2421612739563,
      "citing_paper_id": "246863587",
      "cited_paper_id": 204874165
    },
    {
      "context_text": "No wonder most previous works [6, 13, 26, 48, 57] adopt RNN, such as Long Short-Term Memory (LSTM) [22] and Gated Recurrent Unit (GRU) [14], or small unpretrained Transformer [29] for explanation generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.207648515701294,
      "citing_paper_id": "246863587",
      "cited_paper_id": 204874165
    },
    {
      "context_text": "Previous works [6, 13, 26, 57] mostly rely on RNN, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works and a partial sentence about RNNs.",
      "processing_time": 56.202308654785156,
      "citing_paper_id": "246863587",
      "cited_paper_id": 204874165
    },
    {
      "context_text": "We conducted a user survey in NETE [26, 27] and showed that the explanations generated by NETE were perceived useful by participants.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'NETE' but does not specify it as a dataset, method, or other resource. It appears to be a system or tool used for generating explanations, which is not a dataset.",
      "processing_time": 59.10616135597229,
      "citing_paper_id": "246863587",
      "cited_paper_id": 218522173
    },
    {
      "context_text": "Moreover, the explanation quality of PETER [29] is much better than that of NETE on the same automatic evaluation metrics.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between two methods (PETER and NETE) on automatic evaluation metrics.",
      "processing_time": 56.791175365448,
      "citing_paper_id": "246863587",
      "cited_paper_id": 218522173
    },
    {
      "context_text": "No wonder most previous works [6, 14, 27, 50, 60] adopt RNN, such as Long Short-Term Memory (LSTM) [23] and Gated Recurrent Unit (GRU) [15], or small unpretrained Transformer [29] for explanation generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.37008452415466,
      "citing_paper_id": "246863587",
      "cited_paper_id": 231698507
    },
    {
      "context_text": "However, previous works [6, 14, 27, 60] mostly rely on RNN, e.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a reference to previous works and RNNs. There is no clear indication of a reusable resource.",
      "processing_time": 57.237879514694214,
      "citing_paper_id": "246863587",
      "cited_paper_id": 231698507
    },
    {
      "context_text": "Prompt learning has been successfully applied to many applications, such as domain adaptation [3], text summarization [32] and image captioning [52], but it has been less investigated how prompt learning would benefit recommender systems.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of prompt learning. No verifiable resources are identified.",
      "processing_time": 55.79786467552185,
      "citing_paper_id": "246863587",
      "cited_paper_id": 235658331
    },
    {
      "context_text": "There is a variety of explanation style, such as pre-defined templates [25, 51, 64], highlighted image regions [10] and automatically generated sentences [6, 27, 29].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to various styles of explanations, which are not datasets.",
      "processing_time": 56.565051555633545,
      "citing_paper_id": "246863587",
      "cited_paper_id": 237278204
    },
    {
      "context_text": "This is evidenced by [57] that users’ perception towards machine-generated explanations are highly correlated with the factors of relevance, repetition and feature appearance, which correspond to BLEU/ROUGE, USR and FMR in this work.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only metrics and user perception factors. No clear, verifiable dataset names are present.",
      "processing_time": 56.56443977355957,
      "citing_paper_id": "246863587",
      "cited_paper_id": 248367454
    },
    {
      "context_text": "Language models are at the very heart of many modern NLP systems and applications (Young et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to language models in NLP systems.",
      "processing_time": 55.63726830482483,
      "citing_paper_id": "239050492",
      "cited_paper_id": 3397190
    },
    {
      "context_text": "These methods learn word embeddings for each speciﬁc social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for learning word embeddings in social contexts. No verifiable resources are identified.",
      "processing_time": 56.502586364746094,
      "citing_paper_id": "239050492",
      "cited_paper_id": 5480561
    },
    {
      "context_text": "These methods learn word embeddings for each speciﬁc social context and can capture how word meanings vary across these dimensions (Bamman et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016; Welch et al., 2020a,b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods for learning word embeddings in social contexts. No verifiable resources are identified.",
      "processing_time": 56.502586364746094,
      "citing_paper_id": "239050492",
      "cited_paper_id": 9298083
    },
    {
      "context_text": "…ignore this social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers. The context suggests a discussion about the importance of social context in NLP models, but no datasets are explicitly named.",
      "processing_time": 57.81669640541077,
      "citing_paper_id": "239050492",
      "cited_paper_id": 9048146
    },
    {
      "context_text": "…social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.96147704124451,
      "citing_paper_id": "239050492",
      "cited_paper_id": 26192090
    },
    {
      "context_text": "…social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.96147704124451,
      "citing_paper_id": "239050492",
      "cited_paper_id": 52116412
    },
    {
      "context_text": "…social context, they may perform sub-optimally underscoring the need for a richer integration of social contexts into NLP models (Pavalanathan et al., 2015; Lynn et al., 2017; Zamani et al., 2018; Lynn et al., 2019; May et al., 2019; Kurita et al., 2019; Welch et al., 2020a; Hovy and Yang, 2021).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research works. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.96147704124451,
      "citing_paper_id": "239050492",
      "cited_paper_id": 198905491
    },
    {
      "context_text": "Recent approaches have addressed the ﬁrst limitation by learning word representations that are contextualized by their token-speciﬁc usage context (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019b,a).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 55.49174904823303,
      "citing_paper_id": "239050492",
      "cited_paper_id": 59258832
    },
    {
      "context_text": "Many of such projects use ASP as a paradigm for declarative specification of content constraints, and rely on state-of-the-art ASP solvers for combinatorial model space exploration [Smith and Mateas, 2011].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (ASP) and its application in procedural content generation.",
      "processing_time": 55.663132190704346,
      "citing_paper_id": "11767561",
      "cited_paper_id": 2411164
    },
    {
      "context_text": "Multiple studies have conjectured that this is caused by language understanding, conceptual knowledge, discourse comprehension, and other aspects required to build a mental representation of a word problem [Cummins et al., 1988; Schumacher and Fuchs, 2012].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of cognitive psychology. No verifiable resources are identified.",
      "processing_time": 55.86970376968384,
      "citing_paper_id": "11767561",
      "cited_paper_id": 7532458
    },
    {
      "context_text": "Multiple studies have conjectured that this is caused by language understanding, conceptual knowledge, discourse comprehension , and other aspects required to build a mental representation of a word problem [Cummins et al., 1988; Schumacher and Fuchs, 2012].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of cognitive processes. No verifiable resources are identified.",
      "processing_time": 55.87026333808899,
      "citing_paper_id": "11767561",
      "cited_paper_id": 7532458
    },
    {
      "context_text": "Our method is inspired by classic NLG guidelines [Krahmer and Van Deemter, 2012], and is based on type attributes.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a survey paper on computational generation of referring expressions.",
      "processing_time": 55.867913246154785,
      "citing_paper_id": "11767561",
      "cited_paper_id": 7983519
    },
    {
      "context_text": "It is built according to the classic guidelines of NLG systems [Reiter and Dale, 1997].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to classic guidelines for NLG systems.",
      "processing_time": 55.14849257469177,
      "citing_paper_id": "11767561",
      "cited_paper_id": 8460470
    },
    {
      "context_text": "It is built according to the classic guidelines of NLG systems [Reiter and Dale, 1997].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to classic guidelines for NLG systems.",
      "processing_time": 55.14849257469177,
      "citing_paper_id": "11767561",
      "cited_paper_id": 18117920
    },
    {
      "context_text": "The solution is then chosen according to subset minimality semantics: no answer set is emitted as a solution if any of its subsets is also a valid solution [Gebser et al., 2013].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for solving answer sets.",
      "processing_time": 54.44494295120239,
      "citing_paper_id": "11767561",
      "cited_paper_id": 8513260
    },
    {
      "context_text": "One might imagine more complex requirements like constraining any solution to include certain steps [Smith et al., 2013].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It refers to a concept in puzzle design, which is not relevant to personalized text generation.",
      "processing_time": 57.39610195159912,
      "citing_paper_id": "11767561",
      "cited_paper_id": 15403044
    },
    {
      "context_text": "Recently it has gained new interest with novel approaches in problem generation for natural deduction [Ahmed et al., 2013], algebraic proof problems [Singh et al., 2012], procedural problems [Andersen et al., 2013], embedded systems [Sadigh et al., 2012], etc.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only various types of problems and approaches. No clear, verifiable datasets are identified.",
      "processing_time": 56.38493585586548,
      "citing_paper_id": "11767561",
      "cited_paper_id": 15654068
    },
    {
      "context_text": ", 2013], algebraic proof problems [Singh et al., 2012], procedural problems [Andersen et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of problems. No clear, verifiable resource names are provided.",
      "processing_time": 56.203354597091675,
      "citing_paper_id": "11767561",
      "cited_paper_id": 15654068
    },
    {
      "context_text": "One way to resolve it is a richer ontology with commonsense background knowledge such as Cyc [Lenat, 1995], which is an important area of future work.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Cyc as a source of commonsense background knowledge, but it is not used as a dataset. It is referenced as a method or tool for future work.",
      "processing_time": 57.81271481513977,
      "citing_paper_id": "11767561",
      "cited_paper_id": 16147141
    },
    {
      "context_text": "They focus on natural language generation using Frame Semantics [Fillmore, 1976] with the standard NLG architecture introduced by Reiter and Dale [1997], and explore distance/speed word problems as an example domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and methods. No verifiable resources are identified.",
      "processing_time": 55.86860132217407,
      "citing_paper_id": "11767561",
      "cited_paper_id": 85223883
    },
    {
      "context_text": "They focus on natural language generation using Frame Semantics [Fillmore, 1976] with the standard NLG architecture introduced by Reiter and Dale [1997], and explore distance/speed word problems as an example domain.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only theoretical frameworks and methods. No verifiable resources are identified.",
      "processing_time": 55.86860132217407,
      "citing_paper_id": "11767561",
      "cited_paper_id": 219579821
    },
    {
      "context_text": "Children have been reported to perform up to 30% worse on word problems than on corresponding algebraic equations [Carpenter et al., 1980].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a finding about children's performance on word problems compared to algebraic equations.",
      "processing_time": 56.172871351242065,
      "citing_paper_id": "11767561",
      "cited_paper_id": 126193976
    },
    {
      "context_text": "Automatic problem generation for various educational domains has been studied since the mid-1960s [Wexler, 1968].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only references a historical study on automatic problem generation.",
      "processing_time": 55.99024415016174,
      "citing_paper_id": "11767561",
      "cited_paper_id": null
    },
    {
      "context_text": "Automatic problem generation for various educational domains has been studied since the mid-1960s [Wexler, 1968].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only references a historical study on automatic problem generation.",
      "processing_time": 55.99024415016174,
      "citing_paper_id": "11767561",
      "cited_paper_id": null
    },
    {
      "context_text": "In our setting, the innermost ∃ quantifier is expanded during grounding using skolemization [Benedetti, 2005].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or technique related to skolemization.",
      "processing_time": 54.56182813644409,
      "citing_paper_id": "11767561",
      "cited_paper_id": null
    },
    {
      "context_text": "In ASP [Gebser et al., 2012], programs are composed of facts and rules in a first-order logic representation (syntactically similar to Prolog).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or system (ASP) and its components. There are no verifiable resources that meet the criteria.",
      "processing_time": 56.50189757347107,
      "citing_paper_id": "11767561",
      "cited_paper_id": null
    },
    {
      "context_text": "Synthesis of logical graphs is implemented with answer-set programming (ASP), a logic programming paradigm, wellsuited for exploration of a huge space of possible models under declarative constraints [Gebser et al., 2012].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (answer-set programming).",
      "processing_time": 54.7349066734314,
      "citing_paper_id": "11767561",
      "cited_paper_id": null
    },
    {
      "context_text": "Examples include using reinforcement learning to control quality metrics (Ranzato et al., 2016), adjusting sampling weights to control for poetry style (Ghazvininejad et al., 2017), and learning to condition on valence or domain-speciﬁc codes (Keskar et al., 2019; Peng et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.86018896102905,
      "citing_paper_id": "235368201",
      "cited_paper_id": 7147309
    },
    {
      "context_text": "Examples include using reinforcement learning to control quality metrics (Ranzato et al., 2016), adjusting sampling weights to control for poetry style (Ghazvininejad et al., 2017), and learning to condition on valence or domain-speciﬁc codes (Keskar et al., 2019; Peng et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and approaches. The cited papers' titles do not provide additional context to identify datasets.",
      "processing_time": 56.86018896102905,
      "citing_paper_id": "235368201",
      "cited_paper_id": 51729727
    },
    {
      "context_text": "Examples include using reinforcement learning to control quality metrics (Ranzato et al., 2016), adjusting sampling weights to control for poetry style (Ghazvininejad et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches. The context is too vague to identify any verifiable resources.",
      "processing_time": 56.351516008377075,
      "citing_paper_id": "235368201",
      "cited_paper_id": 7147309
    },
    {
      "context_text": "Traditionally, adaptive education methods involve planning over a pool of pre-made questions (Atkinson, 1972; Hunziker et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to adaptive education methods and pre-made questions. No verifiable resources are identified.",
      "processing_time": 56.51027870178223,
      "citing_paper_id": "235368201",
      "cited_paper_id": 15870909
    },
    {
      "context_text": "Knowledge Tracing (KT) seeks to model a student’s knowledge state from their answer history in order to help individualize exercise sequences (Corbett and Anderson, 1995).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for modeling student knowledge states.",
      "processing_time": 55.022913694381714,
      "citing_paper_id": "235368201",
      "cited_paper_id": 19228797
    },
    {
      "context_text": "We use the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (Settles et al., 2018) dataset, which contains questions and responses for Duolingo users over the first 30 days of learning a second language.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "2018 Duolingo Shared Task on Second Language Acquisition Modeling"
      ],
      "dataset_descriptions": {
        "2018 Duolingo Shared Task on Second Language Acquisition Modeling": "Used to model second language acquisition, focusing on user responses over the first 30 days of learning. The dataset supports the investigation of personalized learning patterns and user engagement."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset used for modeling second language acquisition, which is directly relevant to personalized text generation.",
      "processing_time": 65.28395819664001,
      "citing_paper_id": "235368201",
      "cited_paper_id": 46939026
    },
    {
      "context_text": "The 2018 Duolingo Shared Task on Second Language Acquisition Modeling (Settles et al., 2018) dataset contains questions and responses for Duolingo users over the ﬁrst 30 days of learning a second language.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Duolingo Shared Task on Second Language Acquisition Modeling 2018"
      ],
      "dataset_descriptions": {
        "Duolingo Shared Task on Second Language Acquisition Modeling 2018": "Used to model second language acquisition, focusing on user responses over the first 30 days of learning, to understand learning patterns and personalize educational content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset used for modeling second language acquisition, which is relevant to personalized text generation.",
      "processing_time": 65.11291241645813,
      "citing_paper_id": "235368201",
      "cited_paper_id": 46939026
    },
    {
      "context_text": "We use the 2018 Duolingo Shared Task on Second Language Acquisition Modeling (Settles et al., 2018) dataset, which contains questions and responses for Duolingo users over the ﬁrst 30 days of learning a second language.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "2018 Duolingo Shared Task on Second Language Acquisition Modeling"
      ],
      "dataset_descriptions": {
        "2018 Duolingo Shared Task on Second Language Acquisition Modeling": "Used to model second language acquisition, focusing on user responses over the first 30 days of learning, to understand learning patterns and personalize educational content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions a specific dataset used for modeling second language acquisition, which is directly relevant to personalized text generation.",
      "processing_time": 65.02473640441895,
      "citing_paper_id": "235368201",
      "cited_paper_id": 46939026
    },
    {
      "context_text": "However, these methods, which have been used to develop efﬁcient standardized tests, do not necessarily optimize a student’s learning experience (Mu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or resources. It refers to methods used in developing standardized tests, which are not considered datasets.",
      "processing_time": 57.135823249816895,
      "citing_paper_id": "235368201",
      "cited_paper_id": 49305736
    },
    {
      "context_text": "However, these methods, which have been used to develop efficient standardized tests, do not necessarily optimize a student’s learning experience (Mu et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or resources. It refers to methods used in developing standardized tests, which are not considered datasets.",
      "processing_time": 57.1320378780365,
      "citing_paper_id": "235368201",
      "cited_paper_id": 49305736
    },
    {
      "context_text": "However, achieving equitable outcomes across diverse learning needs benefits from systems that are adaptive and individualized to each student (Doroudi and Brunskill, 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of adaptive and individualized systems for equitable outcomes in learning.",
      "processing_time": 56.17914271354675,
      "citing_paper_id": "235368201",
      "cited_paper_id": 67871849
    },
    {
      "context_text": "In this work, we focus on natural language sequences, where recent progress in language modeling has shown great success at capturing abstract properties of language (Hewitt and Manning, 2019; Liu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to progress in language modeling.",
      "processing_time": 55.15680909156799,
      "citing_paper_id": "235368201",
      "cited_paper_id": 106402715
    },
    {
      "context_text": "For both French and Spanish question generation models, we select 15 students unseen during training and generate 30 questions across 9 difﬁculties from 0.1 to 0.9, using nucleus sampling (Holtzman et al., 2020) ( p = 0 .",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (nucleus sampling) and a general description of the experimental setup. No verifiable resources are identified.",
      "processing_time": 57.84495663642883,
      "citing_paper_id": "235368201",
      "cited_paper_id": 127986954
    },
    {
      "context_text": "For example, Item Response Theory (IRT) seeks to model individual student ability based on their responses to different questions, creating a strong factorization between students and test items (Lord, 1980; Hambelton and Jodoin, 2003).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation discusses Item Response Theory (IRT) but does not mention any specific datasets. The context is about modeling student abilities and test items, which is a methodological discussion rather than a dataset reference.",
      "processing_time": 59.22658681869507,
      "citing_paper_id": "235368201",
      "cited_paper_id": 142740299
    },
    {
      "context_text": "Meanwhile, Computer Adaptive Testing (CAT) techniques are used to determine a fixed student ability as quickly as possible by selecting test items based on information utility (Weiss and Kingsbury, 1984; Thissen and Mislevy, 2000; Settles et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. The context is about CAT techniques and their application in determining student ability.",
      "processing_time": 56.98028635978699,
      "citing_paper_id": "235368201",
      "cited_paper_id": 216554078
    },
    {
      "context_text": "Meanwhile, Computer Adaptive Testing (CAT) techniques are used to determine a ﬁxed student ability as quickly as possible by selecting test items based on information utility (Weiss and Kingsbury, 1984; Thissen and Mislevy, 2000; Settles et al., 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and techniques. The context is about CAT techniques and their application in determining student ability.",
      "processing_time": 56.976927042007446,
      "citing_paper_id": "235368201",
      "cited_paper_id": 216554078
    },
    {
      "context_text": "Meanwhile, pre-trained LMs can effectively handle sequences from a wide range of modalities (Madani et al., 2020; Polu and Sutskever, 2020).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only pre-trained language models handling various modalities.",
      "processing_time": 55.34911847114563,
      "citing_paper_id": "235368201",
      "cited_paper_id": 221535103
    },
    {
      "context_text": "Recently, SAINT+ (Shin et al., 2020) showed state-of-the-art performance on the popular EdNet KT task using a Transformer model to capture temporal information across activities, motivating our use of Transformer LMs.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions the EdNet KT task but does not specify a dataset. It focuses on the method (SAINT+) and its performance on a task, not on a specific dataset.",
      "processing_time": 58.3719687461853,
      "citing_paper_id": "235368201",
      "cited_paper_id": 225062098
    },
    {
      "context_text": "Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for modeling open-domain conversation.",
      "processing_time": 55.720590591430664,
      "citing_paper_id": "202788651",
      "cited_paper_id": 780171
    },
    {
      "context_text": "Given this, various data-driven approaches have been proposed for modeling open-domain conversation, including retrieval-based methods (Yan et al., 2016; Tao et al., 2019), statistical machine translation model (Ritter et al., 2011), and neural networks (Serban et al., 2015; Hu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general approaches and methods for modeling open-domain conversation.",
      "processing_time": 55.720590591430664,
      "citing_paper_id": "202788651",
      "cited_paper_id": 196186961
    },
    {
      "context_text": "To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for open-domain response generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional information about datasets.",
      "processing_time": 56.301352977752686,
      "citing_paper_id": "202788651",
      "cited_paper_id": 3620643
    },
    {
      "context_text": "To improve wording diversity, CVAE models (Serban et al., 2017; Zhao et al., 2017; Shen et al., 2018) are well-investigated for open-domain response generation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers do not provide additional information about datasets.",
      "processing_time": 56.301352977752686,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "Conditional VAE (CVAE) is a popular framework for dialogue generation (Zhao et al., 2017; Shen et al., 2017, 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CVAE) used for dialogue generation. No verifiable resources are identified.",
      "processing_time": 56.82431745529175,
      "citing_paper_id": "202788651",
      "cited_paper_id": 3620643
    },
    {
      "context_text": "Conditional VAE (CVAE) is a popular framework for dialogue generation (Zhao et al., 2017; Shen et al., 2017, 2018).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CVAE) used for dialogue generation. No verifiable resources are identified.",
      "processing_time": 56.82431745529175,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "Over the past decade, a myriad of conversational systems have been proposed in the ﬁeld of artiﬁ-cial intelligence and achieved remarkable success in various industry scenarios, such as e-commerce assistant (Li et al., 2017) and chit-chat machine XiaoIce (Shum et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to conversational systems and their applications. No verifiable resources are identified.",
      "processing_time": 56.4521906375885,
      "citing_paper_id": "202788651",
      "cited_paper_id": 4325193
    },
    {
      "context_text": "In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user proﬁles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.950145959854126,
      "citing_paper_id": "202788651",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user proﬁles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods and approaches. No verifiable resources are identified.",
      "processing_time": 55.950145959854126,
      "citing_paper_id": "202788651",
      "cited_paper_id": 7421176
    },
    {
      "context_text": "…building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user proﬁles as personalizations for generating personalized responses (Zhang et al., 2017, 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general approaches to personalization in dialogue systems.",
      "processing_time": 55.31275916099548,
      "citing_paper_id": "202788651",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "Adaptation , the domain adaptation solution for building personalized conversation systems (Zhang et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for domain adaptation in personalized conversation systems.",
      "processing_time": 55.38861680030823,
      "citing_paper_id": "202788651",
      "cited_paper_id": 6862403
    },
    {
      "context_text": "…building a personalized conversation system has been attached more attention, e.g., implicitly learning user personalizations from dialog history (Li et al., 2015), explicitly collecting and modeling user profiles as personalizations for generating personalized responses (Zhang et al., 2017, 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions personalized conversation systems and user profiles, but does not specify any named datasets. The cited papers are used to support the discussion of methods and approaches.",
      "processing_time": 57.39871144294739,
      "citing_paper_id": "202788651",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user profiles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general methods and approaches for user-level information modeling.",
      "processing_time": 56.67736268043518,
      "citing_paper_id": "202788651",
      "cited_paper_id": 6869582
    },
    {
      "context_text": "In user-level information modeling, existing models either implicitly learn user information from training data such as learning user embedding (Li et al., 2015) or explicitly collect user profiles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to general methods and approaches for user-level information modeling.",
      "processing_time": 56.67736268043518,
      "citing_paper_id": "202788651",
      "cited_paper_id": 21669082
    },
    {
      "context_text": "RL-Persona , the personalized conversational system (Yang et al., 2018), which takes the advantages of deep reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RL-Persona, which is a personalized conversational system, but does not refer to it as a dataset. It is described as a method or system.",
      "processing_time": 58.142263650894165,
      "citing_paper_id": "202788651",
      "cited_paper_id": 7421176
    },
    {
      "context_text": "To facilitate the combination of user personalization u i and decoder hidden states, we incorporate a gate module (Tu et al., 2018) in our model: where f is the sigmoid funtion and o t refers to the decoder output in time step t .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (gate module) used in the model. The context focuses on the technical details of the model rather than the data used.",
      "processing_time": 58.569305419921875,
      "citing_paper_id": "202788651",
      "cited_paper_id": 7421176
    },
    {
      "context_text": "The KL-divergence can be replaced by Wasserstein distance which is implemented by Arjovsky et al. (2017) and is proved to be superior to KL-divergence by many experiments.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Wasserstein distance) and its superiority over KL-divergence.",
      "processing_time": 56.51904559135437,
      "citing_paper_id": "202788651",
      "cited_paper_id": 13943041
    },
    {
      "context_text": ", 2015) or explicitly collect user profiles as the accurate personalization (Zhang et al., 2017; Yang et al., 2018; Zhang et al., 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to collecting user profiles for personalization, which is too generic.",
      "processing_time": 57.03211736679077,
      "citing_paper_id": "202788651",
      "cited_paper_id": 21669082
    },
    {
      "context_text": "RL-Persona, the personalized conversational system (Yang et al., 2018), which takes the advantages of deep reinforcement learning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions RL-Persona, which is a personalized conversational system using deep reinforcement learning. However, it does not mention any specific datasets.",
      "processing_time": 57.10020971298218,
      "citing_paper_id": "202788651",
      "cited_paper_id": 21669082
    },
    {
      "context_text": "We follow (Gu et al., 2019) and compute these parameters as: akµk\nlog σ2k  =Wk(ui) + bk πk =\neak∑K i=1 e ak\n(5)\nTo obtain vk, we use the Gumbel-Softmax reparametrization to replace the exact sampling:\ngi = − log(− log(bi)) vk = e(ak+gk)/τ∑K i=1 e (ai+gi)/τ\n(6)\nwhere bi is a sample from U(0, 1), and…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only mathematical formulas and methods. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.03291392326355,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "Specifically, we follow the conventional setting in previous work (Gu et al., 2019) to compute BLEU scores using smoothing techniques (smoothing 7) 4.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing BLEU scores. No verifiable resources are identified.",
      "processing_time": 56.188738107681274,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "1939 CVAE, Wasserstein autoencoder (Gu et al., 2019) is also used for open-domain response generation to solve the issues of posterior collapse and vanishing latent variables.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of CVAE and Wasserstein autoencoder for response generation.",
      "processing_time": 58.065245389938354,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "As the extension of\nCVAE, Wasserstein autoencoder (Gu et al., 2019) is also used for open-domain response generation to solve the issues of posterior collapse and vanishing latent variables.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Wasserstein autoencoder) used for response generation.",
      "processing_time": 56.36657643318176,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "To this end, we build our model upon the state-of-the-art conversation model WAE (Gu et al., 2019) to model utterance-level and the user-level information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (WAE) which is not a dataset. The citation is focused on the method used.",
      "processing_time": 57.329161405563354,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "Such results indicate that the Wasserstein distance and the adversarial training can enhance model learning and address KLvanishing issue in VAEs, as a result of which achieves better results of generated responses, which is also confirmed in the previous research (Gu et al., 2019).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methods and findings.",
      "processing_time": 54.49870228767395,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "In consistent with previous study (Gu et al., 2019), we compute the similarity between the bag-of-words (BOW) embeddings representations of generated results and reference.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing similarity using BOW embeddings.",
      "processing_time": 55.129934549331665,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "We follow (Gu et al., 2019) and compute these parameters as:  ak μk log σ2 k  =Wk(ui) + bk",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for computing parameters in a conditional Wasserstein Auto-Encoder.",
      "processing_time": 55.993149280548096,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "DiaWAE-GMD, where the former is the stateof-the-art model for open-domain conversation generation (Gu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a model (DialogWAE) and its application in open-domain conversation generation.",
      "processing_time": 56.37162756919861,
      "citing_paper_id": "202788651",
      "cited_paper_id": 44129557
    },
    {
      "context_text": "2) benefited by the semantic capturing ability of WAEs, plenitude persona information can be gathered into the continuous space (Li et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (WAEs) and a general concept (persona information).",
      "processing_time": 55.84846901893616,
      "citing_paper_id": "202788651",
      "cited_paper_id": 69299722
    },
    {
      "context_text": "Such results indicate that the Wasserstein distance and the adversarial training can enhance model learning and address KL-vanishing issue in VAEs, as a result of which achieves better results of generated responses, which is also conﬁrmed in the previous research (Gu et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses methodological improvements in VAEs.",
      "processing_time": 54.61361002922058,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "As for utterance-level information modeling, previous works mainly construct models upon variational autoencoders (VAE) (Kingma and Welling, 2014).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VAE).",
      "processing_time": 54.183255672454834,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "CVAE, as an extension of VAE, supervises the generation process under an extra condition c .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CVAE) which is an extension of another method (VAE). No verifiable resources are identified.",
      "processing_time": 57.36459136009216,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "It is veriﬁed in various open-domain response generation situations that conditional variational autoencoders (CVAE) (Serban et al., 2017; Zhao et al., 2017) are effective for addressing the “universal response” issue.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the effectiveness of CVAE in response generation.",
      "processing_time": 56.01167964935303,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "To train a CVAE model, the log-likelihood objective log p θ ( x | c ) is maximized through pushing up its variational lower bound: where q θ ( z | x, c ) and p θ ( z | c ) represent the approximated conditional posterior and the conditional prior respectively, log p θ ( x | z, c ) represents the probability of reconstructing x conditioned on both z and c .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the mathematical formulation of the CVAE model. No verifiable resources are identified.",
      "processing_time": 56.00931692123413,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "CVAE , which is the conventional CVAE model and trained by KL-divergence.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (CVAE) and a training method (KL-divergence).",
      "processing_time": 56.18958306312561,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "As the extension of CVAE, Wasserstein autoencoder (Gu et al., 2019) is also used for open-domain response generation to solve the issues of posterior collapse and vanishing latent variables.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is about using a method (Wasserstein autoencoder) for a specific task (open-domain response generation).",
      "processing_time": 58.08433127403259,
      "citing_paper_id": "202788651",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "We use RMSprop optimizer (Hinton et al., 2012) to update the parameters of the generator and the discriminator, where the initial learn-3",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (RMSprop optimizer).",
      "processing_time": 54.58664393424988,
      "citing_paper_id": "202788651",
      "cited_paper_id": null
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 1872405
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 148574293
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 209082489
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 210164558
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 216527211
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 216529041
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 225103257
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 226227169
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": 257495891
    },
    {
      "context_text": "Though, this was not a requirement for this challenge.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, models, or methods. It only states a general requirement for a challenge, which is not related to a dataset.",
      "processing_time": 56.89996361732483,
      "citing_paper_id": "251307839",
      "cited_paper_id": null
    },
    {
      "context_text": "In this challenge, participants could use any datasets including external corpora, challenge training datasets to do model training.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to the possibility of using any datasets, which is too generic.",
      "processing_time": 56.05121564865112,
      "citing_paper_id": "251307839",
      "cited_paper_id": 9949285
    },
    {
      "context_text": "Impulse responses used for training this model consists of 150,000 simulated impulse responses at sampling rate 48kHz using the Image Source method [17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'simulated impulse responses' but does not specify a named dataset. The method 'Image Source method' is referenced, but it is not a dataset.",
      "processing_time": 57.20281481742859,
      "citing_paper_id": "251307839",
      "cited_paper_id": 10721495
    },
    {
      "context_text": "Participants were required to describe the datasets used for training their models in sufﬁcient detail in their extended journal papers, and provide a brief coverage in 2-page ICASSP grand challenge paper.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a requirement for participants to describe their datasets. No clear, verifiable resource names are provided.",
      "processing_time": 56.53219699859619,
      "citing_paper_id": "251307839",
      "cited_paper_id": 21340187
    },
    {
      "context_text": "We introduced the following changes in this challenge: (i) there are two tracks: Headset, and Speakerphone both has desktop and mobile recordings in testsets; (ii) All testclips in both tracks has 10-30s enrollment speech (primary talker) with or without noise; (iii) Personalized P.835 framework is…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only describes changes in a challenge setup. No clear identifiers for datasets are present.",
      "processing_time": 55.53632307052612,
      "citing_paper_id": "251307839",
      "cited_paper_id": 21722177
    },
    {
      "context_text": "Deep Speech Enhancement (DSE) models perform signiﬁcantly better than their classical counterparts [1–6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a comparison between DSE models and classical counterparts.",
      "processing_time": 54.6729736328125,
      "citing_paper_id": "251307839",
      "cited_paper_id": 221655409
    },
    {
      "context_text": "The resulting noise dataset has 152 audio classes and 60,000 clips [1].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions a 'noise dataset' with specific characteristics but does not provide a specific name. The dataset is described generically, which does not meet the criteria for inclusion.",
      "processing_time": 57.60927128791809,
      "citing_paper_id": "251307839",
      "cited_paper_id": 221655409
    },
    {
      "context_text": "Noise dataset and impulse responses are same as in 4th DNS Challenge [1].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions '4th DNS Challenge' which is a competition, but does not specify a downloadable dataset. The term 'noise dataset' is too generic without a specific identifier.",
      "processing_time": 57.42678904533386,
      "citing_paper_id": "251307839",
      "cited_paper_id": 221655409
    },
    {
      "context_text": "Noise data included in the training set is chosen from Au-dioSet [12] and is identical to noise set in 4th DNS Challenge [1].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "AudioSet"
      ],
      "dataset_descriptions": {
        "AudioSet": "Used to incorporate noise data into the training set for deep noise suppression, enhancing model robustness in noisy environments."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'AudioSet' and '4th DNS Challenge'. AudioSet is a well-known dataset, while the DNS Challenge is a competition. Only AudioSet fits the criteria for a dataset.",
      "processing_time": 62.07221841812134,
      "citing_paper_id": "251307839",
      "cited_paper_id": 221655409
    },
    {
      "context_text": "; (v) Personalized and non-personalized models for a track were sent to same subjective evaluation and ranked together, i.e. personalized and non-personalized models are treated alike and compared against each other.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for evaluating models. There are no clear identifiers for datasets in the text.",
      "processing_time": 55.962515354156494,
      "citing_paper_id": "251307839",
      "cited_paper_id": 225103257
    },
    {
      "context_text": "The non-causal DSE model is a fullband time domain domain based on end-to-end enhancement network (E3Net) which was originally proposed for the task of personalized speech enhancement using wideband signals [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions E3Net but does not refer to it as a dataset. It is described as a model or method for personalized speech enhancement.",
      "processing_time": 56.38870573043823,
      "citing_paper_id": "251307839",
      "cited_paper_id": 247940115
    },
    {
      "context_text": "We cleaned (enhanced) enrollment clips with a non-causal model based on E3Net architecture [15].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model (E3Net) and does not refer to a specific dataset. The context is about using a model for speech enhancement, which is not directly related to personalized text generation.",
      "processing_time": 58.339173555374146,
      "citing_paper_id": "251307839",
      "cited_paper_id": 247940115
    },
    {
      "context_text": "We evaluated all challenge models through our Personalized P.835 framework to get the subjective ratings used for computing the challenge metric.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a framework for evaluating models, which is not a dataset.",
      "processing_time": 56.027496576309204,
      "citing_paper_id": "251307839",
      "cited_paper_id": 252923993
    },
    {
      "context_text": "Baseline models were personalized and non-personalized variants of models presented in [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 55.34081983566284,
      "citing_paper_id": "251307839",
      "cited_paper_id": 253384308
    },
    {
      "context_text": "These contains standard forms of par-alanguage [13] including but not limited to The throat-clear, ”hmm” or ”mhm”, ”Huh?” or ”what?, Gasps, Sighs, Moans and groans, De-ceptive speech, Sincere speech, Speech with high-base, Speech with high-pitch, Speech with low-pitch, Conﬁdent speech, Tired speech…",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only lists examples of paralanguage forms without referring to a named dataset.",
      "processing_time": 55.82780051231384,
      "citing_paper_id": "251307839",
      "cited_paper_id": 271854712
    },
    {
      "context_text": "We do not prohibit the use of headset corpora for training speak-erphone models and vice versa.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generic 'headset corpora' and 'speakerphone models', which are not specific enough to be considered valid datasets.",
      "processing_time": 57.87124466896057,
      "citing_paper_id": "251307839",
      "cited_paper_id": null
    },
    {
      "context_text": "We do not prohibit the use of headset corpora for training speak-erphone models and vice versa.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generic 'headset corpora' and 'speakerphone models', which are not specific enough to be considered valid datasets.",
      "processing_time": 57.87124466896057,
      "citing_paper_id": "251307839",
      "cited_paper_id": null
    },
    {
      "context_text": "Other related topics include: personalized expression [17, 18], stylistic description [19, 20], online context-aware heuristic search [ 21, 22], and word-speciﬁc discriminative captioning [15] etc .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general topics and methods. No verifiable resources are identified.",
      "processing_time": 55.06999635696411,
      "citing_paper_id": "202770245",
      "cited_paper_id": 44614
    },
    {
      "context_text": "Other related topics include: personalized expression [17, 18], stylistic description [19, 20], online context-aware heuristic search [ 21, 22], and word-speciﬁc discriminative captioning [15] etc .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general topics and methods. No verifiable resources are identified.",
      "processing_time": 55.06999635696411,
      "citing_paper_id": "202770245",
      "cited_paper_id": 23414983
    },
    {
      "context_text": "Other related topics include: personalized expression [17, 18], stylistic description [19, 20], online context-aware heuristic search [ 21, 22], and word-speciﬁc discriminative captioning [15] etc .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general topics and methods. No verifiable resources are identified.",
      "processing_time": 55.06999635696411,
      "citing_paper_id": "202770245",
      "cited_paper_id": 29161017
    },
    {
      "context_text": "Other related topics include: personalized expression [17, 18], stylistic description [19, 20], online context-aware heuristic search [ 21, 22], and word-speciﬁc discriminative captioning [15] etc .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general topics and methods. No verifiable resources are identified.",
      "processing_time": 55.06999635696411,
      "citing_paper_id": "202770245",
      "cited_paper_id": 52830169
    },
    {
      "context_text": "Therefore, generative models, such as GAN and VAE, are typically exploited to handle diverse image captioning [5, 6, 7, 8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models (GAN and VAE) and their application to image captioning. No verifiable datasets are named.",
      "processing_time": 57.213361978530884,
      "citing_paper_id": "202770245",
      "cited_paper_id": 665667
    },
    {
      "context_text": "Therefore, generative models, such as GAN and VAE, are typically exploited to handle diverse image captioning [5, 6, 7, 8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only generative models (GAN and VAE) and their application to image captioning. No verifiable datasets are named.",
      "processing_time": 57.213361978530884,
      "citing_paper_id": "202770245",
      "cited_paper_id": 6282691
    },
    {
      "context_text": "The quality of captioning results lies in both accuracy (a basic evaluation of captioning quality and has been used together with the subsequent diversity metrics in [8, 5, 6]) and diversity.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general aspects of captioning quality and diversity metrics.",
      "processing_time": 54.57369327545166,
      "citing_paper_id": "202770245",
      "cited_paper_id": 665667
    },
    {
      "context_text": "…(only aiming at accuracy) seems far-fetched due to the mutual interference between accuracy and diversity (a more diverse caption tends to be more inconsistent with the ground truth caption) [9, 6, 5], where, therefore, the pure image captioning methods are taken as extraessential references.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses the challenges and methods related to image captioning.",
      "processing_time": 54.913665533065796,
      "citing_paper_id": "202770245",
      "cited_paper_id": 665667
    },
    {
      "context_text": "Besides, we compare VSSI-cap with 1) other recent diverse image captioning methods, including G-GAN [6], GMM-CVAE [8], and CAL [9], 2) the state-of-the-art image captioning method, i.e. , Up-Down (beam search) [3], and 3) Human : a sentence randomly sampled from ground-truth/manually-labeled…",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The term 'ground-truth/manually-labeled' is too generic and lacks a specific identifier.",
      "processing_time": 57.55531716346741,
      "citing_paper_id": "202770245",
      "cited_paper_id": 665667
    },
    {
      "context_text": "Several recent works have been proposed to investigate diverse image captioning [5, 6, 7, 8, 9], which typically employed a Generative Adversarial Network (GAN) or Variational Auto-Encoder (VAE) as the generative model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models (GAN and VAE). No verifiable resources are identified.",
      "processing_time": 55.918591260910034,
      "citing_paper_id": "202770245",
      "cited_paper_id": 665667
    },
    {
      "context_text": "Several recent works have been proposed to investigate diverse image captioning [5, 6, 7, 8, 9], which typically employed a Generative Adversarial Network (GAN) or Variational Auto-Encoder (VAE) as the generative model.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only generative models (GAN and VAE). No verifiable resources are identified.",
      "processing_time": 55.918591260910034,
      "citing_paper_id": "202770245",
      "cited_paper_id": 6282691
    },
    {
      "context_text": "KL annealing method [34] is adopted to reduce the KL vanishing (see the supplementary material for the training details).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (KL annealing) used in training. The context is focused on a technical detail of the training process.",
      "processing_time": 56.901578187942505,
      "citing_paper_id": "202770245",
      "cited_paper_id": 748227
    },
    {
      "context_text": "We compare the proposed VSSI-cap with four baselines: 1) ErDr-cap : a caption generator trained based on encoder-decoder (beam search) [29] that represents the mainstream of general image captioning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (ErDr-cap) which is a caption generator. The title 'Show and tell: A neural image caption generator' confirms that the cited paper is about a method, not a dataset.",
      "processing_time": 59.391379833221436,
      "citing_paper_id": "202770245",
      "cited_paper_id": 1169492
    },
    {
      "context_text": "In LSTM, we use the same vector dimensions of the hidden states as [29], which is set as 512.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LSTM) and a parameter setting. There are no verifiable resources or datasets mentioned.",
      "processing_time": 56.55347537994385,
      "citing_paper_id": "202770245",
      "cited_paper_id": 1169492
    },
    {
      "context_text": "Most state-of-the-art image captioning models adopt an encoder-decoder architecture [1, 2, 3], which encodes the image into a feature representation via Convolutional Neural Network (CNN) and then decodes the feature into a caption via Recurrent Neural Networks with Long-Short Term Memory units…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.886128187179565,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3120635
    },
    {
      "context_text": "Most state-of-the-art image captioning models adopt an encoder-decoder architecture [1, 2, 3], which encodes the image into a feature representation via Convolutional Neural Network (CNN) and then decodes the feature into a caption via Recurrent Neural Networks with Long-Short Term Memory units…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.886128187179565,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "Most state-of-the-art image captioning models adopt an encoder-decoder architecture [1, 2, 3], which encodes the image into a feature representation via Convolutional Neural Network (CNN) and then decodes the feature into a caption via Recurrent Neural Networks with Long-Short Term Memory units…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and architectures. The cited papers' titles do not introduce any specific datasets either.",
      "processing_time": 55.886128187179565,
      "citing_paper_id": "202770245",
      "cited_paper_id": 18347865
    },
    {
      "context_text": "We conduct all the experiments on the MSCOCO dataset 5 [30], which is widely used for image captioning [1, 3] and diverse image captioning [5, 8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used for image captioning experiments, focusing on generating diverse and accurate captions for images using attention mechanisms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the MSCOCO dataset for image captioning experiments. The dataset is a well-known resource in the field and is used for generating captions for images.",
      "processing_time": 61.16654109954834,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3120635
    },
    {
      "context_text": "We conduct all the experiments on the MSCOCO dataset 5 [30], which is widely used for image captioning [1, 3] and diverse image captioning [5, 8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MSCOCO"
      ],
      "dataset_descriptions": {
        "MSCOCO": "Used for image captioning experiments, focusing on generating diverse and accurate captions for images using attention mechanisms."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the use of the MSCOCO dataset for image captioning experiments. The dataset is a well-known resource in the field and is used for generating captions for images.",
      "processing_time": 61.16654109954834,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "Many state-of-the-art methods [2, 3] adopt a maximum likelihood principle to train the models by using the image-caption pair set 3.2).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'image-caption pair set' but does not specify a named dataset. The cited papers are about image captioning but do not provide a specific dataset name in the context.",
      "processing_time": 57.85761642456055,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "Many state-of-the-art methods [2, 3] adopt a maximum likelihood principle to train the models by using the image-caption pair set 3.2).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'image-caption pair set' but does not specify a named dataset. The cited papers are about image captioning but do not provide a specific dataset name in the context.",
      "processing_time": 57.85761642456055,
      "citing_paper_id": "202770245",
      "cited_paper_id": 18347865
    },
    {
      "context_text": "…image captioning methods, including G-GAN [6], GMM-CVAE [8], and CAL [9], 2) the state-of-the-art image captioning method, i.e. , Up-Down (beam search) [3], and 3) Human : a sentence randomly sampled from ground-truth/manually-labeled annotations of each image is used as the output of this method.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'ground-truth/manually-labeled annotations' but does not provide a specific dataset name. The reference to 'Human' as a method using these annotations is clear, but the dataset itself is not named.",
      "processing_time": 58.97001361846924,
      "citing_paper_id": "202770245",
      "cited_paper_id": 3753452
    },
    {
      "context_text": "In the encoder, the visual feature v is extracted from the last fully-connected layer of CNN [27] while the j -th word’s feature e j ( j ∈ { 1 , . . . , M } corresponds to the j -th tree node) of the caption S is extracted by textual parsing and word embedding as aforementioned.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (CNN) and a process (word embedding).",
      "processing_time": 56.20561456680298,
      "citing_paper_id": "202770245",
      "cited_paper_id": 14124313
    },
    {
      "context_text": "1 (Left-Top), it is quite intuitive to derive heterogeneous understanding from human being, while the traditional models typically tend to generate homogeneous sentences due to the limited variation in the maximum likelihood objective [4].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to traditional models and their limitations.",
      "processing_time": 54.72994422912598,
      "citing_paper_id": "202770245",
      "cited_paper_id": 15208089
    },
    {
      "context_text": "We brieﬂy present the variational auto-encoder (VAE) [23, 24] and its conditional variant [ 25, 26], which serves as the fundamental framework of the proposed structured encoder-inferer-decoder scheme.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about VAEs and their variants, which are not datasets.",
      "processing_time": 56.40043926239014,
      "citing_paper_id": "202770245",
      "cited_paper_id": 16895865
    },
    {
      "context_text": "We brieﬂy present the variational auto-encoder (VAE) [23, 24] and its conditional variant [ 25, 26], which serves as the fundamental framework of the proposed structured encoder-inferer-decoder scheme.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are about VAEs and their variants, which are not datasets.",
      "processing_time": 56.40043926239014,
      "citing_paper_id": "202770245",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "For diversity modeling, we infer the lexical and syntactic variables from the visual content by leveraging the visual parsing tree (VP-tree) [13, 14, 15, 16], which predicts the probability distributions of the lexical and syntactic categories to weight the latent variables in variational…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The VP-tree is described as a method for predicting probability distributions, not a dataset.",
      "processing_time": 56.39474701881409,
      "citing_paper_id": "202770245",
      "cited_paper_id": 21003857
    },
    {
      "context_text": "For diversity modeling, we infer the lexical and syntactic variables from the visual content by leveraging the visual parsing tree (VP-tree) [13, 14, 15, 16], which predicts the probability distributions of the lexical and syntactic categories to weight the latent variables in variational…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The VP-tree is described as a method for predicting probability distributions, not a dataset.",
      "processing_time": 56.39474701881409,
      "citing_paper_id": "202770245",
      "cited_paper_id": 52830169
    },
    {
      "context_text": "We parse the captions by using the Stanford Parser [ 32] as well as pruning the textual parsing results by using the pos-tag tool and the lemmatizer tool in NTLK [33], where the dynamic textual parsing trees are converted to a ﬁxed-structured, three-layer, complete binary tree as designed in [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only tools and methods used for processing text. The cited papers are not referenced for their datasets.",
      "processing_time": 55.65350532531738,
      "citing_paper_id": "202770245",
      "cited_paper_id": 21003857
    },
    {
      "context_text": "2) inferer : Inspired by the recent work in visual semantic parsing [13], a VarMI-tree is proposed to infer the latent variables with variations for the lexical and syntactic diversities.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or model (VarMI-tree) inspired by another work. The context is about inferring latent variables for lexical and syntactic diversities, which is not directly related to a dataset.",
      "processing_time": 58.96050214767456,
      "citing_paper_id": "202770245",
      "cited_paper_id": 21003857
    },
    {
      "context_text": "Visual parsing tree (VP-tree) is ﬁrstly proposed in [13], which serves as a robust parser to discover visual entities and their relations from a given image.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (VP-tree) for parsing images. The context is about a technique for image captioning, not a dataset.",
      "processing_time": 57.194188356399536,
      "citing_paper_id": "202770245",
      "cited_paper_id": 21003857
    },
    {
      "context_text": "…obtained: To enable the differentiability in the end-to-end manner, we reparameterize z ( ℓ ) j into ~ z ( ℓ ) j via the reparameterization trick [23] as: ~ z ( ℓ ) j = (cid:22) ( ℓ ) j + (cid:27) ( ℓ ) ⊙ \" ( ℓ ) , (8) where \" ( ℓ ) obeys a standard Gaussian distribution to introduce noise for…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (reparameterization trick) from the cited paper 'Auto-Encoding Variational Bayes'.",
      "processing_time": 56.31569981575012,
      "citing_paper_id": "202770245",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "Contrary to (Paulus et al., 2017; See et al., 2017; Nal-lapati et al., 2016), we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of Byte-Pair Encoding (BPE) to improve the model copy mechanism.",
      "processing_time": 57.30110955238342,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "Contrary to (Paulus et al., 2017; See et al., 2017; Nal-lapati et al., 2016), we rely on Byte-Pair Encoding (BPE) (Sennrich et al., 2016) to improve the model copy mechanism instead of using an additional pointer mechanism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on the use of Byte-Pair Encoding (BPE) to improve the model copy mechanism.",
      "processing_time": 57.30110955238342,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "For models not trained with BPE, the source and target vocabulary consists of all words appearing at least 20 times, creating a source vocabulary size of 47,174 and a target vocabulary size of 21,214.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only describes the creation of vocabularies for a machine translation model.",
      "processing_time": 55.74217247962952,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "Our improvements with BPE are comparable to results on machine translation (Sennrich et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (BPE) and a reference to machine translation results.",
      "processing_time": 55.347808837890625,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "The BPE vocabulary improves the ability to copy proper nouns and rare inﬂections, both of which are difﬁcult to model in word-based vocabularies.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses the BPE vocabulary, which is a method, not a dataset.",
      "processing_time": 56.18440914154053,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "We use BPE with 30K types (Sennrich et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions BPE (Byte Pair Encoding) but does not refer to a specific dataset. BPE is a method for handling rare words in neural machine translation.",
      "processing_time": 56.89218521118164,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "We use byte-pair-encoding (BPE) to tokenize the data which has been shown to enable copying of proper nouns in translation (Sennrich et al., 2016).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (byte-pair-encoding) and its application in tokenizing data.",
      "processing_time": 56.977694272994995,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "Finally, we switch data tokenization to BPE instead of a word-based vocabulary which gives another +0.79 F1-ROUGE1.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses a change in tokenization method from word-based to BPE.",
      "processing_time": 55.7340669631958,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1114678
    },
    {
      "context_text": "…et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of attention mechanisms in NLP tasks. No verifiable resources are identified.",
      "processing_time": 56.2315399646759,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "…et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of attention mechanisms in NLP tasks. No verifiable resources are identified.",
      "processing_time": 56.2315399646759,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": ", 2017) following their success in translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods and models used in neural machine translation and parsing.",
      "processing_time": 55.1664936542511,
      "citing_paper_id": "22716243",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "Table 2 details the effect of our design choices on top of Gehring et al. (2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to another paper's design choices.",
      "processing_time": 54.58039832115173,
      "citing_paper_id": "22716243",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Following (Gehring et al., 2017), we rely on convolutional networks, in contrast to previous work us-ing recurrent networks (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.065189361572266,
      "citing_paper_id": "22716243",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Following (Gehring et al., 2017), we rely on convolutional networks, in contrast to previous work us-ing recurrent networks (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 56.065189361572266,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Our approach builds upon the convolutional encoder-decoder model from (Gehring et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model from the paper but does not refer to any specific dataset. The context is about building upon a method, not using a dataset.",
      "processing_time": 56.80496168136597,
      "citing_paper_id": "22716243",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "We introduce multi-hop intra-attention inspired by multi-hop source attention from (Gehring et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (multi-hop source attention).",
      "processing_time": 54.41918110847473,
      "citing_paper_id": "22716243",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "We train our models following (Gehring et al., 2017), using Nesterov’s accelerated gradient method (Sutskever et al., 2013) with gradient clipping 0.1 (Pascanu et al., 2013), momentum 0.99, and learning rate 0.2.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only training methods and parameters. No verifiable resources are identified.",
      "processing_time": 55.15168833732605,
      "citing_paper_id": "22716243",
      "cited_paper_id": 3648736
    },
    {
      "context_text": "Text generation is an established research area (McKeown, 1992).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It only references a general research area.",
      "processing_time": 54.82520866394043,
      "citing_paper_id": "22716243",
      "cited_paper_id": 5053249
    },
    {
      "context_text": "Efforts in both extractive and abstractive meth-ods have followed advances in the ﬁeld of natural language processing, pattern recognition, and machine learning (Nenkova et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general advancements in NLP, pattern recognition, and machine learning.",
      "processing_time": 55.22848892211914,
      "citing_paper_id": "22716243",
      "cited_paper_id": 5393989
    },
    {
      "context_text": "Extractive algorithms form summaries by pasting relevant portions of the input together, while abstractive summarization algorithms may generate new text that is not present in the initial document (Das and Martins, 2007; Nenkova et al., 2011).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only types of summarization algorithms. No verifiable resources are identified.",
      "processing_time": 55.471983671188354,
      "citing_paper_id": "22716243",
      "cited_paper_id": 5393989
    },
    {
      "context_text": ", 2015b), parsing (Luong et al., 2015a) and image captioning (Vinyals et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only tasks and methods. There are no clear identifiers for datasets.",
      "processing_time": 55.29740333557129,
      "citing_paper_id": "22716243",
      "cited_paper_id": 6954272
    },
    {
      "context_text": "Recently, sequence-to-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about the application of sequence-to-sequence neural networks in various NLP tasks, including abstractive summarization and machine translation.",
      "processing_time": 57.88652205467224,
      "citing_paper_id": "22716243",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Recently, sequence-to-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in both machine translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The context is about the application of sequence-to-sequence neural networks in various NLP tasks, including abstractive summarization and machine translation.",
      "processing_time": 57.88652205467224,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Our work builds on sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015), which have been extensively applied to the task of abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers are about sequence-to-sequence models and their application to abstractive summarization, but do not introduce or use specific datasets.",
      "processing_time": 57.8818039894104,
      "citing_paper_id": "22716243",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Our work builds on sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015), which have been extensively applied to the task of abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers are about sequence-to-sequence models and their application to abstractive summarization, but do not introduce or use specific datasets.",
      "processing_time": 57.8818039894104,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Our work builds on sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015), which have been extensively applied to the task of abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only models and methods. The cited papers are about sequence-to-sequence models and their application to abstractive summarization, but do not introduce or use specific datasets.",
      "processing_time": 57.8818039894104,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": ", 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in translation (Bahdanau et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and models used in abstractive summarization and translation.",
      "processing_time": 55.6918888092041,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "This combination allows us to forgo an additional pointer mechanism unlike (Paulus et al., 2017; See et al., 2017; Nallapati et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to methods or models. The context is about summarization techniques and does not indicate the use of a particular dataset.",
      "processing_time": 56.928340911865234,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "On the original text, we report 39.75 F1-ROUGE1 as opposed to 39.53 for (See et al., 2017).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a performance metric comparison.",
      "processing_time": 53.88609862327576,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Pointer mechanisms (Vinyals et al., 2015a) have been useful for abstractive summarization where copying entities and other rare words from the input is highly advantageous (See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and their applications. The cited papers are about pointer mechanisms and their use in abstractive summarization, but do not specify datasets.",
      "processing_time": 57.41395568847656,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": ", 2015), which have been extensively applied to abstractive summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods and research works.",
      "processing_time": 54.097421169281006,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": ", 2017) and Table 4 reports results on the full text data like (See et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The citation does not mention any specific dataset names, only references to 'full text data'. This is too generic to be considered a verifiable dataset.",
      "processing_time": 56.40090274810791,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Our study compares summarization with fixed value control variables on full text CNN-Dailymail with (See et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CNN-Dailymail"
      ],
      "dataset_descriptions": {
        "CNN-Dailymail": "Used to compare summarization techniques with fixed value control variables, focusing on generating summaries from full text articles."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'CNN-Dailymail' as a dataset used for summarization experiments. The dataset is clearly identified and relevant to the research topic of personalized text generation.",
      "processing_time": 60.23033857345581,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": ", 2017) and the full text version (See et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 53.79796838760376,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Research in abstractive summarization with sequence-to-sequence models focuses on neural architectures (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and learning objectives (Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to neural architectures and learning objectives in abstractive summarization.",
      "processing_time": 55.377912521362305,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Table 4 shows results on the entity-anonymized version of the dataset used by (Nallapati et al., 2016; Paulus et al., 2017) and Table 5 reports re-sults on the original version of the dataset used by (See et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions 'entity-anonymized version of the dataset' and 'original version of the dataset', but does not provide specific names. The cited papers do not clarify the dataset name.",
      "processing_time": 56.98353958129883,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "For instance, (See et al., 2017) pairs attention with a coverage mechanism to avoid repetition and (Paulus et al., 2017) relies on intra-decoder attention to enable generating coherent multi-sentence summaries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only methods and models. The titles of the cited papers do not provide additional information about datasets.",
      "processing_time": 55.81800317764282,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "The statistics of the dataset are re-ported in Table 1 after limiting the length of the train documents to 400, as suggested by (See et al., 2017).",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions a dataset but does not provide a specific name. It only refers to 'the dataset' without any additional qualifiers or identifiers.",
      "processing_time": 55.81534028053284,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "To address this impediment, (See et al., 2017) introduce coverage modeling, (Paulus et al.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (coverage modeling) introduced in the cited paper.",
      "processing_time": 54.50044298171997,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "To address this impediment, (See et al., 2017) introduce coverage modeling, (Paulus et al., 2017) propose intra-decoder attention, and (Suzuki and Nagata, 2017) equip the decoder with an estimator of unigram frequency.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods and models. The cited papers are referenced for their contributions to summarization techniques, not for datasets.",
      "processing_time": 56.25775957107544,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "We compare to existing abstractive baselines (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and report results on the Lead-3 extraction base-line which simply selects the ﬁrst three sentences of the input article as its summary.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to other works and a baseline method. No verifiable resources are identified.",
      "processing_time": 55.525707721710205,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "We compare to existing abstractive baselines (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other research works. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 55.88692808151245,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Most of our experiments are performed with articles truncated at 400 tokens, as suggested by (See et al., 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset names, only a method for truncating articles. The cited paper title confirms it is about summarization techniques, not a specific dataset.",
      "processing_time": 56.37376403808594,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Recent summarization models build upon pointer networks (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) and have a few main architectural differences.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The cited papers are used to reference architectural differences in summarization models.",
      "processing_time": 55.723883628845215,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": ", 2015a) help abstractive summarization to copy entities from the input (See et al., 2017; Paulus et al., 2017; Nallapati et al., 2016).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods or models used for abstractive summarization.",
      "processing_time": 54.49233102798462,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "We evaluate on two versions of this dataset, the entity anonymized version (Hermann et al., 2015; Nallapati et al., 2016; Paulus et al., 2017) and the full text version (See et al., 2017) 1 .",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "entity anonymized version",
        "full text version"
      ],
      "dataset_descriptions": {
        "entity anonymized version": "Used to evaluate summarization models, focusing on entity anonymization to test model performance without specific entity information.",
        "full text version": "Used to evaluate summarization models, focusing on full text to test model performance with complete information."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions two specific versions of a dataset used for evaluation, which aligns with the topic of personalized text generation, particularly in summarization.",
      "processing_time": 60.949448347091675,
      "citing_paper_id": "22716243",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "The field follows recent advances in generative models, such as the introduction of variational auto-encoders (Kingma and Welling, 2013) and adversarial networks (Goodfellow et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models and methods.",
      "processing_time": 53.32789206504822,
      "citing_paper_id": "22716243",
      "cited_paper_id": 10319744
    },
    {
      "context_text": "In addition to attending over encoder states (Bahdanau et al., 2015), we also use intra-attention in the decoder to enable the model to refer back to previously generated words at any time scale.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. There are no verifiable resources that meet the criteria.",
      "processing_time": 54.64248204231262,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The decoder is connected to the encoder through an attention module (Bahdanau et al., 2015) after each GLU.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention module) and a component (GLU).",
      "processing_time": 54.520140647888184,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "The decoder is connected to the encoder through an attention module (Bahdanau et al., 2015) after each GLU.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (attention module) and a component (GLU).",
      "processing_time": 54.520140647888184,
      "citing_paper_id": "22716243",
      "cited_paper_id": 16119010
    },
    {
      "context_text": "The encoder module builds a representation of the input document and the de-coder generates a summary by attending to the the source representation (Bahdanau et al., 2015).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method for generating summaries using an encoder-decoder model with attention.",
      "processing_time": 54.516544342041016,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "For instance, attention mechanisms (Bahdanau et al., 2015) enable generation to focus on a targeted part of the source document.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (attention mechanisms).",
      "processing_time": 53.27355337142944,
      "citing_paper_id": "22716243",
      "cited_paper_id": 11212020
    },
    {
      "context_text": "We implement our models in Torch (Collobert et al., 2011) and on top of the fairseq library 2 .",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions using Torch and fairseq library but does not refer to any specific datasets. These are tools and libraries, not datasets.",
      "processing_time": 54.84990620613098,
      "citing_paper_id": "22716243",
      "cited_paper_id": 14365368
    },
    {
      "context_text": "Both are composed of several layers which start with a word embedding layer followed by alternating convolutions and non-linear Gated Linear Units, GLU (Dauphin et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only model architectures and components.",
      "processing_time": 52.80317544937134,
      "citing_paper_id": "22716243",
      "cited_paper_id": 16119010
    },
    {
      "context_text": "Research in computer vision includes style transfer (Gatys et al., 2015) or controllable image generation (Lample et al., 2017).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and research works.",
      "processing_time": 52.999232053756714,
      "citing_paper_id": "22716243",
      "cited_paper_id": 27009824
    },
    {
      "context_text": "The ﬁeld follows recent advances in generative models, such as the introduction of variational auto-encoders (Kingma and Welling, 2013) and adversarial networks (Good-fellow et al., 2014).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only generative models and methods. No dataset names are present in the citation span.",
      "processing_time": 54.71647000312805,
      "citing_paper_id": "22716243",
      "cited_paper_id": 216078090
    },
    {
      "context_text": "We use the CNN/Dailymail dataset (Hermann et al., 2015; Nallapati et al., 2016) which consists of online news articles along with multi-sentence summaries.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CNN/Dailymail"
      ],
      "dataset_descriptions": {
        "CNN/Dailymail": "Used for training and evaluating summarization models, focusing on generating multi-sentence summaries from online news articles."
      },
      "confidence_score": 1.0,
      "reasoning": "The context clearly mentions the CNN/Dailymail dataset, which is a well-known dataset used for summarization tasks in NLP.",
      "processing_time": 58.0140540599823,
      "citing_paper_id": "22716243",
      "cited_paper_id": null
    },
    {
      "context_text": "We are interested in applying captioning models in this paper, so we chose state-of-the-art standard models, in particular, Xu et al. [38] where they used an LSTM model with attention and Cornia et al. [9] where they adopted a transformer for the captioning task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation intent is to reference research work, not datasets.",
      "processing_time": 54.68572688102722,
      "citing_paper_id": "248227685",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "We also train “Show, Attend and Tell” (SAT) [38] which is based on LSTM [19] and Meshed-Memory Transformers (M(2)) [9], the state-of-the-art captioning model on MS-COCO dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to evaluate the performance of the 'Show, Attend and Tell' and Meshed-Memory Transformers models on image captioning tasks, focusing on state-of-the-art results."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MS-COCO dataset, which is a well-known image captioning dataset. The dataset is used to evaluate the performance of the 'Show, Attend and Tell' model and the Meshed-Memory Transformers model.",
      "processing_time": 60.104076862335205,
      "citing_paper_id": "248227685",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "We also train “Show, Attend and Tell” (SAT) [38] which is based on LSTM [19] and Meshed-Memory Transformers (M(2)) [9], the state-of-the-art captioning model on MS-COCO dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to evaluate the performance of the 'Show, Attend and Tell' and Meshed-Memory Transformers models on image captioning tasks, focusing on state-of-the-art results."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MS-COCO dataset, which is a well-known image captioning dataset. The dataset is used to evaluate the performance of the 'Show, Attend and Tell' model and the Meshed-Memory Transformers model.",
      "processing_time": 60.104076862335205,
      "citing_paper_id": "248227685",
      "cited_paper_id": 219635470
    },
    {
      "context_text": "We also train “Show, Attend and Tell” (SAT) [38] which is based on LSTM [19] and Meshed-Memory Transformers (M2) [9], the state-of-the-art captioning model on MS-COCO dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MS-COCO"
      ],
      "dataset_descriptions": {
        "MS-COCO": "Used to train and evaluate the state-of-the-art captioning model, Meshed-Memory Transformers (M2), focusing on image captioning performance."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the MS-COCO dataset, which is a well-known image captioning dataset. The dataset is used to train and evaluate the state-of-the-art captioning model, Meshed-Memory Transformers (M2).",
      "processing_time": 59.86721134185791,
      "citing_paper_id": "248227685",
      "cited_paper_id": 1915014
    },
    {
      "context_text": "We view ArtEmis as a unique dataset that can act as a bridge between different emotion theories, in particular the well established Theory of Constructed Emotions [3, 4, 5, 6] and the recently proposed Semantic Space Theory of Emotions [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ArtEmis"
      ],
      "dataset_descriptions": {
        "ArtEmis": "Used to explore the relationship between different emotion theories, specifically bridging the Theory of Constructed Emotions and the Semantic Space Theory of Emotions through active inference and categorization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ArtEmis' as a dataset, which is described as a bridge between different emotion theories. No other datasets are mentioned.",
      "processing_time": 59.04742741584778,
      "citing_paper_id": "248227685",
      "cited_paper_id": 2032354
    },
    {
      "context_text": "We view ArtEmis as a unique dataset that can act as a bridge between different emotion theories, in particular the well established Theory of Constructed Emotions [3, 4, 5, 6] and the recently proposed Semantic Space Theory of Emotions [11].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "ArtEmis"
      ],
      "dataset_descriptions": {
        "ArtEmis": "Used to explore the relationship between different emotion theories, specifically bridging the Theory of Constructed Emotions and the Semantic Space Theory of Emotions through active inference and categorization."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'ArtEmis' as a dataset, which is described as a bridge between different emotion theories. No other datasets are mentioned.",
      "processing_time": 59.04742741584778,
      "citing_paper_id": "248227685",
      "cited_paper_id": 189277712
    },
    {
      "context_text": "Captioning datasets, such as [22, 23, 24, 27, 32, 33, 39], contain only factual explanations for a given image.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 55.3028302192688,
      "citing_paper_id": "248227685",
      "cited_paper_id": 3104920
    },
    {
      "context_text": "Captioning datasets, such as [22, 23, 24, 27, 32, 33, 39], contain only factual explanations for a given image.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 55.3028302192688,
      "citing_paper_id": "248227685",
      "cited_paper_id": 6308361
    },
    {
      "context_text": "Captioning datasets, such as [22, 23, 24, 27, 32, 33, 39], contain only factual explanations for a given image.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'captioning datasets' but does not specify any particular dataset names. The cited paper titles do not provide additional clarity on specific datasets.",
      "processing_time": 55.3028302192688,
      "citing_paper_id": "248227685",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "The proliferation of large-scale captioning datasets allowed for the development of many deep neural network based captioning methods [26, 27, 28, 37, 40, 41].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'large-scale captioning datasets' but does not specify any particular dataset names. The citation is used to support the statement about the development of captioning methods.",
      "processing_time": 55.662649631500244,
      "citing_paper_id": "248227685",
      "cited_paper_id": 4406645
    },
    {
      "context_text": "Moreover, they show that these categories have smooth boundaries between them contrary to the belief that emotional categories are discrete [7, 8, 15, 20, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to emotional categories and their boundaries. No verifiable resources are identified.",
      "processing_time": 54.74393606185913,
      "citing_paper_id": "248227685",
      "cited_paper_id": 27947192
    },
    {
      "context_text": "Moreover, they show that these categories have smooth boundaries between them contrary to the belief that emotional categories are discrete [7, 8, 15, 20, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to emotional categories and their boundaries. No verifiable resources are identified.",
      "processing_time": 54.74393606185913,
      "citing_paper_id": "248227685",
      "cited_paper_id": 33644442
    },
    {
      "context_text": "Moreover, they show that these categories have smooth boundaries between them contrary to the belief that emotional categories are discrete [7, 8, 15, 20, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to emotional categories and their boundaries. No verifiable resources are identified.",
      "processing_time": 54.74393606185913,
      "citing_paper_id": "248227685",
      "cited_paper_id": 141839578
    },
    {
      "context_text": "Moreover, they show that these categories have smooth boundaries between them contrary to the belief that emotional categories are discrete [7, 8, 15, 20, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to emotional categories and their boundaries. No verifiable resources are identified.",
      "processing_time": 54.74393606185913,
      "citing_paper_id": "248227685",
      "cited_paper_id": 141954122
    },
    {
      "context_text": "Moreover, they show that these categories have smooth boundaries between them contrary to the belief that emotional categories are discrete [7, 8, 15, 20, 35].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to emotional categories and their boundaries. No verifiable resources are identified.",
      "processing_time": 54.74393606185913,
      "citing_paper_id": "248227685",
      "cited_paper_id": 259806197
    },
    {
      "context_text": "[10, 12, 13, 14] collected emotional experiences induced as a result of different sensory information.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to collecting emotional experiences, which is too generic.",
      "processing_time": 54.83274483680725,
      "citing_paper_id": "248227685",
      "cited_paper_id": 58563174
    },
    {
      "context_text": "However, a major drawback of GoEmotions and similar datasets [10, 12, 13, 14] is that they attribute emotional experiences to a single stimulus.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "GoEmotions"
      ],
      "dataset_descriptions": {
        "GoEmotions": "Mentioned as a dataset attributing emotional experiences to a single stimulus, highlighting a limitation in its design for complex emotional analysis."
      },
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'GoEmotions' as a dataset but does not provide specific details on its usage or the research context. The cited paper titles do not help in disambiguating further.",
      "processing_time": 58.961658239364624,
      "citing_paper_id": "248227685",
      "cited_paper_id": 58563174
    },
    {
      "context_text": "These biases can sometimes be mild, but they can also be very problematic, especially in ethical judgment and applications that interact with humans [17, 21, 31, 36].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses biases in ethical judgments and human interactions. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 55.486557483673096,
      "citing_paper_id": "248227685",
      "cited_paper_id": 145741260
    },
    {
      "context_text": "These biases can sometimes be mild, but they can also be very problematic, especially in ethical judgment and applications that interact with humans [17, 21, 31, 36].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only discusses biases in ethical judgments and human interactions. The cited papers do not provide additional context to identify a dataset.",
      "processing_time": 55.486557483673096,
      "citing_paper_id": "248227685",
      "cited_paper_id": 149581024
    },
    {
      "context_text": "[17] showed that people ethically condemn certain behaviors based on a bad outcome, even though the outcome is determined randomly.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general finding about ethical judgments. There are no verifiable resources or specific datasets mentioned.",
      "processing_time": 55.03117394447327,
      "citing_paper_id": "248227685",
      "cited_paper_id": 145741260
    },
    {
      "context_text": "They also trained Meshed Memory Transformers [9] as well as “Show, Attend and Tell” model [38].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions training models but does not refer to any specific datasets. The focus is on the models themselves rather than the data used.",
      "processing_time": 54.82763671875,
      "citing_paper_id": "248227685",
      "cited_paper_id": 219635470
    },
    {
      "context_text": "[9] where they adopted a transformer for the captioning task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method (transformer) used for image captioning.",
      "processing_time": 54.36266279220581,
      "citing_paper_id": "248227685",
      "cited_paper_id": 219635470
    },
    {
      "context_text": "Communication grounded in images is naturally engaging to humans [18], for example billions are shared and discussed daily online.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general observation about image sharing on Instagram.",
      "processing_time": 54.085397481918335,
      "citing_paper_id": "53022581",
      "cited_paper_id": 239717
    },
    {
      "context_text": "e 2: Generative model performance on COCO caption using the test split of [24] Model Text Pre- Flickr30k COCO training R@1 R@5 R@10 R@1 R@5 R@10 UVS [25] - 23.0 50.7 62.9 43.4 75.7 85.8 Embedding Net [51] - 40.7 69.7 79.2 50.4 79.3 69.4 sm-LSTM [19] - 42.5 71.9 81.5 53.2 83.1 91.5 VSE++ (ResNet, FT) [13] - 52.9 80.5 87.2 64.6 90.0 95.7 GXN (i2t+t2i) [15] - 56.8 - 89.6 68.5 - 97.9 TransResNet model var",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO": "Used to evaluate generative model performance on image captioning, focusing on recall metrics at different ranks.",
        "Flickr30k": "Used to evaluate generative model performance on image-caption pairs, focusing on recall and precision metrics."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO caption' and 'Flickr30k', which are known image captioning datasets. These are used to evaluate generative model performance.",
      "processing_time": 59.96797513961792,
      "citing_paper_id": "53022581",
      "cited_paper_id": 897596
    },
    {
      "context_text": "In this paper we also perform experiments on the COCO [8] and Flickr30k [57] datasets, comparing to a range of models, including both generative models such as in [50, 54, 3] and retrieval based such as in [15, 13, 38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO": "Used to compare generative and retrieval-based models in image captioning, focusing on multimodal reasoning and matching.",
        "Flickr30k": "Used to evaluate personalized text generation models, focusing on generating captions that align with visual content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, COCO and Flickr30k, which are used for experiments in the paper. These datasets are well-known in the field of image captioning and multimodal reasoning.",
      "processing_time": 60.32617664337158,
      "citing_paper_id": "53022581",
      "cited_paper_id": 945386
    },
    {
      "context_text": "In this paper we also perform experiments on the COCO [8] and Flickr30k [57] datasets, comparing to a range of models, including both generative models such as in [50, 54, 3] and retrieval based such as in [15, 13, 38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO": "Used to compare generative and retrieval-based models in image captioning, focusing on multimodal reasoning and matching.",
        "Flickr30k": "Used to evaluate personalized text generation models, focusing on generating captions that align with visual content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, COCO and Flickr30k, which are used for experiments in the paper. These datasets are well-known in the field of image captioning and multimodal reasoning.",
      "processing_time": 60.32617664337158,
      "citing_paper_id": "53022581",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "In this paper we also perform experiments on the COCO [8] and Flickr30k [57] datasets, comparing to a range of models, including both generative models such as in [50, 54, 3] and retrieval based such as in [15, 13, 38].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO": "Used to compare generative and retrieval-based models in image captioning, focusing on multimodal reasoning and matching.",
        "Flickr30k": "Used to evaluate personalized text generation models, focusing on generating captions that align with visual content."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions two specific datasets, COCO and Flickr30k, which are used for experiments in the paper. These datasets are well-known in the field of image captioning and multimodal reasoning.",
      "processing_time": 60.32617664337158,
      "citing_paper_id": "53022581",
      "cited_paper_id": 195347576
    },
    {
      "context_text": "Besides, as in COCO [8], measures like BLEU do not correlate well with human judgements (see top row in Tables 2 and 4) hence we perform human evaluation of our models in Section 5.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions COCO but does not indicate it is used as a dataset in the current research. It is referenced to highlight a limitation of BLEU scores.",
      "processing_time": 55.26309156417847,
      "citing_paper_id": "53022581",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "For example, in the COCO [8] and Flickr30k [57] tasks, some examples of captions include “a large bus sitting next to a very tall building” and “a butcher cutting an animal to sell”, which describe the contents of those images in a personality-free, factual manner.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO": "Used to generate factual captions for images, focusing on describing contents without personality. The dataset includes diverse images and corresponding captions.",
        "Flickr30k": "Used to generate factual captions for images, focusing on the content and style of the text. The dataset includes images with multiple captions per image."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions COCO and Flickr30k as datasets used for captioning tasks, providing specific examples of captions. These are well-known image captioning datasets.",
      "processing_time": 60.28964686393738,
      "citing_paper_id": "53022581",
      "cited_paper_id": 2210455
    },
    {
      "context_text": "Yet the majority of studies in the research community have so far focused on function only: standard image captioning [40] requires the machine to generate a sentence which factually describes the elements of the scene in a neutral tone.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general task of image captioning. No verifiable resources are identified.",
      "processing_time": 54.47682499885559,
      "citing_paper_id": "53022581",
      "cited_paper_id": 2592435
    },
    {
      "context_text": "The ﬁrst is a residual network with 152 layers described in [17] trained on Imagenet [44] to classify images among 1000 classes, which we refer to in the rest of the paper as ResNet152 features.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Imagenet"
      ],
      "dataset_descriptions": {
        "Imagenet": "Used to train the ResNet152 model for image classification among 1000 classes, providing a large-scale visual recognition challenge."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'Imagenet' as the dataset used for training the ResNet152 model, but does not provide specific details about its usage in the current research context.",
      "processing_time": 58.50582194328308,
      "citing_paper_id": "53022581",
      "cited_paper_id": 2930547
    },
    {
      "context_text": "Model Text PreFlickr30k COCO training R@1 R@5 R@10 R@1 R@5 R@10 UVS [25] - 23.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "PreFlickr30k",
        "COCO"
      ],
      "dataset_descriptions": {
        "PreFlickr30k": "Used for training and evaluating multimodal neural language models, focusing on image-text pairs from the English language domain.",
        "COCO": "Used for training and evaluating multimodal neural language models, focusing on image-text pairs from diverse categories."
      },
      "confidence_score": 0.8,
      "reasoning": "The context mentions 'PreFlickr30k' and 'COCO', which are known datasets used for image captioning and visual-semantic embedding tasks. However, the context does not provide specific details on how these datasets are used in the research.",
      "processing_time": 60.42824029922485,
      "citing_paper_id": "53022581",
      "cited_paper_id": 7732372
    },
    {
      "context_text": "The models are evaluated in terms of R@1, where for each sample there are 500 candidates to rank: 495 randomly cho-Method Table 2: Generative model performance on COCO caption using the test split of [24] Model Text Pre-Flickr30k COCO training R@1 R@5 R@10 R@1 R@5 R@10 UVS [ Table 3: Retrieval model performance on Flickr30k and COCO caption using the splits of [24].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO caption",
        "Flickr30k"
      ],
      "dataset_descriptions": {
        "COCO caption": "Used to evaluate generative model performance, focusing on ranking accuracy (R@1, R@5, R@10) in image captioning tasks.",
        "Flickr": "Used to evaluate personalized text generation models, specifically evaluating ranking accuracy (R@1, R@5, R@10) in generating captions for images."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions 'COCO caption' and 'Flickr30k' as datasets used for evaluation. These are well-known image captioning datasets.",
      "processing_time": 60.38838791847229,
      "citing_paper_id": "53022581",
      "cited_paper_id": 7732372
    },
    {
      "context_text": "tion using the test split of [24] Model Text Pre- Flickr30k COCO training R@1 R@5 R@10 R@1 R@5 R@10 UVS [25] - 23.0 50.7 62.9 43.4 75.7 85.8 Embedding Net [51] - 40.7 69.7 79.2 50.4 79.3 69.4 sm-LSTM [19] - 42.5 71.9 81.5 53.2 83.1 91.5 VSE++ (ResNet, FT) [13] - 52.9 80.5 87.2 64.6 90.0 95.7 GXN (i2t+t2i) [15] - 56.8 - 89.6 68.5 - 97.9 TransResNet model variants: Transformer, ResNet152 Full 10.3 27.3",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Flickr30k",
        "COCO"
      ],
      "dataset_descriptions": {
        "Flickr30k": "Used to evaluate image-to-text matching models, focusing on recall at ranks 1, 5, and 10.",
        "COCO": "Used to train and evaluate personalized text generation models, focusing on recall at ranks 1, 5, and 10."
      },
      "confidence_score": 0.8,
      "reasoning": "The citation mentions 'Flickr30k' and 'COCO' as datasets used for evaluation, but does not provide specific details on their usage beyond the test splits.",
      "processing_time": 60.12349057197571,
      "citing_paper_id": "53022581",
      "cited_paper_id": 8039072
    },
    {
      "context_text": "More importantly, our best model (U P D OWN ) either outperforms or is competitive with state-of-the-art single model performance [3] across most metrics (especially CIDEr).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (CIDEr) which is excluded according to the instructions.",
      "processing_time": 54.4885311126709,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "Adding the embedding of the personality trait allows our best model to reach a CIDEr score of 16.5, showing the importance of modeling personality in our new task.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only discusses the impact of adding personality trait embeddings on the CIDEr score.",
      "processing_time": 54.8475775718689,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "In the second stage, we perform policy gradient with REINFORCE to optimize the non-differentiable reward function (CIDEr score in our case).",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (REINFORCE) and a metric (CIDEr score).",
      "processing_time": 54.667558431625366,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "We computed the BLEU, CIDEr, SPICE, and ROUGE-L scores for our best TransResNet model.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics (BLEU, CIDEr, SPICE, ROUGE-L) but does not refer to any specific datasets. The cited papers are about metrics, not datasets.",
      "processing_time": 55.5125207901001,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "We computed the BLEU, CIDEr, SPICE, and ROUGE-L scores for our best TransResNet model.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions several metrics (BLEU, CIDEr, SPICE, ROUGE-L) but does not refer to any specific datasets. The cited papers are about metrics, not datasets.",
      "processing_time": 55.5125207901001,
      "citing_paper_id": "53022581",
      "cited_paper_id": 11933981
    },
    {
      "context_text": "We evaluate BLEU [41], ROUGE-L [26], CIDEr [48] and SPICE [2] and compare models’ performances to state-of-the-art models under the setting of [24].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics (BLEU, ROUGE-L, CIDEr, SPICE) but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.67853140830994,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "We evaluate BLEU [41], ROUGE-L [26], CIDEr [48] and SPICE [2] and compare models’ performances to state-of-the-art models under the setting of [24].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics (BLEU, ROUGE-L, CIDEr, SPICE) but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.67853140830994,
      "citing_paper_id": "53022581",
      "cited_paper_id": 11933981
    },
    {
      "context_text": "We evaluate BLEU [41], ROUGEL [26], CIDEr [48] and SPICE [2] and compare models’ performances to state-of-the-art models under the setting of [24].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics (BLEU, ROUGEL, CIDEr, SPICE) but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.753796339035034,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9026666
    },
    {
      "context_text": "We evaluate BLEU [41], ROUGEL [26], CIDEr [48] and SPICE [2] and compare models’ performances to state-of-the-art models under the setting of [24].",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions evaluation metrics (BLEU, ROUGEL, CIDEr, SPICE) but does not refer to any specific datasets. The cited papers are about evaluation metrics, not datasets.",
      "processing_time": 55.753796339035034,
      "citing_paper_id": "53022581",
      "cited_paper_id": 11933981
    },
    {
      "context_text": "Our work can also be linked to the more general area of human communication, separate from just factual captioning, in particular image grounded conversations between humans [37] or dialogue in general where displaying personality is important [58].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general areas of research. No clear identifiers for datasets are present.",
      "processing_time": 54.26008462905884,
      "citing_paper_id": "53022581",
      "cited_paper_id": 9142609
    },
    {
      "context_text": "The second is a ResNeXt 32 × 48 d [53] trained on 3.5 billion Instagram pictures following the procedure described by [32], which we refer to in the rest of the paper as ResNeXt-IG-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a model (ResNeXt) and a training procedure, but does not specify a dataset name.",
      "processing_time": 55.50022864341736,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13751202
    },
    {
      "context_text": "For image representations, we employ the work of [32] that uses a ResNeXt architecture trained on 3.5 billion social media images which we apply to both.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.2,
      "reasoning": "The context mentions '3.5 billion social media images' but does not provide a specific, named dataset. The reference is too generic and lacks a clear identifier.",
      "processing_time": 55.05479192733765,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13751202
    },
    {
      "context_text": "5B beyond the original image classiﬁcation and detection results in [32].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to a paper. There are no clear identifiers for datasets in the citation span.",
      "processing_time": 54.5170624256134,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13751202
    },
    {
      "context_text": "In this work we make use of the latest advancements in image encoding by using the work of [32] which provides state-of-the-art performance on ImagenNet image classiﬁcation, but has so far not been applied to captioning.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'ImagenNet' but does not indicate it is used as a dataset in this research. It is referenced for image classification performance, not for direct use in the current work.",
      "processing_time": 55.4730749130249,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13751202
    },
    {
      "context_text": "Caption Encoders Each caption is encoded into a vector rC of the same size using a Transformer architecture [47], followed by a two layer perceptron.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformer architecture) which is excluded according to the instructions.",
      "processing_time": 53.984002113342285,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "For text encoding we use the latest advances in attention-based representations using Transformers [47]; in particular, their use in retrieval models for dialogue by large-scale pretraining [36] is adapted here for our captioning tasks.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only models and methods. The citation is focused on the use of attention-based representations and Transformers, which are not datasets.",
      "processing_time": 54.87827825546265,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "For Transformers pretraining the whole network performed better than just pretraining the word embeddings, see Appendix A.",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Transformers) and a finding about pretraining. No verifiable resources are identified.",
      "processing_time": 54.69494819641113,
      "citing_paper_id": "53022581",
      "cited_paper_id": 13756489
    },
    {
      "context_text": "We report performance on the COCO caption dataset [27].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "COCO caption dataset"
      ],
      "dataset_descriptions": {
        "COCO caption dataset": "Used to evaluate image captioning models, focusing on generating descriptive captions for images containing common objects in various contexts."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the COCO caption dataset, which is a well-known dataset used for image captioning tasks. The cited paper title confirms it is a dataset.",
      "processing_time": 57.35531711578369,
      "citing_paper_id": "53022581",
      "cited_paper_id": 14113767
    },
    {
      "context_text": "Personality traits A large number of studies are dedicated to producing a model of the personality of an individual [20], such as the Big-Five [1], the Big-Two [1] and 16PF among others [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions personality models but does not specify any datasets. The cited papers are about personality models, not datasets.",
      "processing_time": 53.90845608711243,
      "citing_paper_id": "53022581",
      "cited_paper_id": 16809893
    },
    {
      "context_text": "Personality traits A large number of studies are dedicated to producing a model of the personality of an individual [20], such as the Big-Five [1], the Big-Two [1] and 16PF among others [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions personality models but does not specify any datasets. The cited papers are about personality models, not datasets.",
      "processing_time": 53.90845608711243,
      "citing_paper_id": "53022581",
      "cited_paper_id": 22826657
    },
    {
      "context_text": "Those models usually project personality in a low dimension space, for instance the Big-Five describes a personality by weighting openness to experience, conscientiousness, extraversion, agreeableness and neuroticism.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to the Big-Five personality model, which is a method or framework, not a dataset.",
      "processing_time": 54.880361795425415,
      "citing_paper_id": "53022581",
      "cited_paper_id": 22826657
    },
    {
      "context_text": "Personality traits A large number of studies are dedicated to produce a model of the personality of an individual [20], such as the Big-Five [1], the Big-Two [1] and 16PF among others [6].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models or frameworks for personality traits. No verifiable resources are identified.",
      "processing_time": 54.07913327217102,
      "citing_paper_id": "53022581",
      "cited_paper_id": 22826657
    },
    {
      "context_text": "[14] collect a somewhat bigger dataset with 10,000 images, FlickrStyle10K, but only covers two types of style (romantic and humorous).",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 525), response: ```json\n{\n    \"reasoning\": \"FlickrStyle10K is mentioned as a dataset with a specific size and style ",
      "processing_time": 62.59912109375,
      "citing_paper_id": "53022581",
      "cited_paper_id": 23414983
    },
    {
      "context_text": "Another research direction is to attempt to produce amusing captions either through wordplay (puns) [7] or training on data from humour websites [55].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'training on data from humour websites' but does not specify a named dataset. The reference to [55] could imply a specific dataset, but without more information, it is too ambiguous to include.",
      "processing_time": 55.920655965805054,
      "citing_paper_id": "53022581",
      "cited_paper_id": 44074048
    },
    {
      "context_text": "Training and Inference We perform a two-stage training strategy to train such caption generation models as proposed by [43].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a training strategy for caption generation models.",
      "processing_time": 53.801596879959106,
      "citing_paper_id": "53022581",
      "cited_paper_id": 206594923
    },
    {
      "context_text": "In SHOWATTTELL, while the overall architecture is similar to [54], we adopt the modification suggested by [43] and input the attention-derived image features to the cell node of the LSTM.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only modifications to an architecture. No verifiable resources are identified.",
      "processing_time": 53.966508865356445,
      "citing_paper_id": "53022581",
      "cited_paper_id": 206594923
    },
    {
      "context_text": "5 Att2in [43] ResNet - 33.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context is about image captioning but does not specify any dataset used.",
      "processing_time": 54.43423581123352,
      "citing_paper_id": "53022581",
      "cited_paper_id": 206594923
    },
    {
      "context_text": "where we initialize word vectors trained using fastText [5] trained on Wikipedia, or pretrain the entire encoder.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'fastText' but does not refer to it as a dataset. It is used as a method for training word vectors, which is not a dataset.",
      "processing_time": 54.76780986785889,
      "citing_paper_id": "53022581",
      "cited_paper_id": 207556454
    },
    {
      "context_text": "For quality con-6 https://multimediacommons.wordpress.com/yfcc100m-core-dataset/ ; [46] trol, crowdworkers were manually monitored and removed for poor performance.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "YFCC100M Core Dataset"
      ],
      "dataset_descriptions": {
        "YFCC100M Core Dataset": "Used for quality control in crowdworking tasks, specifically monitoring and removing underperforming workers. The dataset provides a large corpus of multimedia content."
      },
      "confidence_score": 0.9,
      "reasoning": "The citation mentions 'YFCC100M Core Dataset' which is a specific, verifiable dataset. It is used for quality control in crowdworking tasks.",
      "processing_time": 57.88949990272522,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "5B beyond the original image classification and detection results in [32].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to extending results beyond image classification and detection.",
      "processing_time": 54.099300384521484,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "7 billion dialogue examples; and (ii) image representations [32] with ResNets trained on 3.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to generic 'dialogue examples' and 'image representations', which do not meet the criteria for inclusion.",
      "processing_time": 54.93198347091675,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "5 billion Instagram pictures following the procedure described by [32], which we refer to in the rest of the paper as ResNeXt-IG-3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions '5 billion Instagram pictures' but does not provide a specific, named dataset. It refers to a method or model (ResNeXt-IG-3) rather than a dataset.",
      "processing_time": 55.2520067691803,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "In this work we make use of the latest advancements in image encoding by using the work of [32] which provides state-of-the-art performance on ImagenNet image classification, but has so far not been applied to captioning.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific dataset, only a method or model for image classification. ImagenNet is referenced but not as a dataset used in this research.",
      "processing_time": 54.60878562927246,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "For image representations, we employ the work of [32] that uses a ResNeXt architecture trained on 3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model architecture (ResNeXt). No verifiable resources are identified.",
      "processing_time": 54.107239961624146,
      "citing_paper_id": "53022581",
      "cited_paper_id": null
    },
    {
      "context_text": "Early work on query suggestion has used frequency-based statistical (probabilistic) meth-ods, which include Markov or LDA models [11, 33, 37, 50, 72].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is about query suggestion methods, not datasets.",
      "processing_time": 54.019030809402466,
      "citing_paper_id": "265150332",
      "cited_paper_id": 805182
    },
    {
      "context_text": "…contextual query suggestion is different from existing context-aware query suggestion in the literature [5, 15, 34, 37], since the latter neither conditions recommendations on the body of the web-page being viewed by the user, nor explicitly captures the user’s knowledge, instead focusing on…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a distinction in methodology compared to existing work.",
      "processing_time": 53.4827094078064,
      "citing_paper_id": "265150332",
      "cited_paper_id": 805182
    },
    {
      "context_text": "This task is notably different from existing query suggestion work that leverages previously clicked pages [5, 15, 34, 37] only through surface-level association (such as relationships between past queries and page titles), because it requires contextualizing the full text of the page.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to 'previously clicked pages'. This is too generic and lacks a clear identifier.",
      "processing_time": 54.51665186882019,
      "citing_paper_id": "265150332",
      "cited_paper_id": 805182
    },
    {
      "context_text": "This task is both highly practical and useful, having been shipped in web-scale search engines such as Google and Bing, as well as been widely applied to other tasks and domains, such as task-oriented search [25, 28] and recruitment platforms [82].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications and domains. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 54.150381326675415,
      "citing_paper_id": "265150332",
      "cited_paper_id": 3654098
    },
    {
      "context_text": "One such challenging task is a new variant of query suggestion [4, 12, 23, 66], that we call contextual query suggestion.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a new variant of a task called contextual query suggestion.",
      "processing_time": 53.41842317581177,
      "citing_paper_id": "265150332",
      "cited_paper_id": 13689271
    },
    {
      "context_text": "More recently, neural network methods based on recurrent or attention-based architectures [12, 23, 34, 66, 81] have been leveraged to better model past query sequences and generalize to unseen and long-tail queries.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only neural network methods and architectures. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.85102367401123,
      "citing_paper_id": "265150332",
      "cited_paper_id": 13689271
    },
    {
      "context_text": "Over several rounds of judgement and refinement, we obtain manual evaluation results for 1 , 309 sets of contextual query suggestion results from all four models listed in Section 4.2 (effectively a total of 5 , 236 annotations for individual query suggestions).",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a set of manual evaluations and annotations, which are not considered datasets according to the extraction rules.",
      "processing_time": 54.579671144485474,
      "citing_paper_id": "265150332",
      "cited_paper_id": 35001435
    },
    {
      "context_text": "Language models [24, 47, 56, 57], which are pre-trained on unannotated text corpora using Transformer architectures [71] based on self-supervised learning objectives, have been shown to acquire knowledge from text corpora [55, 59, 67] and have been successfully used for various natural language…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to text corpora and language models. No verifiable resources are identified.",
      "processing_time": 54.00887632369995,
      "citing_paper_id": "265150332",
      "cited_paper_id": 204838007
    },
    {
      "context_text": "Language models [24, 47, 56, 57], which are pre-trained on unannotated text corpora using Transformer architectures [71] based on self-supervised learning objectives, have been shown to acquire knowledge from text corpora [55, 59, 67] and have been successfully used for various natural language…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to text corpora and language models. No verifiable resources are identified.",
      "processing_time": 54.00887632369995,
      "citing_paper_id": "265150332",
      "cited_paper_id": null
    },
    {
      "context_text": "Additionally, because they tend to be relatively short and easy to aggregate, and because entity recognition and linking [18, 41] are well-studied problems, the process of operationalizing the creation of this store is greatly simplified.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general processes and problems related to entity recognition and linking.",
      "processing_time": 53.48997926712036,
      "citing_paper_id": "265150332",
      "cited_paper_id": 207853467
    },
    {
      "context_text": "…text corpora using Transformer architectures [71] based on self-supervised learning objectives, have been shown to acquire knowledge from text corpora [55, 59, 67] and have been successfully used for various natural language tasks, such as question answering and dialogue generation tasks [35, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to text corpora and language models. No verifiable resources are identified.",
      "processing_time": 53.84366750717163,
      "citing_paper_id": "265150332",
      "cited_paper_id": 211205183
    },
    {
      "context_text": "…text corpora using Transformer architectures [71] based on self-supervised learning objectives, have been shown to acquire knowledge from text corpora [55, 59, 67] and have been successfully used for various natural language tasks, such as question answering and dialogue generation tasks [35, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to text corpora and language models. No verifiable resources are identified.",
      "processing_time": 53.84366750717163,
      "citing_paper_id": "265150332",
      "cited_paper_id": 237513360
    },
    {
      "context_text": "…text corpora using Transformer architectures [71] based on self-supervised learning objectives, have been shown to acquire knowledge from text corpora [55, 59, 67] and have been successfully used for various natural language tasks, such as question answering and dialogue generation tasks [35, 68].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only general references to text corpora and language models. No verifiable resources are identified.",
      "processing_time": 53.84366750717163,
      "citing_paper_id": "265150332",
      "cited_paper_id": null
    },
    {
      "context_text": "Large Language Models (LLMs) [6, 9, 16, 53, 69, 70], such as GPT-4, are multi-billion parameter models trained on massive text corpora, which are capable of internalizing general knowledge across diverse domains [55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only large language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 53.5872368812561,
      "citing_paper_id": "265150332",
      "cited_paper_id": 211205183
    },
    {
      "context_text": "Large Language Models (LLMs) [6, 9, 16, 53, 69, 70], such as GPT-4, are multi-billion parameter models trained on massive text corpora, which are capable of internalizing general knowledge across diverse domains [55, 59].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only large language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 53.5872368812561,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "Finally, our knowledge augmentation method is cost efficient since the knowledge injection employs entities as atoms.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only discusses a knowledge augmentation method.",
      "processing_time": 52.999003887176514,
      "citing_paper_id": "265150332",
      "cited_paper_id": 235294302
    },
    {
      "context_text": "Meanwhile, for some real-world applications, such as query suggestion [22, 65], item recommendations [26, 77], snippet generation [13] or question answering [31], reflecting personal preferences, knowledge and needs of users in the model’s outputs is essential.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No verifiable resources are named.",
      "processing_time": 53.40268564224243,
      "citing_paper_id": "265150332",
      "cited_paper_id": 235294302
    },
    {
      "context_text": "Meanwhile, other studies have proposed to improve training strategies by performing either multi-task learning with a document ranker [4, 5, 15] or reinforcement learning [8].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and approaches such as multi-task learning and reinforcement learning.",
      "processing_time": 53.227630853652954,
      "citing_paper_id": "265150332",
      "cited_paper_id": 237006567
    },
    {
      "context_text": "For example, to generate factually correct answers in response to input questions, existing work [7, 40, 63] typically augments the internalized knowledge in LLMs with externally relevant factual knowledge related to questions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the general concept of augmenting LLMs with external factual knowledge. No verifiable resource names are provided.",
      "processing_time": 53.96475100517273,
      "citing_paper_id": "265150332",
      "cited_paper_id": 247362809
    },
    {
      "context_text": "For example, to generate factually correct answers in response to input questions, existing work [7, 40, 63] typically augments the internalized knowledge in LLMs with externally relevant factual knowledge related to questions.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the general concept of augmenting LLMs with external factual knowledge. No verifiable resource names are provided.",
      "processing_time": 53.96475100517273,
      "citing_paper_id": "265150332",
      "cited_paper_id": 260063238
    },
    {
      "context_text": "Due to the often large volume of users’ historical information, more recent work [60, 64, 73, 79] proposes to inject only a fraction of the most relevant history by retrieving it from a complete interaction memory.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to users' historical information and interaction memory. No verifiable resource names are provided.",
      "processing_time": 53.87862205505371,
      "citing_paper_id": "265150332",
      "cited_paper_id": 251371589
    },
    {
      "context_text": "Based on this observation, recent work [60, 64, 73, 79] instead retrieves relevant content from an external memory [54] that stores the user’s historical information.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to an external memory storing user’s historical information, which is a method or system rather than a dataset.",
      "processing_time": 54.113913774490356,
      "citing_paper_id": "265150332",
      "cited_paper_id": 251371589
    },
    {
      "context_text": "These include product or content recommendations [14, 27, 30, 73], dialogue generations [64, 79], writing assistants [44, 60], and even robotic systems [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No verifiable resources are identified.",
      "processing_time": 53.30674195289612,
      "citing_paper_id": "265150332",
      "cited_paper_id": 251371589
    },
    {
      "context_text": "These include product or content recommendations [14, 27, 30, 73], dialogue generations [64, 79], writing assistants [44, 60], and even robotic systems [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No verifiable resources are identified.",
      "processing_time": 53.30674195289612,
      "citing_paper_id": "265150332",
      "cited_paper_id": 258685399
    },
    {
      "context_text": "These include product or content recommendations [14, 27, 30, 73], dialogue generations [64, 79], writing assistants [44, 60], and even robotic systems [76].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of personalized text generation. No verifiable resources are identified.",
      "processing_time": 53.30674195289612,
      "citing_paper_id": "265150332",
      "cited_paper_id": 261049680
    },
    {
      "context_text": "In particular, LLMs have shown increased capacity for knowledge acquisition and retention thanks to their very large number of parameters [49, 78], as well as a remarkable ability to generalize across new domains with no need for additional task-specific fine-tuning and training data [62, 74].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of large language models.",
      "processing_time": 52.601233959198,
      "citing_paper_id": "265150332",
      "cited_paper_id": 252408513
    },
    {
      "context_text": "For example, in order to suggest the next item that the user may interact with, they prepend a sequence of their past item interactions into the LLM’s input [14, 46].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general method of using past item interactions. No verifiable resource names are provided.",
      "processing_time": 53.39569878578186,
      "citing_paper_id": "265150332",
      "cited_paper_id": 256459451
    },
    {
      "context_text": "…by retrieval from the knowledge store without requiring explicit profiling of users [38, 80]; the knowledge represented as entities is succinct, thereby leading to efficiency gains through reductions in input context length when compared with existing LLM contextualization work [58].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method or approach involving retrieval-augmented language models.",
      "processing_time": 53.06106972694397,
      "citing_paper_id": "265150332",
      "cited_paper_id": 256459451
    },
    {
      "context_text": "We refer to our framework as K nowledge-augmented large La nguage M odels for P ersonalization, or K-LaMP.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a framework called K-LaMP. There are no verifiable resources or datasets mentioned.",
      "processing_time": 53.46927094459534,
      "citing_paper_id": "265150332",
      "cited_paper_id": 256459451
    },
    {
      "context_text": "Moreover, they are able to understand the context of given inputs and then generate contextually coherent responses, allowing users and system designers to easily customize LLM responses through prompt engineering [39, 58, 75].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the capability of language models to generate contextually coherent responses through prompt engineering.",
      "processing_time": 53.30926513671875,
      "citing_paper_id": "265150332",
      "cited_paper_id": 256459451
    },
    {
      "context_text": "Firstly, they reaffirm the fact",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It is too vague to extract any verifiable resources.",
      "processing_time": 53.303937673568726,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259089062
    },
    {
      "context_text": "In each of these three metrics 4 , similarity is computed by calculating the dot product of representations obtained from Contriever [42].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Contriever) which is excluded according to the rules.",
      "processing_time": 53.14681911468506,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259089062
    },
    {
      "context_text": "To operationalize the retrieval step, we first represent all records in the knowledge-store K using embeddings, then compute embedding-level similarities with the representation of current query 𝑞 𝑗 using Contriever [42].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (Contriever) for computing embedding-level similarities. No verifiable datasets are referenced.",
      "processing_time": 53.543741941452026,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259089062
    },
    {
      "context_text": "Several recent studies [20, 29, 32, 36, 46, 48] have tackled the problem of LLM personalization through augmenting the user’s input with relevant information, a process known as in-context learning [9, 39, 58, 75].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only the concept of in-context learning and personalization of LLMs. No verifiable resources are identified.",
      "processing_time": 53.73269009590149,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259332879
    },
    {
      "context_text": "Specifically, early work [20, 29, 32, 36, 46, 48] proposes to incorporate the historical sequence of the user’s interactions (e.g., recent purchase logs of items) into LLMs prompts, thereby allowing LLMs to generate outputs that are personalized (e.g., next item recommendation).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of incorporating user interaction history into LLM prompts for personalization.",
      "processing_time": 53.173521280288696,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259332879
    },
    {
      "context_text": "…allows them to generate plausible, reasonable, and helpful outputs in response to user inputs that been leveraged with impressive results for a diverse range of natural language tasks, including question answering and dialogue generation, even without any task-specific training [6, 53, 70].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general capabilities of models. No dataset names are present in the citation span.",
      "processing_time": 53.16936898231506,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "Recently, Large Language Models (LLMs) [6, 53, 70], which are scaled-up versions of language models, have demonstrated the capability of handling diverse language tasks across various domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only Large Language Models (LLMs). No dataset names are present in the context.",
      "processing_time": 53.330039739608765,
      "citing_paper_id": "265150332",
      "cited_paper_id": 259950998
    },
    {
      "context_text": "In order to yield outputs that are customized to individual users, recent studies [10, 45] propose to personalize the generations of LLMs, with applications spanning various tasks and domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only a general concept of personalizing LLM generations. No verifiable resources are identified.",
      "processing_time": 53.32775831222534,
      "citing_paper_id": "265150332",
      "cited_paper_id": 261531422
    },
    {
      "context_text": "Meanwhile, some studies [38, 80] enable personalization through construction of deep user profiles that are then incorporated into LLM prompts.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of constructing deep user profiles for personalization.",
      "processing_time": 52.757256507873535,
      "citing_paper_id": "265150332",
      "cited_paper_id": null
    },
    {
      "context_text": "…has two additional advantages: it enables light-weight personalization by retrieval from the knowledge store without requiring explicit profiling of users [38, 80]; the knowledge represented as entities is succinct, thereby leading to efficiency gains through reductions in input context length…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about personalization and knowledge representation.",
      "processing_time": 52.43657302856445,
      "citing_paper_id": "265150332",
      "cited_paper_id": null
    },
    {
      "context_text": "Recently, sequence-to-sequence (seq2seq) framework (Sutskever, Vinyals, and Le 2014) has been proved effective for the task of abstractive summarization (Chopra, Auli, and Rush 2016; See, Liu, and Manning 2017) and other text generation tasks (Tao et al. 2018; Gao et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The seq2seq framework and abstractive summarization are discussed, but no datasets are explicitly named.",
      "processing_time": 53.861565828323364,
      "citing_paper_id": "55461757",
      "cited_paper_id": 133195
    },
    {
      "context_text": "Recently, sequence-to-sequence (seq2seq) framework (Sutskever, Vinyals, and Le 2014) has been proved effective for the task of abstractive summarization (Chopra, Auli, and Rush 2016; See, Liu, and Manning 2017) and other text generation tasks (Tao et al. 2018; Gao et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The seq2seq framework and abstractive summarization are discussed, but no datasets are explicitly named.",
      "processing_time": 53.861565828323364,
      "citing_paper_id": "55461757",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "Recently, sequence-to-sequence (seq2seq) framework (Sutskever, Vinyals, and Le 2014) has been proved effective for the task of abstractive summarization (Chopra, Auli, and Rush 2016; See, Liu, and Manning 2017) and other text generation tasks (Tao et al. 2018; Gao et al. 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. The seq2seq framework and abstractive summarization are discussed, but no datasets are explicitly named.",
      "processing_time": 53.861565828323364,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "The effect of comments or social contexts in document summarization have been explored by several previous works (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015; Li, Bing, and Lam 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works exploring the effect of comments or social contexts in document summarization.",
      "processing_time": 53.243545055389404,
      "citing_paper_id": "55461757",
      "cited_paper_id": 704517
    },
    {
      "context_text": "The effect of comments or social contexts in document summarization have been explored by several previous works (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015; Li, Bing, and Lam 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works exploring the effect of comments or social contexts in document summarization.",
      "processing_time": 53.243545055389404,
      "citing_paper_id": "55461757",
      "cited_paper_id": 14777460
    },
    {
      "context_text": "The effect of comments or social contexts in document summarization have been explored by several previous works (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015; Li, Bing, and Lam 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous works exploring the effect of comments or social contexts in document summarization.",
      "processing_time": 53.243545055389404,
      "citing_paper_id": "55461757",
      "cited_paper_id": 207168360
    },
    {
      "context_text": "Unlike these approaches that directly extract sentences from the original document (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015), we aim to generate a natural-sounding summary from scratch instead of extracting words from the document.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach to generating summaries.",
      "processing_time": 52.428837060928345,
      "citing_paper_id": "55461757",
      "cited_paper_id": 704517
    },
    {
      "context_text": "Unlike these approaches that directly extract sentences from the original document (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015), we aim to generate a natural-sounding sum-mary from scratch instead of extracting words from the document.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach to generating summaries. No verifiable resources are named.",
      "processing_time": 53.081878662109375,
      "citing_paper_id": "55461757",
      "cited_paper_id": 704517
    },
    {
      "context_text": "Unlike these approaches that directly extract sentences from the original document (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015), we aim to generate a natural-sounding sum-mary from scratch instead of extracting words from the document.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach to generating summaries. No verifiable resources are named.",
      "processing_time": 53.081878662109375,
      "citing_paper_id": "55461757",
      "cited_paper_id": 14777460
    },
    {
      "context_text": "Unlike these approaches that directly extract sentences from the original document (Hu, Sun, and Lim 2008; Yang et al. 2011; Li et al. 2015), we aim to generate a natural-sounding sum-mary from scratch instead of extracting words from the document.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a methodological approach to generating summaries. No verifiable resources are named.",
      "processing_time": 53.081878662109375,
      "citing_paper_id": "55461757",
      "cited_paper_id": 207168360
    },
    {
      "context_text": "For evaluation metrics, we adopt ROUGE score (Lin 2004) which is widely applied for summarization evaluation (Sun et al. 2018; Chen et al. 2018).",
      "catation_intent": "findings",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions ROUGE score but does not refer to a specific dataset. ROUGE is a metric, not a dataset, and thus should not be included.",
      "processing_time": 53.408427476882935,
      "citing_paper_id": "55461757",
      "cited_paper_id": 964287
    },
    {
      "context_text": "To focus on the main aspect, some summarization methods (Sun et al. 2018; Zhou et al. 2017; Bansal and Chen 2018) ﬁrst select several sentences about the main aspect and then generate the summary.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only summarization methods. There are no clear identifiers for datasets in the provided context.",
      "processing_time": 52.93530607223511,
      "citing_paper_id": "55461757",
      "cited_paper_id": 1770102
    },
    {
      "context_text": "To focus on the main aspect, some summarization methods (Sun et al. 2018; Zhou et al. 2017; Bansal and Chen 2018) first select several sentences about the main aspect and then generate the summary.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only summarization methods. There are no clear identifiers for datasets in the context.",
      "processing_time": 52.856231927871704,
      "citing_paper_id": "55461757",
      "cited_paper_id": 1770102
    },
    {
      "context_text": "Most methods for ab-stractive text summarization are based on the sequence-to-sequence model (Sutskever, Vinyals, and Le 2014), which encodes the source texts into the semantic representation with an encoder, and generates the summaries from the representation with a decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (sequence-to-sequence model). The cited paper title confirms this is about a method, not a dataset.",
      "processing_time": 53.406800746917725,
      "citing_paper_id": "55461757",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "To evaluate the performance of our proposed dataset and model, we compare it with the following baselines: (1) S2S : Sequence-to-sequence framework (Sutskever, Vinyals, and Le 2014) has been proposed for language generation task.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions a model (S2S) rather than a dataset. No specific, verifiable dataset is referenced.",
      "processing_time": 52.70472693443298,
      "citing_paper_id": "55461757",
      "cited_paper_id": 7961699
    },
    {
      "context_text": "To tackle the out-of-vocabulary problem, some researchers employ the copy mechanism to copy some words from the input document to summary (Gu et al. 2016; See, Liu, and Manning 2017).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (copy mechanism) used in summarization.",
      "processing_time": 52.3767192363739,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "The design of the pointer network is the same as the model used in (See, Liu, and Manning 2017), thus we omit this procedure in our paper due to the limited space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a model but does not refer to any specific dataset. The context is about the design of the pointer network, which is a method.",
      "processing_time": 52.85490393638611,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Following (See, Liu, and Manning 2017; Ma et al. 2018a), we choose the long short-term memory (LSTM) as the Bi-RNN cell.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the choice of LSTM as the Bi-RNN cell. No verifiable resources are identified.",
      "processing_time": 52.84591627120972,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Following (See, Liu, and Manning 2017; Ma et al. 2018a), we choose the long short-term memory (LSTM) as the Bi-RNN cell.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only the choice of LSTM as the Bi-RNN cell. No verifiable resources are identified.",
      "processing_time": 52.84591627120972,
      "citing_paper_id": "55461757",
      "cited_paper_id": 44142790
    },
    {
      "context_text": "In order to handle the out-of-vocabulary (OOV) problem, we equip the pointer network (Gu et al. 2016; Vinyals, Fortu-nato, and Jaitly 2015; See, Liu, and Manning 2017) with our decoder, which makes our decoder capable to copy words from the source text.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (pointer network) and its application to handle OOV problems. No verifiable datasets are referenced.",
      "processing_time": 53.15339112281799,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "(4) LEAD1 : LEAD1 is a commonly used baseline (Nallapati, Zhai, and Zhou 2017; See, Liu, and Manning 2017), which selects the ﬁrst sentence of document as the summary.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method called LEAD1. The cited papers are used to support the claim about LEAD1 being a common baseline, not to introduce datasets.",
      "processing_time": 53.87738394737244,
      "citing_paper_id": "55461757",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Wang et al. (2018) use convolutional seq2seq model to summarize text and use the policy gradient algorithm to directly optimize the ROUGE score.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and algorithms. The context focuses on the methodological approach rather than the data used.",
      "processing_time": 52.7947211265564,
      "citing_paper_id": "55461757",
      "cited_paper_id": 13663262
    },
    {
      "context_text": "(3) CGU : Lin et al. (2018) propose to use the convolutional gated unit to reﬁne the source representation, which achieves the state-of-the-art performance on social media text summarization dataset.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "social media text summarization dataset"
      ],
      "dataset_descriptions": {
        "social media text summarization dataset": "Used to evaluate the performance of the proposed method in personalized text generation, focusing on the effectiveness of the convolutional gated unit in refining source representations."
      },
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a 'social media text summarization dataset' but does not provide a specific name. The dataset is used for evaluating the performance of the convolutional gated unit in text summarization.",
      "processing_time": 55.956639528274536,
      "citing_paper_id": "55461757",
      "cited_paper_id": 13707541
    },
    {
      "context_text": "Lin et al. (2018) propose a seq2seq based model which uses an CNN to reﬁne the representation of source context.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (seq2seq model with CNN).",
      "processing_time": 52.08845138549805,
      "citing_paper_id": "55461757",
      "cited_paper_id": 13707541
    },
    {
      "context_text": "Li et al. (2015) pro-pose an unsupervised compressive multi-document summarization model using sparse coding method.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for multi-document summarization.",
      "processing_time": 51.844419956207275,
      "citing_paper_id": "55461757",
      "cited_paper_id": 14777460
    },
    {
      "context_text": "Recently, Nguyen et al. (2016) publish a small extractive sentence-comment dataset which can not be used to train neural models due to its small size.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "SoLSCSum"
      ],
      "dataset_descriptions": {
        "SoLSCSum": "Used to highlight limitations in training neural models due to its small size, focusing on extractive summarization and personalization."
      },
      "confidence_score": 0.85,
      "reasoning": "The context mentions a specific dataset by Nguyen et al. (2016) that is too small for training neural models. The title confirms it is a dataset.",
      "processing_time": 55.15031623840332,
      "citing_paper_id": "55461757",
      "cited_paper_id": 17215563
    },
    {
      "context_text": "Graph-based method has been used for comment oriented summarization task such as (Hu, Sun, and Lim 2007; 2008), where they identify three relations (topic, quotation, and mention) by which comments can be linked to one another.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and relations used in comment-oriented summarization.",
      "processing_time": 52.07822275161743,
      "citing_paper_id": "55461757",
      "cited_paper_id": 17337656
    },
    {
      "context_text": "Graph-based method has been used for comment oriented summarization task such as (Hu, Sun, and Lim 2007; 2008), where they identify three relations (topic, quotation, and mention) by which comments can be linked to one another.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and relations used in comment-oriented summarization.",
      "processing_time": 52.07822275161743,
      "citing_paper_id": "55461757",
      "cited_paper_id": 207168360
    },
    {
      "context_text": "ain aspect and the bad summary describes another trivial aspect that is not the main point of the document. To focus on the main aspect, some summarization methods (Sun et al. 2018; Zhou et al. 2017; Bansal and Chen 2018) ﬁrst select several sentences about the main aspect and then generate the summary. However, it is very challenging to discover which is the main aspect of the news document. Nowadays, a great number",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only summarization methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 52.61582398414612,
      "citing_paper_id": "55461757",
      "cited_paper_id": 44129061
    },
    {
      "context_text": "uffer from redundancy problem. Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization is dedicated to abstractive summarization (Bansal and Chen 2018; Ma et al. 2018b; Zhou et al. 2018). On the text summarization benchmark dataset CNN/DailyMail, the state-ofthe-art abstractive methods outperform the best extractive method in terms of ROUGE score.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "CNN/DailyMail"
      ],
      "dataset_descriptions": {
        "CNN/DailyMail": "Used to evaluate abstractive summarization methods, comparing their performance in personalized text generation, focusing on the performance of the models in terms of ROUGE score."
      },
      "confidence_score": 1.0,
      "reasoning": "The context mentions the CNN/DailyMail dataset, which is a well-known benchmark for text summarization. The dataset is used to evaluate abstractive summarization methods.",
      "processing_time": 55.50082039833069,
      "citing_paper_id": "55461757",
      "cited_paper_id": 44129061
    },
    {
      "context_text": "Recently, with the emergence of neural network models for text generation, a vast majority of the literature on summarization is dedicated to abstractive summarization (Bansal and Chen 2018; Ma et al. 2018b; Zhou et al. 2018).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to literature on abstractive summarization.",
      "processing_time": 51.98689341545105,
      "citing_paper_id": "55461757",
      "cited_paper_id": 44142790
    },
    {
      "context_text": "Retrieval-based methods aim to select a suitable response from a large repository [38, 49, 50], while generationbased methods aim at generating a response from scratch [26, 28, 32, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for response selection and generation. No verifiable resources are identified.",
      "processing_time": 52.60589098930359,
      "citing_paper_id": "235792273",
      "cited_paper_id": 94285
    },
    {
      "context_text": "Retrieval-based methods aim to select a suitable response from a large repository [38, 49, 50], while generationbased methods aim at generating a response from scratch [26, 28, 32, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for response selection and generation. No verifiable resources are identified.",
      "processing_time": 52.60589098930359,
      "citing_paper_id": "235792273",
      "cited_paper_id": 218763557
    },
    {
      "context_text": "Retrieval-based methods aim to select a suitable response from a large repository [38, 49, 50], while generationbased methods aim at generating a response from scratch [26, 28, 32, 48].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general methods for response selection and generation. No verifiable resources are identified.",
      "processing_time": 52.60589098930359,
      "citing_paper_id": "235792273",
      "cited_paper_id": 231662221
    },
    {
      "context_text": "Some early studies treat the response generation task as a statistical machine translation problem because of its end-to-end and data-driven features [25, 34].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general approach to response generation. No verifiable resources are identified.",
      "processing_time": 52.374356508255005,
      "citing_paper_id": "235792273",
      "cited_paper_id": 780171
    },
    {
      "context_text": "To achieve this, DHAP uses a key-value memory network [18] to store the user’s historical post-response pairs.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions a method (key-value memory network) but does not specify a dataset. No dataset names are present in the citation context.",
      "processing_time": 52.18508434295654,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2711679
    },
    {
      "context_text": "Traditional personalized chatbots focus on modeling the user’s psychological behavior such as the “Big Five” of speakers [17].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference to personality generation in dialogue systems.",
      "processing_time": 51.94561195373535,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2817528
    },
    {
      "context_text": "With personality, a chatbot can generate more informative and user-specific responses [9, 41], and has the potential to perform similar behaviors as real humans.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of using personality in chatbots. No verifiable resources are identified.",
      "processing_time": 52.485007762908936,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Speaker [9] is also based on Seq2SeqWA but using user ID embeddings as additional input to the decoder.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Seq2SeqWA) and an additional input (user ID embeddings).",
      "processing_time": 52.393961668014526,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Early studies tried to integrate the user ID embeddings to a sequence-to-sequence (Seq2Seq)model for identifying the user and generating user-related responses [3, 4, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to integrating user ID embeddings into a Seq2Seq model, which is a methodological approach.",
      "processing_time": 52.892056465148926,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Early studies tried to integrate the user ID embeddings to a sequence-to-sequence (Seq2Seq)model for identifying the user and generating user-related responses [3, 4, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific, verifiable datasets. It only refers to integrating user ID embeddings into a Seq2Seq model, which is a methodological approach.",
      "processing_time": 52.892056465148926,
      "citing_paper_id": "235792273",
      "cited_paper_id": 202775604
    },
    {
      "context_text": "Many Seq2Seq-based extensions have been applied to tackle the “safe response” problem [8]; to incorporate external knowledge [44]; to generate responses with emotions or personas [9, 23, 43]; and to model the hierarchical structure of the dialogue context [26, 27].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only various applications of Seq2Seq models in dialogue systems. No verifiable resources are identified.",
      "processing_time": 52.465503215789795,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Some researchers first tried to input the user ID embeddings into the decoder of a Seq2Seq model to generate more personalized responses [1, 3, 4, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving user ID embeddings in a Seq2Seq model.",
      "processing_time": 52.07361841201782,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Some researchers first tried to input the user ID embeddings into the decoder of a Seq2Seq model to generate more personalized responses [1, 3, 4, 9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a methodological approach involving user ID embeddings in a Seq2Seq model.",
      "processing_time": 52.07361841201782,
      "citing_paper_id": "235792273",
      "cited_paper_id": 202775604
    },
    {
      "context_text": "Thus, building accurate encoding of the input post is difficult, which further leads to poor quality of the generated responses [9, 28].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a challenge in building accurate encodings for input posts.",
      "processing_time": 51.763163566589355,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "In recent years, deep-learning based methods were proposed to learn the persona information directly from large-scale dialogue datasets via end-to-end neural networks [4, 9, 23, 41].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'large-scale dialogue datasets' but does not specify any particular dataset names. The term 'datasets' is too generic and lacks a specific identifier.",
      "processing_time": 52.51164221763611,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "A length penalty is applied as [9] to alleviate the generation of meaningless responses.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a method to improve conversation models.",
      "processing_time": 51.299818992614746,
      "citing_paper_id": "235792273",
      "cited_paper_id": 2955580
    },
    {
      "context_text": "Inspired by CopyNet [7], in addition to leveraging the personalized information captured by the implicit user profile, we construct a personalized vocabulary so that the model is allowed to directly select personalized words that the user frequently used in history.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It discusses a method for constructing a personalized vocabulary but does not reference a named dataset.",
      "processing_time": 52.138023138046265,
      "citing_paper_id": "235792273",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "Inspired by CopyNet [7], DHAP can switch between generating a word from a generic vocabulary ( 𝑝 ( 𝑦 | 𝑚 ) ) and copying a word from a user’s personalized vocabulary ( 𝑝 ( 𝑦 | 𝑚 ) ) as: where 𝑝 ( 𝑚 ) and 𝑝 ( 𝑚 ) are computed by our designed decoding switcher.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (CopyNet) and a model (DHAP). The citation is focused on explaining a mechanism rather than using a dataset.",
      "processing_time": 52.649760007858276,
      "citing_paper_id": "235792273",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "Inspired by copy mechanism [7], the probability of selecting a word 𝑦 𝑡 is computed as: where 𝛾 𝑡,𝑖 is the attention weight calculated by the personalized post representation c attentively reading the representation of historical responses E with the same attention process in Equation (6).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (copy mechanism) used in sequence-to-sequence learning.",
      "processing_time": 51.565589427948,
      "citing_paper_id": "235792273",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "(1) We use BLEU-1, BLEU-2 [20], and ROUGE-L [12] to measure word overlaps between the generated response and ground truth.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions evaluation metrics (BLEU-1, BLEU-2, ROUGE-L) but does not refer to any specific datasets. These metrics are used to measure word overlaps between generated responses and ground truth.",
      "processing_time": 52.976372957229614,
      "citing_paper_id": "235792273",
      "cited_paper_id": 11080756
    },
    {
      "context_text": "The pre-trained word embeddings for Weibo and Reddit corpus are offered by Li et al. [10] and Pennington et al. [21], respectively.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'Weibo and Reddit corpus' but does not specify them as datasets. It refers to pre-trained word embeddings, which are not datasets themselves.",
      "processing_time": 52.402005434036255,
      "citing_paper_id": "235792273",
      "cited_paper_id": 21716001
    },
    {
      "context_text": "Open-domain chatbots have attracted more and more attention, due to their broad application in real applications, such as Microsoft XiaoIce [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a chatbot system. There are no verifiable resources or datasets mentioned.",
      "processing_time": 51.63312768936157,
      "citing_paper_id": "235792273",
      "cited_paper_id": 56657857
    },
    {
      "context_text": "Open-domain chatbots have attracted more and more attention, due to their broad application in real applications, such asMicrosoft XiaoIce [45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions Microsoft XiaoIce but does not refer to it as a dataset, model, or method. It is cited as an example of a real-world application of open-domain chatbots.",
      "processing_time": 52.35416007041931,
      "citing_paper_id": "235792273",
      "cited_paper_id": 56657857
    },
    {
      "context_text": "Towards this ultimate goal, in the dialogue system area, building digital agents has attracted more and more attention and had some preliminary applications in our daily life [14, 24, 45].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general applications of dialogue systems.",
      "processing_time": 50.76685953140259,
      "citing_paper_id": "235792273",
      "cited_paper_id": 56657857
    },
    {
      "context_text": "Faced with extensive information available on the Internet, it is very appealing to have an intelligent assistant that can provide the most relevant information [16, 46, 47], collaborate with us on dayto-day problems [14, 45], or even act as our agent for some specific tasks [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of intelligent assistants. No dataset names are present in the text.",
      "processing_time": 51.903979539871216,
      "citing_paper_id": "235792273",
      "cited_paper_id": 56657857
    },
    {
      "context_text": "Faced with extensive information available on the Internet, it is very appealing to have an intelligent assistant that can provide the most relevant information [16, 46, 47], collaborate with us on dayto-day problems [14, 45], or even act as our agent for some specific tasks [37].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only general capabilities of intelligent assistants. No dataset names are present in the text.",
      "processing_time": 51.903979539871216,
      "citing_paper_id": "235792273",
      "cited_paper_id": 221471467
    },
    {
      "context_text": "On this dataset, many methods have achieved encouraging performance, such as variational autoencoders [30], pre-trained language models [29, 36], and multi-task modeling [29, 31, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not specify a named dataset, only mentions methods and models. No specific, verifiable dataset is referenced.",
      "processing_time": 51.56508493423462,
      "citing_paper_id": "235792273",
      "cited_paper_id": 208006638
    },
    {
      "context_text": "On this dataset, many methods have achieved encouraging performance, such as variational autoencoders [30], pre-trained language models [29, 36], and multi-task modeling [29, 31, 35].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not specify a named dataset, only mentions methods and models. No specific, verifiable dataset is referenced.",
      "processing_time": 51.56508493423462,
      "citing_paper_id": "235792273",
      "cited_paper_id": 215786161
    },
    {
      "context_text": "Recently, some studies proposed to assign pre-defined personas to chatbots so as to generate more personalized responses [23, 31, 41].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the concept of assigning pre-defined personas to chatbots for personalized responses.",
      "processing_time": 51.5613751411438,
      "citing_paper_id": "235792273",
      "cited_paper_id": 208006638
    },
    {
      "context_text": "Here we choose RNN-based architectures because they are better at capturing local correlations and encoding positional information than Transformers for short texts [19, 39].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a preference for RNN-based architectures over Transformers for short texts. No verifiable resources are identified.",
      "processing_time": 51.96857690811157,
      "citing_paper_id": "235792273",
      "cited_paper_id": 208163680
    },
    {
      "context_text": "Following [11, 15], we use Persona F1 (P-F1) to measure the unigram F1 between the generated response and user’s historical responses.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a metric (Persona F1) used for evaluation. The context is focused on the evaluation metric rather than a dataset.",
      "processing_time": 52.104246377944946,
      "citing_paper_id": "235792273",
      "cited_paper_id": 221839676
    },
    {
      "context_text": "For data cleaning, we remove hashtags, URLs, emoticons, and duplicate text as [22].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific dataset, only a general data cleaning process. The cited paper title suggests a dataset, but it is not referenced in the citation context.",
      "processing_time": 52.04185700416565,
      "citing_paper_id": "235792273",
      "cited_paper_id": 221969995
    },
    {
      "context_text": "Weibo dataset is a subset of PChatbotW [22], which is collected from Weibo for the one-year period beginning from Sept.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Weibo dataset",
        "PChatbotW"
      ],
      "dataset_descriptions": {
        "Weibo dataset": "The Weibo dataset, a subset of PChatbotW, is used to train and evaluate personalized chatbot responses, focusing on the nuances of social media interactions over a one-year period.",
        "PChatbotW": "PChatbotW is a large-scale dataset collected from Weibo, used to develop and test personalized chatbot systems, emphasizing the diversity and complexity of user interactions."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Weibo dataset' and 'PChatbotW', both of which are specific datasets. The Weibo dataset is described as a subset of PChatbotW, which is relevant to personalized chatbot research.",
      "processing_time": 56.364758014678955,
      "citing_paper_id": "235792273",
      "cited_paper_id": 221969995
    },
    {
      "context_text": "Weibo dataset is a subset of PChatbotW [22], which is collected from Weibo for the one-year period beginning from Sept. 10, 2018.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "Weibo dataset",
        "PChatbotW"
      ],
      "dataset_descriptions": {
        "Weibo": "Used to train and evaluate personalized chatbot responses, focusing on user interactions from the Weibo social platform over a year."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions 'Weibo dataset' and 'PChatbotW', both of which are specific datasets. The Weibo dataset is described as a subset of PChatbotW, collected from Weibo over a specific time period.",
      "processing_time": 54.54884123802185,
      "citing_paper_id": "235792273",
      "cited_paper_id": 221969995
    },
    {
      "context_text": "The dataset was split into 60% training, 10% validation, and 30% test subsets, and we experimented with a range of models, including Decision Tree classifiers (Breiman et al., 1984), Random Forests (Breiman, 2001), Logistic Regression (Bishop, 2006), and Support Vector Machines (Smola & Schölkopf, 2004), using the scikit-learn implementation.",
      "catation_intent": "reusable resource",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The context does not mention any specific, verifiable dataset names. It only describes the splitting of a generic dataset and the models used.",
      "processing_time": 51.800801515579224,
      "citing_paper_id": "237684656",
      "cited_paper_id": 89141
    },
    {
      "context_text": "…and 30% test subsets, and we experimented with a range of models, including Decision Tree classifiers (Breiman et al., 1984), Random Forests (Breiman, 2001), Logistic Regression (Bishop, 2006), and Support Vector Machines (Smola & Schölkopf, 2004), using the scikit-learn implementation.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models and methods. No verifiable resources are identified.",
      "processing_time": 51.23127317428589,
      "citing_paper_id": "237684656",
      "cited_paper_id": 89141
    },
    {
      "context_text": "We employ a machine learning approach and utilize the Random Forest classifier from the scikit-learn3 suite (Breiman, 2001).",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the use of a Random Forest classifier from scikit-learn, which is a method, not a dataset. No specific dataset is mentioned.",
      "processing_time": 51.75209307670593,
      "citing_paper_id": "237684656",
      "cited_paper_id": 89141
    },
    {
      "context_text": "Many ITS incorporate explicit student models and consider the development of a personalized curriculum and personalized feedback (Albacete et al., 2019; Chi et al., 2011; Lin et al., 2013; Munshi & Biswas, 2019; Rus et al., 2014a, b).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers discussing personalized curricula and feedback in Intelligent Tutoring Systems (ITS).",
      "processing_time": 51.80943441390991,
      "citing_paper_id": "237684656",
      "cited_paper_id": 1773683
    },
    {
      "context_text": "2017) to multiple-choice quizzes (Guo et al., 2016; Tamura et al., 2015) across a number of subject domains.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to multiple-choice quizzes across various subject domains, which is too generic.",
      "processing_time": 51.591171741485596,
      "citing_paper_id": "237684656",
      "cited_paper_id": 2152636
    },
    {
      "context_text": "…as Wikipedia can be utilized to generate various types of educational content and interactions with the aim of scaling up computer-based learning systems and addressing the needs of their students (Brunskill et al., 2018; Dinan et al., 2018; Guo et al., 2016; Liu et al., 2012; Willis et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to the use of Wikipedia for generating educational content, which is a general reference rather than a specific dataset.",
      "processing_time": 51.989908933639526,
      "citing_paper_id": "237684656",
      "cited_paper_id": 2152636
    },
    {
      "context_text": "…as Wikipedia can be utilized to generate various types of educational content and interactions with the aim of scaling up computer-based learning systems and addressing the needs of their students (Brunskill et al., 2018; Dinan et al., 2018; Guo et al., 2016; Liu et al., 2012; Willis et al., 2019).",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to the use of Wikipedia for generating educational content, which is a general reference rather than a specific dataset.",
      "processing_time": 51.989908933639526,
      "citing_paper_id": "237684656",
      "cited_paper_id": 156054563
    },
    {
      "context_text": "…Medio et al., 2016; Ram´ırez-Noriega et al., 2018; Talukdar & Cohen, 2012); and generating a variety of pedagogical interventions ranging from open questions (Liu et al., 2012; Shah et al., 2017) to multiple-choice quizzes (Guo et al., 2016; Tamura et al., 2015) across a number of subject domains.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and methods. No verifiable resources are identified.",
      "processing_time": 51.34524703025818,
      "citing_paper_id": "237684656",
      "cited_paper_id": 2152636
    },
    {
      "context_text": "Of particular relevance here is the line of related work, where researchers have investigated how machine learning and large-scale, open-access resources such as Wikipedia can be utilized to generate various types of educational content and interactions with the aim of scaling up computer-based learning systems and addressing the needs of their students (Brunskill et al., 2018; Dinan et al., 2018; Guo et al., 2016; Liu et al., 2012; Willis et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions 'large-scale, open-access resources such as Wikipedia' but does not specify it as a dataset. No other specific datasets are mentioned.",
      "processing_time": 51.64145874977112,
      "citing_paper_id": "237684656",
      "cited_paper_id": 2152636
    },
    {
      "context_text": "Personalization and adapt-ability of ITS to individual student needs have been shown to not only help students in independent learning, but also help teachers personalize feedback and instruction, in particular in blended and flipped-classroom environments (Baker, 2016; Holstein et al., 2017, 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general concepts about personalization and adaptability in intelligent tutoring systems.",
      "processing_time": 51.26682925224304,
      "citing_paper_id": "237684656",
      "cited_paper_id": 2837961
    },
    {
      "context_text": "…1998; D’Agostino & Endriss, 1998; Hendriks et al., 2010; Scheines & Sieg, 1994; Stamper et al., 2013; Sufrin & Bornat, 1996), and algorithms (Leelawong & Biswas, 2008); to assisting students in knowledge and skill acquisition in natural sciences (Hume et al., 1996; Makatchev et al., 2011;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers and systems. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 51.74180459976196,
      "citing_paper_id": "237684656",
      "cited_paper_id": 5415133
    },
    {
      "context_text": "…et al., 2002; Dietrich & Buckley, 2008; Goguadze et al., 2005; Hrastinski et al., 2019; Koedinger & Anderson, 1993; Melis & Siekmann, 2004; Passier & Jeuring, 2006; Sommer & Nuckols, 2004), logic (Abel et al., 2001; Andrews et al., 2004; Burstall, 1998; D’Agostino & Endriss, 1998;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 51.79533910751343,
      "citing_paper_id": "237684656",
      "cited_paper_id": 5664447
    },
    {
      "context_text": "Personalization and adaptability of ITS to individual student needs have been shown to not only help students in independent learning, but also help teachers personalize feedback and instruction, in particular in blended and flipped-classroom environments (Baker, 2016; Holstein et al., 2017, 2019).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only general research findings about personalization and adaptability of ITS.",
      "processing_time": 51.149104833602905,
      "citing_paper_id": "237684656",
      "cited_paper_id": 5701360
    },
    {
      "context_text": "…2016), language model score for the model built using a state-of-the-art LSTM neural network (Merity et al., 2017), 8 textual entailment-based relations using a state-of-the-art attention-based neural network (Parikh et al., 2016), TF-IDF scores, and named entity classes (Nothman et al., 2013).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific, verifiable datasets. It only refers to methods, models, and scores.",
      "processing_time": 51.26817202568054,
      "citing_paper_id": "237684656",
      "cited_paper_id": 8495258
    },
    {
      "context_text": ", 2017),8 textual entailment-based relations using a state-of-theart attention-based neural network (Parikh et al., 2016), TF-IDF scores, and named entity classes (Nothman et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on textual entailment and attention-based neural networks, which are not datasets.",
      "processing_time": 51.59246826171875,
      "citing_paper_id": "237684656",
      "cited_paper_id": 8495258
    },
    {
      "context_text": "Many ITS rely heavily on expert design and hand-crafted rules to generate system interventions, which makes them difficult to build and transfer across domains, and limits their potential efficacy and scalability (Folsom-Kovarik et al., 2010; Olney & Cade, 2015).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses limitations of ITS systems.",
      "processing_time": 50.58282804489136,
      "citing_paper_id": "237684656",
      "cited_paper_id": 8579776
    },
    {
      "context_text": ", 2005; Hennecke, 1999), or involving a human tutor (Cukurova et al., 2017; Hrastinski et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to papers. No verifiable resources are identified.",
      "processing_time": 51.06116271018982,
      "citing_paper_id": "237684656",
      "cited_paper_id": 38995897
    },
    {
      "context_text": "…to be particularly challenging in the past, with many systems aiming to provide feedback on mathematical expressions resorting to hand-crafted rules (B¨udenbender et al., 2002; Goguadze et al., 2005; Hennecke, 1999), or involving a human tutor (Cukurova et al., 2017; Hrastinski et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to previous work and methods. There are no clear identifiers for datasets or other verifiable resources.",
      "processing_time": 51.58284378051758,
      "citing_paper_id": "237684656",
      "cited_paper_id": 38995897
    },
    {
      "context_text": "In this respect, dialogue-based ITS have been shown to be some of the most promising tools for learning (Ahn et al., 2018; Graesser et al. 2001, 2005, Nye et al., 2014; Ventura et al., 2018), as they simulate the familiar learning environment of student–tutor interaction, which helps improve student confidence and motivation and leads to a better learning experience.",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research findings about dialogue-based intelligent tutoring systems.",
      "processing_time": 50.74699950218201,
      "citing_paper_id": "237684656",
      "cited_paper_id": 49319238
    },
    {
      "context_text": "In this respect, dialogue-based ITS have been shown to be some of the most promising tools for learning (Ahn et al., 2018; Graesser et al. 2001, 2005, Nye et al., 2014; Ventura et al., 2018), as they simulate the familiar learning environment of student–tutor interaction, which helps improve…",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research findings about dialogue-based intelligent tutoring systems.",
      "processing_time": 50.74244546890259,
      "citing_paper_id": "237684656",
      "cited_paper_id": 49319238
    },
    {
      "context_text": "In addition, the selection of questions to present students with and the analysis of their performance in answering these questions is critical for curriculum structuring itself, both for human tutors and in ITS (Boaler & Brodie, 2004; Jiang, 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the importance of question selection and performance analysis in educational contexts.",
      "processing_time": 51.26028656959534,
      "citing_paper_id": "237684656",
      "cited_paper_id": 145664123
    },
    {
      "context_text": "In addition, the selection of questions to present students with and the analysis of their performance in answering these questions is critical for curriculum structuring itself, both for human tutors and in ITS (Boaler & Brodie, 2004; Jiang, 2014).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, models, or methods. It focuses on the importance of question selection and performance analysis in educational contexts.",
      "processing_time": 51.26028656959534,
      "citing_paper_id": "237684656",
      "cited_paper_id": null
    },
    {
      "context_text": "There is a growing body of research on automated hint generation for programming exercises (McBroom et al., 2019; Price et al., 2019).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only references to research on automated hint generation for programming exercises.",
      "processing_time": 50.85352897644043,
      "citing_paper_id": "237684656",
      "cited_paper_id": 201698369
    },
    {
      "context_text": ", 2019; Koedinger & Anderson, 1993; Melis & Siekmann, 2004; Passier & Jeuring, 2006; Sommer & Nuckols, 2004), logic (Abel et al., 2001; Andrews et al., 2004; Burstall, 1998; D’Agostino & Endriss, 1998; Hendriks et al., 2010; Scheines & Sieg, 1994; Stamper et al., 2013; Sufrin & Bornat, 1996), and algorithms (Leelawong & Biswas, 2008); to assisting students in knowledge and skill acquisition in natural sciences (Hume et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The provided context does not mention any specific datasets, only references to various studies and papers. There are no clear identifiers for datasets, corpora, or other verifiable resources.",
      "processing_time": 51.58517265319824,
      "citing_paper_id": "237684656",
      "cited_paper_id": 215516200
    },
    {
      "context_text": "…2004; Passier & Jeuring, 2006; Sommer & Nuckols, 2004), logic (Abel et al., 2001; Andrews et al., 2004; Burstall, 1998; D’Agostino & Endriss, 1998; Hendriks et al., 2010; Scheines & Sieg, 1994; Stamper et al., 2013; Sufrin & Bornat, 1996), and algorithms (Leelawong & Biswas, 2008); to assisting…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies and topics such as logic and algorithms. There are no clear identifiers for datasets.",
      "processing_time": 51.42262887954712,
      "citing_paper_id": "237684656",
      "cited_paper_id": 215516200
    },
    {
      "context_text": "…students acquire knowledge about mathematics (B¨udenbender et al., 2002; Dietrich & Buckley, 2008; Goguadze et al., 2005; Hrastinski et al., 2019; Koedinger & Anderson, 1993; Melis & Siekmann, 2004; Passier & Jeuring, 2006; Sommer & Nuckols, 2004), logic (Abel et al., 2001; Andrews et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to various studies. There are no clear identifiers for datasets, benchmarks, or other verifiable resources.",
      "processing_time": 51.22924852371216,
      "citing_paper_id": "237684656",
      "cited_paper_id": null
    },
    {
      "context_text": "Personalized tutoring helps students achieve their learning goals effectively (Anania, 1983; Bloom, 1984; Burke, 1983; Hrastinski et al., 2019; Hume et al., 1996).",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to research papers discussing personalized tutoring.",
      "processing_time": 50.36157274246216,
      "citing_paper_id": "237684656",
      "cited_paper_id": null
    },
    {
      "context_text": "Hume et al. (1993) define a hint as “a rhetorical device that is intended to either: (1) provide the student with a piece of information that the tutor hopes will stimulate the student’s recall of the facts needed to answer a question, or (2) provide a piece of information that can facilitate the…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a definition of a hint in educational contexts.",
      "processing_time": 50.360673666000366,
      "citing_paper_id": "237684656",
      "cited_paper_id": null
    },
    {
      "context_text": "We choose 10 identities, 7 from VGGFace [2] and 3 in-the-wild identities gathered from the internet.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "VGGFace"
      ],
      "dataset_descriptions": {
        "VGGFace": "Used to train a model for personalized text generation, focusing on recognizing faces across pose and age, enhancing the personalization aspect of the generated content."
      },
      "confidence_score": 0.8,
      "reasoning": "VGGFace is mentioned as a source of identities for the research, which aligns with the topic of personalized text generation, particularly in the context of face recognition.",
      "processing_time": 53.01363134384155,
      "citing_paper_id": "261705666",
      "cited_paper_id": 216009
    },
    {
      "context_text": "Distinct from FastComposer and Break-A-Scene, our attention loss only targets regions in the attention map not present in the ground-truth mask ( A k [ i, j ] for all ( i, j ) ∈ { ( i, j ) | M v [ i, j ] = 0 } ), allowing for the varying optimal values for other areas.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only discusses a method for attention loss in image generation.",
      "processing_time": 50.267473220825195,
      "citing_paper_id": "261705666",
      "cited_paper_id": 8923541
    },
    {
      "context_text": "Text-to-image diffusion models Diffusion models [10, 27, 28, 26] have recently achieved remarkable success in image generation, driving advancements in various applications and fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in diffusion models for image generation.",
      "processing_time": 50.32127404212952,
      "citing_paper_id": "261705666",
      "cited_paper_id": 196470871
    },
    {
      "context_text": "Text-to-image diffusion models Diffusion models [10, 27, 28, 26] have recently achieved remarkable success in image generation, driving advancements in various applications and fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in diffusion models for image generation.",
      "processing_time": 50.32127404212952,
      "citing_paper_id": "261705666",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "Text-to-image diffusion models Diffusion models [10, 27, 28, 26] have recently achieved remarkable success in image generation, driving advancements in various applications and fields.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general advancements in diffusion models for image generation.",
      "processing_time": 50.32127404212952,
      "citing_paper_id": "261705666",
      "cited_paper_id": 227209335
    },
    {
      "context_text": "Diffusion models [10, 27, 28, 26] are a class of generative models that create images through an iterative denoising process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models and their processes. No verifiable resources are identified.",
      "processing_time": 50.318867444992065,
      "citing_paper_id": "261705666",
      "cited_paper_id": 196470871
    },
    {
      "context_text": "Diffusion models [10, 27, 28, 26] are a class of generative models that create images through an iterative denoising process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models and their processes. No verifiable resources are identified.",
      "processing_time": 50.318867444992065,
      "citing_paper_id": "261705666",
      "cited_paper_id": 222140788
    },
    {
      "context_text": "Diffusion models [10, 27, 28, 26] are a class of generative models that create images through an iterative denoising process.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only generative models and their processes. No verifiable resources are identified.",
      "processing_time": 50.318867444992065,
      "citing_paper_id": "261705666",
      "cited_paper_id": 227209335
    },
    {
      "context_text": "With the rise of GANs, there have been efforts to fine-tune GANs, like Pivotal Tuning [20], based on GAN inversion [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 50.486074924468994,
      "citing_paper_id": "261705666",
      "cited_paper_id": 214743564
    },
    {
      "context_text": "With the rise of GANs, there have been efforts to fine-tune GANs, like Pivotal Tuning [20], based on GAN inversion [36].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and techniques. The cited papers' titles do not introduce any datasets either.",
      "processing_time": 50.486074924468994,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "…an auxiliary identity loss that utilizes a pre-trained face recognition model [5] R and cropping function B conditioned by the face detection model [4]: where cos denotes the cosine similarity and ˆ x (0) = D (ˆ z (0) ) refers to the estimated clean image from z ( t id ) r using Tweedie’s formula…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of a pre-trained face recognition model and a face detection model, which are excluded according to the instructions.",
      "processing_time": 51.2003653049469,
      "citing_paper_id": "261705666",
      "cited_paper_id": 219964874
    },
    {
      "context_text": "Specifically, the structure loss comprises two components: the self-similarity loss L ssim [29] and the patch contrastive loss L contra [17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions losses but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 50.278024673461914,
      "citing_paper_id": "261705666",
      "cited_paper_id": 220871180
    },
    {
      "context_text": "Specifically, the structure loss comprises two components: the self-similarity loss L ssim [29] and the patch contrastive loss L contra [17].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context mentions losses but does not refer to any specific datasets. The cited papers are about methods and models, not datasets.",
      "processing_time": 50.278024673461914,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245650694
    },
    {
      "context_text": "Similar to Pivotal Tuning [20] in GAN inversion, our method consists of two-phase optimization.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 340), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only a method calle",
      "processing_time": 54.00909233093262,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "With the rise of GANs, there have been efforts to fine-tune GANs, like Pivotal Tuning (Roich et al. 2022), based on GAN inversion (Zhu et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context focuses on GANs and fine-tuning techniques.",
      "processing_time": 50.29390525817871,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "Two-phase Optimization Similar to Pivotal Tuning (Roich et al. 2022) in GAN inversion, our method consists of two-phase optimization.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (Pivotal Tuning) used in GAN inversion. The context focuses on describing a similar two-phase optimization method.",
      "processing_time": 50.611918926239014,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235390635
    },
    {
      "context_text": "…identity loss that utilizes a pre-trained face recognition model [5] R and cropping function B conditioned by the face detection model [4]: where cos denotes the cosine similarity and ˆ x (0) = D (ˆ z (0) ) refers to the estimated clean image from z ( t id ) r using Tweedie’s formula [13].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of a pre-trained face recognition model and a face detection model, which are excluded according to the instructions.",
      "processing_time": 50.84209656715393,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235422658
    },
    {
      "context_text": "Rather than optimizing the entire model, we apply the LoRA [11], where only the residuals ∆ W of the projection layers in the cross-attention module are trained using low-rank decomposition.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (LoRA) for adapting large language models.",
      "processing_time": 49.8969349861145,
      "citing_paper_id": "261705666",
      "cited_paper_id": 235458009
    },
    {
      "context_text": "Recent advancements in large-scale text-to-image models, such as Stable Diffusion [21] and Imagen [23], have made it possible to generate high-fidelity, photorealistic por-trait images.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models (Stable Diffusion and Imagen). The context focuses on the capabilities of these models rather than the datasets they were trained on.",
      "processing_time": 50.7401909828186,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "In Stable Diffusion [21], CLIP text encoder [18] is used to produce text embedding features.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (CLIP text encoder) used for producing text embedding features.",
      "processing_time": 49.95871710777283,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Latent diffusion models (LDM) [21] are a variant of diffusion models where the denoising process occurs in the latent space.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (latent diffusion models).",
      "processing_time": 49.61654329299927,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "5 [21].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "The citation does not provide any context about the use of datasets, methods, or findings. The cited paper title is unrelated to personalized text generation.",
      "processing_time": 49.81400394439697,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "With a learning rate of 5e-6, we train the model for 750 steps, optimizing the cross-attention layers of the Stable Diffusion model [21], as detailed in the original paper.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a model (Stable Diffusion) and training parameters. The cited paper title confirms the focus on a model rather than a dataset.",
      "processing_time": 50.35111713409424,
      "citing_paper_id": "261705666",
      "cited_paper_id": 245335280
    },
    {
      "context_text": "Specifically, we employ a pre-trained super-resolution model [31] and a face restoration model [35] to further improve the quality of the generated samples.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions models but does not refer to any specific datasets. The context is about improving the quality of generated samples using pre-trained models.",
      "processing_time": 49.899489402770996,
      "citing_paper_id": "261705666",
      "cited_paper_id": 249926514
    },
    {
      "context_text": "Moreover, they can be altered or manipulated for the purpose of image editing, as demonstrated in Prompt-to-Prompt [9].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for image editing.",
      "processing_time": 49.211907386779785,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251252882
    },
    {
      "context_text": "Textual Inversion For the training of Textual Inversion [6], we adopt the optimal settings, including a batch size of 2, a learning rate of 5e-3, and a total of 5000 training steps.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only training parameters for Textual Inversion. No verifiable resources are identified.",
      "processing_time": 49.824708223342896,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "We compare our method with other personalization methods including DreamBooth [22] Textual Inversion [6], and Custom Diffusion [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only models or methods. No verifiable resources are identified.",
      "processing_time": 49.60170912742615,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "In the first phase, we optimize the text embed-dings for the special tokens [ V ∗ ] using the reconstruction objective as in [6].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for optimizing text embeddings.",
      "processing_time": 49.25745701789856,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Inversion [6], and Custom Diffusion [14] using the same source and reference images.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 330), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only methods or mod",
      "processing_time": 49.921327352523804,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "DreamBooth [22] fine-tunes entire weights, Textual Inversion [6] adjusts text embeddings, and Custom Diffusion [14] adapts the mapping matrix for the cross-attention layer.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 351), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only methods and mo",
      "processing_time": 53.17984437942505,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "Formally, the masked reconstruction loss for the source and the reference prompts are given by: [22], Textual Inversion [6], and Custom Diffusion [14].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to methods or papers. No dataset names are present in the context.",
      "processing_time": 49.67715525627136,
      "citing_paper_id": "261705666",
      "cited_paper_id": 251253049
    },
    {
      "context_text": "…reconstruction objective given by: For the facial regions, we use an auxiliary identity loss that utilizes a pre-trained face recognition model [5] R and cropping function B conditioned by the face detection model [4]: where cos denotes the cosine similarity and ˆ x (0) = D (ˆ z (0) ) refers…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on the use of a pre-trained face recognition model and a face detection model.",
      "processing_time": 49.83530879020691,
      "citing_paper_id": "261705666",
      "cited_paper_id": 252223915
    },
    {
      "context_text": "For composed prompt learning, we employ structure loss [15] that maximizes structural similarity between the estimated image ˆ x (0) and the source images using a pre-trained DINO ViT [3].",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (structure loss) and a pre-trained model (DINO ViT).",
      "processing_time": 49.59546494483948,
      "citing_paper_id": "261705666",
      "cited_paper_id": 252668838
    },
    {
      "context_text": "Methods like ELITE [32] and In-stantBooth [25] employ a data-driven approach for encoder-based domain tuning, which is not directly comparable to our approach.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 314), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only methods. There",
      "processing_time": 52.43172240257263,
      "citing_paper_id": "261705666",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Methods like ELITE (Wei et al. 2023) and InstantBooth (Shi et al.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only methods. There are no clear identifiers for datasets within the given context.",
      "processing_time": 49.48414731025696,
      "citing_paper_id": "261705666",
      "cited_paper_id": 257219968
    },
    {
      "context_text": "Our method differs from concurrent works like SVD-iff [8], FastComposer [33], and Break-A-Scene [1], which use similar techniques like attention loss or composed prompts.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only other methods and works. There are no verifiable resources or datasets mentioned in the context.",
      "processing_time": 49.63314938545227,
      "citing_paper_id": "261705666",
      "cited_paper_id": 257631648
    },
    {
      "context_text": "Our method outperforms other baselines in terms of identity similarity measured between the source images ( CSIM ), masked CLIP similarity measure ( Style ), and Aes-thetic score [24]. where z s and z r are the source and reference noised latent at timestep t ∼ Uniform(1, T ) and ϵ ∼ N ( 0 , I ) .",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only evaluation metrics and methods. No verifiable resources are identified.",
      "processing_time": 49.368104457855225,
      "citing_paper_id": "261705666",
      "cited_paper_id": null
    },
    {
      "context_text": "Finally, we evaluate the overall image fidelity with the LAION aesthetic predictor [24].",
      "catation_intent": "none",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation mentions the LAION aesthetic predictor, which is a model or method, not a dataset. No specific dataset is referenced.",
      "processing_time": 49.36813163757324,
      "citing_paper_id": "261705666",
      "cited_paper_id": null
    },
    {
      "context_text": "Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve ﬁrst explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it [16, 9, 12, 34, 19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods for preference-based reinforcement learning.",
      "processing_time": 48.92618155479431,
      "citing_paper_id": "258959321",
      "cited_paper_id": 282570
    },
    {
      "context_text": "Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve ﬁrst explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it [16, 9, 12, 34, 19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only algorithms and methods for preference-based reinforcement learning.",
      "processing_time": 48.92618155479431,
      "citing_paper_id": "258959321",
      "cited_paper_id": 4751379
    },
    {
      "context_text": "In controlled sentiment generation , x is a preﬁx of a movie review the IMDb dataset [22], and the policy must generate y with positive sentiment.",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [
        "MIMDb"
      ],
      "dataset_descriptions": {
        "MIMIC-III": "Used to train and evaluate models for controlled sentiment generation, specifically generating text with positive sentiment from movie reviews."
      },
      "confidence_score": 0.9,
      "reasoning": "The context mentions the IMDb dataset, which is a well-known dataset used for sentiment analysis in movie reviews. The dataset is used for controlled sentiment generation, specifically to generate text with positive sentiment.",
      "processing_time": 53.855403661727905,
      "citing_paper_id": "258959321",
      "cited_paper_id": 1428702
    },
    {
      "context_text": "Similarly, preference-based RL (PbRL) learns from binary preferences generated by an unknown ‘scoring’ function rather than rewards [9, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (preference-based RL). The context is about the approach and not a particular dataset.",
      "processing_time": 49.377182960510254,
      "citing_paper_id": "258959321",
      "cited_paper_id": 4751379
    },
    {
      "context_text": "Similarly, preference-based RL (PbRL) learns from binary preferences generated by an unknown ‘scoring’ function rather than rewards [9, 35].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (preference-based RL). The context is about the approach and not a particular dataset.",
      "processing_time": 49.377182960510254,
      "citing_paper_id": "258959321",
      "cited_paper_id": 243860719
    },
    {
      "context_text": "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 37, 8].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only refers to 'very large datasets' in a generic sense.",
      "processing_time": 48.922189235687256,
      "citing_paper_id": "258959321",
      "cited_paper_id": 11759366
    },
    {
      "context_text": "the reward model) and subsequently optimizing it [15, 9, 12, 31, 17].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to other papers. There are no clear identifiers for datasets, models, or methods.",
      "processing_time": 49.242581367492676,
      "citing_paper_id": "258959321",
      "cited_paper_id": 13285792
    },
    {
      "context_text": "These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives [30, 24, 42] and another body of work on general methods for learning from human preferences [12, 17].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 325), response: ```json\n{\n    \"reasoning\": \"The citation span does not mention any specific datasets, only reference",
      "processing_time": 52.01138114929199,
      "citing_paper_id": "258959321",
      "cited_paper_id": 13285792
    },
    {
      "context_text": "We can draw connections to the control as inference framework [20] for the constrained RL problem outlined in 3.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a theoretical framework. No verifiable resources are identified.",
      "processing_time": 48.7377769947052,
      "citing_paper_id": "258959321",
      "cited_paper_id": 19077536
    },
    {
      "context_text": "These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives [33, 27, 45] and another body of work on general methods for learning from human preferences [12, 19].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only references to bodies of work and methods. There are no clear identifiers for datasets.",
      "processing_time": 48.89564323425293,
      "citing_paper_id": "258959321",
      "cited_paper_id": 21850704
    },
    {
      "context_text": "Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; [5]) that measures how well a given reward function aligns with empirical preference data.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It refers to a method (Bradley-Terry model) and empirical preference data, but does not name a specific dataset.",
      "processing_time": 49.30359864234924,
      "citing_paper_id": "258959321",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "…a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model [5], then ﬁne-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE [44], proximal policy…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable dataset. It discusses methods and models but does not reference a dataset by name.",
      "processing_time": 48.79263734817505,
      "citing_paper_id": "258959321",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "There are a number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the framework if we have access to several ranked answers).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on preference modeling using the Bradley-Terry and Plackett-Luce models.",
      "processing_time": 49.034568548202515,
      "citing_paper_id": "258959321",
      "cited_paper_id": 121987403
    },
    {
      "context_text": "There are a number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the framework if we have access to several ranked answers).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models and methods. The context focuses on preference modeling using the Bradley-Terry and Plackett-Luce models.",
      "processing_time": 49.034568548202515,
      "citing_paper_id": "258959321",
      "cited_paper_id": 123178801
    },
    {
      "context_text": "6 uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models [30, 21], shown in Appendix A.3.",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only models. The context is about deriving expressions using statistical models.",
      "processing_time": 48.50540566444397,
      "citing_paper_id": "258959321",
      "cited_paper_id": 123178801
    },
    {
      "context_text": "However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and humanwritten completions [21, 33, 13, 36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets of instructions and human-written completions' but does not provide specific names. The cited paper titles do not help disambiguate any specific datasets.",
      "processing_time": 49.15224623680115,
      "citing_paper_id": "258959321",
      "cited_paper_id": 127667495
    },
    {
      "context_text": "However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and humanwritten completions [21, 33, 13, 36].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The context mentions 'datasets of instructions and human-written completions' but does not provide specific names. The cited paper titles do not help disambiguate any specific datasets.",
      "processing_time": 49.15224623680115,
      "citing_paper_id": "258959321",
      "cited_paper_id": 237421373
    },
    {
      "context_text": "Self-supervised language models of increasing scale learn to complete some tasks zero-shot [31] or with few-shot prompts [6, 25, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 48.70386362075806,
      "citing_paper_id": "258959321",
      "cited_paper_id": 160025533
    },
    {
      "context_text": "Self-supervised language models of increasing scale learn to complete some tasks zero-shot [31] or with few-shot prompts [6, 25, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 48.70386362075806,
      "citing_paper_id": "258959321",
      "cited_paper_id": 218971783
    },
    {
      "context_text": "Self-supervised language models of increasing scale learn to complete some tasks zero-shot [31] or with few-shot prompts [6, 25, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 48.70386362075806,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236635565
    },
    {
      "context_text": "Self-supervised language models of increasing scale learn to complete some tasks zero-shot [31] or with few-shot prompts [6, 25, 11].",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to language models and their capabilities. No verifiable resources are identified.",
      "processing_time": 48.70386362075806,
      "citing_paper_id": "258959321",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "…human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have ﬁne-tuned LLMs with datasets of human preferences, improving proﬁciency in translation [18], summarization [38, 48], story-telling [48], and instruction-following [26, 32].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'datasets of human preferences' but does not specify any named datasets. The cited paper title confirms the focus on human preferences but does not introduce a specific dataset.",
      "processing_time": 48.95781922340393,
      "citing_paper_id": "258959321",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "| {z } This is the same objective optimized in prior works [48, 38, 1, 26] using the DPO-equivalent reward for the reward class of r � .",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reward class used in prior works. No verifiable resources are identified.",
      "processing_time": 48.45030498504639,
      "citing_paper_id": "258959321",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "The standard approach [48, 38, 1, 26] has been to construct the reward function r ( x, y ) = r � ( x, y ) , and maximize using PPO [37].",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 340), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only references to ",
      "processing_time": 51.63084530830383,
      "citing_paper_id": "258959321",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "…loss: ⇥ ⇤ In the context of LMs, the network r � ( x, y ) is often initialized from the SFT model ⇡ SFT ( y | x ) with the addition of a linear layer on top of the ﬁnal transformer layer that produces a single scalar prediction for the reward value [48]. where � is the logistic function.",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method for initializing a network from a SFT model. There are no clear identifiers for datasets in the given context.",
      "processing_time": 48.85648322105408,
      "citing_paper_id": "258959321",
      "cited_paper_id": 202660943
    },
    {
      "context_text": "Following prior work [29, 28, 17, 15], it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to prior work in the context of a mathematical objective.",
      "processing_time": 48.57220244407654,
      "citing_paper_id": "258959321",
      "cited_paper_id": 203610423
    },
    {
      "context_text": "In particular, we formulate the following optimization problem max where � is a parameter controlling the deviation from the base reference policy ⇡ ref , namely the initial SFT model ⇡ SFT .",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reference policy which is part of a model or method.",
      "processing_time": 48.23224997520447,
      "citing_paper_id": "258959321",
      "cited_paper_id": 222291028
    },
    {
      "context_text": "preferences [35], and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries.",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to human preferences which is too generic.",
      "processing_time": 48.12159848213196,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236503748
    },
    {
      "context_text": ", which has also been adopted in subsequent work [35, 1, 23].",
      "catation_intent": "none",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, models, or methods. It only refers to subsequent work adopting something, but the details are not provided.",
      "processing_time": 48.47567629814148,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236503748
    },
    {
      "context_text": "This is the same objective optimized in prior works [45, 35, 1, 23] using the DPO-equivalent reward for the reward class of rφ.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a reward class used in prior works. The context is too generic and does not provide any specific dataset names.",
      "processing_time": 48.55987763404846,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236503748
    },
    {
      "context_text": "Despite the success of instruction tuning, relative human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation [16], summarization [35, 45], story-telling [45], and instruction-following [23, 29].",
      "catation_intent": "findings",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific, verifiable datasets. It only refers to 'datasets of human preferences' in a generic sense without naming any particular dataset.",
      "processing_time": 48.587382316589355,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236503748
    },
    {
      "context_text": "The standard approach [45, 35, 1, 23] has been to construct the reward function r(x, y) = rφ(x, y)− β(log πθ(y | x)− log πref(y | x)), and maximize using PPO [34].",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation context does not mention any specific datasets, only references to methods and approaches. The cited paper titles do not help in identifying any datasets.",
      "processing_time": 48.457871437072754,
      "citing_paper_id": "258959321",
      "cited_paper_id": 236503748
    },
    {
      "context_text": "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 40, 8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "No specific dataset names are mentioned or inferred from the context or cited papers.",
      "processing_time": 49.195130586624146,
      "citing_paper_id": "258959321",
      "cited_paper_id": 247951931
    },
    {
      "context_text": "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities [11, 7, 40, 8].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.0,
      "reasoning": "No specific dataset names are mentioned or inferred from the context or cited papers.",
      "processing_time": 49.195130586624146,
      "citing_paper_id": "258959321",
      "cited_paper_id": 257219404
    },
    {
      "context_text": "However, their performance on downstream tasks and alignment with user intent can be signiﬁcantly improved by ﬁne-tuning on datasets of instructions and human-written completions [23, 36, 13, 39].",
      "catation_intent": "reusable resource",
      "resource_type": "dataset",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "The citation mentions 'datasets of instructions and human-written completions' but does not provide specific names. The context is too generic to identify a specific dataset.",
      "processing_time": 48.47395730018616,
      "citing_paper_id": "258959321",
      "cited_paper_id": 257663729
    },
    {
      "context_text": "While existing studies suggest LMs can be better automated evaluators than existing metrics [10], we conduct a human study to justify our usage of GPT-4 for evaluation in Sec.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only the use of GPT-4 for evaluation. No verifiable resources are identified.",
      "processing_time": 48.25976014137268,
      "citing_paper_id": "258959321",
      "cited_paper_id": 257913780
    },
    {
      "context_text": "We use an SFT model ﬁne-tuned on human-written forum post summaries 2 with the TRLX [42] framework for RLHF.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific, verifiable datasets. It only refers to a method (SFT model fine-tuned on human-written forum post summaries) and a framework (TRLX).",
      "processing_time": 48.62726902961731,
      "citing_paper_id": "258959321",
      "cited_paper_id": null
    },
    {
      "context_text": "In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version [39] as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running ‘normal’ PPO with learned rewards).",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only implementations of PPO-GT and modifications to PPO. No verifiable resources are identified.",
      "processing_time": 48.17261624336243,
      "citing_paper_id": "258959321",
      "cited_paper_id": null
    },
    {
      "context_text": "We use an SFT model fine-tuned on human-written forum post summaries2 with the TRLX [39] framework for RLHF.",
      "catation_intent": "reusable resource",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context mentions 'human-written forum post summaries' but does not provide a specific, identifiable dataset name. The reference to TRLX is a framework, not a dataset.",
      "processing_time": 48.45606088638306,
      "citing_paper_id": "258959321",
      "cited_paper_id": null
    },
    {
      "context_text": "We consider a real world dataset from Amazon Electronics (McAuley et al., 2015) to evaluate our model.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 11 column 1 (char 585), response: ```json\n{\n    \"reasoning\": \"The context mentions a specific dataset from Amazon Electronics, which i",
      "processing_time": 54.53291940689087,
      "citing_paper_id": "29161455",
      "cited_paper_id": 1012652
    },
    {
      "context_text": "For the sequence encoder, the attention vector is defined as in many other applications (Bahdanau et al., 2014; Luong et al., 2015):",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only methods and models. The context is focused on the definition of the attention vector in neural machine translation.",
      "processing_time": 47.9158570766449,
      "citing_paper_id": "29161455",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "For the sequence encoder, the attention vector is deﬁned as in many other applications (Bahdanau et al., 2014; Luong et al., 2015): α 1 tj e j (7) where a 1 t ∈ R n is the attention vector on the sequence encoder at time-step t , α 1 tj is the attention score over the encoder hidden state e j and…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are about attention mechanisms in neural machine translation, which do not introduce specific datasets.",
      "processing_time": 47.865813970565796,
      "citing_paper_id": "29161455",
      "cited_paper_id": 1998416
    },
    {
      "context_text": "Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step: where s ui ∈ R k is the aspect importance considering the interaction between u and i , e t is the de-coder…",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a method (copy mechanism) used in sequence-to-sequence learning.",
      "processing_time": 47.522615909576416,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are about copying mechanisms and summarization techniques, which do not introduce specific datasets.",
      "processing_time": 47.75344967842102,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8174613
    },
    {
      "context_text": "Inspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next time-step:",
      "catation_intent": "method",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only methods and models. The cited papers are about copying mechanisms and summarization techniques, which do not introduce specific datasets.",
      "processing_time": 47.75344967842102,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "…encoder, and α2tj is the attention score between the attribute latent factor γj and decoder hidden state ht.\nInspired by the copy mechanism (Gu et al., 2016; See et al., 2017), we design an attention vector that estimates the probability that each aspect will be discussed in the next…",
      "catation_intent": "research work",
      "resource_type": "method",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The context does not mention any specific datasets, only a method (copy mechanism) and its application in designing an attention vector.",
      "processing_time": 47.497071743011475,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "…to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016; Ghosh et al.,…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of data-to-text natural language generation. No specific, verifiable datasets are named.",
      "processing_time": 47.99015188217163,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al.",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 298), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only general applic",
      "processing_time": 47.21949744224548,
      "citing_paper_id": "29161455",
      "cited_paper_id": 8314118
    },
    {
      "context_text": "Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various fields (Gatt and Krahmer, 2017).",
      "catation_intent": [],
      "resource_type": [],
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.3,
      "reasoning": "JSON decode error: Extra data: line 9 column 1 (char 296), response: ```json\n{\n    \"reasoning\": \"The citation does not mention any specific datasets, only a general refe",
      "processing_time": 47.2587993144989,
      "citing_paper_id": "29161455",
      "cited_paper_id": 16946362
    },
    {
      "context_text": "Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al.",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to data-to-text natural language generation. No specific, verifiable datasets are named.",
      "processing_time": 47.43626689910889,
      "citing_paper_id": "29161455",
      "cited_paper_id": 16946362
    },
    {
      "context_text": "Review generation belongs to a large body of work on data-to-text natural language generation (Gatt and Krahmer, 2017), which has applications including summarization (See et al., 2017), image captioning (Vinyals et al., 2015), and dialogue response generation (Xing et al., 2017; Li et al., 2016;…",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation span does not mention any specific datasets, only general applications of natural language generation. No verifiable resources are identified.",
      "processing_time": 47.21966814994812,
      "citing_paper_id": "29161455",
      "cited_paper_id": 16946362
    },
    {
      "context_text": "Contextual, or ‘data-to-text’ natural language generation is one of the core tasks in natural language processing and has a considerable impact on various ﬁelds (Gatt and Krahmer, 2017).",
      "catation_intent": "research work",
      "resource_type": "none",
      "extracted_datasets": [],
      "dataset_descriptions": {},
      "confidence_score": 0.6,
      "reasoning": "The citation does not mention any specific datasets, only a general reference to the field of natural language generation.",
      "processing_time": 47.090088844299316,
      "citing_paper_id": "29161455",
      "cited_paper_id": 16946362
    }
  ],
  "filtering_stats": {
    "original_papers_count": 1120,
    "filtered_papers_count": 162,
    "filtered_percentage": "14.5%"
  },
  "extraction_stats": {
    "unique_contexts_processed": 4438,
    "total_citation_instances": 5734,
    "successful_extractions": 420,
    "failed_extractions": 5314,
    "total_processing_time": 260.5333843231201
  }
}